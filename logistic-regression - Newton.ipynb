{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Rahul Govindkumar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is library for scientific computing in Python. It has efficient implementation of n-dimensional array (tensor) manupulations, which is useful for machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a list into numpy array (tensor)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 4],\n",
       "       [2, 6, 9]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [[1, 2, 4], [2, 6, 9]]\n",
    "a = np.array(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the dimensions of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply simple arithmetic operation on all element of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  6, 12],\n",
       "       [ 6, 18, 27]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can transpose a tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 6],\n",
       "       [4, 9]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a.T.shape)\n",
    "a.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply aggregate functions on the whole tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or on one dimension of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  8, 13])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 17])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do element-wise arithmetic operation on two tensors (of the same size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6, 20],\n",
       "       [ 2, 12,  9]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = np.array([[1, 2, 4], [2, 6, 9]])\n",
    "c2 = np.array([[2, 3, 5], [1, 2, 1]])\n",
    "c1 * c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to multiply all columns of a tensor by vector (for example if you want to multiply all data features by their lables) you need a trick. This multiplication shows up in calculating the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 4]\n",
      " [2 6 9]]\n",
      "[ 1 -1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 4], [2, 6, 9]])\n",
    "b = np.array([1,-1])\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to multiply the first row of a by 1 and the second row of a by -1. Simply multiplying a by b does not work because a and b do not have the same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this multiplication we first have to assume b has one column and then repeat the column of b with the number of columns in a. We use tile function to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1],\n",
       "       [-1, -1, -1]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_repeat = np.tile(b,  (a.shape[1],1)).T\n",
    "print(b_repeat.shape)\n",
    "b_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can multiply each column of a by b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  4],\n",
       "       [-2, -6, -9]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create inital random vector using numpy (using N(0,1)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0 #mean\n",
    "sigma = 1 #standard deviation\n",
    "r = np.random.normal(mu,sigma, 1000) #draws 1000 samples from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply functions on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of Normal distribution\n",
    "def normal(x, mu, sigma):\n",
    "    return np.exp( -0.5 * ((x-mu)/sigma)**2)/np.sqrt(2.0*np.pi*sigma**2)\n",
    "\n",
    "#probability of samples on the Normal distribution\n",
    "probabilities = normal(r, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has useful APIs for analysis. Here we plot the histogram of samples and also plot the probabilies to see if the samples follow the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1b7812d4790>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(r,50,density=True)\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.scatter(r, probabilities, c='b', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    p = re.compile(',')\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    header = f.readline().strip()\n",
    "    varnames = p.split(header)\n",
    "    namehash = {}\n",
    "    for l in f:\n",
    "        li = p.split(l.strip())\n",
    "        xdata.append([float(x) for x in li[:-1]])\n",
    "        ydata.append(float(li[-1]))\n",
    "    \n",
    "    return np.array(xdata), np.array(ydata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming our data is x is available in numpy we use numpy to implement logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain_whole, ytrain_whole) = read_data('datasets/spambase-train.csv')\n",
    "(xtest, ytest) = read_data('datasets/spambase-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of xtrain: (3601, 54)\n",
      "The shape of ytrain: (3601,)\n",
      "The shape of xtest: (1000, 54)\n",
      "The shape of ytest: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of xtrain:\", xtrain_whole.shape)\n",
    "print(\"The shape of ytrain:\", ytrain_whole.shape)\n",
    "print(\"The shape of xtest:\", xtest.shape)\n",
    "print(\"The shape of ytest:\", ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before training make we normalize the input data (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmean = np.mean(xtrain_whole, axis=0)\n",
    "xstd = np.std(xtrain_whole, axis=0)\n",
    "xtrain_normal_whole = (xtrain_whole-xmean) / xstd\n",
    "xtest_normal = (xtest-xmean) / xstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a validation set. We create an array of indecies and permute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "premute_indicies = np.random.permutation(np.arange(xtrain_whole.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the first 2600 data points as the training data and rest as the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_normal = xtrain_normal_whole[premute_indicies[:2600]]\n",
    "ytrain = ytrain_whole[premute_indicies[:2600]]\n",
    "xval_normal = xtrain_normal_whole[premute_indicies[2600:]]\n",
    "yval = ytrain_whole[premute_indicies[2600:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiallizing the weights and bias with random values from N(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(0, 1, xtrain_normal.shape[1]);\n",
    "bias = np.random.normal(0,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sigmoid function\n",
    "def sigmoid(v):\n",
    "    #return np.exp(-np.logaddexp(0, -v)) #numerically stable implementation of sigmoid function \n",
    "    return 1.0 / (1+np.exp(-v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use dot-product from numpy to calculate the margin and pass it to the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w: weight vector (numpy array of size n)\n",
    "#b: numpy array of size 1\n",
    "#returns p(y=1|x, w, b)\n",
    "def prob(x, w, b):\n",
    "    return sigmoid(np.dot(x,w) + b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate $l_2$ penalty using linalg library of numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.742391924271915"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Cross Entropy Loss} = -\\frac{1}{|D|}[\\sum_{(y^i,\\mathbf{x}^i)\\in\\mathcal{D}} \n",
    " y^i \\log p(y=1|\\mathbf{x}^i;\\mathbf{w},b)  +  (1-y^i) \\log (1 - p(y=1|\\mathbf{x}^i;\\mathbf{w},b))]+\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w: weight vector (numpy array of size n)\n",
    "#y_prob: p(y|x, w, b)\n",
    "#y_true: class variable data\n",
    "#lambda_: l2 penalty coefficient\n",
    "#returns the cross entropy loss\n",
    "def loss(w, y_prob, y_true, lambda_):\n",
    "    \n",
    "    n_sample=len(w)\n",
    "    \n",
    "    cross_entropy_Loss=-(1/abs(n_sample))*np.sum((y_true * np.log(y_prob + 0.0000000000000000001)) + (1-y_true)*np.log(1-y_prob + 0.0000000000000000001)) + lambda_*0.5*np.linalg.norm(w)\n",
    "    \n",
    "    return cross_entropy_Loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x: input variables (data of size m x n with m data point and n features)\n",
    "#w: weight vector (numpy array of size n)\n",
    "#y_prob: p(y|x, w, b)\n",
    "#y_true: class variable data\n",
    "#lambda_: l2 penalty coefficient\n",
    "#returns tuple of gradient w.r.t w and w.r.t to bias\n",
    "\n",
    "def grad_w_b(x, w, y_prob, y_true, lambda_):\n",
    "    \n",
    "    n_samples=len(x)\n",
    "#     grad_w = np.zeros(w.shape)\n",
    "#     grad_b = 0.0\n",
    "    \n",
    "    grad_w = (1/n_samples) * np.dot(x.T, (y_prob - y_true))\n",
    "    grad_b = (1/n_samples) * np.sum(y_prob -y_true)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x: input variables (data of size m x n with m data point and n features)\n",
    "#w: weight vector (numpy array of size n)\n",
    "#y_prob: p(y|x, w, b)\n",
    "#y_true: class variable data\n",
    "#lambda_: l2 penalty coefficient\n",
    "#returns tuple of gradient w.r.t w and w.r.t to bias\n",
    "\n",
    "def second_grad_w(x, w, y_prob, y_true, lambda_):\n",
    "    n_sample = len(x)\n",
    "    second_grad_w = np.zeros(w.shape)\n",
    "    second_grad_b = 0.0\n",
    "    second_grad_w = (1/n_sample) * -1 * np.dot(np.dot(x.T,x), np.dot(y_prob,(1-y_prob) ))\n",
    "    second_grad_b = y_prob - y_true\n",
    "    return (second_grad_w,second_grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#lambda_ is the coeffienct of l2 norm penalty\n",
    "#learning_rate is learning rate of gradient descent algorithm\n",
    "#max_iter determines the maximum number of iterations if the gradients descent does not converge.\n",
    "#continue the training while gradient > 0.1 or the number steps is less max_iter\n",
    "\n",
    "#returns model as tuple of (weights,bias)\n",
    "\n",
    "def fit(x, y_true, learning_rate, lambda_, max_iter, verbose=0):\n",
    "    weights = np.random.normal(0, 1, x.shape[1]);\n",
    "    bias = np.random.normal(0,1,1)\n",
    "    \n",
    "    for i in range (max_iter):\n",
    "        \n",
    "        y_prod = prob(x, weights, bias)\n",
    "        dw,db = grad_w_b(x, weights, y_prod, y_true, lambda_)\n",
    "        \n",
    "        H, second_db =  second_grad_w(x,weights,y_prod,y_true,lambda_)\n",
    "        second_gradident = np.dot(np.linalg.inv(H),dw)\n",
    "        \n",
    "        weight_old = weights\n",
    "        weights += second_gradident\n",
    "        \n",
    "        cross_loss = loss(weights, y_prod, y_true, lambda_)\n",
    "        \n",
    "        # #         H_inv = np.linalg.inv(H)*(dw)\n",
    "        \n",
    "# #         H_ingradv = 1/(H)*(dw)\n",
    "#         weights = weights + np.dot(np.linalg.inv(H), dw)\n",
    "# #         weights = weights - (np.array([grad])).T\n",
    "# #         bias = bias - learning_rate * db\n",
    "        \n",
    "        \n",
    "# #         if(diff <= diffThreshHold or iteration > maxIteration):\n",
    "# #             break\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Iteration Number: \"+str(i)+\"\\nLoss: \"+ str(cross_loss)+\"\\nl2 norm of gradients: \"+str(np.linalg.norm(dw))+\"\\nl2 norm of weights: \"+str(np.linalg.norm(weights)))\n",
    "            print(\"---------------------\")\n",
    "            cost_list.append(cross_loss)\n",
    "    #change the condition appropriately\n",
    "#     while True:\n",
    "        \n",
    "#         if verbose: #verbose is used for debugging purposes\n",
    "#             #print iteration number, loss, l2 norm of gradients, l2 norm of weights\n",
    "#             pass\n",
    "    return (weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y_true, model):\n",
    "    w, b = model\n",
    "    return np.sum((prob(x, w, b)>0.5).astype(np.double) == y_true)  / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 0\n",
      "Loss: 104.73476703485291\n",
      "l2 norm of gradients: 0.9574963604652689\n",
      "l2 norm of weights: 7.187806999686712\n",
      "---------------------\n",
      "Iteration Number: 1\n",
      "Loss: 104.67191021481929\n",
      "l2 norm of gradients: 0.9572160215495435\n",
      "l2 norm of weights: 7.185626093478667\n",
      "---------------------\n",
      "Iteration Number: 2\n",
      "Loss: 104.60909534461909\n",
      "l2 norm of gradients: 0.9569356860230818\n",
      "l2 norm of weights: 7.183446134562879\n",
      "---------------------\n",
      "Iteration Number: 3\n",
      "Loss: 104.54632239787972\n",
      "l2 norm of gradients: 0.9566553539330455\n",
      "l2 norm of weights: 7.181267122411418\n",
      "---------------------\n",
      "Iteration Number: 4\n",
      "Loss: 104.48359134820552\n",
      "l2 norm of gradients: 0.9563750253271222\n",
      "l2 norm of weights: 7.179089056494968\n",
      "---------------------\n",
      "Iteration Number: 5\n",
      "Loss: 104.42090216915959\n",
      "l2 norm of gradients: 0.9560947002535223\n",
      "l2 norm of weights: 7.1769119362828375\n",
      "---------------------\n",
      "Iteration Number: 6\n",
      "Loss: 104.35825483425808\n",
      "l2 norm of gradients: 0.9558143787609775\n",
      "l2 norm of weights: 7.174735761242965\n",
      "---------------------\n",
      "Iteration Number: 7\n",
      "Loss: 104.29564931697502\n",
      "l2 norm of gradients: 0.9555340608987396\n",
      "l2 norm of weights: 7.172560530841921\n",
      "---------------------\n",
      "Iteration Number: 8\n",
      "Loss: 104.23308559078133\n",
      "l2 norm of gradients: 0.9552537467165783\n",
      "l2 norm of weights: 7.170386244544923\n",
      "---------------------\n",
      "Iteration Number: 9\n",
      "Loss: 104.1705636290629\n",
      "l2 norm of gradients: 0.9549734362647789\n",
      "l2 norm of weights: 7.1682129018158305\n",
      "---------------------\n",
      "Iteration Number: 10\n",
      "Loss: 104.10808340520175\n",
      "l2 norm of gradients: 0.9546931295941423\n",
      "l2 norm of weights: 7.166040502117163\n",
      "---------------------\n",
      "Iteration Number: 11\n",
      "Loss: 104.04564489253661\n",
      "l2 norm of gradients: 0.9544128267559807\n",
      "l2 norm of weights: 7.163869044910095\n",
      "---------------------\n",
      "Iteration Number: 12\n",
      "Loss: 103.98324806436487\n",
      "l2 norm of gradients: 0.9541325278021179\n",
      "l2 norm of weights: 7.161698529654476\n",
      "---------------------\n",
      "Iteration Number: 13\n",
      "Loss: 103.92089289394646\n",
      "l2 norm of gradients: 0.9538522327848865\n",
      "l2 norm of weights: 7.159528955808827\n",
      "---------------------\n",
      "Iteration Number: 14\n",
      "Loss: 103.85857935450461\n",
      "l2 norm of gradients: 0.953571941757126\n",
      "l2 norm of weights: 7.157360322830349\n",
      "---------------------\n",
      "Iteration Number: 15\n",
      "Loss: 103.79630741923653\n",
      "l2 norm of gradients: 0.9532916547721817\n",
      "l2 norm of weights: 7.1551926301749305\n",
      "---------------------\n",
      "Iteration Number: 16\n",
      "Loss: 103.73407706129632\n",
      "l2 norm of gradients: 0.9530113718839016\n",
      "l2 norm of weights: 7.153025877297159\n",
      "---------------------\n",
      "Iteration Number: 17\n",
      "Loss: 103.67188825378975\n",
      "l2 norm of gradients: 0.9527310931466361\n",
      "l2 norm of weights: 7.1508600636503195\n",
      "---------------------\n",
      "Iteration Number: 18\n",
      "Loss: 103.60974096980432\n",
      "l2 norm of gradients: 0.9524508186152347\n",
      "l2 norm of weights: 7.1486951886864105\n",
      "---------------------\n",
      "Iteration Number: 19\n",
      "Loss: 103.54763518239503\n",
      "l2 norm of gradients: 0.9521705483450444\n",
      "l2 norm of weights: 7.146531251856143\n",
      "---------------------\n",
      "Iteration Number: 20\n",
      "Loss: 103.48557086455033\n",
      "l2 norm of gradients: 0.9518902823919093\n",
      "l2 norm of weights: 7.14436825260895\n",
      "---------------------\n",
      "Iteration Number: 21\n",
      "Loss: 103.42354798927853\n",
      "l2 norm of gradients: 0.9516100208121661\n",
      "l2 norm of weights: 7.142206190392998\n",
      "---------------------\n",
      "Iteration Number: 22\n",
      "Loss: 103.36156652949984\n",
      "l2 norm of gradients: 0.951329763662644\n",
      "l2 norm of weights: 7.140045064655189\n",
      "---------------------\n",
      "Iteration Number: 23\n",
      "Loss: 103.29962645811463\n",
      "l2 norm of gradients: 0.9510495110006626\n",
      "l2 norm of weights: 7.137884874841171\n",
      "---------------------\n",
      "Iteration Number: 24\n",
      "Loss: 103.23772774801203\n",
      "l2 norm of gradients: 0.950769262884029\n",
      "l2 norm of weights: 7.135725620395341\n",
      "---------------------\n",
      "Iteration Number: 25\n",
      "Loss: 103.17587037200725\n",
      "l2 norm of gradients: 0.950489019371037\n",
      "l2 norm of weights: 7.133567300760859\n",
      "---------------------\n",
      "Iteration Number: 26\n",
      "Loss: 103.11405430292567\n",
      "l2 norm of gradients: 0.9502087805204645\n",
      "l2 norm of weights: 7.131409915379648\n",
      "---------------------\n",
      "Iteration Number: 27\n",
      "Loss: 103.05227951352236\n",
      "l2 norm of gradients: 0.9499285463915711\n",
      "l2 norm of weights: 7.12925346369241\n",
      "---------------------\n",
      "Iteration Number: 28\n",
      "Loss: 102.99054597653011\n",
      "l2 norm of gradients: 0.9496483170440971\n",
      "l2 norm of weights: 7.127097945138623\n",
      "---------------------\n",
      "Iteration Number: 29\n",
      "Loss: 102.92885366465588\n",
      "l2 norm of gradients: 0.9493680925382608\n",
      "l2 norm of weights: 7.124943359156559\n",
      "---------------------\n",
      "Iteration Number: 30\n",
      "Loss: 102.86720255056335\n",
      "l2 norm of gradients: 0.9490878729347564\n",
      "l2 norm of weights: 7.122789705183283\n",
      "---------------------\n",
      "Iteration Number: 31\n",
      "Loss: 102.80559260688307\n",
      "l2 norm of gradients: 0.9488076582947526\n",
      "l2 norm of weights: 7.12063698265467\n",
      "---------------------\n",
      "Iteration Number: 32\n",
      "Loss: 102.74402380621441\n",
      "l2 norm of gradients: 0.94852744867989\n",
      "l2 norm of weights: 7.1184851910054\n",
      "---------------------\n",
      "Iteration Number: 33\n",
      "Loss: 102.6824961211294\n",
      "l2 norm of gradients: 0.9482472441522792\n",
      "l2 norm of weights: 7.116334329668976\n",
      "---------------------\n",
      "Iteration Number: 34\n",
      "Loss: 102.62100952417093\n",
      "l2 norm of gradients: 0.9479670447744987\n",
      "l2 norm of weights: 7.114184398077734\n",
      "---------------------\n",
      "Iteration Number: 35\n",
      "Loss: 102.55956398781599\n",
      "l2 norm of gradients: 0.9476868506095928\n",
      "l2 norm of weights: 7.112035395662839\n",
      "---------------------\n",
      "Iteration Number: 36\n",
      "Loss: 102.49815948456035\n",
      "l2 norm of gradients: 0.9474066617210697\n",
      "l2 norm of weights: 7.109887321854303\n",
      "---------------------\n",
      "Iteration Number: 37\n",
      "Loss: 102.436795986837\n",
      "l2 norm of gradients: 0.9471264781728994\n",
      "l2 norm of weights: 7.107740176080986\n",
      "---------------------\n",
      "Iteration Number: 38\n",
      "Loss: 102.3754734670343\n",
      "l2 norm of gradients: 0.946846300029511\n",
      "l2 norm of weights: 7.105593957770613\n",
      "---------------------\n",
      "Iteration Number: 39\n",
      "Loss: 102.31419189754477\n",
      "l2 norm of gradients: 0.9465661273557918\n",
      "l2 norm of weights: 7.103448666349771\n",
      "---------------------\n",
      "Iteration Number: 40\n",
      "Loss: 102.25295125070042\n",
      "l2 norm of gradients: 0.9462859602170836\n",
      "l2 norm of weights: 7.101304301243928\n",
      "---------------------\n",
      "Iteration Number: 41\n",
      "Loss: 102.19175149880829\n",
      "l2 norm of gradients: 0.9460057986791817\n",
      "l2 norm of weights: 7.099160861877432\n",
      "---------------------\n",
      "Iteration Number: 42\n",
      "Loss: 102.13059261416198\n",
      "l2 norm of gradients: 0.9457256428083325\n",
      "l2 norm of weights: 7.097018347673524\n",
      "---------------------\n",
      "Iteration Number: 43\n",
      "Loss: 102.06947456899931\n",
      "l2 norm of gradients: 0.9454454926712313\n",
      "l2 norm of weights: 7.094876758054346\n",
      "---------------------\n",
      "Iteration Number: 44\n",
      "Loss: 102.00839733554423\n",
      "l2 norm of gradients: 0.9451653483350201\n",
      "l2 norm of weights: 7.092736092440947\n",
      "---------------------\n",
      "Iteration Number: 45\n",
      "Loss: 101.94736088598138\n",
      "l2 norm of gradients: 0.9448852098672852\n",
      "l2 norm of weights: 7.0905963502532945\n",
      "---------------------\n",
      "Iteration Number: 46\n",
      "Loss: 101.88636519246614\n",
      "l2 norm of gradients: 0.9446050773360553\n",
      "l2 norm of weights: 7.08845753091028\n",
      "---------------------\n",
      "Iteration Number: 47\n",
      "Loss: 101.82541022712553\n",
      "l2 norm of gradients: 0.9443249508097993\n",
      "l2 norm of weights: 7.086319633829729\n",
      "---------------------\n",
      "Iteration Number: 48\n",
      "Loss: 101.76449596206447\n",
      "l2 norm of gradients: 0.9440448303574239\n",
      "l2 norm of weights: 7.084182658428408\n",
      "---------------------\n",
      "Iteration Number: 49\n",
      "Loss: 101.70362236934162\n",
      "l2 norm of gradients: 0.9437647160482716\n",
      "l2 norm of weights: 7.082046604122034\n",
      "---------------------\n",
      "Iteration Number: 50\n",
      "Loss: 101.64278942100269\n",
      "l2 norm of gradients: 0.9434846079521184\n",
      "l2 norm of weights: 7.079911470325285\n",
      "---------------------\n",
      "Iteration Number: 51\n",
      "Loss: 101.58199708905185\n",
      "l2 norm of gradients: 0.9432045061391713\n",
      "l2 norm of weights: 7.077777256451803\n",
      "---------------------\n",
      "Iteration Number: 52\n",
      "Loss: 101.52124534546665\n",
      "l2 norm of gradients: 0.9429244106800664\n",
      "l2 norm of weights: 7.075643961914209\n",
      "---------------------\n",
      "Iteration Number: 53\n",
      "Loss: 101.46053416220386\n",
      "l2 norm of gradients: 0.9426443216458666\n",
      "l2 norm of weights: 7.0735115861241065\n",
      "---------------------\n",
      "Iteration Number: 54\n",
      "Loss: 101.39986351118625\n",
      "l2 norm of gradients: 0.9423642391080594\n",
      "l2 norm of weights: 7.071380128492092\n",
      "---------------------\n",
      "Iteration Number: 55\n",
      "Loss: 101.33923336430733\n",
      "l2 norm of gradients: 0.9420841631385539\n",
      "l2 norm of weights: 7.069249588427766\n",
      "---------------------\n",
      "Iteration Number: 56\n",
      "Loss: 101.27864369343713\n",
      "l2 norm of gradients: 0.9418040938096797\n",
      "l2 norm of weights: 7.067119965339739\n",
      "---------------------\n",
      "Iteration Number: 57\n",
      "Loss: 101.2180944704041\n",
      "l2 norm of gradients: 0.9415240311941834\n",
      "l2 norm of weights: 7.064991258635638\n",
      "---------------------\n",
      "Iteration Number: 58\n",
      "Loss: 101.15758566702965\n",
      "l2 norm of gradients: 0.9412439753652276\n",
      "l2 norm of weights: 7.062863467722122\n",
      "---------------------\n",
      "Iteration Number: 59\n",
      "Loss: 101.0971172551021\n",
      "l2 norm of gradients: 0.9409639263963873\n",
      "l2 norm of weights: 7.060736592004885\n",
      "---------------------\n",
      "Iteration Number: 60\n",
      "Loss: 101.03668920635423\n",
      "l2 norm of gradients: 0.9406838843616484\n",
      "l2 norm of weights: 7.058610630888666\n",
      "---------------------\n",
      "Iteration Number: 61\n",
      "Loss: 100.9763014925474\n",
      "l2 norm of gradients: 0.9404038493354052\n",
      "l2 norm of weights: 7.05648558377726\n",
      "---------------------\n",
      "Iteration Number: 62\n",
      "Loss: 100.91595408536243\n",
      "l2 norm of gradients: 0.9401238213924578\n",
      "l2 norm of weights: 7.054361450073525\n",
      "---------------------\n",
      "Iteration Number: 63\n",
      "Loss: 100.85564695648331\n",
      "l2 norm of gradients: 0.9398438006080101\n",
      "l2 norm of weights: 7.052238229179393\n",
      "---------------------\n",
      "Iteration Number: 64\n",
      "Loss: 100.79538007756501\n",
      "l2 norm of gradients: 0.9395637870576669\n",
      "l2 norm of weights: 7.050115920495873\n",
      "---------------------\n",
      "Iteration Number: 65\n",
      "Loss: 100.73515342022472\n",
      "l2 norm of gradients: 0.9392837808174329\n",
      "l2 norm of weights: 7.04799452342307\n",
      "---------------------\n",
      "Iteration Number: 66\n",
      "Loss: 100.6749669560575\n",
      "l2 norm of gradients: 0.9390037819637079\n",
      "l2 norm of weights: 7.0458740373601865\n",
      "---------------------\n",
      "Iteration Number: 67\n",
      "Loss: 100.61482065665852\n",
      "l2 norm of gradients: 0.9387237905732874\n",
      "l2 norm of weights: 7.043754461705531\n",
      "---------------------\n",
      "Iteration Number: 68\n",
      "Loss: 100.55471449355105\n",
      "l2 norm of gradients: 0.9384438067233577\n",
      "l2 norm of weights: 7.041635795856532\n",
      "---------------------\n",
      "Iteration Number: 69\n",
      "Loss: 100.49464843826892\n",
      "l2 norm of gradients: 0.9381638304914945\n",
      "l2 norm of weights: 7.039518039209747\n",
      "---------------------\n",
      "Iteration Number: 70\n",
      "Loss: 100.43462246231039\n",
      "l2 norm of gradients: 0.937883861955661\n",
      "l2 norm of weights: 7.037401191160861\n",
      "---------------------\n",
      "Iteration Number: 71\n",
      "Loss: 100.37463653715434\n",
      "l2 norm of gradients: 0.9376039011942047\n",
      "l2 norm of weights: 7.035285251104716\n",
      "---------------------\n",
      "Iteration Number: 72\n",
      "Loss: 100.31469063424811\n",
      "l2 norm of gradients: 0.9373239482858551\n",
      "l2 norm of weights: 7.0331702184352975\n",
      "---------------------\n",
      "Iteration Number: 73\n",
      "Loss: 100.25478472501364\n",
      "l2 norm of gradients: 0.9370440033097214\n",
      "l2 norm of weights: 7.031056092545761\n",
      "---------------------\n",
      "Iteration Number: 74\n",
      "Loss: 100.19491878085461\n",
      "l2 norm of gradients: 0.9367640663452903\n",
      "l2 norm of weights: 7.0289428728284316\n",
      "---------------------\n",
      "Iteration Number: 75\n",
      "Loss: 100.1350927731482\n",
      "l2 norm of gradients: 0.9364841374724237\n",
      "l2 norm of weights: 7.026830558674818\n",
      "---------------------\n",
      "Iteration Number: 76\n",
      "Loss: 100.07530667325453\n",
      "l2 norm of gradients: 0.9362042167713547\n",
      "l2 norm of weights: 7.024719149475618\n",
      "---------------------\n",
      "Iteration Number: 77\n",
      "Loss: 100.01556045250167\n",
      "l2 norm of gradients: 0.9359243043226877\n",
      "l2 norm of weights: 7.022608644620732\n",
      "---------------------\n",
      "Iteration Number: 78\n",
      "Loss: 99.95585408220703\n",
      "l2 norm of gradients: 0.9356444002073933\n",
      "l2 norm of weights: 7.020499043499269\n",
      "---------------------\n",
      "Iteration Number: 79\n",
      "Loss: 99.8961875336392\n",
      "l2 norm of gradients: 0.9353645045068079\n",
      "l2 norm of weights: 7.018390345499558\n",
      "---------------------\n",
      "Iteration Number: 80\n",
      "Loss: 99.8365607780898\n",
      "l2 norm of gradients: 0.9350846173026304\n",
      "l2 norm of weights: 7.016282550009156\n",
      "---------------------\n",
      "Iteration Number: 81\n",
      "Loss: 99.776973786784\n",
      "l2 norm of gradients: 0.9348047386769189\n",
      "l2 norm of weights: 7.014175656414858\n",
      "---------------------\n",
      "Iteration Number: 82\n",
      "Loss: 99.71742653094323\n",
      "l2 norm of gradients: 0.9345248687120897\n",
      "l2 norm of weights: 7.012069664102707\n",
      "---------------------\n",
      "Iteration Number: 83\n",
      "Loss: 99.6579189817787\n",
      "l2 norm of gradients: 0.9342450074909143\n",
      "l2 norm of weights: 7.009964572458002\n",
      "---------------------\n",
      "Iteration Number: 84\n",
      "Loss: 99.59845111045281\n",
      "l2 norm of gradients: 0.9339651550965158\n",
      "l2 norm of weights: 7.007860380865307\n",
      "---------------------\n",
      "Iteration Number: 85\n",
      "Loss: 99.5390228881358\n",
      "l2 norm of gradients: 0.9336853116123678\n",
      "l2 norm of weights: 7.005757088708466\n",
      "---------------------\n",
      "Iteration Number: 86\n",
      "Loss: 99.47963428595651\n",
      "l2 norm of gradients: 0.933405477122291\n",
      "l2 norm of weights: 7.003654695370604\n",
      "---------------------\n",
      "Iteration Number: 87\n",
      "Loss: 99.42028527504485\n",
      "l2 norm of gradients: 0.9331256517104519\n",
      "l2 norm of weights: 7.001553200234142\n",
      "---------------------\n",
      "Iteration Number: 88\n",
      "Loss: 99.36097582647965\n",
      "l2 norm of gradients: 0.9328458354613585\n",
      "l2 norm of weights: 6.999452602680805\n",
      "---------------------\n",
      "Iteration Number: 89\n",
      "Loss: 99.30170591134437\n",
      "l2 norm of gradients: 0.9325660284598581\n",
      "l2 norm of weights: 6.997352902091632\n",
      "---------------------\n",
      "Iteration Number: 90\n",
      "Loss: 99.24247550070248\n",
      "l2 norm of gradients: 0.9322862307911366\n",
      "l2 norm of weights: 6.995254097846987\n",
      "---------------------\n",
      "Iteration Number: 91\n",
      "Loss: 99.18328456558076\n",
      "l2 norm of gradients: 0.9320064425407134\n",
      "l2 norm of weights: 6.993156189326563\n",
      "---------------------\n",
      "Iteration Number: 92\n",
      "Loss: 99.12413307701308\n",
      "l2 norm of gradients: 0.9317266637944407\n",
      "l2 norm of weights: 6.9910591759094\n",
      "---------------------\n",
      "Iteration Number: 93\n",
      "Loss: 99.0650210059739\n",
      "l2 norm of gradients: 0.9314468946384998\n",
      "l2 norm of weights: 6.988963056973886\n",
      "---------------------\n",
      "Iteration Number: 94\n",
      "Loss: 99.00594832346528\n",
      "l2 norm of gradients: 0.9311671351593994\n",
      "l2 norm of weights: 6.986867831897775\n",
      "---------------------\n",
      "Iteration Number: 95\n",
      "Loss: 98.94691500043659\n",
      "l2 norm of gradients: 0.9308873854439721\n",
      "l2 norm of weights: 6.984773500058187\n",
      "---------------------\n",
      "Iteration Number: 96\n",
      "Loss: 98.88792100783925\n",
      "l2 norm of gradients: 0.9306076455793723\n",
      "l2 norm of weights: 6.982680060831627\n",
      "---------------------\n",
      "Iteration Number: 97\n",
      "Loss: 98.82896631660061\n",
      "l2 norm of gradients: 0.9303279156530737\n",
      "l2 norm of weights: 6.980587513593989\n",
      "---------------------\n",
      "Iteration Number: 98\n",
      "Loss: 98.7700508976199\n",
      "l2 norm of gradients: 0.9300481957528662\n",
      "l2 norm of weights: 6.978495857720569\n",
      "---------------------\n",
      "Iteration Number: 99\n",
      "Loss: 98.71117472179256\n",
      "l2 norm of gradients: 0.9297684859668544\n",
      "l2 norm of weights: 6.976405092586069\n",
      "---------------------\n",
      "Iteration Number: 100\n",
      "Loss: 98.65233776000397\n",
      "l2 norm of gradients: 0.9294887863834532\n",
      "l2 norm of weights: 6.974315217564615\n",
      "---------------------\n",
      "Iteration Number: 101\n",
      "Loss: 98.59353998309969\n",
      "l2 norm of gradients: 0.9292090970913869\n",
      "l2 norm of weights: 6.972226232029761\n",
      "---------------------\n",
      "Iteration Number: 102\n",
      "Loss: 98.53478136192165\n",
      "l2 norm of gradients: 0.9289294181796852\n",
      "l2 norm of weights: 6.970138135354497\n",
      "---------------------\n",
      "Iteration Number: 103\n",
      "Loss: 98.47606186729912\n",
      "l2 norm of gradients: 0.9286497497376817\n",
      "l2 norm of weights: 6.968050926911265\n",
      "---------------------\n",
      "Iteration Number: 104\n",
      "Loss: 98.41738147003497\n",
      "l2 norm of gradients: 0.9283700918550105\n",
      "l2 norm of weights: 6.9659646060719655\n",
      "---------------------\n",
      "Iteration Number: 105\n",
      "Loss: 98.3587401409332\n",
      "l2 norm of gradients: 0.9280904446216037\n",
      "l2 norm of weights: 6.963879172207966\n",
      "---------------------\n",
      "Iteration Number: 106\n",
      "Loss: 98.30013785075664\n",
      "l2 norm of gradients: 0.9278108081276887\n",
      "l2 norm of weights: 6.961794624690111\n",
      "---------------------\n",
      "Iteration Number: 107\n",
      "Loss: 98.24157457027653\n",
      "l2 norm of gradients: 0.9275311824637863\n",
      "l2 norm of weights: 6.959710962888734\n",
      "---------------------\n",
      "Iteration Number: 108\n",
      "Loss: 98.18305027024408\n",
      "l2 norm of gradients: 0.9272515677207063\n",
      "l2 norm of weights: 6.957628186173666\n",
      "---------------------\n",
      "Iteration Number: 109\n",
      "Loss: 98.12456492138273\n",
      "l2 norm of gradients: 0.9269719639895466\n",
      "l2 norm of weights: 6.955546293914245\n",
      "---------------------\n",
      "Iteration Number: 110\n",
      "Loss: 98.06611849441506\n",
      "l2 norm of gradients: 0.9266923713616897\n",
      "l2 norm of weights: 6.953465285479325\n",
      "---------------------\n",
      "Iteration Number: 111\n",
      "Loss: 98.00771096005353\n",
      "l2 norm of gradients: 0.9264127899287999\n",
      "l2 norm of weights: 6.951385160237287\n",
      "---------------------\n",
      "Iteration Number: 112\n",
      "Loss: 97.94934228896958\n",
      "l2 norm of gradients: 0.9261332197828208\n",
      "l2 norm of weights: 6.94930591755605\n",
      "---------------------\n",
      "Iteration Number: 113\n",
      "Loss: 97.89101245185961\n",
      "l2 norm of gradients: 0.9258536610159729\n",
      "l2 norm of weights: 6.947227556803076\n",
      "---------------------\n",
      "Iteration Number: 114\n",
      "Loss: 97.83272141937917\n",
      "l2 norm of gradients: 0.9255741137207502\n",
      "l2 norm of weights: 6.945150077345388\n",
      "---------------------\n",
      "Iteration Number: 115\n",
      "Loss: 97.77446916218048\n",
      "l2 norm of gradients: 0.9252945779899182\n",
      "l2 norm of weights: 6.943073478549568\n",
      "---------------------\n",
      "Iteration Number: 116\n",
      "Loss: 97.71625565089127\n",
      "l2 norm of gradients: 0.9250150539165105\n",
      "l2 norm of weights: 6.94099775978178\n",
      "---------------------\n",
      "Iteration Number: 117\n",
      "Loss: 97.65808085615421\n",
      "l2 norm of gradients: 0.9247355415938264\n",
      "l2 norm of weights: 6.93892292040777\n",
      "---------------------\n",
      "Iteration Number: 118\n",
      "Loss: 97.59994474857518\n",
      "l2 norm of gradients: 0.9244560411154287\n",
      "l2 norm of weights: 6.936848959792879\n",
      "---------------------\n",
      "Iteration Number: 119\n",
      "Loss: 97.54184729874896\n",
      "l2 norm of gradients: 0.9241765525751401\n",
      "l2 norm of weights: 6.934775877302054\n",
      "---------------------\n",
      "Iteration Number: 120\n",
      "Loss: 97.48378847727611\n",
      "l2 norm of gradients: 0.9238970760670402\n",
      "l2 norm of weights: 6.932703672299858\n",
      "---------------------\n",
      "Iteration Number: 121\n",
      "Loss: 97.42576825472568\n",
      "l2 norm of gradients: 0.9236176116854644\n",
      "l2 norm of weights: 6.930632344150475\n",
      "---------------------\n",
      "Iteration Number: 122\n",
      "Loss: 97.3677866016737\n",
      "l2 norm of gradients: 0.9233381595249996\n",
      "l2 norm of weights: 6.928561892217726\n",
      "---------------------\n",
      "Iteration Number: 123\n",
      "Loss: 97.30984348865967\n",
      "l2 norm of gradients: 0.9230587196804818\n",
      "l2 norm of weights: 6.926492315865075\n",
      "---------------------\n",
      "Iteration Number: 124\n",
      "Loss: 97.25193888625472\n",
      "l2 norm of gradients: 0.9227792922469938\n",
      "l2 norm of weights: 6.924423614455642\n",
      "---------------------\n",
      "Iteration Number: 125\n",
      "Loss: 97.19407276497401\n",
      "l2 norm of gradients: 0.9224998773198616\n",
      "l2 norm of weights: 6.922355787352207\n",
      "---------------------\n",
      "Iteration Number: 126\n",
      "Loss: 97.13624509534617\n",
      "l2 norm of gradients: 0.9222204749946525\n",
      "l2 norm of weights: 6.920288833917226\n",
      "---------------------\n",
      "Iteration Number: 127\n",
      "Loss: 97.0784558478929\n",
      "l2 norm of gradients: 0.9219410853671721\n",
      "l2 norm of weights: 6.91822275351284\n",
      "---------------------\n",
      "Iteration Number: 128\n",
      "Loss: 97.0207049931131\n",
      "l2 norm of gradients: 0.9216617085334606\n",
      "l2 norm of weights: 6.9161575455008775\n",
      "---------------------\n",
      "Iteration Number: 129\n",
      "Loss: 96.96299250151053\n",
      "l2 norm of gradients: 0.9213823445897915\n",
      "l2 norm of weights: 6.914093209242876\n",
      "---------------------\n",
      "Iteration Number: 130\n",
      "Loss: 96.90531834357253\n",
      "l2 norm of gradients: 0.9211029936326678\n",
      "l2 norm of weights: 6.912029744100081\n",
      "---------------------\n",
      "Iteration Number: 131\n",
      "Loss: 96.84768248976388\n",
      "l2 norm of gradients: 0.9208236557588192\n",
      "l2 norm of weights: 6.909967149433462\n",
      "---------------------\n",
      "Iteration Number: 132\n",
      "Loss: 96.79008491057621\n",
      "l2 norm of gradients: 0.9205443310652002\n",
      "l2 norm of weights: 6.907905424603723\n",
      "---------------------\n",
      "Iteration Number: 133\n",
      "Loss: 96.73252557645583\n",
      "l2 norm of gradients: 0.920265019648986\n",
      "l2 norm of weights: 6.905844568971307\n",
      "---------------------\n",
      "Iteration Number: 134\n",
      "Loss: 96.6750044578619\n",
      "l2 norm of gradients: 0.9199857216075705\n",
      "l2 norm of weights: 6.903784581896409\n",
      "---------------------\n",
      "Iteration Number: 135\n",
      "Loss: 96.61752152524888\n",
      "l2 norm of gradients: 0.9197064370385637\n",
      "l2 norm of weights: 6.9017254627389875\n",
      "---------------------\n",
      "Iteration Number: 136\n",
      "Loss: 96.56007674904768\n",
      "l2 norm of gradients: 0.9194271660397876\n",
      "l2 norm of weights: 6.8996672108587696\n",
      "---------------------\n",
      "Iteration Number: 137\n",
      "Loss: 96.50267009969362\n",
      "l2 norm of gradients: 0.9191479087092751\n",
      "l2 norm of weights: 6.897609825615266\n",
      "---------------------\n",
      "Iteration Number: 138\n",
      "Loss: 96.44530154761344\n",
      "l2 norm of gradients: 0.918868665145266\n",
      "l2 norm of weights: 6.895553306367774\n",
      "---------------------\n",
      "Iteration Number: 139\n",
      "Loss: 96.38797106322436\n",
      "l2 norm of gradients: 0.9185894354462044\n",
      "l2 norm of weights: 6.893497652475397\n",
      "---------------------\n",
      "Iteration Number: 140\n",
      "Loss: 96.33067861694015\n",
      "l2 norm of gradients: 0.918310219710736\n",
      "l2 norm of weights: 6.891442863297045\n",
      "---------------------\n",
      "Iteration Number: 141\n",
      "Loss: 96.27342417917237\n",
      "l2 norm of gradients: 0.9180310180377053\n",
      "l2 norm of weights: 6.889388938191448\n",
      "---------------------\n",
      "Iteration Number: 142\n",
      "Loss: 96.21620772031608\n",
      "l2 norm of gradients: 0.9177518305261524\n",
      "l2 norm of weights: 6.887335876517167\n",
      "---------------------\n",
      "Iteration Number: 143\n",
      "Loss: 96.1590292107682\n",
      "l2 norm of gradients: 0.9174726572753107\n",
      "l2 norm of weights: 6.8852836776326\n",
      "---------------------\n",
      "Iteration Number: 144\n",
      "Loss: 96.1018886209253\n",
      "l2 norm of gradients: 0.917193498384603\n",
      "l2 norm of weights: 6.883232340895996\n",
      "---------------------\n",
      "Iteration Number: 145\n",
      "Loss: 96.04478592115777\n",
      "l2 norm of gradients: 0.9169143539536404\n",
      "l2 norm of weights: 6.881181865665464\n",
      "---------------------\n",
      "Iteration Number: 146\n",
      "Loss: 95.98772108186643\n",
      "l2 norm of gradients: 0.9166352240822176\n",
      "l2 norm of weights: 6.879132251298977\n",
      "---------------------\n",
      "Iteration Number: 147\n",
      "Loss: 95.9306940734154\n",
      "l2 norm of gradients: 0.9163561088703115\n",
      "l2 norm of weights: 6.87708349715439\n",
      "---------------------\n",
      "Iteration Number: 148\n",
      "Loss: 95.87370486618747\n",
      "l2 norm of gradients: 0.9160770084180765\n",
      "l2 norm of weights: 6.875035602589445\n",
      "---------------------\n",
      "Iteration Number: 149\n",
      "Loss: 95.81675343054204\n",
      "l2 norm of gradients: 0.9157979228258437\n",
      "l2 norm of weights: 6.872988566961777\n",
      "---------------------\n",
      "Iteration Number: 150\n",
      "Loss: 95.75983973684501\n",
      "l2 norm of gradients: 0.9155188521941173\n",
      "l2 norm of weights: 6.870942389628936\n",
      "---------------------\n",
      "Iteration Number: 151\n",
      "Loss: 95.70296375546798\n",
      "l2 norm of gradients: 0.9152397966235706\n",
      "l2 norm of weights: 6.86889706994838\n",
      "---------------------\n",
      "Iteration Number: 152\n",
      "Loss: 95.64612545676141\n",
      "l2 norm of gradients: 0.9149607562150444\n",
      "l2 norm of weights: 6.8668526072775\n",
      "---------------------\n",
      "Iteration Number: 153\n",
      "Loss: 95.58932481108543\n",
      "l2 norm of gradients: 0.9146817310695436\n",
      "l2 norm of weights: 6.864809000973619\n",
      "---------------------\n",
      "Iteration Number: 154\n",
      "Loss: 95.53256178878975\n",
      "l2 norm of gradients: 0.9144027212882352\n",
      "l2 norm of weights: 6.862766250394007\n",
      "---------------------\n",
      "Iteration Number: 155\n",
      "Loss: 95.47583636023205\n",
      "l2 norm of gradients: 0.9141237269724432\n",
      "l2 norm of weights: 6.860724354895886\n",
      "---------------------\n",
      "Iteration Number: 156\n",
      "Loss: 95.4191484957627\n",
      "l2 norm of gradients: 0.9138447482236479\n",
      "l2 norm of weights: 6.858683313836448\n",
      "---------------------\n",
      "Iteration Number: 157\n",
      "Loss: 95.36249816572105\n",
      "l2 norm of gradients: 0.9135657851434825\n",
      "l2 norm of weights: 6.856643126572853\n",
      "---------------------\n",
      "Iteration Number: 158\n",
      "Loss: 95.30588534046161\n",
      "l2 norm of gradients: 0.9132868378337295\n",
      "l2 norm of weights: 6.854603792462248\n",
      "---------------------\n",
      "Iteration Number: 159\n",
      "Loss: 95.24930999032938\n",
      "l2 norm of gradients: 0.9130079063963178\n",
      "l2 norm of weights: 6.852565310861773\n",
      "---------------------\n",
      "Iteration Number: 160\n",
      "Loss: 95.19277208566304\n",
      "l2 norm of gradients: 0.9127289909333206\n",
      "l2 norm of weights: 6.85052768112857\n",
      "---------------------\n",
      "Iteration Number: 161\n",
      "Loss: 95.13627159681495\n",
      "l2 norm of gradients: 0.9124500915469523\n",
      "l2 norm of weights: 6.848490902619792\n",
      "---------------------\n",
      "Iteration Number: 162\n",
      "Loss: 95.0798084941268\n",
      "l2 norm of gradients: 0.9121712083395644\n",
      "l2 norm of weights: 6.846454974692615\n",
      "---------------------\n",
      "Iteration Number: 163\n",
      "Loss: 95.02338274793688\n",
      "l2 norm of gradients: 0.9118923414136445\n",
      "l2 norm of weights: 6.844419896704246\n",
      "---------------------\n",
      "Iteration Number: 164\n",
      "Loss: 94.96699432860132\n",
      "l2 norm of gradients: 0.9116134908718115\n",
      "l2 norm of weights: 6.84238566801193\n",
      "---------------------\n",
      "Iteration Number: 165\n",
      "Loss: 94.9106432064467\n",
      "l2 norm of gradients: 0.9113346568168142\n",
      "l2 norm of weights: 6.840352287972966\n",
      "---------------------\n",
      "Iteration Number: 166\n",
      "Loss: 94.8543293518373\n",
      "l2 norm of gradients: 0.9110558393515271\n",
      "l2 norm of weights: 6.838319755944707\n",
      "---------------------\n",
      "Iteration Number: 167\n",
      "Loss: 94.79805273511354\n",
      "l2 norm of gradients: 0.9107770385789484\n",
      "l2 norm of weights: 6.83628807128458\n",
      "---------------------\n",
      "Iteration Number: 168\n",
      "Loss: 94.7418133266135\n",
      "l2 norm of gradients: 0.9104982546021968\n",
      "l2 norm of weights: 6.834257233350086\n",
      "---------------------\n",
      "Iteration Number: 169\n",
      "Loss: 94.68561109668973\n",
      "l2 norm of gradients: 0.9102194875245081\n",
      "l2 norm of weights: 6.832227241498814\n",
      "---------------------\n",
      "Iteration Number: 170\n",
      "Loss: 94.62944601570311\n",
      "l2 norm of gradients: 0.9099407374492329\n",
      "l2 norm of weights: 6.830198095088453\n",
      "---------------------\n",
      "Iteration Number: 171\n",
      "Loss: 94.57331805399318\n",
      "l2 norm of gradients: 0.9096620044798331\n",
      "l2 norm of weights: 6.828169793476792\n",
      "---------------------\n",
      "Iteration Number: 172\n",
      "Loss: 94.51722718193233\n",
      "l2 norm of gradients: 0.9093832887198797\n",
      "l2 norm of weights: 6.826142336021742\n",
      "---------------------\n",
      "Iteration Number: 173\n",
      "Loss: 94.4611733698599\n",
      "l2 norm of gradients: 0.909104590273049\n",
      "l2 norm of weights: 6.824115722081332\n",
      "---------------------\n",
      "Iteration Number: 174\n",
      "Loss: 94.40515658814601\n",
      "l2 norm of gradients: 0.9088259092431199\n",
      "l2 norm of weights: 6.822089951013729\n",
      "---------------------\n",
      "Iteration Number: 175\n",
      "Loss: 94.3491768071537\n",
      "l2 norm of gradients: 0.9085472457339716\n",
      "l2 norm of weights: 6.820065022177244\n",
      "---------------------\n",
      "Iteration Number: 176\n",
      "Loss: 94.29323399724976\n",
      "l2 norm of gradients: 0.9082685998495796\n",
      "l2 norm of weights: 6.818040934930335\n",
      "---------------------\n",
      "Iteration Number: 177\n",
      "Loss: 94.23732812880347\n",
      "l2 norm of gradients: 0.9079899716940136\n",
      "l2 norm of weights: 6.816017688631626\n",
      "---------------------\n",
      "Iteration Number: 178\n",
      "Loss: 94.18145917219027\n",
      "l2 norm of gradients: 0.9077113613714339\n",
      "l2 norm of weights: 6.813995282639908\n",
      "---------------------\n",
      "Iteration Number: 179\n",
      "Loss: 94.12562709779058\n",
      "l2 norm of gradients: 0.9074327689860892\n",
      "l2 norm of weights: 6.811973716314158\n",
      "---------------------\n",
      "Iteration Number: 180\n",
      "Loss: 94.06983187598063\n",
      "l2 norm of gradients: 0.9071541946423127\n",
      "l2 norm of weights: 6.809952989013535\n",
      "---------------------\n",
      "Iteration Number: 181\n",
      "Loss: 94.01407347716291\n",
      "l2 norm of gradients: 0.9068756384445195\n",
      "l2 norm of weights: 6.8079331000974\n",
      "---------------------\n",
      "Iteration Number: 182\n",
      "Loss: 93.95835187171222\n",
      "l2 norm of gradients: 0.9065971004972045\n",
      "l2 norm of weights: 6.805914048925318\n",
      "---------------------\n",
      "Iteration Number: 183\n",
      "Loss: 93.90266703004104\n",
      "l2 norm of gradients: 0.906318580904938\n",
      "l2 norm of weights: 6.803895834857074\n",
      "---------------------\n",
      "Iteration Number: 184\n",
      "Loss: 93.84701892254267\n",
      "l2 norm of gradients: 0.9060400797723637\n",
      "l2 norm of weights: 6.801878457252674\n",
      "---------------------\n",
      "Iteration Number: 185\n",
      "Loss: 93.79140751963105\n",
      "l2 norm of gradients: 0.9057615972041955\n",
      "l2 norm of weights: 6.799861915472364\n",
      "---------------------\n",
      "Iteration Number: 186\n",
      "Loss: 93.73583279172243\n",
      "l2 norm of gradients: 0.905483133305214\n",
      "l2 norm of weights: 6.797846208876625\n",
      "---------------------\n",
      "Iteration Number: 187\n",
      "Loss: 93.6802947092361\n",
      "l2 norm of gradients: 0.9052046881802647\n",
      "l2 norm of weights: 6.795831336826199\n",
      "---------------------\n",
      "Iteration Number: 188\n",
      "Loss: 93.62479324259792\n",
      "l2 norm of gradients: 0.904926261934254\n",
      "l2 norm of weights: 6.793817298682081\n",
      "---------------------\n",
      "Iteration Number: 189\n",
      "Loss: 93.5693283622451\n",
      "l2 norm of gradients: 0.9046478546721464\n",
      "l2 norm of weights: 6.791804093805543\n",
      "---------------------\n",
      "Iteration Number: 190\n",
      "Loss: 93.51390003861692\n",
      "l2 norm of gradients: 0.9043694664989617\n",
      "l2 norm of weights: 6.78979172155813\n",
      "---------------------\n",
      "Iteration Number: 191\n",
      "Loss: 93.45850824216646\n",
      "l2 norm of gradients: 0.9040910975197727\n",
      "l2 norm of weights: 6.78778018130168\n",
      "---------------------\n",
      "Iteration Number: 192\n",
      "Loss: 93.40315294334226\n",
      "l2 norm of gradients: 0.9038127478397008\n",
      "l2 norm of weights: 6.785769472398324\n",
      "---------------------\n",
      "Iteration Number: 193\n",
      "Loss: 93.34783411260544\n",
      "l2 norm of gradients: 0.9035344175639137\n",
      "l2 norm of weights: 6.7837595942105\n",
      "---------------------\n",
      "Iteration Number: 194\n",
      "Loss: 93.29255172043942\n",
      "l2 norm of gradients: 0.9032561067976234\n",
      "l2 norm of weights: 6.781750546100961\n",
      "---------------------\n",
      "Iteration Number: 195\n",
      "Loss: 93.23730573732618\n",
      "l2 norm of gradients: 0.9029778156460814\n",
      "l2 norm of weights: 6.779742327432779\n",
      "---------------------\n",
      "Iteration Number: 196\n",
      "Loss: 93.182096133732\n",
      "l2 norm of gradients: 0.9026995442145771\n",
      "l2 norm of weights: 6.777734937569364\n",
      "---------------------\n",
      "Iteration Number: 197\n",
      "Loss: 93.12692288017661\n",
      "l2 norm of gradients: 0.9024212926084344\n",
      "l2 norm of weights: 6.775728375874462\n",
      "---------------------\n",
      "Iteration Number: 198\n",
      "Loss: 93.07178594715916\n",
      "l2 norm of gradients: 0.9021430609330086\n",
      "l2 norm of weights: 6.773722641712171\n",
      "---------------------\n",
      "Iteration Number: 199\n",
      "Loss: 93.01668530518609\n",
      "l2 norm of gradients: 0.9018648492936835\n",
      "l2 norm of weights: 6.771717734446943\n",
      "---------------------\n",
      "Iteration Number: 200\n",
      "Loss: 92.96162092479048\n",
      "l2 norm of gradients: 0.9015866577958687\n",
      "l2 norm of weights: 6.7697136534436\n",
      "---------------------\n",
      "Iteration Number: 201\n",
      "Loss: 92.90659277650484\n",
      "l2 norm of gradients: 0.9013084865449963\n",
      "l2 norm of weights: 6.767710398067338\n",
      "---------------------\n",
      "Iteration Number: 202\n",
      "Loss: 92.85160083087771\n",
      "l2 norm of gradients: 0.9010303356465178\n",
      "l2 norm of weights: 6.7657079676837375\n",
      "---------------------\n",
      "Iteration Number: 203\n",
      "Loss: 92.79664505845318\n",
      "l2 norm of gradients: 0.9007522052059018\n",
      "l2 norm of weights: 6.7637063616587705\n",
      "---------------------\n",
      "Iteration Number: 204\n",
      "Loss: 92.74172542980882\n",
      "l2 norm of gradients: 0.9004740953286305\n",
      "l2 norm of weights: 6.761705579358807\n",
      "---------------------\n",
      "Iteration Number: 205\n",
      "Loss: 92.68684191550643\n",
      "l2 norm of gradients: 0.9001960061201965\n",
      "l2 norm of weights: 6.7597056201506325\n",
      "---------------------\n",
      "Iteration Number: 206\n",
      "Loss: 92.631994486144\n",
      "l2 norm of gradients: 0.8999179376861003\n",
      "l2 norm of weights: 6.757706483401445\n",
      "---------------------\n",
      "Iteration Number: 207\n",
      "Loss: 92.57718311231186\n",
      "l2 norm of gradients: 0.8996398901318476\n",
      "l2 norm of weights: 6.75570816847887\n",
      "---------------------\n",
      "Iteration Number: 208\n",
      "Loss: 92.52240776462344\n",
      "l2 norm of gradients: 0.8993618635629456\n",
      "l2 norm of weights: 6.75371067475097\n",
      "---------------------\n",
      "Iteration Number: 209\n",
      "Loss: 92.46766841369346\n",
      "l2 norm of gradients: 0.8990838580849005\n",
      "l2 norm of weights: 6.7517140015862465\n",
      "---------------------\n",
      "Iteration Number: 210\n",
      "Loss: 92.41296503016129\n",
      "l2 norm of gradients: 0.8988058738032141\n",
      "l2 norm of weights: 6.749718148353654\n",
      "---------------------\n",
      "Iteration Number: 211\n",
      "Loss: 92.358297584672\n",
      "l2 norm of gradients: 0.8985279108233819\n",
      "l2 norm of weights: 6.747723114422609\n",
      "---------------------\n",
      "Iteration Number: 212\n",
      "Loss: 92.30366604787534\n",
      "l2 norm of gradients: 0.8982499692508886\n",
      "l2 norm of weights: 6.745728899162993\n",
      "---------------------\n",
      "Iteration Number: 213\n",
      "Loss: 92.24907039044193\n",
      "l2 norm of gradients: 0.8979720491912065\n",
      "l2 norm of weights: 6.743735501945163\n",
      "---------------------\n",
      "Iteration Number: 214\n",
      "Loss: 92.19451058305951\n",
      "l2 norm of gradients: 0.8976941507497918\n",
      "l2 norm of weights: 6.741742922139962\n",
      "---------------------\n",
      "Iteration Number: 215\n",
      "Loss: 92.13998659642033\n",
      "l2 norm of gradients: 0.8974162740320818\n",
      "l2 norm of weights: 6.739751159118728\n",
      "---------------------\n",
      "Iteration Number: 216\n",
      "Loss: 92.08549840122629\n",
      "l2 norm of gradients: 0.8971384191434923\n",
      "l2 norm of weights: 6.737760212253295\n",
      "---------------------\n",
      "Iteration Number: 217\n",
      "Loss: 92.03104596821647\n",
      "l2 norm of gradients: 0.8968605861894144\n",
      "l2 norm of weights: 6.735770080916008\n",
      "---------------------\n",
      "Iteration Number: 218\n",
      "Loss: 91.97662926811073\n",
      "l2 norm of gradients: 0.8965827752752111\n",
      "l2 norm of weights: 6.7337807644797305\n",
      "---------------------\n",
      "Iteration Number: 219\n",
      "Loss: 91.92224827166369\n",
      "l2 norm of gradients: 0.8963049865062152\n",
      "l2 norm of weights: 6.731792262317847\n",
      "---------------------\n",
      "Iteration Number: 220\n",
      "Loss: 91.86790294964575\n",
      "l2 norm of gradients: 0.8960272199877256\n",
      "l2 norm of weights: 6.729804573804281\n",
      "---------------------\n",
      "Iteration Number: 221\n",
      "Loss: 91.81359327282372\n",
      "l2 norm of gradients: 0.8957494758250051\n",
      "l2 norm of weights: 6.727817698313492\n",
      "---------------------\n",
      "Iteration Number: 222\n",
      "Loss: 91.75931921200157\n",
      "l2 norm of gradients: 0.895471754123277\n",
      "l2 norm of weights: 6.725831635220491\n",
      "---------------------\n",
      "Iteration Number: 223\n",
      "Loss: 91.70508073797963\n",
      "l2 norm of gradients: 0.8951940549877223\n",
      "l2 norm of weights: 6.7238463839008435\n",
      "---------------------\n",
      "Iteration Number: 224\n",
      "Loss: 91.65087782158784\n",
      "l2 norm of gradients: 0.8949163785234763\n",
      "l2 norm of weights: 6.7218619437306835\n",
      "---------------------\n",
      "Iteration Number: 225\n",
      "Loss: 91.59671043366036\n",
      "l2 norm of gradients: 0.8946387248356269\n",
      "l2 norm of weights: 6.719878314086715\n",
      "---------------------\n",
      "Iteration Number: 226\n",
      "Loss: 91.54257854505207\n",
      "l2 norm of gradients: 0.8943610940292107\n",
      "l2 norm of weights: 6.717895494346225\n",
      "---------------------\n",
      "Iteration Number: 227\n",
      "Loss: 91.48848212663276\n",
      "l2 norm of gradients: 0.8940834862092096\n",
      "l2 norm of weights: 6.715913483887085\n",
      "---------------------\n",
      "Iteration Number: 228\n",
      "Loss: 91.43442114929091\n",
      "l2 norm of gradients: 0.8938059014805497\n",
      "l2 norm of weights: 6.713932282087767\n",
      "---------------------\n",
      "Iteration Number: 229\n",
      "Loss: 91.38039558392155\n",
      "l2 norm of gradients: 0.8935283399480968\n",
      "l2 norm of weights: 6.711951888327343\n",
      "---------------------\n",
      "Iteration Number: 230\n",
      "Loss: 91.32640540145587\n",
      "l2 norm of gradients: 0.8932508017166535\n",
      "l2 norm of weights: 6.709972301985501\n",
      "---------------------\n",
      "Iteration Number: 231\n",
      "Loss: 91.27245057281812\n",
      "l2 norm of gradients: 0.8929732868909577\n",
      "l2 norm of weights: 6.707993522442544\n",
      "---------------------\n",
      "Iteration Number: 232\n",
      "Loss: 91.21853106896657\n",
      "l2 norm of gradients: 0.8926957955756784\n",
      "l2 norm of weights: 6.706015549079404\n",
      "---------------------\n",
      "Iteration Number: 233\n",
      "Loss: 91.16464686086815\n",
      "l2 norm of gradients: 0.892418327875413\n",
      "l2 norm of weights: 6.704038381277647\n",
      "---------------------\n",
      "Iteration Number: 234\n",
      "Loss: 91.1107979195116\n",
      "l2 norm of gradients: 0.8921408838946852\n",
      "l2 norm of weights: 6.702062018419481\n",
      "---------------------\n",
      "Iteration Number: 235\n",
      "Loss: 91.0569842158943\n",
      "l2 norm of gradients: 0.8918634637379412\n",
      "l2 norm of weights: 6.700086459887764\n",
      "---------------------\n",
      "Iteration Number: 236\n",
      "Loss: 91.00320572104641\n",
      "l2 norm of gradients: 0.8915860675095474\n",
      "l2 norm of weights: 6.698111705066009\n",
      "---------------------\n",
      "Iteration Number: 237\n",
      "Loss: 90.94946240600419\n",
      "l2 norm of gradients: 0.8913086953137871\n",
      "l2 norm of weights: 6.696137753338396\n",
      "---------------------\n",
      "Iteration Number: 238\n",
      "Loss: 90.89575424183164\n",
      "l2 norm of gradients: 0.8910313472548583\n",
      "l2 norm of weights: 6.694164604089776\n",
      "---------------------\n",
      "Iteration Number: 239\n",
      "Loss: 90.84208119959972\n",
      "l2 norm of gradients: 0.8907540234368702\n",
      "l2 norm of weights: 6.692192256705678\n",
      "---------------------\n",
      "Iteration Number: 240\n",
      "Loss: 90.78844325040724\n",
      "l2 norm of gradients: 0.8904767239638399\n",
      "l2 norm of weights: 6.690220710572321\n",
      "---------------------\n",
      "Iteration Number: 241\n",
      "Loss: 90.73484036536499\n",
      "l2 norm of gradients: 0.8901994489396918\n",
      "l2 norm of weights: 6.688249965076613\n",
      "---------------------\n",
      "Iteration Number: 242\n",
      "Loss: 90.68127251560624\n",
      "l2 norm of gradients: 0.8899221984682518\n",
      "l2 norm of weights: 6.686280019606165\n",
      "---------------------\n",
      "Iteration Number: 243\n",
      "Loss: 90.6277396722905\n",
      "l2 norm of gradients: 0.8896449726532466\n",
      "l2 norm of weights: 6.684310873549299\n",
      "---------------------\n",
      "Iteration Number: 244\n",
      "Loss: 90.57424180658276\n",
      "l2 norm of gradients: 0.8893677715982998\n",
      "l2 norm of weights: 6.682342526295048\n",
      "---------------------\n",
      "Iteration Number: 245\n",
      "Loss: 90.52077888967355\n",
      "l2 norm of gradients: 0.8890905954069297\n",
      "l2 norm of weights: 6.680374977233171\n",
      "---------------------\n",
      "Iteration Number: 246\n",
      "Loss: 90.46735089277945\n",
      "l2 norm of gradients: 0.8888134441825462\n",
      "l2 norm of weights: 6.678408225754156\n",
      "---------------------\n",
      "Iteration Number: 247\n",
      "Loss: 90.41395778713354\n",
      "l2 norm of gradients: 0.8885363180284476\n",
      "l2 norm of weights: 6.676442271249228\n",
      "---------------------\n",
      "Iteration Number: 248\n",
      "Loss: 90.3605995439832\n",
      "l2 norm of gradients: 0.8882592170478188\n",
      "l2 norm of weights: 6.6744771131103535\n",
      "---------------------\n",
      "Iteration Number: 249\n",
      "Loss: 90.30727613460373\n",
      "l2 norm of gradients: 0.8879821413437275\n",
      "l2 norm of weights: 6.672512750730253\n",
      "---------------------\n",
      "Iteration Number: 250\n",
      "Loss: 90.25398753028516\n",
      "l2 norm of gradients: 0.8877050910191221\n",
      "l2 norm of weights: 6.670549183502404\n",
      "---------------------\n",
      "Iteration Number: 251\n",
      "Loss: 90.20073370234904\n",
      "l2 norm of gradients: 0.8874280661768283\n",
      "l2 norm of weights: 6.668586410821047\n",
      "---------------------\n",
      "Iteration Number: 252\n",
      "Loss: 90.1475146221193\n",
      "l2 norm of gradients: 0.8871510669195473\n",
      "l2 norm of weights: 6.666624432081195\n",
      "---------------------\n",
      "Iteration Number: 253\n",
      "Loss: 90.09433026096168\n",
      "l2 norm of gradients: 0.8868740933498517\n",
      "l2 norm of weights: 6.66466324667864\n",
      "---------------------\n",
      "Iteration Number: 254\n",
      "Loss: 90.04118059025417\n",
      "l2 norm of gradients: 0.8865971455701841\n",
      "l2 norm of weights: 6.662702854009961\n",
      "---------------------\n",
      "Iteration Number: 255\n",
      "Loss: 89.98806558139442\n",
      "l2 norm of gradients: 0.8863202236828533\n",
      "l2 norm of weights: 6.660743253472523\n",
      "---------------------\n",
      "Iteration Number: 256\n",
      "Loss: 89.93498520580108\n",
      "l2 norm of gradients: 0.8860433277900323\n",
      "l2 norm of weights: 6.658784444464496\n",
      "---------------------\n",
      "Iteration Number: 257\n",
      "Loss: 89.881939434925\n",
      "l2 norm of gradients: 0.8857664579937549\n",
      "l2 norm of weights: 6.656826426384851\n",
      "---------------------\n",
      "Iteration Number: 258\n",
      "Loss: 89.82892824022413\n",
      "l2 norm of gradients: 0.8854896143959136\n",
      "l2 norm of weights: 6.654869198633374\n",
      "---------------------\n",
      "Iteration Number: 259\n",
      "Loss: 89.7759515931909\n",
      "l2 norm of gradients: 0.8852127970982563\n",
      "l2 norm of weights: 6.652912760610668\n",
      "---------------------\n",
      "Iteration Number: 260\n",
      "Loss: 89.72300946533964\n",
      "l2 norm of gradients: 0.8849360062023844\n",
      "l2 norm of weights: 6.650957111718162\n",
      "---------------------\n",
      "Iteration Number: 261\n",
      "Loss: 89.67010182820388\n",
      "l2 norm of gradients: 0.8846592418097494\n",
      "l2 norm of weights: 6.649002251358113\n",
      "---------------------\n",
      "Iteration Number: 262\n",
      "Loss: 89.61722865333468\n",
      "l2 norm of gradients: 0.88438250402165\n",
      "l2 norm of weights: 6.647048178933623\n",
      "---------------------\n",
      "Iteration Number: 263\n",
      "Loss: 89.56438991231711\n",
      "l2 norm of gradients: 0.8841057929392304\n",
      "l2 norm of weights: 6.645094893848632\n",
      "---------------------\n",
      "Iteration Number: 264\n",
      "Loss: 89.51158557675444\n",
      "l2 norm of gradients: 0.8838291086634769\n",
      "l2 norm of weights: 6.643142395507933\n",
      "---------------------\n",
      "Iteration Number: 265\n",
      "Loss: 89.45881561827579\n",
      "l2 norm of gradients: 0.8835524512952151\n",
      "l2 norm of weights: 6.641190683317178\n",
      "---------------------\n",
      "Iteration Number: 266\n",
      "Loss: 89.40608000853264\n",
      "l2 norm of gradients: 0.8832758209351079\n",
      "l2 norm of weights: 6.639239756682879\n",
      "---------------------\n",
      "Iteration Number: 267\n",
      "Loss: 89.35337871919607\n",
      "l2 norm of gradients: 0.8829992176836524\n",
      "l2 norm of weights: 6.637289615012423\n",
      "---------------------\n",
      "Iteration Number: 268\n",
      "Loss: 89.30071172196952\n",
      "l2 norm of gradients: 0.8827226416411771\n",
      "l2 norm of weights: 6.635340257714066\n",
      "---------------------\n",
      "Iteration Number: 269\n",
      "Loss: 89.2480789885716\n",
      "l2 norm of gradients: 0.88244609290784\n",
      "l2 norm of weights: 6.633391684196954\n",
      "---------------------\n",
      "Iteration Number: 270\n",
      "Loss: 89.19548049075814\n",
      "l2 norm of gradients: 0.8821695715836253\n",
      "l2 norm of weights: 6.631443893871114\n",
      "---------------------\n",
      "Iteration Number: 271\n",
      "Loss: 89.14291620029901\n",
      "l2 norm of gradients: 0.8818930777683408\n",
      "l2 norm of weights: 6.6294968861474715\n",
      "---------------------\n",
      "Iteration Number: 272\n",
      "Loss: 89.09038608899108\n",
      "l2 norm of gradients: 0.8816166115616159\n",
      "l2 norm of weights: 6.627550660437851\n",
      "---------------------\n",
      "Iteration Number: 273\n",
      "Loss: 89.0378901286557\n",
      "l2 norm of gradients: 0.8813401730628984\n",
      "l2 norm of weights: 6.625605216154985\n",
      "---------------------\n",
      "Iteration Number: 274\n",
      "Loss: 88.98542829114407\n",
      "l2 norm of gradients: 0.8810637623714521\n",
      "l2 norm of weights: 6.623660552712517\n",
      "---------------------\n",
      "Iteration Number: 275\n",
      "Loss: 88.93300054833216\n",
      "l2 norm of gradients: 0.8807873795863543\n",
      "l2 norm of weights: 6.621716669525008\n",
      "---------------------\n",
      "Iteration Number: 276\n",
      "Loss: 88.88060687211411\n",
      "l2 norm of gradients: 0.8805110248064939\n",
      "l2 norm of weights: 6.619773566007946\n",
      "---------------------\n",
      "Iteration Number: 277\n",
      "Loss: 88.82824723441962\n",
      "l2 norm of gradients: 0.8802346981305671\n",
      "l2 norm of weights: 6.617831241577745\n",
      "---------------------\n",
      "Iteration Number: 278\n",
      "Loss: 88.7759216071944\n",
      "l2 norm of gradients: 0.8799583996570765\n",
      "l2 norm of weights: 6.6158896956517586\n",
      "---------------------\n",
      "Iteration Number: 279\n",
      "Loss: 88.72362996241617\n",
      "l2 norm of gradients: 0.8796821294843283\n",
      "l2 norm of weights: 6.613948927648278\n",
      "---------------------\n",
      "Iteration Number: 280\n",
      "Loss: 88.67137227209449\n",
      "l2 norm of gradients: 0.8794058877104292\n",
      "l2 norm of weights: 6.6120089369865465\n",
      "---------------------\n",
      "Iteration Number: 281\n",
      "Loss: 88.61914850825264\n",
      "l2 norm of gradients: 0.8791296744332842\n",
      "l2 norm of weights: 6.610069723086755\n",
      "---------------------\n",
      "Iteration Number: 282\n",
      "Loss: 88.56695864295449\n",
      "l2 norm of gradients: 0.8788534897505944\n",
      "l2 norm of weights: 6.608131285370056\n",
      "---------------------\n",
      "Iteration Number: 283\n",
      "Loss: 88.51480264827354\n",
      "l2 norm of gradients: 0.8785773337598538\n",
      "l2 norm of weights: 6.606193623258566\n",
      "---------------------\n",
      "Iteration Number: 284\n",
      "Loss: 88.46268049632475\n",
      "l2 norm of gradients: 0.878301206558348\n",
      "l2 norm of weights: 6.604256736175371\n",
      "---------------------\n",
      "Iteration Number: 285\n",
      "Loss: 88.41059215924578\n",
      "l2 norm of gradients: 0.8780251082431506\n",
      "l2 norm of weights: 6.6023206235445295\n",
      "---------------------\n",
      "Iteration Number: 286\n",
      "Loss: 88.35853760920642\n",
      "l2 norm of gradients: 0.8777490389111211\n",
      "l2 norm of weights: 6.600385284791082\n",
      "---------------------\n",
      "Iteration Number: 287\n",
      "Loss: 88.30651681838344\n",
      "l2 norm of gradients: 0.8774729986589028\n",
      "l2 norm of weights: 6.598450719341057\n",
      "---------------------\n",
      "Iteration Number: 288\n",
      "Loss: 88.25452975901045\n",
      "l2 norm of gradients: 0.8771969875829203\n",
      "l2 norm of weights: 6.59651692662147\n",
      "---------------------\n",
      "Iteration Number: 289\n",
      "Loss: 88.20257640332734\n",
      "l2 norm of gradients: 0.8769210057793767\n",
      "l2 norm of weights: 6.594583906060337\n",
      "---------------------\n",
      "Iteration Number: 290\n",
      "Loss: 88.15065672361439\n",
      "l2 norm of gradients: 0.8766450533442515\n",
      "l2 norm of weights: 6.592651657086671\n",
      "---------------------\n",
      "Iteration Number: 291\n",
      "Loss: 88.09877069217029\n",
      "l2 norm of gradients: 0.8763691303732983\n",
      "l2 norm of weights: 6.590720179130496\n",
      "---------------------\n",
      "Iteration Number: 292\n",
      "Loss: 88.04691828133379\n",
      "l2 norm of gradients: 0.8760932369620426\n",
      "l2 norm of weights: 6.588789471622848\n",
      "---------------------\n",
      "Iteration Number: 293\n",
      "Loss: 87.9950994634537\n",
      "l2 norm of gradients: 0.8758173732057783\n",
      "l2 norm of weights: 6.586859533995775\n",
      "---------------------\n",
      "Iteration Number: 294\n",
      "Loss: 87.94331421092372\n",
      "l2 norm of gradients: 0.8755415391995673\n",
      "l2 norm of weights: 6.584930365682353\n",
      "---------------------\n",
      "Iteration Number: 295\n",
      "Loss: 87.89156249616344\n",
      "l2 norm of gradients: 0.8752657350382355\n",
      "l2 norm of weights: 6.583001966116684\n",
      "---------------------\n",
      "Iteration Number: 296\n",
      "Loss: 87.83984429161288\n",
      "l2 norm of gradients: 0.874989960816371\n",
      "l2 norm of weights: 6.581074334733899\n",
      "---------------------\n",
      "Iteration Number: 297\n",
      "Loss: 87.78815956975541\n",
      "l2 norm of gradients: 0.8747142166283223\n",
      "l2 norm of weights: 6.579147470970169\n",
      "---------------------\n",
      "Iteration Number: 298\n",
      "Loss: 87.73650830308425\n",
      "l2 norm of gradients: 0.8744385025681952\n",
      "l2 norm of weights: 6.577221374262706\n",
      "---------------------\n",
      "Iteration Number: 299\n",
      "Loss: 87.68489046413501\n",
      "l2 norm of gradients: 0.8741628187298514\n",
      "l2 norm of weights: 6.575296044049769\n",
      "---------------------\n",
      "Iteration Number: 300\n",
      "Loss: 87.63330602547897\n",
      "l2 norm of gradients: 0.8738871652069052\n",
      "l2 norm of weights: 6.573371479770669\n",
      "---------------------\n",
      "Iteration Number: 301\n",
      "Loss: 87.58175495970139\n",
      "l2 norm of gradients: 0.8736115420927225\n",
      "l2 norm of weights: 6.571447680865773\n",
      "---------------------\n",
      "Iteration Number: 302\n",
      "Loss: 87.53023723942069\n",
      "l2 norm of gradients: 0.873335949480417\n",
      "l2 norm of weights: 6.569524646776507\n",
      "---------------------\n",
      "Iteration Number: 303\n",
      "Loss: 87.47875283729547\n",
      "l2 norm of gradients: 0.8730603874628503\n",
      "l2 norm of weights: 6.567602376945367\n",
      "---------------------\n",
      "Iteration Number: 304\n",
      "Loss: 87.4273017260069\n",
      "l2 norm of gradients: 0.8727848561326267\n",
      "l2 norm of weights: 6.565680870815914\n",
      "---------------------\n",
      "Iteration Number: 305\n",
      "Loss: 87.37588387826351\n",
      "l2 norm of gradients: 0.8725093555820934\n",
      "l2 norm of weights: 6.563760127832788\n",
      "---------------------\n",
      "Iteration Number: 306\n",
      "Loss: 87.32449926680142\n",
      "l2 norm of gradients: 0.8722338859033375\n",
      "l2 norm of weights: 6.561840147441707\n",
      "---------------------\n",
      "Iteration Number: 307\n",
      "Loss: 87.27314786440476\n",
      "l2 norm of gradients: 0.8719584471881836\n",
      "l2 norm of weights: 6.559920929089472\n",
      "---------------------\n",
      "Iteration Number: 308\n",
      "Loss: 87.22182964386712\n",
      "l2 norm of gradients: 0.8716830395281924\n",
      "l2 norm of weights: 6.558002472223973\n",
      "---------------------\n",
      "Iteration Number: 309\n",
      "Loss: 87.17054457802594\n",
      "l2 norm of gradients: 0.8714076630146573\n",
      "l2 norm of weights: 6.556084776294192\n",
      "---------------------\n",
      "Iteration Number: 310\n",
      "Loss: 87.11929263975108\n",
      "l2 norm of gradients: 0.8711323177386036\n",
      "l2 norm of weights: 6.554167840750207\n",
      "---------------------\n",
      "Iteration Number: 311\n",
      "Loss: 87.06807380192615\n",
      "l2 norm of gradients: 0.8708570037907859\n",
      "l2 norm of weights: 6.5522516650432\n",
      "---------------------\n",
      "Iteration Number: 312\n",
      "Loss: 87.0168880374822\n",
      "l2 norm of gradients: 0.8705817212616858\n",
      "l2 norm of weights: 6.550336248625454\n",
      "---------------------\n",
      "Iteration Number: 313\n",
      "Loss: 86.96573531937635\n",
      "l2 norm of gradients: 0.8703064702415099\n",
      "l2 norm of weights: 6.548421590950366\n",
      "---------------------\n",
      "Iteration Number: 314\n",
      "Loss: 86.91461562060498\n",
      "l2 norm of gradients: 0.8700312508201877\n",
      "l2 norm of weights: 6.546507691472445\n",
      "---------------------\n",
      "Iteration Number: 315\n",
      "Loss: 86.86352891417279\n",
      "l2 norm of gradients: 0.8697560630873707\n",
      "l2 norm of weights: 6.544594549647314\n",
      "---------------------\n",
      "Iteration Number: 316\n",
      "Loss: 86.81247517315059\n",
      "l2 norm of gradients: 0.8694809071324282\n",
      "l2 norm of weights: 6.542682164931724\n",
      "---------------------\n",
      "Iteration Number: 317\n",
      "Loss: 86.76145437060268\n",
      "l2 norm of gradients: 0.869205783044447\n",
      "l2 norm of weights: 6.540770536783548\n",
      "---------------------\n",
      "Iteration Number: 318\n",
      "Loss: 86.71046647965458\n",
      "l2 norm of gradients: 0.8689306909122289\n",
      "l2 norm of weights: 6.538859664661788\n",
      "---------------------\n",
      "Iteration Number: 319\n",
      "Loss: 86.6595114734502\n",
      "l2 norm of gradients: 0.8686556308242884\n",
      "l2 norm of weights: 6.536949548026581\n",
      "---------------------\n",
      "Iteration Number: 320\n",
      "Loss: 86.60858932517095\n",
      "l2 norm of gradients: 0.8683806028688515\n",
      "l2 norm of weights: 6.535040186339202\n",
      "---------------------\n",
      "Iteration Number: 321\n",
      "Loss: 86.55770000802687\n",
      "l2 norm of gradients: 0.868105607133853\n",
      "l2 norm of weights: 6.533131579062067\n",
      "---------------------\n",
      "Iteration Number: 322\n",
      "Loss: 86.50684349526314\n",
      "l2 norm of gradients: 0.8678306437069352\n",
      "l2 norm of weights: 6.531223725658733\n",
      "---------------------\n",
      "Iteration Number: 323\n",
      "Loss: 86.4560197601509\n",
      "l2 norm of gradients: 0.8675557126754448\n",
      "l2 norm of weights: 6.5293166255939115\n",
      "---------------------\n",
      "Iteration Number: 324\n",
      "Loss: 86.40522877600452\n",
      "l2 norm of gradients: 0.8672808141264331\n",
      "l2 norm of weights: 6.527410278333463\n",
      "---------------------\n",
      "Iteration Number: 325\n",
      "Loss: 86.35447051616143\n",
      "l2 norm of gradients: 0.8670059481466517\n",
      "l2 norm of weights: 6.525504683344402\n",
      "---------------------\n",
      "Iteration Number: 326\n",
      "Loss: 86.30374495399488\n",
      "l2 norm of gradients: 0.8667311148225526\n",
      "l2 norm of weights: 6.5235998400949065\n",
      "---------------------\n",
      "Iteration Number: 327\n",
      "Loss: 86.25305206291256\n",
      "l2 norm of gradients: 0.8664563142402852\n",
      "l2 norm of weights: 6.521695748054316\n",
      "---------------------\n",
      "Iteration Number: 328\n",
      "Loss: 86.2023918163553\n",
      "l2 norm of gradients: 0.8661815464856952\n",
      "l2 norm of weights: 6.519792406693134\n",
      "---------------------\n",
      "Iteration Number: 329\n",
      "Loss: 86.15176418779167\n",
      "l2 norm of gradients: 0.8659068116443223\n",
      "l2 norm of weights: 6.517889815483036\n",
      "---------------------\n",
      "Iteration Number: 330\n",
      "Loss: 86.10116915073095\n",
      "l2 norm of gradients: 0.8656321098013976\n",
      "l2 norm of weights: 6.515987973896871\n",
      "---------------------\n",
      "Iteration Number: 331\n",
      "Loss: 86.05060667871356\n",
      "l2 norm of gradients: 0.8653574410418443\n",
      "l2 norm of weights: 6.5140868814086605\n",
      "---------------------\n",
      "Iteration Number: 332\n",
      "Loss: 86.00007674530545\n",
      "l2 norm of gradients: 0.8650828054502735\n",
      "l2 norm of weights: 6.512186537493612\n",
      "---------------------\n",
      "Iteration Number: 333\n",
      "Loss: 85.94957932412017\n",
      "l2 norm of gradients: 0.8648082031109834\n",
      "l2 norm of weights: 6.51028694162811\n",
      "---------------------\n",
      "Iteration Number: 334\n",
      "Loss: 85.89911438878822\n",
      "l2 norm of gradients: 0.8645336341079575\n",
      "l2 norm of weights: 6.508388093289729\n",
      "---------------------\n",
      "Iteration Number: 335\n",
      "Loss: 85.84868191299383\n",
      "l2 norm of gradients: 0.864259098524863\n",
      "l2 norm of weights: 6.506489991957229\n",
      "---------------------\n",
      "Iteration Number: 336\n",
      "Loss: 85.7982818704335\n",
      "l2 norm of gradients: 0.863984596445049\n",
      "l2 norm of weights: 6.504592637110566\n",
      "---------------------\n",
      "Iteration Number: 337\n",
      "Loss: 85.7479142348553\n",
      "l2 norm of gradients: 0.8637101279515448\n",
      "l2 norm of weights: 6.502696028230888\n",
      "---------------------\n",
      "Iteration Number: 338\n",
      "Loss: 85.69757898003151\n",
      "l2 norm of gradients: 0.8634356931270583\n",
      "l2 norm of weights: 6.500800164800543\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 339\n",
      "Loss: 85.64727607976766\n",
      "l2 norm of gradients: 0.8631612920539742\n",
      "l2 norm of weights: 6.49890504630308\n",
      "---------------------\n",
      "Iteration Number: 340\n",
      "Loss: 85.5970055079099\n",
      "l2 norm of gradients: 0.8628869248143525\n",
      "l2 norm of weights: 6.497010672223252\n",
      "---------------------\n",
      "Iteration Number: 341\n",
      "Loss: 85.54676723833701\n",
      "l2 norm of gradients: 0.8626125914899272\n",
      "l2 norm of weights: 6.495117042047016\n",
      "---------------------\n",
      "Iteration Number: 342\n",
      "Loss: 85.49656124495493\n",
      "l2 norm of gradients: 0.8623382921621039\n",
      "l2 norm of weights: 6.493224155261543\n",
      "---------------------\n",
      "Iteration Number: 343\n",
      "Loss: 85.44638750171903\n",
      "l2 norm of gradients: 0.8620640269119592\n",
      "l2 norm of weights: 6.491332011355213\n",
      "---------------------\n",
      "Iteration Number: 344\n",
      "Loss: 85.39624598260058\n",
      "l2 norm of gradients: 0.8617897958202382\n",
      "l2 norm of weights: 6.489440609817623\n",
      "---------------------\n",
      "Iteration Number: 345\n",
      "Loss: 85.34613666161371\n",
      "l2 norm of gradients: 0.861515598967354\n",
      "l2 norm of weights: 6.487549950139586\n",
      "---------------------\n",
      "Iteration Number: 346\n",
      "Loss: 85.29605951281641\n",
      "l2 norm of gradients: 0.8612414364333851\n",
      "l2 norm of weights: 6.485660031813135\n",
      "---------------------\n",
      "Iteration Number: 347\n",
      "Loss: 85.24601451028781\n",
      "l2 norm of gradients: 0.8609673082980749\n",
      "l2 norm of weights: 6.483770854331529\n",
      "---------------------\n",
      "Iteration Number: 348\n",
      "Loss: 85.19600162815011\n",
      "l2 norm of gradients: 0.860693214640829\n",
      "l2 norm of weights: 6.481882417189249\n",
      "---------------------\n",
      "Iteration Number: 349\n",
      "Loss: 85.14602084055426\n",
      "l2 norm of gradients: 0.8604191555407155\n",
      "l2 norm of weights: 6.479994719882002\n",
      "---------------------\n",
      "Iteration Number: 350\n",
      "Loss: 85.09607212169492\n",
      "l2 norm of gradients: 0.8601451310764614\n",
      "l2 norm of weights: 6.478107761906729\n",
      "---------------------\n",
      "Iteration Number: 351\n",
      "Loss: 85.04615544579022\n",
      "l2 norm of gradients: 0.8598711413264531\n",
      "l2 norm of weights: 6.476221542761603\n",
      "---------------------\n",
      "Iteration Number: 352\n",
      "Loss: 84.99627078710145\n",
      "l2 norm of gradients: 0.8595971863687341\n",
      "l2 norm of weights: 6.474336061946027\n",
      "---------------------\n",
      "Iteration Number: 353\n",
      "Loss: 84.94641811992666\n",
      "l2 norm of gradients: 0.8593232662810034\n",
      "l2 norm of weights: 6.472451318960648\n",
      "---------------------\n",
      "Iteration Number: 354\n",
      "Loss: 84.89659741859244\n",
      "l2 norm of gradients: 0.8590493811406146\n",
      "l2 norm of weights: 6.4705673133073445\n",
      "---------------------\n",
      "Iteration Number: 355\n",
      "Loss: 84.84680865746851\n",
      "l2 norm of gradients: 0.8587755310245744\n",
      "l2 norm of weights: 6.468684044489243\n",
      "---------------------\n",
      "Iteration Number: 356\n",
      "Loss: 84.79705181095265\n",
      "l2 norm of gradients: 0.8585017160095407\n",
      "l2 norm of weights: 6.46680151201071\n",
      "---------------------\n",
      "Iteration Number: 357\n",
      "Loss: 84.74732685348332\n",
      "l2 norm of gradients: 0.858227936171823\n",
      "l2 norm of weights: 6.464919715377355\n",
      "---------------------\n",
      "Iteration Number: 358\n",
      "Loss: 84.697633759532\n",
      "l2 norm of gradients: 0.8579541915873788\n",
      "l2 norm of weights: 6.46303865409604\n",
      "---------------------\n",
      "Iteration Number: 359\n",
      "Loss: 84.64797250360479\n",
      "l2 norm of gradients: 0.8576804823318138\n",
      "l2 norm of weights: 6.461158327674871\n",
      "---------------------\n",
      "Iteration Number: 360\n",
      "Loss: 84.59834306025223\n",
      "l2 norm of gradients: 0.8574068084803806\n",
      "l2 norm of weights: 6.459278735623212\n",
      "---------------------\n",
      "Iteration Number: 361\n",
      "Loss: 84.54874540404569\n",
      "l2 norm of gradients: 0.8571331701079763\n",
      "l2 norm of weights: 6.457399877451674\n",
      "---------------------\n",
      "Iteration Number: 362\n",
      "Loss: 84.49917950959866\n",
      "l2 norm of gradients: 0.8568595672891433\n",
      "l2 norm of weights: 6.455521752672125\n",
      "---------------------\n",
      "Iteration Number: 363\n",
      "Loss: 84.44964535157322\n",
      "l2 norm of gradients: 0.8565860000980658\n",
      "l2 norm of weights: 6.453644360797689\n",
      "---------------------\n",
      "Iteration Number: 364\n",
      "Loss: 84.40014290465518\n",
      "l2 norm of gradients: 0.85631246860857\n",
      "l2 norm of weights: 6.4517677013427495\n",
      "---------------------\n",
      "Iteration Number: 365\n",
      "Loss: 84.35067214355686\n",
      "l2 norm of gradients: 0.8560389728941235\n",
      "l2 norm of weights: 6.449891773822951\n",
      "---------------------\n",
      "Iteration Number: 366\n",
      "Loss: 84.30123304305394\n",
      "l2 norm of gradients: 0.8557655130278319\n",
      "l2 norm of weights: 6.4480165777551965\n",
      "---------------------\n",
      "Iteration Number: 367\n",
      "Loss: 84.25182557793053\n",
      "l2 norm of gradients: 0.8554920890824399\n",
      "l2 norm of weights: 6.446142112657654\n",
      "---------------------\n",
      "Iteration Number: 368\n",
      "Loss: 84.20244972301751\n",
      "l2 norm of gradients: 0.8552187011303295\n",
      "l2 norm of weights: 6.444268378049757\n",
      "---------------------\n",
      "Iteration Number: 369\n",
      "Loss: 84.15310545318992\n",
      "l2 norm of gradients: 0.8549453492435178\n",
      "l2 norm of weights: 6.442395373452203\n",
      "---------------------\n",
      "Iteration Number: 370\n",
      "Loss: 84.10379274335196\n",
      "l2 norm of gradients: 0.8546720334936584\n",
      "l2 norm of weights: 6.440523098386961\n",
      "---------------------\n",
      "Iteration Number: 371\n",
      "Loss: 84.05451156844381\n",
      "l2 norm of gradients: 0.8543987539520372\n",
      "l2 norm of weights: 6.438651552377264\n",
      "---------------------\n",
      "Iteration Number: 372\n",
      "Loss: 84.00526190344024\n",
      "l2 norm of gradients: 0.8541255106895742\n",
      "l2 norm of weights: 6.43678073494762\n",
      "---------------------\n",
      "Iteration Number: 373\n",
      "Loss: 83.95604372336217\n",
      "l2 norm of gradients: 0.8538523037768206\n",
      "l2 norm of weights: 6.4349106456238045\n",
      "---------------------\n",
      "Iteration Number: 374\n",
      "Loss: 83.90685700325204\n",
      "l2 norm of gradients: 0.8535791332839587\n",
      "l2 norm of weights: 6.4330412839328694\n",
      "---------------------\n",
      "Iteration Number: 375\n",
      "Loss: 83.85770171820394\n",
      "l2 norm of gradients: 0.8533059992808009\n",
      "l2 norm of weights: 6.431172649403139\n",
      "---------------------\n",
      "Iteration Number: 376\n",
      "Loss: 83.8085778433385\n",
      "l2 norm of gradients: 0.8530329018367878\n",
      "l2 norm of weights: 6.429304741564214\n",
      "---------------------\n",
      "Iteration Number: 377\n",
      "Loss: 83.75948535381794\n",
      "l2 norm of gradients: 0.8527598410209891\n",
      "l2 norm of weights: 6.427437559946969\n",
      "---------------------\n",
      "Iteration Number: 378\n",
      "Loss: 83.71042422484008\n",
      "l2 norm of gradients: 0.8524868169021006\n",
      "l2 norm of weights: 6.425571104083558\n",
      "---------------------\n",
      "Iteration Number: 379\n",
      "Loss: 83.66139443163662\n",
      "l2 norm of gradients: 0.8522138295484444\n",
      "l2 norm of weights: 6.423705373507414\n",
      "---------------------\n",
      "Iteration Number: 380\n",
      "Loss: 83.61239594948225\n",
      "l2 norm of gradients: 0.8519408790279686\n",
      "l2 norm of weights: 6.421840367753249\n",
      "---------------------\n",
      "Iteration Number: 381\n",
      "Loss: 83.56342875368662\n",
      "l2 norm of gradients: 0.8516679654082446\n",
      "l2 norm of weights: 6.419976086357052\n",
      "---------------------\n",
      "Iteration Number: 382\n",
      "Loss: 83.51449281958786\n",
      "l2 norm of gradients: 0.8513950887564685\n",
      "l2 norm of weights: 6.418112528856097\n",
      "---------------------\n",
      "Iteration Number: 383\n",
      "Loss: 83.4655881225755\n",
      "l2 norm of gradients: 0.8511222491394583\n",
      "l2 norm of weights: 6.41624969478894\n",
      "---------------------\n",
      "Iteration Number: 384\n",
      "Loss: 83.41671463806304\n",
      "l2 norm of gradients: 0.8508494466236541\n",
      "l2 norm of weights: 6.414387583695416\n",
      "---------------------\n",
      "Iteration Number: 385\n",
      "Loss: 83.36787234150752\n",
      "l2 norm of gradients: 0.8505766812751177\n",
      "l2 norm of weights: 6.412526195116649\n",
      "---------------------\n",
      "Iteration Number: 386\n",
      "Loss: 83.3190612084082\n",
      "l2 norm of gradients: 0.8503039531595302\n",
      "l2 norm of weights: 6.410665528595041\n",
      "---------------------\n",
      "Iteration Number: 387\n",
      "Loss: 83.27028121428624\n",
      "l2 norm of gradients: 0.8500312623421932\n",
      "l2 norm of weights: 6.408805583674284\n",
      "---------------------\n",
      "Iteration Number: 388\n",
      "Loss: 83.22153233471738\n",
      "l2 norm of gradients: 0.8497586088880268\n",
      "l2 norm of weights: 6.4069463598993535\n",
      "---------------------\n",
      "Iteration Number: 389\n",
      "Loss: 83.17281454529996\n",
      "l2 norm of gradients: 0.8494859928615692\n",
      "l2 norm of weights: 6.405087856816509\n",
      "---------------------\n",
      "Iteration Number: 390\n",
      "Loss: 83.12412782167742\n",
      "l2 norm of gradients: 0.8492134143269764\n",
      "l2 norm of weights: 6.403230073973299\n",
      "---------------------\n",
      "Iteration Number: 391\n",
      "Loss: 83.07547213953109\n",
      "l2 norm of gradients: 0.848940873348021\n",
      "l2 norm of weights: 6.40137301091856\n",
      "---------------------\n",
      "Iteration Number: 392\n",
      "Loss: 83.02684747457302\n",
      "l2 norm of gradients: 0.8486683699880914\n",
      "l2 norm of weights: 6.399516667202411\n",
      "---------------------\n",
      "Iteration Number: 393\n",
      "Loss: 82.97825380256222\n",
      "l2 norm of gradients: 0.8483959043101919\n",
      "l2 norm of weights: 6.397661042376263\n",
      "---------------------\n",
      "Iteration Number: 394\n",
      "Loss: 82.92969109928843\n",
      "l2 norm of gradients: 0.8481234763769416\n",
      "l2 norm of weights: 6.395806135992813\n",
      "---------------------\n",
      "Iteration Number: 395\n",
      "Loss: 82.88115934057629\n",
      "l2 norm of gradients: 0.8478510862505737\n",
      "l2 norm of weights: 6.393951947606047\n",
      "---------------------\n",
      "Iteration Number: 396\n",
      "Loss: 82.83265850229462\n",
      "l2 norm of gradients: 0.8475787339929354\n",
      "l2 norm of weights: 6.39209847677124\n",
      "---------------------\n",
      "Iteration Number: 397\n",
      "Loss: 82.78418856034885\n",
      "l2 norm of gradients: 0.8473064196654865\n",
      "l2 norm of weights: 6.390245723044955\n",
      "---------------------\n",
      "Iteration Number: 398\n",
      "Loss: 82.73574949066912\n",
      "l2 norm of gradients: 0.8470341433292997\n",
      "l2 norm of weights: 6.388393685985042\n",
      "---------------------\n",
      "Iteration Number: 399\n",
      "Loss: 82.68734126924639\n",
      "l2 norm of gradients: 0.84676190504506\n",
      "l2 norm of weights: 6.386542365150643\n",
      "---------------------\n",
      "Iteration Number: 400\n",
      "Loss: 82.63896387208676\n",
      "l2 norm of gradients: 0.8464897048730633\n",
      "l2 norm of weights: 6.384691760102189\n",
      "---------------------\n",
      "Iteration Number: 401\n",
      "Loss: 82.59061727524966\n",
      "l2 norm of gradients: 0.8462175428732172\n",
      "l2 norm of weights: 6.382841870401397\n",
      "---------------------\n",
      "Iteration Number: 402\n",
      "Loss: 82.54230145482002\n",
      "l2 norm of gradients: 0.8459454191050391\n",
      "l2 norm of weights: 6.380992695611275\n",
      "---------------------\n",
      "Iteration Number: 403\n",
      "Loss: 82.49401638692677\n",
      "l2 norm of gradients: 0.8456733336276574\n",
      "l2 norm of weights: 6.3791442352961205\n",
      "---------------------\n",
      "Iteration Number: 404\n",
      "Loss: 82.44576204773864\n",
      "l2 norm of gradients: 0.8454012864998097\n",
      "l2 norm of weights: 6.377296489021517\n",
      "---------------------\n",
      "Iteration Number: 405\n",
      "Loss: 82.3975384134553\n",
      "l2 norm of gradients: 0.8451292777798428\n",
      "l2 norm of weights: 6.375449456354341\n",
      "---------------------\n",
      "Iteration Number: 406\n",
      "Loss: 82.34934546031404\n",
      "l2 norm of gradients: 0.8448573075257125\n",
      "l2 norm of weights: 6.3736031368627515\n",
      "---------------------\n",
      "Iteration Number: 407\n",
      "Loss: 82.3011831646005\n",
      "l2 norm of gradients: 0.8445853757949835\n",
      "l2 norm of weights: 6.371757530116202\n",
      "---------------------\n",
      "Iteration Number: 408\n",
      "Loss: 82.2530515026241\n",
      "l2 norm of gradients: 0.8443134826448276\n",
      "l2 norm of weights: 6.36991263568543\n",
      "---------------------\n",
      "Iteration Number: 409\n",
      "Loss: 82.20495045074\n",
      "l2 norm of gradients: 0.8440416281320257\n",
      "l2 norm of weights: 6.3680684531424605\n",
      "---------------------\n",
      "Iteration Number: 410\n",
      "Loss: 82.15687998533821\n",
      "l2 norm of gradients: 0.8437698123129653\n",
      "l2 norm of weights: 6.366224982060607\n",
      "---------------------\n",
      "Iteration Number: 411\n",
      "Loss: 82.10884008284768\n",
      "l2 norm of gradients: 0.8434980352436413\n",
      "l2 norm of weights: 6.364382222014468\n",
      "---------------------\n",
      "Iteration Number: 412\n",
      "Loss: 82.06083071973221\n",
      "l2 norm of gradients: 0.8432262969796553\n",
      "l2 norm of weights: 6.362540172579932\n",
      "---------------------\n",
      "Iteration Number: 413\n",
      "Loss: 82.01285187249698\n",
      "l2 norm of gradients: 0.8429545975762162\n",
      "l2 norm of weights: 6.360698833334168\n",
      "---------------------\n",
      "Iteration Number: 414\n",
      "Loss: 81.96490351768134\n",
      "l2 norm of gradients: 0.8426829370881384\n",
      "l2 norm of weights: 6.358858203855635\n",
      "---------------------\n",
      "Iteration Number: 415\n",
      "Loss: 81.91698563186411\n",
      "l2 norm of gradients: 0.8424113155698428\n",
      "l2 norm of weights: 6.357018283724072\n",
      "---------------------\n",
      "Iteration Number: 416\n",
      "Loss: 81.86909819166091\n",
      "l2 norm of gradients: 0.8421397330753563\n",
      "l2 norm of weights: 6.355179072520506\n",
      "---------------------\n",
      "Iteration Number: 417\n",
      "Loss: 81.8212411737262\n",
      "l2 norm of gradients: 0.8418681896583114\n",
      "l2 norm of weights: 6.353340569827245\n",
      "---------------------\n",
      "Iteration Number: 418\n",
      "Loss: 81.77341455475117\n",
      "l2 norm of gradients: 0.8415966853719465\n",
      "l2 norm of weights: 6.351502775227881\n",
      "---------------------\n",
      "Iteration Number: 419\n",
      "Loss: 81.72561831146528\n",
      "l2 norm of gradients: 0.8413252202691047\n",
      "l2 norm of weights: 6.349665688307287\n",
      "---------------------\n",
      "Iteration Number: 420\n",
      "Loss: 81.67785242063275\n",
      "l2 norm of gradients: 0.841053794402235\n",
      "l2 norm of weights: 6.347829308651615\n",
      "---------------------\n",
      "Iteration Number: 421\n",
      "Loss: 81.63011685906122\n",
      "l2 norm of gradients: 0.8407824078233915\n",
      "l2 norm of weights: 6.345993635848302\n",
      "---------------------\n",
      "Iteration Number: 422\n",
      "Loss: 81.582411603586\n",
      "l2 norm of gradients: 0.8405110605842324\n",
      "l2 norm of weights: 6.344158669486061\n",
      "---------------------\n",
      "Iteration Number: 423\n",
      "Loss: 81.53473663109182\n",
      "l2 norm of gradients: 0.840239752736022\n",
      "l2 norm of weights: 6.342324409154883\n",
      "---------------------\n",
      "Iteration Number: 424\n",
      "Loss: 81.48709191848953\n",
      "l2 norm of gradients: 0.8399684843296289\n",
      "l2 norm of weights: 6.340490854446037\n",
      "---------------------\n",
      "Iteration Number: 425\n",
      "Loss: 81.43947744273488\n",
      "l2 norm of gradients: 0.8396972554155264\n",
      "l2 norm of weights: 6.33865800495207\n",
      "---------------------\n",
      "Iteration Number: 426\n",
      "Loss: 81.39189318082335\n",
      "l2 norm of gradients: 0.8394260660437929\n",
      "l2 norm of weights: 6.336825860266805\n",
      "---------------------\n",
      "Iteration Number: 427\n",
      "Loss: 81.34433910978441\n",
      "l2 norm of gradients: 0.8391549162641114\n",
      "l2 norm of weights: 6.334994419985332\n",
      "---------------------\n",
      "Iteration Number: 428\n",
      "Loss: 81.29681520667722\n",
      "l2 norm of gradients: 0.8388838061257696\n",
      "l2 norm of weights: 6.333163683704025\n",
      "---------------------\n",
      "Iteration Number: 429\n",
      "Loss: 81.24932144861216\n",
      "l2 norm of gradients: 0.8386127356776601\n",
      "l2 norm of weights: 6.331333651020522\n",
      "---------------------\n",
      "Iteration Number: 430\n",
      "Loss: 81.20185781272806\n",
      "l2 norm of gradients: 0.8383417049682801\n",
      "l2 norm of weights: 6.3295043215337365\n",
      "---------------------\n",
      "Iteration Number: 431\n",
      "Loss: 81.15442427620636\n",
      "l2 norm of gradients: 0.8380707140457316\n",
      "l2 norm of weights: 6.3276756948438475\n",
      "---------------------\n",
      "Iteration Number: 432\n",
      "Loss: 81.1070208162576\n",
      "l2 norm of gradients: 0.8377997629577222\n",
      "l2 norm of weights: 6.325847770552306\n",
      "---------------------\n",
      "Iteration Number: 433\n",
      "Loss: 81.05964741014073\n",
      "l2 norm of gradients: 0.8375288517515634\n",
      "l2 norm of weights: 6.324020548261829\n",
      "---------------------\n",
      "Iteration Number: 434\n",
      "Loss: 81.01230403514643\n",
      "l2 norm of gradients: 0.8372579804741727\n",
      "l2 norm of weights: 6.322194027576397\n",
      "---------------------\n",
      "Iteration Number: 435\n",
      "Loss: 80.96499066860125\n",
      "l2 norm of gradients: 0.8369871491720723\n",
      "l2 norm of weights: 6.3203682081012555\n",
      "---------------------\n",
      "Iteration Number: 436\n",
      "Loss: 80.91770728787095\n",
      "l2 norm of gradients: 0.8367163578913902\n",
      "l2 norm of weights: 6.318543089442917\n",
      "---------------------\n",
      "Iteration Number: 437\n",
      "Loss: 80.87045387036196\n",
      "l2 norm of gradients: 0.8364456066778596\n",
      "l2 norm of weights: 6.31671867120915\n",
      "---------------------\n",
      "Iteration Number: 438\n",
      "Loss: 80.82323039351003\n",
      "l2 norm of gradients: 0.8361748955768196\n",
      "l2 norm of weights: 6.314894953008986\n",
      "---------------------\n",
      "Iteration Number: 439\n",
      "Loss: 80.7760368347956\n",
      "l2 norm of gradients: 0.8359042246332147\n",
      "l2 norm of weights: 6.313071934452713\n",
      "---------------------\n",
      "Iteration Number: 440\n",
      "Loss: 80.7288731717338\n",
      "l2 norm of gradients: 0.8356335938915961\n",
      "l2 norm of weights: 6.311249615151878\n",
      "---------------------\n",
      "Iteration Number: 441\n",
      "Loss: 80.68173938187827\n",
      "l2 norm of gradients: 0.8353630033961209\n",
      "l2 norm of weights: 6.309427994719281\n",
      "---------------------\n",
      "Iteration Number: 442\n",
      "Loss: 80.63463544281399\n",
      "l2 norm of gradients: 0.8350924531905531\n",
      "l2 norm of weights: 6.307607072768975\n",
      "---------------------\n",
      "Iteration Number: 443\n",
      "Loss: 80.58756133217094\n",
      "l2 norm of gradients: 0.8348219433182629\n",
      "l2 norm of weights: 6.305786848916267\n",
      "---------------------\n",
      "Iteration Number: 444\n",
      "Loss: 80.54051702761178\n",
      "l2 norm of gradients: 0.834551473822228\n",
      "l2 norm of weights: 6.303967322777712\n",
      "---------------------\n",
      "Iteration Number: 445\n",
      "Loss: 80.49350250683858\n",
      "l2 norm of gradients: 0.8342810447450332\n",
      "l2 norm of weights: 6.302148493971117\n",
      "---------------------\n",
      "Iteration Number: 446\n",
      "Loss: 80.44651774758557\n",
      "l2 norm of gradients: 0.834010656128871\n",
      "l2 norm of weights: 6.300330362115531\n",
      "---------------------\n",
      "Iteration Number: 447\n",
      "Loss: 80.39956272763176\n",
      "l2 norm of gradients: 0.8337403080155423\n",
      "l2 norm of weights: 6.298512926831249\n",
      "---------------------\n",
      "Iteration Number: 448\n",
      "Loss: 80.35263742478652\n",
      "l2 norm of gradients: 0.8334700004464556\n",
      "l2 norm of weights: 6.29669618773981\n",
      "---------------------\n",
      "Iteration Number: 449\n",
      "Loss: 80.30574181689785\n",
      "l2 norm of gradients: 0.8331997334626281\n",
      "l2 norm of weights: 6.294880144463994\n",
      "---------------------\n",
      "Iteration Number: 450\n",
      "Loss: 80.25887588185539\n",
      "l2 norm of gradients: 0.8329295071046865\n",
      "l2 norm of weights: 6.293064796627819\n",
      "---------------------\n",
      "Iteration Number: 451\n",
      "Loss: 80.21203959758022\n",
      "l2 norm of gradients: 0.8326593214128665\n",
      "l2 norm of weights: 6.291250143856538\n",
      "---------------------\n",
      "Iteration Number: 452\n",
      "Loss: 80.16523294202925\n",
      "l2 norm of gradients: 0.8323891764270137\n",
      "l2 norm of weights: 6.289436185776645\n",
      "---------------------\n",
      "Iteration Number: 453\n",
      "Loss: 80.1184558932029\n",
      "l2 norm of gradients: 0.8321190721865837\n",
      "l2 norm of weights: 6.287622922015858\n",
      "---------------------\n",
      "Iteration Number: 454\n",
      "Loss: 80.07170842913243\n",
      "l2 norm of gradients: 0.831849008730643\n",
      "l2 norm of weights: 6.285810352203136\n",
      "---------------------\n",
      "Iteration Number: 455\n",
      "Loss: 80.02499052788752\n",
      "l2 norm of gradients: 0.8315789860978687\n",
      "l2 norm of weights: 6.283998475968659\n",
      "---------------------\n",
      "Iteration Number: 456\n",
      "Loss: 79.97830216757423\n",
      "l2 norm of gradients: 0.8313090043265503\n",
      "l2 norm of weights: 6.282187292943836\n",
      "---------------------\n",
      "Iteration Number: 457\n",
      "Loss: 79.9316433263351\n",
      "l2 norm of gradients: 0.8310390634545882\n",
      "l2 norm of weights: 6.280376802761302\n",
      "---------------------\n",
      "Iteration Number: 458\n",
      "Loss: 79.88501398235394\n",
      "l2 norm of gradients: 0.8307691635194961\n",
      "l2 norm of weights: 6.278567005054912\n",
      "---------------------\n",
      "Iteration Number: 459\n",
      "Loss: 79.83841411384631\n",
      "l2 norm of gradients: 0.8304993045583999\n",
      "l2 norm of weights: 6.276757899459742\n",
      "---------------------\n",
      "Iteration Number: 460\n",
      "Loss: 79.79184369905954\n",
      "l2 norm of gradients: 0.8302294866080401\n",
      "l2 norm of weights: 6.274949485612086\n",
      "---------------------\n",
      "Iteration Number: 461\n",
      "Loss: 79.74530271629071\n",
      "l2 norm of gradients: 0.8299597097047698\n",
      "l2 norm of weights: 6.273141763149452\n",
      "---------------------\n",
      "Iteration Number: 462\n",
      "Loss: 79.69879114386465\n",
      "l2 norm of gradients: 0.8296899738845581\n",
      "l2 norm of weights: 6.271334731710563\n",
      "---------------------\n",
      "Iteration Number: 463\n",
      "Loss: 79.65230896013685\n",
      "l2 norm of gradients: 0.8294202791829882\n",
      "l2 norm of weights: 6.269528390935349\n",
      "---------------------\n",
      "Iteration Number: 464\n",
      "Loss: 79.60585614351385\n",
      "l2 norm of gradients: 0.8291506256352595\n",
      "l2 norm of weights: 6.267722740464954\n",
      "---------------------\n",
      "Iteration Number: 465\n",
      "Loss: 79.55943267242188\n",
      "l2 norm of gradients: 0.8288810132761876\n",
      "l2 norm of weights: 6.265917779941724\n",
      "---------------------\n",
      "Iteration Number: 466\n",
      "Loss: 79.51303852534348\n",
      "l2 norm of gradients: 0.8286114421402047\n",
      "l2 norm of weights: 6.264113509009207\n",
      "---------------------\n",
      "Iteration Number: 467\n",
      "Loss: 79.46667368077631\n",
      "l2 norm of gradients: 0.8283419122613612\n",
      "l2 norm of weights: 6.262309927312154\n",
      "---------------------\n",
      "Iteration Number: 468\n",
      "Loss: 79.42033811726985\n",
      "l2 norm of gradients: 0.8280724236733251\n",
      "l2 norm of weights: 6.260507034496516\n",
      "---------------------\n",
      "Iteration Number: 469\n",
      "Loss: 79.37403181340099\n",
      "l2 norm of gradients: 0.827802976409383\n",
      "l2 norm of weights: 6.258704830209436\n",
      "---------------------\n",
      "Iteration Number: 470\n",
      "Loss: 79.32775474778684\n",
      "l2 norm of gradients: 0.8275335705024421\n",
      "l2 norm of weights: 6.256903314099252\n",
      "---------------------\n",
      "Iteration Number: 471\n",
      "Loss: 79.28150689907751\n",
      "l2 norm of gradients: 0.8272642059850284\n",
      "l2 norm of weights: 6.255102485815493\n",
      "---------------------\n",
      "Iteration Number: 472\n",
      "Loss: 79.23528824595641\n",
      "l2 norm of gradients: 0.8269948828892895\n",
      "l2 norm of weights: 6.253302345008874\n",
      "---------------------\n",
      "Iteration Number: 473\n",
      "Loss: 79.18909876715665\n",
      "l2 norm of gradients: 0.8267256012469942\n",
      "l2 norm of weights: 6.251502891331297\n",
      "---------------------\n",
      "Iteration Number: 474\n",
      "Loss: 79.14293844142915\n",
      "l2 norm of gradients: 0.8264563610895338\n",
      "l2 norm of weights: 6.249704124435846\n",
      "---------------------\n",
      "Iteration Number: 475\n",
      "Loss: 79.09680724757601\n",
      "l2 norm of gradients: 0.8261871624479223\n",
      "l2 norm of weights: 6.247906043976784\n",
      "---------------------\n",
      "Iteration Number: 476\n",
      "Loss: 79.05070516442083\n",
      "l2 norm of gradients: 0.825918005352798\n",
      "l2 norm of weights: 6.2461086496095515\n",
      "---------------------\n",
      "Iteration Number: 477\n",
      "Loss: 79.00463217083292\n",
      "l2 norm of gradients: 0.8256488898344231\n",
      "l2 norm of weights: 6.244311940990763\n",
      "---------------------\n",
      "Iteration Number: 478\n",
      "Loss: 78.95858824571592\n",
      "l2 norm of gradients: 0.8253798159226848\n",
      "l2 norm of weights: 6.242515917778205\n",
      "---------------------\n",
      "Iteration Number: 479\n",
      "Loss: 78.91257336800369\n",
      "l2 norm of gradients: 0.8251107836470973\n",
      "l2 norm of weights: 6.24072057963083\n",
      "---------------------\n",
      "Iteration Number: 480\n",
      "Loss: 78.86658751667196\n",
      "l2 norm of gradients: 0.8248417930368012\n",
      "l2 norm of weights: 6.238925926208759\n",
      "---------------------\n",
      "Iteration Number: 481\n",
      "Loss: 78.82063067072818\n",
      "l2 norm of gradients: 0.824572844120564\n",
      "l2 norm of weights: 6.237131957173274\n",
      "---------------------\n",
      "Iteration Number: 482\n",
      "Loss: 78.774702809211\n",
      "l2 norm of gradients: 0.8243039369267827\n",
      "l2 norm of weights: 6.235338672186817\n",
      "---------------------\n",
      "Iteration Number: 483\n",
      "Loss: 78.72880391120854\n",
      "l2 norm of gradients: 0.8240350714834832\n",
      "l2 norm of weights: 6.233546070912987\n",
      "---------------------\n",
      "Iteration Number: 484\n",
      "Loss: 78.68293395582609\n",
      "l2 norm of gradients: 0.8237662478183214\n",
      "l2 norm of weights: 6.231754153016536\n",
      "---------------------\n",
      "Iteration Number: 485\n",
      "Loss: 78.63709292221887\n",
      "l2 norm of gradients: 0.8234974659585844\n",
      "l2 norm of weights: 6.229962918163366\n",
      "---------------------\n",
      "Iteration Number: 486\n",
      "Loss: 78.59128078956962\n",
      "l2 norm of gradients: 0.823228725931191\n",
      "l2 norm of weights: 6.22817236602053\n",
      "---------------------\n",
      "Iteration Number: 487\n",
      "Loss: 78.54549753709466\n",
      "l2 norm of gradients: 0.822960027762693\n",
      "l2 norm of weights: 6.226382496256221\n",
      "---------------------\n",
      "Iteration Number: 488\n",
      "Loss: 78.49974314405137\n",
      "l2 norm of gradients: 0.8226913714792754\n",
      "l2 norm of weights: 6.224593308539778\n",
      "---------------------\n",
      "Iteration Number: 489\n",
      "Loss: 78.45401758972831\n",
      "l2 norm of gradients: 0.8224227571067584\n",
      "l2 norm of weights: 6.222804802541675\n",
      "---------------------\n",
      "Iteration Number: 490\n",
      "Loss: 78.40832085344354\n",
      "l2 norm of gradients: 0.8221541846705973\n",
      "l2 norm of weights: 6.2210169779335205\n",
      "---------------------\n",
      "Iteration Number: 491\n",
      "Loss: 78.36265291456395\n",
      "l2 norm of gradients: 0.8218856541958836\n",
      "l2 norm of weights: 6.2192298343880585\n",
      "---------------------\n",
      "Iteration Number: 492\n",
      "Loss: 78.3170137524788\n",
      "l2 norm of gradients: 0.8216171657073467\n",
      "l2 norm of weights: 6.21744337157916\n",
      "---------------------\n",
      "Iteration Number: 493\n",
      "Loss: 78.2714033466172\n",
      "l2 norm of gradients: 0.8213487192293537\n",
      "l2 norm of weights: 6.215657589181819\n",
      "---------------------\n",
      "Iteration Number: 494\n",
      "Loss: 78.22582167643685\n",
      "l2 norm of gradients: 0.8210803147859116\n",
      "l2 norm of weights: 6.213872486872158\n",
      "---------------------\n",
      "Iteration Number: 495\n",
      "Loss: 78.18026872144186\n",
      "l2 norm of gradients: 0.8208119524006673\n",
      "l2 norm of weights: 6.212088064327411\n",
      "---------------------\n",
      "Iteration Number: 496\n",
      "Loss: 78.13474446115644\n",
      "l2 norm of gradients: 0.8205436320969087\n",
      "l2 norm of weights: 6.210304321225933\n",
      "---------------------\n",
      "Iteration Number: 497\n",
      "Loss: 78.08924887514988\n",
      "l2 norm of gradients: 0.8202753538975665\n",
      "l2 norm of weights: 6.208521257247189\n",
      "---------------------\n",
      "Iteration Number: 498\n",
      "Loss: 78.0437819430199\n",
      "l2 norm of gradients: 0.8200071178252143\n",
      "l2 norm of weights: 6.206738872071753\n",
      "---------------------\n",
      "Iteration Number: 499\n",
      "Loss: 77.99834364440036\n",
      "l2 norm of gradients: 0.8197389239020701\n",
      "l2 norm of weights: 6.204957165381308\n",
      "---------------------\n",
      "Iteration Number: 500\n",
      "Loss: 77.95293395896137\n",
      "l2 norm of gradients: 0.8194707721499972\n",
      "l2 norm of weights: 6.203176136858635\n",
      "---------------------\n",
      "Iteration Number: 501\n",
      "Loss: 77.90755286640581\n",
      "l2 norm of gradients: 0.8192026625905051\n",
      "l2 norm of weights: 6.2013957861876134\n",
      "---------------------\n",
      "Iteration Number: 502\n",
      "Loss: 77.8622003464582\n",
      "l2 norm of gradients: 0.818934595244751\n",
      "l2 norm of weights: 6.199616113053223\n",
      "---------------------\n",
      "Iteration Number: 503\n",
      "Loss: 77.81687637890371\n",
      "l2 norm of gradients: 0.8186665701335404\n",
      "l2 norm of weights: 6.197837117141528\n",
      "---------------------\n",
      "Iteration Number: 504\n",
      "Loss: 77.771580943534\n",
      "l2 norm of gradients: 0.818398587277328\n",
      "l2 norm of weights: 6.196058798139689\n",
      "---------------------\n",
      "Iteration Number: 505\n",
      "Loss: 77.7263140201909\n",
      "l2 norm of gradients: 0.81813064669622\n",
      "l2 norm of weights: 6.194281155735946\n",
      "---------------------\n",
      "Iteration Number: 506\n",
      "Loss: 77.68107558874316\n",
      "l2 norm of gradients: 0.8178627484099736\n",
      "l2 norm of weights: 6.192504189619624\n",
      "---------------------\n",
      "Iteration Number: 507\n",
      "Loss: 77.63586562909836\n",
      "l2 norm of gradients: 0.817594892437999\n",
      "l2 norm of weights: 6.190727899481121\n",
      "---------------------\n",
      "Iteration Number: 508\n",
      "Loss: 77.59068412119096\n",
      "l2 norm of gradients: 0.8173270787993607\n",
      "l2 norm of weights: 6.188952285011913\n",
      "---------------------\n",
      "Iteration Number: 509\n",
      "Loss: 77.54553104499159\n",
      "l2 norm of gradients: 0.8170593075127778\n",
      "l2 norm of weights: 6.187177345904547\n",
      "---------------------\n",
      "Iteration Number: 510\n",
      "Loss: 77.50040638050153\n",
      "l2 norm of gradients: 0.8167915785966258\n",
      "l2 norm of weights: 6.185403081852633\n",
      "---------------------\n",
      "Iteration Number: 511\n",
      "Loss: 77.45531010776543\n",
      "l2 norm of gradients: 0.8165238920689375\n",
      "l2 norm of weights: 6.183629492550849\n",
      "---------------------\n",
      "Iteration Number: 512\n",
      "Loss: 77.4102422068487\n",
      "l2 norm of gradients: 0.8162562479474047\n",
      "l2 norm of weights: 6.181856577694931\n",
      "---------------------\n",
      "Iteration Number: 513\n",
      "Loss: 77.3652026578553\n",
      "l2 norm of gradients: 0.8159886462493782\n",
      "l2 norm of weights: 6.180084336981667\n",
      "---------------------\n",
      "Iteration Number: 514\n",
      "Loss: 77.32019144092135\n",
      "l2 norm of gradients: 0.81572108699187\n",
      "l2 norm of weights: 6.1783127701089064\n",
      "---------------------\n",
      "Iteration Number: 515\n",
      "Loss: 77.27520853622119\n",
      "l2 norm of gradients: 0.8154535701915541\n",
      "l2 norm of weights: 6.176541876775537\n",
      "---------------------\n",
      "Iteration Number: 516\n",
      "Loss: 77.23025392394595\n",
      "l2 norm of gradients: 0.8151860958647676\n",
      "l2 norm of weights: 6.174771656681498\n",
      "---------------------\n",
      "Iteration Number: 517\n",
      "Loss: 77.18532758433997\n",
      "l2 norm of gradients: 0.8149186640275125\n",
      "l2 norm of weights: 6.1730021095277685\n",
      "---------------------\n",
      "Iteration Number: 518\n",
      "Loss: 77.14042949766642\n",
      "l2 norm of gradients: 0.8146512746954557\n",
      "l2 norm of weights: 6.171233235016363\n",
      "---------------------\n",
      "Iteration Number: 519\n",
      "Loss: 77.09555964422887\n",
      "l2 norm of gradients: 0.8143839278839313\n",
      "l2 norm of weights: 6.169465032850332\n",
      "---------------------\n",
      "Iteration Number: 520\n",
      "Loss: 77.05071800435296\n",
      "l2 norm of gradients: 0.8141166236079421\n",
      "l2 norm of weights: 6.167697502733753\n",
      "---------------------\n",
      "Iteration Number: 521\n",
      "Loss: 77.00590455840637\n",
      "l2 norm of gradients: 0.8138493618821591\n",
      "l2 norm of weights: 6.165930644371731\n",
      "---------------------\n",
      "Iteration Number: 522\n",
      "Loss: 76.96111928678842\n",
      "l2 norm of gradients: 0.8135821427209247\n",
      "l2 norm of weights: 6.164164457470395\n",
      "---------------------\n",
      "Iteration Number: 523\n",
      "Loss: 76.91636216992384\n",
      "l2 norm of gradients: 0.8133149661382526\n",
      "l2 norm of weights: 6.162398941736888\n",
      "---------------------\n",
      "Iteration Number: 524\n",
      "Loss: 76.87163318827831\n",
      "l2 norm of gradients: 0.8130478321478302\n",
      "l2 norm of weights: 6.16063409687937\n",
      "---------------------\n",
      "Iteration Number: 525\n",
      "Loss: 76.8269323223377\n",
      "l2 norm of gradients: 0.8127807407630185\n",
      "l2 norm of weights: 6.158869922607011\n",
      "---------------------\n",
      "Iteration Number: 526\n",
      "Loss: 76.78225955263463\n",
      "l2 norm of gradients: 0.812513691996855\n",
      "l2 norm of weights: 6.157106418629986\n",
      "---------------------\n",
      "Iteration Number: 527\n",
      "Loss: 76.73761485971963\n",
      "l2 norm of gradients: 0.812246685862053\n",
      "l2 norm of weights: 6.155343584659477\n",
      "---------------------\n",
      "Iteration Number: 528\n",
      "Loss: 76.69299822418674\n",
      "l2 norm of gradients: 0.8119797223710049\n",
      "l2 norm of weights: 6.153581420407659\n",
      "---------------------\n",
      "Iteration Number: 529\n",
      "Loss: 76.64840962664692\n",
      "l2 norm of gradients: 0.8117128015357825\n",
      "l2 norm of weights: 6.151819925587703\n",
      "---------------------\n",
      "Iteration Number: 530\n",
      "Loss: 76.60384904776294\n",
      "l2 norm of gradients: 0.811445923368138\n",
      "l2 norm of weights: 6.150059099913774\n",
      "---------------------\n",
      "Iteration Number: 531\n",
      "Loss: 76.5593164682104\n",
      "l2 norm of gradients: 0.8111790878795062\n",
      "l2 norm of weights: 6.14829894310102\n",
      "---------------------\n",
      "Iteration Number: 532\n",
      "Loss: 76.51481186870835\n",
      "l2 norm of gradients: 0.8109122950810048\n",
      "l2 norm of weights: 6.146539454865574\n",
      "---------------------\n",
      "Iteration Number: 533\n",
      "Loss: 76.47033522999673\n",
      "l2 norm of gradients: 0.8106455449834368\n",
      "l2 norm of weights: 6.1447806349245475\n",
      "---------------------\n",
      "Iteration Number: 534\n",
      "Loss: 76.42588653285485\n",
      "l2 norm of gradients: 0.8103788375972911\n",
      "l2 norm of weights: 6.143022482996024\n",
      "---------------------\n",
      "Iteration Number: 535\n",
      "Loss: 76.3814657580876\n",
      "l2 norm of gradients: 0.810112172932744\n",
      "l2 norm of weights: 6.141264998799061\n",
      "---------------------\n",
      "Iteration Number: 536\n",
      "Loss: 76.33707288653888\n",
      "l2 norm of gradients: 0.8098455509996604\n",
      "l2 norm of weights: 6.139508182053679\n",
      "---------------------\n",
      "Iteration Number: 537\n",
      "Loss: 76.2927078990735\n",
      "l2 norm of gradients: 0.809578971807596\n",
      "l2 norm of weights: 6.137752032480866\n",
      "---------------------\n",
      "Iteration Number: 538\n",
      "Loss: 76.24837077659295\n",
      "l2 norm of gradients: 0.8093124353657973\n",
      "l2 norm of weights: 6.135996549802563\n",
      "---------------------\n",
      "Iteration Number: 539\n",
      "Loss: 76.20406150003205\n",
      "l2 norm of gradients: 0.8090459416832042\n",
      "l2 norm of weights: 6.134241733741668\n",
      "---------------------\n",
      "Iteration Number: 540\n",
      "Loss: 76.15978005034518\n",
      "l2 norm of gradients: 0.8087794907684509\n",
      "l2 norm of weights: 6.132487584022028\n",
      "---------------------\n",
      "Iteration Number: 541\n",
      "Loss: 76.11552640853104\n",
      "l2 norm of gradients: 0.8085130826298668\n",
      "l2 norm of weights: 6.1307341003684375\n",
      "---------------------\n",
      "Iteration Number: 542\n",
      "Loss: 76.07130055560627\n",
      "l2 norm of gradients: 0.8082467172754786\n",
      "l2 norm of weights: 6.128981282506632\n",
      "---------------------\n",
      "Iteration Number: 543\n",
      "Loss: 76.02710247262941\n",
      "l2 norm of gradients: 0.8079803947130115\n",
      "l2 norm of weights: 6.127229130163284\n",
      "---------------------\n",
      "Iteration Number: 544\n",
      "Loss: 75.98293214067613\n",
      "l2 norm of gradients: 0.8077141149498903\n",
      "l2 norm of weights: 6.125477643066001\n",
      "---------------------\n",
      "Iteration Number: 545\n",
      "Loss: 75.93878954086583\n",
      "l2 norm of gradients: 0.8074478779932415\n",
      "l2 norm of weights: 6.123726820943319\n",
      "---------------------\n",
      "Iteration Number: 546\n",
      "Loss: 75.8946746543362\n",
      "l2 norm of gradients: 0.8071816838498939\n",
      "l2 norm of weights: 6.121976663524698\n",
      "---------------------\n",
      "Iteration Number: 547\n",
      "Loss: 75.85058746226306\n",
      "l2 norm of gradients: 0.8069155325263802\n",
      "l2 norm of weights: 6.120227170540523\n",
      "---------------------\n",
      "Iteration Number: 548\n",
      "Loss: 75.80652794584947\n",
      "l2 norm of gradients: 0.8066494240289389\n",
      "l2 norm of weights: 6.118478341722089\n",
      "---------------------\n",
      "Iteration Number: 549\n",
      "Loss: 75.76249608632604\n",
      "l2 norm of gradients: 0.8063833583635155\n",
      "l2 norm of weights: 6.116730176801612\n",
      "---------------------\n",
      "Iteration Number: 550\n",
      "Loss: 75.71849186495034\n",
      "l2 norm of gradients: 0.8061173355357637\n",
      "l2 norm of weights: 6.114982675512209\n",
      "---------------------\n",
      "Iteration Number: 551\n",
      "Loss: 75.67451526302278\n",
      "l2 norm of gradients: 0.8058513555510468\n",
      "l2 norm of weights: 6.113235837587905\n",
      "---------------------\n",
      "Iteration Number: 552\n",
      "Loss: 75.63056626185465\n",
      "l2 norm of gradients: 0.8055854184144398\n",
      "l2 norm of weights: 6.111489662763623\n",
      "---------------------\n",
      "Iteration Number: 553\n",
      "Loss: 75.5866448428079\n",
      "l2 norm of gradients: 0.8053195241307297\n",
      "l2 norm of weights: 6.109744150775185\n",
      "---------------------\n",
      "Iteration Number: 554\n",
      "Loss: 75.54275098724526\n",
      "l2 norm of gradients: 0.8050536727044184\n",
      "l2 norm of weights: 6.1079993013593\n",
      "---------------------\n",
      "Iteration Number: 555\n",
      "Loss: 75.49888467658845\n",
      "l2 norm of gradients: 0.804787864139723\n",
      "l2 norm of weights: 6.106255114253565\n",
      "---------------------\n",
      "Iteration Number: 556\n",
      "Loss: 75.45504589227053\n",
      "l2 norm of gradients: 0.8045220984405776\n",
      "l2 norm of weights: 6.104511589196463\n",
      "---------------------\n",
      "Iteration Number: 557\n",
      "Loss: 75.41123461575198\n",
      "l2 norm of gradients: 0.8042563756106351\n",
      "l2 norm of weights: 6.10276872592735\n",
      "---------------------\n",
      "Iteration Number: 558\n",
      "Loss: 75.36745082853581\n",
      "l2 norm of gradients: 0.8039906956532681\n",
      "l2 norm of weights: 6.101026524186462\n",
      "---------------------\n",
      "Iteration Number: 559\n",
      "Loss: 75.32369451214261\n",
      "l2 norm of gradients: 0.8037250585715705\n",
      "l2 norm of weights: 6.099284983714903\n",
      "---------------------\n",
      "Iteration Number: 560\n",
      "Loss: 75.27996564812261\n",
      "l2 norm of gradients: 0.80345946436836\n",
      "l2 norm of weights: 6.09754410425464\n",
      "---------------------\n",
      "Iteration Number: 561\n",
      "Loss: 75.2362642180579\n",
      "l2 norm of gradients: 0.8031939130461778\n",
      "l2 norm of weights: 6.095803885548503\n",
      "---------------------\n",
      "Iteration Number: 562\n",
      "Loss: 75.19259020355656\n",
      "l2 norm of gradients: 0.8029284046072915\n",
      "l2 norm of weights: 6.094064327340179\n",
      "---------------------\n",
      "Iteration Number: 563\n",
      "Loss: 75.14894358625556\n",
      "l2 norm of gradients: 0.802662939053696\n",
      "l2 norm of weights: 6.09232542937421\n",
      "---------------------\n",
      "Iteration Number: 564\n",
      "Loss: 75.1053243478202\n",
      "l2 norm of gradients: 0.8023975163871152\n",
      "l2 norm of weights: 6.090587191395981\n",
      "---------------------\n",
      "Iteration Number: 565\n",
      "Loss: 75.06173246994508\n",
      "l2 norm of gradients: 0.8021321366090031\n",
      "l2 norm of weights: 6.088849613151726\n",
      "---------------------\n",
      "Iteration Number: 566\n",
      "Loss: 75.01816793435117\n",
      "l2 norm of gradients: 0.8018667997205458\n",
      "l2 norm of weights: 6.087112694388514\n",
      "---------------------\n",
      "Iteration Number: 567\n",
      "Loss: 74.97463072278397\n",
      "l2 norm of gradients: 0.8016015057226631\n",
      "l2 norm of weights: 6.085376434854253\n",
      "---------------------\n",
      "Iteration Number: 568\n",
      "Loss: 74.93112081702807\n",
      "l2 norm of gradients: 0.8013362546160091\n",
      "l2 norm of weights: 6.0836408342976815\n",
      "---------------------\n",
      "Iteration Number: 569\n",
      "Loss: 74.88763819887716\n",
      "l2 norm of gradients: 0.8010710464009747\n",
      "l2 norm of weights: 6.081905892468362\n",
      "---------------------\n",
      "Iteration Number: 570\n",
      "Loss: 74.84418285017314\n",
      "l2 norm of gradients: 0.8008058810776891\n",
      "l2 norm of weights: 6.08017160911668\n",
      "---------------------\n",
      "Iteration Number: 571\n",
      "Loss: 74.80075475277125\n",
      "l2 norm of gradients: 0.8005407586460199\n",
      "l2 norm of weights: 6.078437983993842\n",
      "---------------------\n",
      "Iteration Number: 572\n",
      "Loss: 74.7573538885569\n",
      "l2 norm of gradients: 0.800275679105577\n",
      "l2 norm of weights: 6.076705016851862\n",
      "---------------------\n",
      "Iteration Number: 573\n",
      "Loss: 74.71398023944371\n",
      "l2 norm of gradients: 0.8000106424557114\n",
      "l2 norm of weights: 6.0749727074435675\n",
      "---------------------\n",
      "Iteration Number: 574\n",
      "Loss: 74.67063378737421\n",
      "l2 norm of gradients: 0.7997456486955193\n",
      "l2 norm of weights: 6.0732410555225895\n",
      "---------------------\n",
      "Iteration Number: 575\n",
      "Loss: 74.62731451431893\n",
      "l2 norm of gradients: 0.799480697823842\n",
      "l2 norm of weights: 6.071510060843359\n",
      "---------------------\n",
      "Iteration Number: 576\n",
      "Loss: 74.58402240226955\n",
      "l2 norm of gradients: 0.7992157898392678\n",
      "l2 norm of weights: 6.069779723161102\n",
      "---------------------\n",
      "Iteration Number: 577\n",
      "Loss: 74.54075743324918\n",
      "l2 norm of gradients: 0.7989509247401334\n",
      "l2 norm of weights: 6.068050042231838\n",
      "---------------------\n",
      "Iteration Number: 578\n",
      "Loss: 74.4975195893033\n",
      "l2 norm of gradients: 0.7986861025245262\n",
      "l2 norm of weights: 6.06632101781237\n",
      "---------------------\n",
      "Iteration Number: 579\n",
      "Loss: 74.45430885250845\n",
      "l2 norm of gradients: 0.7984213231902847\n",
      "l2 norm of weights: 6.064592649660288\n",
      "---------------------\n",
      "Iteration Number: 580\n",
      "Loss: 74.41112520496817\n",
      "l2 norm of gradients: 0.7981565867350012\n",
      "l2 norm of weights: 6.062864937533957\n",
      "---------------------\n",
      "Iteration Number: 581\n",
      "Loss: 74.3679686288058\n",
      "l2 norm of gradients: 0.7978918931560223\n",
      "l2 norm of weights: 6.0611378811925185\n",
      "---------------------\n",
      "Iteration Number: 582\n",
      "Loss: 74.3248391061797\n",
      "l2 norm of gradients: 0.7976272424504509\n",
      "l2 norm of weights: 6.059411480395879\n",
      "---------------------\n",
      "Iteration Number: 583\n",
      "Loss: 74.28173661926954\n",
      "l2 norm of gradients: 0.797362634615148\n",
      "l2 norm of weights: 6.057685734904714\n",
      "---------------------\n",
      "Iteration Number: 584\n",
      "Loss: 74.23866115027755\n",
      "l2 norm of gradients: 0.7970980696467336\n",
      "l2 norm of weights: 6.05596064448046\n",
      "---------------------\n",
      "Iteration Number: 585\n",
      "Loss: 74.19561268143723\n",
      "l2 norm of gradients: 0.7968335475415894\n",
      "l2 norm of weights: 6.054236208885306\n",
      "---------------------\n",
      "Iteration Number: 586\n",
      "Loss: 74.152591195009\n",
      "l2 norm of gradients: 0.7965690682958584\n",
      "l2 norm of weights: 6.052512427882195\n",
      "---------------------\n",
      "Iteration Number: 587\n",
      "Loss: 74.1095966732749\n",
      "l2 norm of gradients: 0.7963046319054486\n",
      "l2 norm of weights: 6.0507893012348175\n",
      "---------------------\n",
      "Iteration Number: 588\n",
      "Loss: 74.06662909854361\n",
      "l2 norm of gradients: 0.7960402383660329\n",
      "l2 norm of weights: 6.049066828707607\n",
      "---------------------\n",
      "Iteration Number: 589\n",
      "Loss: 74.02368845315338\n",
      "l2 norm of gradients: 0.7957758876730516\n",
      "l2 norm of weights: 6.047345010065737\n",
      "---------------------\n",
      "Iteration Number: 590\n",
      "Loss: 73.98077471945899\n",
      "l2 norm of gradients: 0.7955115798217138\n",
      "l2 norm of weights: 6.045623845075109\n",
      "---------------------\n",
      "Iteration Number: 591\n",
      "Loss: 73.93788787984758\n",
      "l2 norm of gradients: 0.7952473148069985\n",
      "l2 norm of weights: 6.043903333502363\n",
      "---------------------\n",
      "Iteration Number: 592\n",
      "Loss: 73.89502791672919\n",
      "l2 norm of gradients: 0.7949830926236567\n",
      "l2 norm of weights: 6.042183475114858\n",
      "---------------------\n",
      "Iteration Number: 593\n",
      "Loss: 73.85219481254045\n",
      "l2 norm of gradients: 0.7947189132662124\n",
      "l2 norm of weights: 6.040464269680677\n",
      "---------------------\n",
      "Iteration Number: 594\n",
      "Loss: 73.8093885497466\n",
      "l2 norm of gradients: 0.7944547767289648\n",
      "l2 norm of weights: 6.0387457169686165\n",
      "---------------------\n",
      "Iteration Number: 595\n",
      "Loss: 73.76660911082378\n",
      "l2 norm of gradients: 0.7941906830059895\n",
      "l2 norm of weights: 6.037027816748189\n",
      "---------------------\n",
      "Iteration Number: 596\n",
      "Loss: 73.7238564782875\n",
      "l2 norm of gradients: 0.7939266320911393\n",
      "l2 norm of weights: 6.035310568789612\n",
      "---------------------\n",
      "Iteration Number: 597\n",
      "Loss: 73.68113063467099\n",
      "l2 norm of gradients: 0.7936626239780475\n",
      "l2 norm of weights: 6.033593972863809\n",
      "---------------------\n",
      "Iteration Number: 598\n",
      "Loss: 73.63843156253547\n",
      "l2 norm of gradients: 0.7933986586601283\n",
      "l2 norm of weights: 6.031878028742397\n",
      "---------------------\n",
      "Iteration Number: 599\n",
      "Loss: 73.59575924446605\n",
      "l2 norm of gradients: 0.7931347361305776\n",
      "l2 norm of weights: 6.030162736197695\n",
      "---------------------\n",
      "Iteration Number: 600\n",
      "Loss: 73.5531136630642\n",
      "l2 norm of gradients: 0.7928708563823766\n",
      "l2 norm of weights: 6.028448095002706\n",
      "---------------------\n",
      "Iteration Number: 601\n",
      "Loss: 73.51049480096762\n",
      "l2 norm of gradients: 0.7926070194082916\n",
      "l2 norm of weights: 6.026734104931123\n",
      "---------------------\n",
      "Iteration Number: 602\n",
      "Loss: 73.46790264082932\n",
      "l2 norm of gradients: 0.7923432252008763\n",
      "l2 norm of weights: 6.025020765757317\n",
      "---------------------\n",
      "Iteration Number: 603\n",
      "Loss: 73.4253371653344\n",
      "l2 norm of gradients: 0.7920794737524729\n",
      "l2 norm of weights: 6.023308077256338\n",
      "---------------------\n",
      "Iteration Number: 604\n",
      "Loss: 73.38279835718104\n",
      "l2 norm of gradients: 0.7918157650552144\n",
      "l2 norm of weights: 6.02159603920391\n",
      "---------------------\n",
      "Iteration Number: 605\n",
      "Loss: 73.34028619910146\n",
      "l2 norm of gradients: 0.7915520991010255\n",
      "l2 norm of weights: 6.019884651376422\n",
      "---------------------\n",
      "Iteration Number: 606\n",
      "Loss: 73.29780067384888\n",
      "l2 norm of gradients: 0.7912884758816245\n",
      "l2 norm of weights: 6.018173913550929\n",
      "---------------------\n",
      "Iteration Number: 607\n",
      "Loss: 73.25534176419278\n",
      "l2 norm of gradients: 0.7910248953885243\n",
      "l2 norm of weights: 6.016463825505146\n",
      "---------------------\n",
      "Iteration Number: 608\n",
      "Loss: 73.21290945293633\n",
      "l2 norm of gradients: 0.7907613576130346\n",
      "l2 norm of weights: 6.014754387017441\n",
      "---------------------\n",
      "Iteration Number: 609\n",
      "Loss: 73.17050372290134\n",
      "l2 norm of gradients: 0.7904978625462631\n",
      "l2 norm of weights: 6.013045597866837\n",
      "---------------------\n",
      "Iteration Number: 610\n",
      "Loss: 73.1281245569296\n",
      "l2 norm of gradients: 0.7902344101791172\n",
      "l2 norm of weights: 6.011337457832997\n",
      "---------------------\n",
      "Iteration Number: 611\n",
      "Loss: 73.08577193789247\n",
      "l2 norm of gradients: 0.7899710005023053\n",
      "l2 norm of weights: 6.009629966696234\n",
      "---------------------\n",
      "Iteration Number: 612\n",
      "Loss: 73.04344584868002\n",
      "l2 norm of gradients: 0.7897076335063384\n",
      "l2 norm of weights: 6.007923124237492\n",
      "---------------------\n",
      "Iteration Number: 613\n",
      "Loss: 73.00114627220836\n",
      "l2 norm of gradients: 0.7894443091815321\n",
      "l2 norm of weights: 6.006216930238354\n",
      "---------------------\n",
      "Iteration Number: 614\n",
      "Loss: 72.9588731914104\n",
      "l2 norm of gradients: 0.7891810275180073\n",
      "l2 norm of weights: 6.004511384481028\n",
      "---------------------\n",
      "Iteration Number: 615\n",
      "Loss: 72.91662658924977\n",
      "l2 norm of gradients: 0.7889177885056926\n",
      "l2 norm of weights: 6.002806486748351\n",
      "---------------------\n",
      "Iteration Number: 616\n",
      "Loss: 72.87440644870789\n",
      "l2 norm of gradients: 0.7886545921343253\n",
      "l2 norm of weights: 6.001102236823777\n",
      "---------------------\n",
      "Iteration Number: 617\n",
      "Loss: 72.83221275279013\n",
      "l2 norm of gradients: 0.7883914383934526\n",
      "l2 norm of weights: 5.999398634491377\n",
      "---------------------\n",
      "Iteration Number: 618\n",
      "Loss: 72.79004548452554\n",
      "l2 norm of gradients: 0.7881283272724339\n",
      "l2 norm of weights: 5.997695679535838\n",
      "---------------------\n",
      "Iteration Number: 619\n",
      "Loss: 72.74790462695753\n",
      "l2 norm of gradients: 0.7878652587604421\n",
      "l2 norm of weights: 5.995993371742447\n",
      "---------------------\n",
      "Iteration Number: 620\n",
      "Loss: 72.7057901631647\n",
      "l2 norm of gradients: 0.7876022328464651\n",
      "l2 norm of weights: 5.994291710897105\n",
      "---------------------\n",
      "Iteration Number: 621\n",
      "Loss: 72.66370207623378\n",
      "l2 norm of gradients: 0.7873392495193068\n",
      "l2 norm of weights: 5.992590696786302\n",
      "---------------------\n",
      "Iteration Number: 622\n",
      "Loss: 72.62164034928726\n",
      "l2 norm of gradients: 0.787076308767589\n",
      "l2 norm of weights: 5.99089032919713\n",
      "---------------------\n",
      "Iteration Number: 623\n",
      "Loss: 72.57960496545745\n",
      "l2 norm of gradients: 0.7868134105797537\n",
      "l2 norm of weights: 5.989190607917268\n",
      "---------------------\n",
      "Iteration Number: 624\n",
      "Loss: 72.53759590790803\n",
      "l2 norm of gradients: 0.7865505549440631\n",
      "l2 norm of weights: 5.9874915327349845\n",
      "---------------------\n",
      "Iteration Number: 625\n",
      "Loss: 72.4956131598192\n",
      "l2 norm of gradients: 0.786287741848602\n",
      "l2 norm of weights: 5.985793103439128\n",
      "---------------------\n",
      "Iteration Number: 626\n",
      "Loss: 72.45365670438873\n",
      "l2 norm of gradients: 0.7860249712812792\n",
      "l2 norm of weights: 5.984095319819126\n",
      "---------------------\n",
      "Iteration Number: 627\n",
      "Loss: 72.41172652484549\n",
      "l2 norm of gradients: 0.7857622432298293\n",
      "l2 norm of weights: 5.982398181664978\n",
      "---------------------\n",
      "Iteration Number: 628\n",
      "Loss: 72.3698226044314\n",
      "l2 norm of gradients: 0.7854995576818137\n",
      "l2 norm of weights: 5.980701688767258\n",
      "---------------------\n",
      "Iteration Number: 629\n",
      "Loss: 72.32794492641773\n",
      "l2 norm of gradients: 0.7852369146246214\n",
      "l2 norm of weights: 5.9790058409171\n",
      "---------------------\n",
      "Iteration Number: 630\n",
      "Loss: 72.28609347408461\n",
      "l2 norm of gradients: 0.7849743140454729\n",
      "l2 norm of weights: 5.9773106379062035\n",
      "---------------------\n",
      "Iteration Number: 631\n",
      "Loss: 72.24426823074765\n",
      "l2 norm of gradients: 0.784711755931419\n",
      "l2 norm of weights: 5.975616079526824\n",
      "---------------------\n",
      "Iteration Number: 632\n",
      "Loss: 72.20246917973219\n",
      "l2 norm of gradients: 0.7844492402693435\n",
      "l2 norm of weights: 5.973922165571768\n",
      "---------------------\n",
      "Iteration Number: 633\n",
      "Loss: 72.16069630439061\n",
      "l2 norm of gradients: 0.7841867670459651\n",
      "l2 norm of weights: 5.972228895834392\n",
      "---------------------\n",
      "Iteration Number: 634\n",
      "Loss: 72.11894958809172\n",
      "l2 norm of gradients: 0.7839243362478381\n",
      "l2 norm of weights: 5.9705362701086\n",
      "---------------------\n",
      "Iteration Number: 635\n",
      "Loss: 72.07722901423008\n",
      "l2 norm of gradients: 0.7836619478613537\n",
      "l2 norm of weights: 5.9688442881888335\n",
      "---------------------\n",
      "Iteration Number: 636\n",
      "Loss: 72.03553456621412\n",
      "l2 norm of gradients: 0.7833996018727428\n",
      "l2 norm of weights: 5.967152949870072\n",
      "---------------------\n",
      "Iteration Number: 637\n",
      "Loss: 71.99386622747618\n",
      "l2 norm of gradients: 0.7831372982680758\n",
      "l2 norm of weights: 5.965462254947826\n",
      "---------------------\n",
      "Iteration Number: 638\n",
      "Loss: 71.95222398147166\n",
      "l2 norm of gradients: 0.7828750370332652\n",
      "l2 norm of weights: 5.963772203218136\n",
      "---------------------\n",
      "Iteration Number: 639\n",
      "Loss: 71.91060781167586\n",
      "l2 norm of gradients: 0.7826128181540664\n",
      "l2 norm of weights: 5.962082794477565\n",
      "---------------------\n",
      "Iteration Number: 640\n",
      "Loss: 71.86901770157367\n",
      "l2 norm of gradients: 0.7823506416160796\n",
      "l2 norm of weights: 5.9603940285232\n",
      "---------------------\n",
      "Iteration Number: 641\n",
      "Loss: 71.82745363468366\n",
      "l2 norm of gradients: 0.7820885074047511\n",
      "l2 norm of weights: 5.958705905152638\n",
      "---------------------\n",
      "Iteration Number: 642\n",
      "Loss: 71.78591559453974\n",
      "l2 norm of gradients: 0.7818264155053747\n",
      "l2 norm of weights: 5.957018424163997\n",
      "---------------------\n",
      "Iteration Number: 643\n",
      "Loss: 71.74440356468858\n",
      "l2 norm of gradients: 0.7815643659030931\n",
      "l2 norm of weights: 5.955331585355893\n",
      "---------------------\n",
      "Iteration Number: 644\n",
      "Loss: 71.70291752870726\n",
      "l2 norm of gradients: 0.7813023585828993\n",
      "l2 norm of weights: 5.953645388527454\n",
      "---------------------\n",
      "Iteration Number: 645\n",
      "Loss: 71.66145747018608\n",
      "l2 norm of gradients: 0.7810403935296384\n",
      "l2 norm of weights: 5.951959833478305\n",
      "---------------------\n",
      "Iteration Number: 646\n",
      "Loss: 71.62002337273833\n",
      "l2 norm of gradients: 0.7807784707280081\n",
      "l2 norm of weights: 5.9502749200085665\n",
      "---------------------\n",
      "Iteration Number: 647\n",
      "Loss: 71.57861521998854\n",
      "l2 norm of gradients: 0.7805165901625616\n",
      "l2 norm of weights: 5.948590647918852\n",
      "---------------------\n",
      "Iteration Number: 648\n",
      "Loss: 71.53723299559476\n",
      "l2 norm of gradients: 0.7802547518177072\n",
      "l2 norm of weights: 5.946907017010266\n",
      "---------------------\n",
      "Iteration Number: 649\n",
      "Loss: 71.4958766832178\n",
      "l2 norm of gradients: 0.7799929556777117\n",
      "l2 norm of weights: 5.945224027084392\n",
      "---------------------\n",
      "Iteration Number: 650\n",
      "Loss: 71.45454626655356\n",
      "l2 norm of gradients: 0.7797312017266994\n",
      "l2 norm of weights: 5.9435416779432995\n",
      "---------------------\n",
      "Iteration Number: 651\n",
      "Loss: 71.41324172930423\n",
      "l2 norm of gradients: 0.7794694899486564\n",
      "l2 norm of weights: 5.94185996938953\n",
      "---------------------\n",
      "Iteration Number: 652\n",
      "Loss: 71.37196305519403\n",
      "l2 norm of gradients: 0.7792078203274292\n",
      "l2 norm of weights: 5.940178901226103\n",
      "---------------------\n",
      "Iteration Number: 653\n",
      "Loss: 71.33071022797287\n",
      "l2 norm of gradients: 0.7789461928467282\n",
      "l2 norm of weights: 5.938498473256499\n",
      "---------------------\n",
      "Iteration Number: 654\n",
      "Loss: 71.28948323140257\n",
      "l2 norm of gradients: 0.7786846074901275\n",
      "l2 norm of weights: 5.936818685284671\n",
      "---------------------\n",
      "Iteration Number: 655\n",
      "Loss: 71.24828204926096\n",
      "l2 norm of gradients: 0.7784230642410672\n",
      "l2 norm of weights: 5.935139537115028\n",
      "---------------------\n",
      "Iteration Number: 656\n",
      "Loss: 71.20710666535456\n",
      "l2 norm of gradients: 0.7781615630828551\n",
      "l2 norm of weights: 5.933461028552441\n",
      "---------------------\n",
      "Iteration Number: 657\n",
      "Loss: 71.16595706349553\n",
      "l2 norm of gradients: 0.7779001039986665\n",
      "l2 norm of weights: 5.9317831594022286\n",
      "---------------------\n",
      "Iteration Number: 658\n",
      "Loss: 71.12483322752838\n",
      "l2 norm of gradients: 0.7776386869715473\n",
      "l2 norm of weights: 5.930105929470165\n",
      "---------------------\n",
      "Iteration Number: 659\n",
      "Loss: 71.08373514130412\n",
      "l2 norm of gradients: 0.7773773119844142\n",
      "l2 norm of weights: 5.928429338562466\n",
      "---------------------\n",
      "Iteration Number: 660\n",
      "Loss: 71.04266278869187\n",
      "l2 norm of gradients: 0.7771159790200568\n",
      "l2 norm of weights: 5.926753386485791\n",
      "---------------------\n",
      "Iteration Number: 661\n",
      "Loss: 71.00161615359181\n",
      "l2 norm of gradients: 0.7768546880611384\n",
      "l2 norm of weights: 5.9250780730472385\n",
      "---------------------\n",
      "Iteration Number: 662\n",
      "Loss: 70.96059521990836\n",
      "l2 norm of gradients: 0.7765934390901975\n",
      "l2 norm of weights: 5.923403398054343\n",
      "---------------------\n",
      "Iteration Number: 663\n",
      "Loss: 70.91959997156594\n",
      "l2 norm of gradients: 0.776332232089649\n",
      "l2 norm of weights: 5.921729361315065\n",
      "---------------------\n",
      "Iteration Number: 664\n",
      "Loss: 70.87863039251597\n",
      "l2 norm of gradients: 0.776071067041786\n",
      "l2 norm of weights: 5.9200559626377975\n",
      "---------------------\n",
      "Iteration Number: 665\n",
      "Loss: 70.83768646671295\n",
      "l2 norm of gradients: 0.7758099439287802\n",
      "l2 norm of weights: 5.918383201831355\n",
      "---------------------\n",
      "Iteration Number: 666\n",
      "Loss: 70.79676817814347\n",
      "l2 norm of gradients: 0.7755488627326845\n",
      "l2 norm of weights: 5.91671107870497\n",
      "---------------------\n",
      "Iteration Number: 667\n",
      "Loss: 70.75587551080174\n",
      "l2 norm of gradients: 0.7752878234354332\n",
      "l2 norm of weights: 5.915039593068296\n",
      "---------------------\n",
      "Iteration Number: 668\n",
      "Loss: 70.71500844870096\n",
      "l2 norm of gradients: 0.7750268260188432\n",
      "l2 norm of weights: 5.913368744731396\n",
      "---------------------\n",
      "Iteration Number: 669\n",
      "Loss: 70.67416697587599\n",
      "l2 norm of gradients: 0.7747658704646168\n",
      "l2 norm of weights: 5.911698533504743\n",
      "---------------------\n",
      "Iteration Number: 670\n",
      "Loss: 70.63335107637143\n",
      "l2 norm of gradients: 0.774504956754341\n",
      "l2 norm of weights: 5.910028959199215\n",
      "---------------------\n",
      "Iteration Number: 671\n",
      "Loss: 70.59256073425787\n",
      "l2 norm of gradients: 0.7742440848694899\n",
      "l2 norm of weights: 5.908360021626092\n",
      "---------------------\n",
      "Iteration Number: 672\n",
      "Loss: 70.55179593361602\n",
      "l2 norm of gradients: 0.7739832547914258\n",
      "l2 norm of weights: 5.906691720597054\n",
      "---------------------\n",
      "Iteration Number: 673\n",
      "Loss: 70.51105665854799\n",
      "l2 norm of gradients: 0.7737224665014009\n",
      "l2 norm of weights: 5.905024055924175\n",
      "---------------------\n",
      "Iteration Number: 674\n",
      "Loss: 70.4703428931667\n",
      "l2 norm of gradients: 0.773461719980557\n",
      "l2 norm of weights: 5.903357027419919\n",
      "---------------------\n",
      "Iteration Number: 675\n",
      "Loss: 70.42965462160505\n",
      "l2 norm of gradients: 0.773201015209929\n",
      "l2 norm of weights: 5.90169063489714\n",
      "---------------------\n",
      "Iteration Number: 676\n",
      "Loss: 70.3889918280196\n",
      "l2 norm of gradients: 0.7729403521704439\n",
      "l2 norm of weights: 5.9000248781690745\n",
      "---------------------\n",
      "Iteration Number: 677\n",
      "Loss: 70.34835449656792\n",
      "l2 norm of gradients: 0.7726797308429234\n",
      "l2 norm of weights: 5.898359757049343\n",
      "---------------------\n",
      "Iteration Number: 678\n",
      "Loss: 70.3077426114359\n",
      "l2 norm of gradients: 0.7724191512080855\n",
      "l2 norm of weights: 5.896695271351942\n",
      "---------------------\n",
      "Iteration Number: 679\n",
      "Loss: 70.26715615682672\n",
      "l2 norm of gradients: 0.7721586132465436\n",
      "l2 norm of weights: 5.895031420891241\n",
      "---------------------\n",
      "Iteration Number: 680\n",
      "Loss: 70.22659511695163\n",
      "l2 norm of gradients: 0.7718981169388103\n",
      "l2 norm of weights: 5.893368205481982\n",
      "---------------------\n",
      "Iteration Number: 681\n",
      "Loss: 70.18605947604267\n",
      "l2 norm of gradients: 0.7716376622652967\n",
      "l2 norm of weights: 5.891705624939274\n",
      "---------------------\n",
      "Iteration Number: 682\n",
      "Loss: 70.14554921834817\n",
      "l2 norm of gradients: 0.7713772492063147\n",
      "l2 norm of weights: 5.89004367907859\n",
      "---------------------\n",
      "Iteration Number: 683\n",
      "Loss: 70.1050643281291\n",
      "l2 norm of gradients: 0.7711168777420775\n",
      "l2 norm of weights: 5.888382367715764\n",
      "---------------------\n",
      "Iteration Number: 684\n",
      "Loss: 70.06460478966903\n",
      "l2 norm of gradients: 0.7708565478527012\n",
      "l2 norm of weights: 5.886721690666987\n",
      "---------------------\n",
      "Iteration Number: 685\n",
      "Loss: 70.02417058725928\n",
      "l2 norm of gradients: 0.7705962595182054\n",
      "l2 norm of weights: 5.885061647748806\n",
      "---------------------\n",
      "Iteration Number: 686\n",
      "Loss: 69.98376170521678\n",
      "l2 norm of gradients: 0.7703360127185155\n",
      "l2 norm of weights: 5.8834022387781175\n",
      "---------------------\n",
      "Iteration Number: 687\n",
      "Loss: 69.94337812785781\n",
      "l2 norm of gradients: 0.7700758074334623\n",
      "l2 norm of weights: 5.881743463572164\n",
      "---------------------\n",
      "Iteration Number: 688\n",
      "Loss: 69.90301983953408\n",
      "l2 norm of gradients: 0.7698156436427848\n",
      "l2 norm of weights: 5.8800853219485365\n",
      "---------------------\n",
      "Iteration Number: 689\n",
      "Loss: 69.86268682459803\n",
      "l2 norm of gradients: 0.7695555213261295\n",
      "l2 norm of weights: 5.878427813725165\n",
      "---------------------\n",
      "Iteration Number: 690\n",
      "Loss: 69.82237906742432\n",
      "l2 norm of gradients: 0.7692954404630534\n",
      "l2 norm of weights: 5.876770938720316\n",
      "---------------------\n",
      "Iteration Number: 691\n",
      "Loss: 69.78209655240015\n",
      "l2 norm of gradients: 0.7690354010330236\n",
      "l2 norm of weights: 5.875114696752595\n",
      "---------------------\n",
      "Iteration Number: 692\n",
      "Loss: 69.74183926392838\n",
      "l2 norm of gradients: 0.7687754030154197\n",
      "l2 norm of weights: 5.873459087640938\n",
      "---------------------\n",
      "Iteration Number: 693\n",
      "Loss: 69.70160718643064\n",
      "l2 norm of gradients: 0.7685154463895335\n",
      "l2 norm of weights: 5.8718041112046055\n",
      "---------------------\n",
      "Iteration Number: 694\n",
      "Loss: 69.66140030433768\n",
      "l2 norm of gradients: 0.7682555311345709\n",
      "l2 norm of weights: 5.870149767263191\n",
      "---------------------\n",
      "Iteration Number: 695\n",
      "Loss: 69.62121860210009\n",
      "l2 norm of gradients: 0.7679956572296537\n",
      "l2 norm of weights: 5.868496055636606\n",
      "---------------------\n",
      "Iteration Number: 696\n",
      "Loss: 69.58106206417767\n",
      "l2 norm of gradients: 0.7677358246538194\n",
      "l2 norm of weights: 5.86684297614508\n",
      "---------------------\n",
      "Iteration Number: 697\n",
      "Loss: 69.54093067505488\n",
      "l2 norm of gradients: 0.7674760333860224\n",
      "l2 norm of weights: 5.8651905286091655\n",
      "---------------------\n",
      "Iteration Number: 698\n",
      "Loss: 69.50082441921958\n",
      "l2 norm of gradients: 0.767216283405136\n",
      "l2 norm of weights: 5.863538712849721\n",
      "---------------------\n",
      "Iteration Number: 699\n",
      "Loss: 69.46074328117953\n",
      "l2 norm of gradients: 0.7669565746899526\n",
      "l2 norm of weights: 5.86188752868792\n",
      "---------------------\n",
      "Iteration Number: 700\n",
      "Loss: 69.42068724545891\n",
      "l2 norm of gradients: 0.766696907219185\n",
      "l2 norm of weights: 5.860236975945244\n",
      "---------------------\n",
      "Iteration Number: 701\n",
      "Loss: 69.38065629659512\n",
      "l2 norm of gradients: 0.7664372809714676\n",
      "l2 norm of weights: 5.858587054443478\n",
      "---------------------\n",
      "Iteration Number: 702\n",
      "Loss: 69.34065041913642\n",
      "l2 norm of gradients: 0.7661776959253573\n",
      "l2 norm of weights: 5.856937764004706\n",
      "---------------------\n",
      "Iteration Number: 703\n",
      "Loss: 69.30066959764723\n",
      "l2 norm of gradients: 0.765918152059334\n",
      "l2 norm of weights: 5.855289104451318\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 704\n",
      "Loss: 69.26071381671434\n",
      "l2 norm of gradients: 0.765658649351803\n",
      "l2 norm of weights: 5.853641075605991\n",
      "---------------------\n",
      "Iteration Number: 705\n",
      "Loss: 69.2207830609237\n",
      "l2 norm of gradients: 0.7653991877810941\n",
      "l2 norm of weights: 5.851993677291704\n",
      "---------------------\n",
      "Iteration Number: 706\n",
      "Loss: 69.18087731488691\n",
      "l2 norm of gradients: 0.7651397673254641\n",
      "l2 norm of weights: 5.850346909331719\n",
      "---------------------\n",
      "Iteration Number: 707\n",
      "Loss: 69.14099656322507\n",
      "l2 norm of gradients: 0.7648803879630977\n",
      "l2 norm of weights: 5.848700771549591\n",
      "---------------------\n",
      "Iteration Number: 708\n",
      "Loss: 69.10114079057514\n",
      "l2 norm of gradients: 0.7646210496721071\n",
      "l2 norm of weights: 5.847055263769159\n",
      "---------------------\n",
      "Iteration Number: 709\n",
      "Loss: 69.06130998158758\n",
      "l2 norm of gradients: 0.7643617524305346\n",
      "l2 norm of weights: 5.845410385814539\n",
      "---------------------\n",
      "Iteration Number: 710\n",
      "Loss: 69.02150412091997\n",
      "l2 norm of gradients: 0.7641024962163527\n",
      "l2 norm of weights: 5.843766137510133\n",
      "---------------------\n",
      "Iteration Number: 711\n",
      "Loss: 68.98172319325964\n",
      "l2 norm of gradients: 0.763843281007465\n",
      "l2 norm of weights: 5.8421225186806165\n",
      "---------------------\n",
      "Iteration Number: 712\n",
      "Loss: 68.94196718329023\n",
      "l2 norm of gradients: 0.7635841067817078\n",
      "l2 norm of weights: 5.840479529150938\n",
      "---------------------\n",
      "Iteration Number: 713\n",
      "Loss: 68.90223607571957\n",
      "l2 norm of gradients: 0.76332497351685\n",
      "l2 norm of weights: 5.838837168746321\n",
      "---------------------\n",
      "Iteration Number: 714\n",
      "Loss: 68.86252985526112\n",
      "l2 norm of gradients: 0.763065881190595\n",
      "l2 norm of weights: 5.837195437292254\n",
      "---------------------\n",
      "Iteration Number: 715\n",
      "Loss: 68.82284850665337\n",
      "l2 norm of gradients: 0.7628068297805811\n",
      "l2 norm of weights: 5.8355543346144945\n",
      "---------------------\n",
      "Iteration Number: 716\n",
      "Loss: 68.78319201463808\n",
      "l2 norm of gradients: 0.7625478192643824\n",
      "l2 norm of weights: 5.833913860539061\n",
      "---------------------\n",
      "Iteration Number: 717\n",
      "Loss: 68.74356036397323\n",
      "l2 norm of gradients: 0.7622888496195099\n",
      "l2 norm of weights: 5.832274014892234\n",
      "---------------------\n",
      "Iteration Number: 718\n",
      "Loss: 68.7039535394282\n",
      "l2 norm of gradients: 0.7620299208234126\n",
      "l2 norm of weights: 5.830634797500554\n",
      "---------------------\n",
      "Iteration Number: 719\n",
      "Loss: 68.66437152579202\n",
      "l2 norm of gradients: 0.7617710328534774\n",
      "l2 norm of weights: 5.828996208190815\n",
      "---------------------\n",
      "Iteration Number: 720\n",
      "Loss: 68.62481430785787\n",
      "l2 norm of gradients: 0.7615121856870309\n",
      "l2 norm of weights: 5.827358246790067\n",
      "---------------------\n",
      "Iteration Number: 721\n",
      "Loss: 68.58528187044007\n",
      "l2 norm of gradients: 0.7612533793013404\n",
      "l2 norm of weights: 5.825720913125608\n",
      "---------------------\n",
      "Iteration Number: 722\n",
      "Loss: 68.54577419836164\n",
      "l2 norm of gradients: 0.7609946136736138\n",
      "l2 norm of weights: 5.8240842070249865\n",
      "---------------------\n",
      "Iteration Number: 723\n",
      "Loss: 68.50629127645766\n",
      "l2 norm of gradients: 0.7607358887810008\n",
      "l2 norm of weights: 5.8224481283159975\n",
      "---------------------\n",
      "Iteration Number: 724\n",
      "Loss: 68.46683308958305\n",
      "l2 norm of gradients: 0.7604772046005949\n",
      "l2 norm of weights: 5.82081267682668\n",
      "---------------------\n",
      "Iteration Number: 725\n",
      "Loss: 68.42739962259216\n",
      "l2 norm of gradients: 0.7602185611094318\n",
      "l2 norm of weights: 5.819177852385312\n",
      "---------------------\n",
      "Iteration Number: 726\n",
      "Loss: 68.38799086036389\n",
      "l2 norm of gradients: 0.7599599582844929\n",
      "l2 norm of weights: 5.817543654820414\n",
      "---------------------\n",
      "Iteration Number: 727\n",
      "Loss: 68.34860678778496\n",
      "l2 norm of gradients: 0.759701396102704\n",
      "l2 norm of weights: 5.81591008396074\n",
      "---------------------\n",
      "Iteration Number: 728\n",
      "Loss: 68.30924738975769\n",
      "l2 norm of gradients: 0.7594428745409373\n",
      "l2 norm of weights: 5.814277139635283\n",
      "---------------------\n",
      "Iteration Number: 729\n",
      "Loss: 68.26991265119085\n",
      "l2 norm of gradients: 0.7591843935760116\n",
      "l2 norm of weights: 5.812644821673262\n",
      "---------------------\n",
      "Iteration Number: 730\n",
      "Loss: 68.23060255701607\n",
      "l2 norm of gradients: 0.7589259531846937\n",
      "l2 norm of weights: 5.81101312990413\n",
      "---------------------\n",
      "Iteration Number: 731\n",
      "Loss: 68.19131709216337\n",
      "l2 norm of gradients: 0.7586675533436978\n",
      "l2 norm of weights: 5.809382064157569\n",
      "---------------------\n",
      "Iteration Number: 732\n",
      "Loss: 68.15205624158845\n",
      "l2 norm of gradients: 0.7584091940296886\n",
      "l2 norm of weights: 5.807751624263484\n",
      "---------------------\n",
      "Iteration Number: 733\n",
      "Loss: 68.11281999024763\n",
      "l2 norm of gradients: 0.7581508752192793\n",
      "l2 norm of weights: 5.806121810052005\n",
      "---------------------\n",
      "Iteration Number: 734\n",
      "Loss: 68.0736083231217\n",
      "l2 norm of gradients: 0.7578925968890343\n",
      "l2 norm of weights: 5.804492621353481\n",
      "---------------------\n",
      "Iteration Number: 735\n",
      "Loss: 68.03442122519357\n",
      "l2 norm of gradients: 0.7576343590154693\n",
      "l2 norm of weights: 5.802864057998483\n",
      "---------------------\n",
      "Iteration Number: 736\n",
      "Loss: 67.99525868146236\n",
      "l2 norm of gradients: 0.7573761615750519\n",
      "l2 norm of weights: 5.801236119817797\n",
      "---------------------\n",
      "Iteration Number: 737\n",
      "Loss: 67.9561206769389\n",
      "l2 norm of gradients: 0.7571180045442026\n",
      "l2 norm of weights: 5.799608806642426\n",
      "---------------------\n",
      "Iteration Number: 738\n",
      "Loss: 67.91700719664392\n",
      "l2 norm of gradients: 0.7568598878992955\n",
      "l2 norm of weights: 5.797982118303584\n",
      "---------------------\n",
      "Iteration Number: 739\n",
      "Loss: 67.8779182256163\n",
      "l2 norm of gradients: 0.7566018116166581\n",
      "l2 norm of weights: 5.796356054632699\n",
      "---------------------\n",
      "Iteration Number: 740\n",
      "Loss: 67.83885374889731\n",
      "l2 norm of gradients: 0.7563437756725736\n",
      "l2 norm of weights: 5.794730615461403\n",
      "---------------------\n",
      "Iteration Number: 741\n",
      "Loss: 67.79981375154861\n",
      "l2 norm of gradients: 0.75608578004328\n",
      "l2 norm of weights: 5.79310580062154\n",
      "---------------------\n",
      "Iteration Number: 742\n",
      "Loss: 67.76079821863868\n",
      "l2 norm of gradients: 0.7558278247049718\n",
      "l2 norm of weights: 5.791481609945158\n",
      "---------------------\n",
      "Iteration Number: 743\n",
      "Loss: 67.72180713524811\n",
      "l2 norm of gradients: 0.7555699096338004\n",
      "l2 norm of weights: 5.789858043264504\n",
      "---------------------\n",
      "Iteration Number: 744\n",
      "Loss: 67.68284048647257\n",
      "l2 norm of gradients: 0.7553120348058743\n",
      "l2 norm of weights: 5.788235100412033\n",
      "---------------------\n",
      "Iteration Number: 745\n",
      "Loss: 67.6438982574176\n",
      "l2 norm of gradients: 0.7550542001972607\n",
      "l2 norm of weights: 5.786612781220394\n",
      "---------------------\n",
      "Iteration Number: 746\n",
      "Loss: 67.60498043319349\n",
      "l2 norm of gradients: 0.7547964057839843\n",
      "l2 norm of weights: 5.784991085522437\n",
      "---------------------\n",
      "Iteration Number: 747\n",
      "Loss: 67.5660869989353\n",
      "l2 norm of gradients: 0.7545386515420309\n",
      "l2 norm of weights: 5.783370013151205\n",
      "---------------------\n",
      "Iteration Number: 748\n",
      "Loss: 67.52721793978039\n",
      "l2 norm of gradients: 0.7542809374473447\n",
      "l2 norm of weights: 5.781749563939937\n",
      "---------------------\n",
      "Iteration Number: 749\n",
      "Loss: 67.48837324087393\n",
      "l2 norm of gradients: 0.7540232634758317\n",
      "l2 norm of weights: 5.7801297377220635\n",
      "---------------------\n",
      "Iteration Number: 750\n",
      "Loss: 67.44955288738535\n",
      "l2 norm of gradients: 0.7537656296033577\n",
      "l2 norm of weights: 5.778510534331206\n",
      "---------------------\n",
      "Iteration Number: 751\n",
      "Loss: 67.41075686448549\n",
      "l2 norm of gradients: 0.7535080358057514\n",
      "l2 norm of weights: 5.776891953601173\n",
      "---------------------\n",
      "Iteration Number: 752\n",
      "Loss: 67.37198515735636\n",
      "l2 norm of gradients: 0.7532504820588037\n",
      "l2 norm of weights: 5.775273995365962\n",
      "---------------------\n",
      "Iteration Number: 753\n",
      "Loss: 67.3332377511965\n",
      "l2 norm of gradients: 0.7529929683382675\n",
      "l2 norm of weights: 5.773656659459756\n",
      "---------------------\n",
      "Iteration Number: 754\n",
      "Loss: 67.29451463121174\n",
      "l2 norm of gradients: 0.75273549461986\n",
      "l2 norm of weights: 5.772039945716919\n",
      "---------------------\n",
      "Iteration Number: 755\n",
      "Loss: 67.2558157826205\n",
      "l2 norm of gradients: 0.7524780608792618\n",
      "l2 norm of weights: 5.770423853972\n",
      "---------------------\n",
      "Iteration Number: 756\n",
      "Loss: 67.21714119064707\n",
      "l2 norm of gradients: 0.7522206670921189\n",
      "l2 norm of weights: 5.768808384059728\n",
      "---------------------\n",
      "Iteration Number: 757\n",
      "Loss: 67.17849084053782\n",
      "l2 norm of gradients: 0.7519633132340415\n",
      "l2 norm of weights: 5.767193535815009\n",
      "---------------------\n",
      "Iteration Number: 758\n",
      "Loss: 67.13986471754015\n",
      "l2 norm of gradients: 0.7517059992806057\n",
      "l2 norm of weights: 5.765579309072928\n",
      "---------------------\n",
      "Iteration Number: 759\n",
      "Loss: 67.10126280691865\n",
      "l2 norm of gradients: 0.7514487252073536\n",
      "l2 norm of weights: 5.763965703668746\n",
      "---------------------\n",
      "Iteration Number: 760\n",
      "Loss: 67.06268509394198\n",
      "l2 norm of gradients: 0.7511914909897943\n",
      "l2 norm of weights: 5.762352719437898\n",
      "---------------------\n",
      "Iteration Number: 761\n",
      "Loss: 67.02413156389603\n",
      "l2 norm of gradients: 0.7509342966034036\n",
      "l2 norm of weights: 5.76074035621599\n",
      "---------------------\n",
      "Iteration Number: 762\n",
      "Loss: 66.9856022020744\n",
      "l2 norm of gradients: 0.7506771420236248\n",
      "l2 norm of weights: 5.759128613838804\n",
      "---------------------\n",
      "Iteration Number: 763\n",
      "Loss: 66.9470969937807\n",
      "l2 norm of gradients: 0.7504200272258699\n",
      "l2 norm of weights: 5.757517492142288\n",
      "---------------------\n",
      "Iteration Number: 764\n",
      "Loss: 66.9086159243316\n",
      "l2 norm of gradients: 0.7501629521855188\n",
      "l2 norm of weights: 5.755906990962559\n",
      "---------------------\n",
      "Iteration Number: 765\n",
      "Loss: 66.87015897905358\n",
      "l2 norm of gradients: 0.7499059168779206\n",
      "l2 norm of weights: 5.754297110135902\n",
      "---------------------\n",
      "Iteration Number: 766\n",
      "Loss: 66.83172614328325\n",
      "l2 norm of gradients: 0.7496489212783944\n",
      "l2 norm of weights: 5.752687849498769\n",
      "---------------------\n",
      "Iteration Number: 767\n",
      "Loss: 66.79331740236762\n",
      "l2 norm of gradients: 0.7493919653622286\n",
      "l2 norm of weights: 5.751079208887774\n",
      "---------------------\n",
      "Iteration Number: 768\n",
      "Loss: 66.7549327416635\n",
      "l2 norm of gradients: 0.7491350491046821\n",
      "l2 norm of weights: 5.749471188139695\n",
      "---------------------\n",
      "Iteration Number: 769\n",
      "Loss: 66.71657214654175\n",
      "l2 norm of gradients: 0.748878172480985\n",
      "l2 norm of weights: 5.747863787091473\n",
      "---------------------\n",
      "Iteration Number: 770\n",
      "Loss: 66.67823560237969\n",
      "l2 norm of gradients: 0.7486213354663378\n",
      "l2 norm of weights: 5.746257005580209\n",
      "---------------------\n",
      "Iteration Number: 771\n",
      "Loss: 66.63992309456592\n",
      "l2 norm of gradients: 0.7483645380359136\n",
      "l2 norm of weights: 5.7446508434431625\n",
      "---------------------\n",
      "Iteration Number: 772\n",
      "Loss: 66.60163460849796\n",
      "l2 norm of gradients: 0.7481077801648571\n",
      "l2 norm of weights: 5.743045300517753\n",
      "---------------------\n",
      "Iteration Number: 773\n",
      "Loss: 66.5633701295914\n",
      "l2 norm of gradients: 0.7478510618282855\n",
      "l2 norm of weights: 5.741440376641555\n",
      "---------------------\n",
      "Iteration Number: 774\n",
      "Loss: 66.5251296432595\n",
      "l2 norm of gradients: 0.7475943830012888\n",
      "l2 norm of weights: 5.7398360716523\n",
      "---------------------\n",
      "Iteration Number: 775\n",
      "Loss: 66.48691313493734\n",
      "l2 norm of gradients: 0.7473377436589301\n",
      "l2 norm of weights: 5.738232385387874\n",
      "---------------------\n",
      "Iteration Number: 776\n",
      "Loss: 66.44872059006529\n",
      "l2 norm of gradients: 0.7470811437762466\n",
      "l2 norm of weights: 5.736629317686316\n",
      "---------------------\n",
      "Iteration Number: 777\n",
      "Loss: 66.41055199409055\n",
      "l2 norm of gradients: 0.7468245833282491\n",
      "l2 norm of weights: 5.735026868385817\n",
      "---------------------\n",
      "Iteration Number: 778\n",
      "Loss: 66.37240733247658\n",
      "l2 norm of gradients: 0.7465680622899231\n",
      "l2 norm of weights: 5.733425037324721\n",
      "---------------------\n",
      "Iteration Number: 779\n",
      "Loss: 66.33428659069358\n",
      "l2 norm of gradients: 0.7463115806362282\n",
      "l2 norm of weights: 5.73182382434152\n",
      "---------------------\n",
      "Iteration Number: 780\n",
      "Loss: 66.29618975422405\n",
      "l2 norm of gradients: 0.7460551383420998\n",
      "l2 norm of weights: 5.730223229274859\n",
      "---------------------\n",
      "Iteration Number: 781\n",
      "Loss: 66.25811680855736\n",
      "l2 norm of gradients: 0.7457987353824483\n",
      "l2 norm of weights: 5.728623251963526\n",
      "---------------------\n",
      "Iteration Number: 782\n",
      "Loss: 66.22006773919529\n",
      "l2 norm of gradients: 0.7455423717321598\n",
      "l2 norm of weights: 5.72702389224646\n",
      "---------------------\n",
      "Iteration Number: 783\n",
      "Loss: 66.18204253164856\n",
      "l2 norm of gradients: 0.7452860473660966\n",
      "l2 norm of weights: 5.725425149962745\n",
      "---------------------\n",
      "Iteration Number: 784\n",
      "Loss: 66.14404117143808\n",
      "l2 norm of gradients: 0.7450297622590973\n",
      "l2 norm of weights: 5.7238270249516106\n",
      "---------------------\n",
      "Iteration Number: 785\n",
      "Loss: 66.1060636440937\n",
      "l2 norm of gradients: 0.7447735163859776\n",
      "l2 norm of weights: 5.72222951705243\n",
      "---------------------\n",
      "Iteration Number: 786\n",
      "Loss: 66.06810993515967\n",
      "l2 norm of gradients: 0.7445173097215296\n",
      "l2 norm of weights: 5.72063262610472\n",
      "---------------------\n",
      "Iteration Number: 787\n",
      "Loss: 66.0301800301823\n",
      "l2 norm of gradients: 0.7442611422405228\n",
      "l2 norm of weights: 5.719036351948142\n",
      "---------------------\n",
      "Iteration Number: 788\n",
      "Loss: 65.9922739147249\n",
      "l2 norm of gradients: 0.7440050139177049\n",
      "l2 norm of weights: 5.717440694422496\n",
      "---------------------\n",
      "Iteration Number: 789\n",
      "Loss: 65.9543915743546\n",
      "l2 norm of gradients: 0.7437489247278011\n",
      "l2 norm of weights: 5.715845653367723\n",
      "---------------------\n",
      "Iteration Number: 790\n",
      "Loss: 65.91653299465557\n",
      "l2 norm of gradients: 0.7434928746455145\n",
      "l2 norm of weights: 5.714251228623909\n",
      "---------------------\n",
      "Iteration Number: 791\n",
      "Loss: 65.8786981612151\n",
      "l2 norm of gradients: 0.7432368636455271\n",
      "l2 norm of weights: 5.712657420031273\n",
      "---------------------\n",
      "Iteration Number: 792\n",
      "Loss: 65.84088705963245\n",
      "l2 norm of gradients: 0.7429808917024994\n",
      "l2 norm of weights: 5.7110642274301755\n",
      "---------------------\n",
      "Iteration Number: 793\n",
      "Loss: 65.80309967551892\n",
      "l2 norm of gradients: 0.7427249587910708\n",
      "l2 norm of weights: 5.709471650661112\n",
      "---------------------\n",
      "Iteration Number: 794\n",
      "Loss: 65.76533599448896\n",
      "l2 norm of gradients: 0.7424690648858604\n",
      "l2 norm of weights: 5.707879689564721\n",
      "---------------------\n",
      "Iteration Number: 795\n",
      "Loss: 65.72759600217694\n",
      "l2 norm of gradients: 0.742213209961466\n",
      "l2 norm of weights: 5.706288343981771\n",
      "---------------------\n",
      "Iteration Number: 796\n",
      "Loss: 65.68987968421322\n",
      "l2 norm of gradients: 0.7419573939924661\n",
      "l2 norm of weights: 5.704697613753167\n",
      "---------------------\n",
      "Iteration Number: 797\n",
      "Loss: 65.65218702625278\n",
      "l2 norm of gradients: 0.7417016169534179\n",
      "l2 norm of weights: 5.7031074987199535\n",
      "---------------------\n",
      "Iteration Number: 798\n",
      "Loss: 65.61451801394902\n",
      "l2 norm of gradients: 0.74144587881886\n",
      "l2 norm of weights: 5.701517998723303\n",
      "---------------------\n",
      "Iteration Number: 799\n",
      "Loss: 65.57687263297036\n",
      "l2 norm of gradients: 0.741190179563311\n",
      "l2 norm of weights: 5.699929113604526\n",
      "---------------------\n",
      "Iteration Number: 800\n",
      "Loss: 65.53925086898971\n",
      "l2 norm of gradients: 0.7409345191612697\n",
      "l2 norm of weights: 5.698340843205063\n",
      "---------------------\n",
      "Iteration Number: 801\n",
      "Loss: 65.50165270769718\n",
      "l2 norm of gradients: 0.7406788975872167\n",
      "l2 norm of weights: 5.6967531873664905\n",
      "---------------------\n",
      "Iteration Number: 802\n",
      "Loss: 65.46407813478316\n",
      "l2 norm of gradients: 0.7404233148156125\n",
      "l2 norm of weights: 5.695166145930512\n",
      "---------------------\n",
      "Iteration Number: 803\n",
      "Loss: 65.42652713595633\n",
      "l2 norm of gradients: 0.7401677708209\n",
      "l2 norm of weights: 5.693579718738966\n",
      "---------------------\n",
      "Iteration Number: 804\n",
      "Loss: 65.38899969692986\n",
      "l2 norm of gradients: 0.7399122655775026\n",
      "l2 norm of weights: 5.691993905633819\n",
      "---------------------\n",
      "Iteration Number: 805\n",
      "Loss: 65.35149580342518\n",
      "l2 norm of gradients: 0.7396567990598261\n",
      "l2 norm of weights: 5.69040870645717\n",
      "---------------------\n",
      "Iteration Number: 806\n",
      "Loss: 65.31401544117732\n",
      "l2 norm of gradients: 0.7394013712422576\n",
      "l2 norm of weights: 5.688824121051246\n",
      "---------------------\n",
      "Iteration Number: 807\n",
      "Loss: 65.27655859592542\n",
      "l2 norm of gradients: 0.7391459820991665\n",
      "l2 norm of weights: 5.687240149258404\n",
      "---------------------\n",
      "Iteration Number: 808\n",
      "Loss: 65.23912525342448\n",
      "l2 norm of gradients: 0.7388906316049048\n",
      "l2 norm of weights: 5.685656790921128\n",
      "---------------------\n",
      "Iteration Number: 809\n",
      "Loss: 65.20171539943155\n",
      "l2 norm of gradients: 0.738635319733806\n",
      "l2 norm of weights: 5.684074045882034\n",
      "---------------------\n",
      "Iteration Number: 810\n",
      "Loss: 65.16432901972205\n",
      "l2 norm of gradients: 0.7383800464601868\n",
      "l2 norm of weights: 5.682491913983861\n",
      "---------------------\n",
      "Iteration Number: 811\n",
      "Loss: 65.12696610006918\n",
      "l2 norm of gradients: 0.7381248117583462\n",
      "l2 norm of weights: 5.680910395069478\n",
      "---------------------\n",
      "Iteration Number: 812\n",
      "Loss: 65.08962662626709\n",
      "l2 norm of gradients: 0.7378696156025668\n",
      "l2 norm of weights: 5.679329488981881\n",
      "---------------------\n",
      "Iteration Number: 813\n",
      "Loss: 65.05231058410945\n",
      "l2 norm of gradients: 0.7376144579671134\n",
      "l2 norm of weights: 5.677749195564193\n",
      "---------------------\n",
      "Iteration Number: 814\n",
      "Loss: 65.01501795940554\n",
      "l2 norm of gradients: 0.7373593388262345\n",
      "l2 norm of weights: 5.6761695146596605\n",
      "---------------------\n",
      "Iteration Number: 815\n",
      "Loss: 64.97774873797202\n",
      "l2 norm of gradients: 0.7371042581541615\n",
      "l2 norm of weights: 5.67459044611166\n",
      "---------------------\n",
      "Iteration Number: 816\n",
      "Loss: 64.94050290563268\n",
      "l2 norm of gradients: 0.7368492159251095\n",
      "l2 norm of weights: 5.67301198976369\n",
      "---------------------\n",
      "Iteration Number: 817\n",
      "Loss: 64.90328044822411\n",
      "l2 norm of gradients: 0.7365942121132778\n",
      "l2 norm of weights: 5.671434145459376\n",
      "---------------------\n",
      "Iteration Number: 818\n",
      "Loss: 64.86608135158923\n",
      "l2 norm of gradients: 0.7363392466928486\n",
      "l2 norm of weights: 5.6698569130424685\n",
      "---------------------\n",
      "Iteration Number: 819\n",
      "Loss: 64.82890560158285\n",
      "l2 norm of gradients: 0.7360843196379879\n",
      "l2 norm of weights: 5.66828029235684\n",
      "---------------------\n",
      "Iteration Number: 820\n",
      "Loss: 64.79175318406503\n",
      "l2 norm of gradients: 0.7358294309228467\n",
      "l2 norm of weights: 5.666704283246494\n",
      "---------------------\n",
      "Iteration Number: 821\n",
      "Loss: 64.7546240849068\n",
      "l2 norm of gradients: 0.735574580521559\n",
      "l2 norm of weights: 5.665128885555551\n",
      "---------------------\n",
      "Iteration Number: 822\n",
      "Loss: 64.71751828999022\n",
      "l2 norm of gradients: 0.7353197684082438\n",
      "l2 norm of weights: 5.663554099128258\n",
      "---------------------\n",
      "Iteration Number: 823\n",
      "Loss: 64.68043578520596\n",
      "l2 norm of gradients: 0.7350649945570042\n",
      "l2 norm of weights: 5.661979923808987\n",
      "---------------------\n",
      "Iteration Number: 824\n",
      "Loss: 64.6433765564503\n",
      "l2 norm of gradients: 0.734810258941928\n",
      "l2 norm of weights: 5.660406359442233\n",
      "---------------------\n",
      "Iteration Number: 825\n",
      "Loss: 64.60634058962961\n",
      "l2 norm of gradients: 0.7345555615370871\n",
      "l2 norm of weights: 5.658833405872613\n",
      "---------------------\n",
      "Iteration Number: 826\n",
      "Loss: 64.56932787066455\n",
      "l2 norm of gradients: 0.7343009023165388\n",
      "l2 norm of weights: 5.65726106294487\n",
      "---------------------\n",
      "Iteration Number: 827\n",
      "Loss: 64.53233838548006\n",
      "l2 norm of gradients: 0.7340462812543245\n",
      "l2 norm of weights: 5.655689330503865\n",
      "---------------------\n",
      "Iteration Number: 828\n",
      "Loss: 64.49537212000799\n",
      "l2 norm of gradients: 0.733791698324471\n",
      "l2 norm of weights: 5.654118208394587\n",
      "---------------------\n",
      "Iteration Number: 829\n",
      "Loss: 64.45842906019425\n",
      "l2 norm of gradients: 0.7335371535009899\n",
      "l2 norm of weights: 5.652547696462147\n",
      "---------------------\n",
      "Iteration Number: 830\n",
      "Loss: 64.4215091919947\n",
      "l2 norm of gradients: 0.7332826467578781\n",
      "l2 norm of weights: 5.650977794551776\n",
      "---------------------\n",
      "Iteration Number: 831\n",
      "Loss: 64.38461250136763\n",
      "l2 norm of gradients: 0.7330281780691176\n",
      "l2 norm of weights: 5.649408502508828\n",
      "---------------------\n",
      "Iteration Number: 832\n",
      "Loss: 64.3477389742841\n",
      "l2 norm of gradients: 0.7327737474086756\n",
      "l2 norm of weights: 5.647839820178783\n",
      "---------------------\n",
      "Iteration Number: 833\n",
      "Loss: 64.31088859672639\n",
      "l2 norm of gradients: 0.7325193547505047\n",
      "l2 norm of weights: 5.64627174740724\n",
      "---------------------\n",
      "Iteration Number: 834\n",
      "Loss: 64.27406135468345\n",
      "l2 norm of gradients: 0.7322650000685432\n",
      "l2 norm of weights: 5.644704284039921\n",
      "---------------------\n",
      "Iteration Number: 835\n",
      "Loss: 64.23725723414935\n",
      "l2 norm of gradients: 0.7320106833367152\n",
      "l2 norm of weights: 5.643137429922671\n",
      "---------------------\n",
      "Iteration Number: 836\n",
      "Loss: 64.20047622113594\n",
      "l2 norm of gradients: 0.7317564045289293\n",
      "l2 norm of weights: 5.641571184901458\n",
      "---------------------\n",
      "Iteration Number: 837\n",
      "Loss: 64.16371830165636\n",
      "l2 norm of gradients: 0.7315021636190812\n",
      "l2 norm of weights: 5.64000554882237\n",
      "---------------------\n",
      "Iteration Number: 838\n",
      "Loss: 64.12698346173724\n",
      "l2 norm of gradients: 0.7312479605810516\n",
      "l2 norm of weights: 5.638440521531619\n",
      "---------------------\n",
      "Iteration Number: 839\n",
      "Loss: 64.09027168741027\n",
      "l2 norm of gradients: 0.7309937953887076\n",
      "l2 norm of weights: 5.636876102875539\n",
      "---------------------\n",
      "Iteration Number: 840\n",
      "Loss: 64.05358296471898\n",
      "l2 norm of gradients: 0.7307396680159018\n",
      "l2 norm of weights: 5.635312292700587\n",
      "---------------------\n",
      "Iteration Number: 841\n",
      "Loss: 64.01691727971419\n",
      "l2 norm of gradients: 0.7304855784364734\n",
      "l2 norm of weights: 5.633749090853341\n",
      "---------------------\n",
      "Iteration Number: 842\n",
      "Loss: 63.9802746184589\n",
      "l2 norm of gradients: 0.730231526624247\n",
      "l2 norm of weights: 5.632186497180501\n",
      "---------------------\n",
      "Iteration Number: 843\n",
      "Loss: 63.943654967021025\n",
      "l2 norm of gradients: 0.729977512553034\n",
      "l2 norm of weights: 5.630624511528893\n",
      "---------------------\n",
      "Iteration Number: 844\n",
      "Loss: 63.90705831147817\n",
      "l2 norm of gradients: 0.7297235361966321\n",
      "l2 norm of weights: 5.629063133745461\n",
      "---------------------\n",
      "Iteration Number: 845\n",
      "Loss: 63.87048463792085\n",
      "l2 norm of gradients: 0.7294695975288249\n",
      "l2 norm of weights: 5.627502363677275\n",
      "---------------------\n",
      "Iteration Number: 846\n",
      "Loss: 63.83393393244153\n",
      "l2 norm of gradients: 0.729215696523383\n",
      "l2 norm of weights: 5.625942201171525\n",
      "---------------------\n",
      "Iteration Number: 847\n",
      "Loss: 63.79740618114753\n",
      "l2 norm of gradients: 0.7289618331540629\n",
      "l2 norm of weights: 5.624382646075527\n",
      "---------------------\n",
      "Iteration Number: 848\n",
      "Loss: 63.760901370153164\n",
      "l2 norm of gradients: 0.7287080073946085\n",
      "l2 norm of weights: 5.622823698236717\n",
      "---------------------\n",
      "Iteration Number: 849\n",
      "Loss: 63.72441948558257\n",
      "l2 norm of gradients: 0.7284542192187492\n",
      "l2 norm of weights: 5.621265357502656\n",
      "---------------------\n",
      "Iteration Number: 850\n",
      "Loss: 63.68796051356472\n",
      "l2 norm of gradients: 0.7282004686002023\n",
      "l2 norm of weights: 5.619707623721027\n",
      "---------------------\n",
      "Iteration Number: 851\n",
      "Loss: 63.65152444024156\n",
      "l2 norm of gradients: 0.7279467555126711\n",
      "l2 norm of weights: 5.618150496739638\n",
      "---------------------\n",
      "Iteration Number: 852\n",
      "Loss: 63.61511125176162\n",
      "l2 norm of gradients: 0.727693079929846\n",
      "l2 norm of weights: 5.616593976406417\n",
      "---------------------\n",
      "Iteration Number: 853\n",
      "Loss: 63.57872093428631\n",
      "l2 norm of gradients: 0.7274394418254045\n",
      "l2 norm of weights: 5.615038062569421\n",
      "---------------------\n",
      "Iteration Number: 854\n",
      "Loss: 63.5423534739838\n",
      "l2 norm of gradients: 0.727185841173011\n",
      "l2 norm of weights: 5.613482755076826\n",
      "---------------------\n",
      "Iteration Number: 855\n",
      "Loss: 63.50600885702494\n",
      "l2 norm of gradients: 0.7269322779463169\n",
      "l2 norm of weights: 5.611928053776936\n",
      "---------------------\n",
      "Iteration Number: 856\n",
      "Loss: 63.46968706959848\n",
      "l2 norm of gradients: 0.7266787521189608\n",
      "l2 norm of weights: 5.610373958518174\n",
      "---------------------\n",
      "Iteration Number: 857\n",
      "Loss: 63.43338809790158\n",
      "l2 norm of gradients: 0.7264252636645687\n",
      "l2 norm of weights: 5.608820469149094\n",
      "---------------------\n",
      "Iteration Number: 858\n",
      "Loss: 63.397111928130506\n",
      "l2 norm of gradients: 0.7261718125567537\n",
      "l2 norm of weights: 5.607267585518369\n",
      "---------------------\n",
      "Iteration Number: 859\n",
      "Loss: 63.360858546502605\n",
      "l2 norm of gradients: 0.7259183987691159\n",
      "l2 norm of weights: 5.605715307474802\n",
      "---------------------\n",
      "Iteration Number: 860\n",
      "Loss: 63.32462793923612\n",
      "l2 norm of gradients: 0.725665022275244\n",
      "l2 norm of weights: 5.604163634867316\n",
      "---------------------\n",
      "Iteration Number: 861\n",
      "Loss: 63.288420092560436\n",
      "l2 norm of gradients: 0.7254116830487131\n",
      "l2 norm of weights: 5.602612567544962\n",
      "---------------------\n",
      "Iteration Number: 862\n",
      "Loss: 63.25223499271389\n",
      "l2 norm of gradients: 0.7251583810630862\n",
      "l2 norm of weights: 5.601062105356917\n",
      "---------------------\n",
      "Iteration Number: 863\n",
      "Loss: 63.21607262594691\n",
      "l2 norm of gradients: 0.7249051162919145\n",
      "l2 norm of weights: 5.599512248152483\n",
      "---------------------\n",
      "Iteration Number: 864\n",
      "Loss: 63.179932978512774\n",
      "l2 norm of gradients: 0.7246518887087364\n",
      "l2 norm of weights: 5.597962995781088\n",
      "---------------------\n",
      "Iteration Number: 865\n",
      "Loss: 63.1438160366757\n",
      "l2 norm of gradients: 0.7243986982870783\n",
      "l2 norm of weights: 5.596414348092288\n",
      "---------------------\n",
      "Iteration Number: 866\n",
      "Loss: 63.10772178671234\n",
      "l2 norm of gradients: 0.7241455450004547\n",
      "l2 norm of weights: 5.594866304935762\n",
      "---------------------\n",
      "Iteration Number: 867\n",
      "Loss: 63.07165021490597\n",
      "l2 norm of gradients: 0.7238924288223678\n",
      "l2 norm of weights: 5.59331886616132\n",
      "---------------------\n",
      "Iteration Number: 868\n",
      "Loss: 63.03560130754389\n",
      "l2 norm of gradients: 0.7236393497263086\n",
      "l2 norm of weights: 5.5917720316188975\n",
      "---------------------\n",
      "Iteration Number: 869\n",
      "Loss: 62.99957505093262\n",
      "l2 norm of gradients: 0.7233863076857554\n",
      "l2 norm of weights: 5.590225801158556\n",
      "---------------------\n",
      "Iteration Number: 870\n",
      "Loss: 62.9635714313776\n",
      "l2 norm of gradients: 0.7231333026741752\n",
      "l2 norm of weights: 5.588680174630488\n",
      "---------------------\n",
      "Iteration Number: 871\n",
      "Loss: 62.92759043519717\n",
      "l2 norm of gradients: 0.7228803346650237\n",
      "l2 norm of weights: 5.587135151885011\n",
      "---------------------\n",
      "Iteration Number: 872\n",
      "Loss: 62.89163204872183\n",
      "l2 norm of gradients: 0.7226274036317443\n",
      "l2 norm of weights: 5.585590732772574\n",
      "---------------------\n",
      "Iteration Number: 873\n",
      "Loss: 62.85569625828303\n",
      "l2 norm of gradients: 0.7223745095477697\n",
      "l2 norm of weights: 5.584046917143753\n",
      "---------------------\n",
      "Iteration Number: 874\n",
      "Loss: 62.819783050230335\n",
      "l2 norm of gradients: 0.7221216523865214\n",
      "l2 norm of weights: 5.582503704849253\n",
      "---------------------\n",
      "Iteration Number: 875\n",
      "Loss: 62.7838924109141\n",
      "l2 norm of gradients: 0.7218688321214087\n",
      "l2 norm of weights: 5.580961095739907\n",
      "---------------------\n",
      "Iteration Number: 876\n",
      "Loss: 62.748024326700055\n",
      "l2 norm of gradients: 0.721616048725831\n",
      "l2 norm of weights: 5.579419089666682\n",
      "---------------------\n",
      "Iteration Number: 877\n",
      "Loss: 62.71217878395761\n",
      "l2 norm of gradients: 0.7213633021731755\n",
      "l2 norm of weights: 5.5778776864806705\n",
      "---------------------\n",
      "Iteration Number: 878\n",
      "Loss: 62.67635576906848\n",
      "l2 norm of gradients: 0.7211105924368195\n",
      "l2 norm of weights: 5.5763368860331\n",
      "---------------------\n",
      "Iteration Number: 879\n",
      "Loss: 62.64055526842342\n",
      "l2 norm of gradients: 0.7208579194901292\n",
      "l2 norm of weights: 5.574796688175323\n",
      "---------------------\n",
      "Iteration Number: 880\n",
      "Loss: 62.60477726841667\n",
      "l2 norm of gradients: 0.7206052833064598\n",
      "l2 norm of weights: 5.573257092758829\n",
      "---------------------\n",
      "Iteration Number: 881\n",
      "Loss: 62.56902175545919\n",
      "l2 norm of gradients: 0.7203526838591563\n",
      "l2 norm of weights: 5.571718099635236\n",
      "---------------------\n",
      "Iteration Number: 882\n",
      "Loss: 62.53328871596688\n",
      "l2 norm of gradients: 0.7201001211215533\n",
      "l2 norm of weights: 5.570179708656293\n",
      "---------------------\n",
      "Iteration Number: 883\n",
      "Loss: 62.49757813636341\n",
      "l2 norm of gradients: 0.7198475950669749\n",
      "l2 norm of weights: 5.568641919673884\n",
      "---------------------\n",
      "Iteration Number: 884\n",
      "Loss: 62.46189000308453\n",
      "l2 norm of gradients: 0.7195951056687354\n",
      "l2 norm of weights: 5.567104732540023\n",
      "---------------------\n",
      "Iteration Number: 885\n",
      "Loss: 62.4262243025713\n",
      "l2 norm of gradients: 0.7193426529001389\n",
      "l2 norm of weights: 5.565568147106859\n",
      "---------------------\n",
      "Iteration Number: 886\n",
      "Loss: 62.390581021276525\n",
      "l2 norm of gradients: 0.719090236734479\n",
      "l2 norm of weights: 5.564032163226672\n",
      "---------------------\n",
      "Iteration Number: 887\n",
      "Loss: 62.35496014566138\n",
      "l2 norm of gradients: 0.7188378571450407\n",
      "l2 norm of weights: 5.5624967807518795\n",
      "---------------------\n",
      "Iteration Number: 888\n",
      "Loss: 62.31936166219371\n",
      "l2 norm of gradients: 0.7185855141050984\n",
      "l2 norm of weights: 5.560961999535029\n",
      "---------------------\n",
      "Iteration Number: 889\n",
      "Loss: 62.28378555735501\n",
      "l2 norm of gradients: 0.7183332075879177\n",
      "l2 norm of weights: 5.559427819428804\n",
      "---------------------\n",
      "Iteration Number: 890\n",
      "Loss: 62.248231817630085\n",
      "l2 norm of gradients: 0.7180809375667546\n",
      "l2 norm of weights: 5.557894240286024\n",
      "---------------------\n",
      "Iteration Number: 891\n",
      "Loss: 62.21270042951855\n",
      "l2 norm of gradients: 0.717828704014856\n",
      "l2 norm of weights: 5.556361261959644\n",
      "---------------------\n",
      "Iteration Number: 892\n",
      "Loss: 62.177191379520735\n",
      "l2 norm of gradients: 0.7175765069054598\n",
      "l2 norm of weights: 5.554828884302751\n",
      "---------------------\n",
      "Iteration Number: 893\n",
      "Loss: 62.14170465415704\n",
      "l2 norm of gradients: 0.7173243462117951\n",
      "l2 norm of weights: 5.553297107168572\n",
      "---------------------\n",
      "Iteration Number: 894\n",
      "Loss: 62.10624023994809\n",
      "l2 norm of gradients: 0.7170722219070825\n",
      "l2 norm of weights: 5.551765930410469\n",
      "---------------------\n",
      "Iteration Number: 895\n",
      "Loss: 62.07079812342343\n",
      "l2 norm of gradients: 0.7168201339645339\n",
      "l2 norm of weights: 5.550235353881941\n",
      "---------------------\n",
      "Iteration Number: 896\n",
      "Loss: 62.03537829112983\n",
      "l2 norm of gradients: 0.716568082357353\n",
      "l2 norm of weights: 5.548705377436624\n",
      "---------------------\n",
      "Iteration Number: 897\n",
      "Loss: 61.999980729614364\n",
      "l2 norm of gradients: 0.7163160670587354\n",
      "l2 norm of weights: 5.5471760009282916\n",
      "---------------------\n",
      "Iteration Number: 898\n",
      "Loss: 61.96460542543456\n",
      "l2 norm of gradients: 0.7160640880418685\n",
      "l2 norm of weights: 5.545647224210856\n",
      "---------------------\n",
      "Iteration Number: 899\n",
      "Loss: 61.92925236516197\n",
      "l2 norm of gradients: 0.7158121452799329\n",
      "l2 norm of weights: 5.544119047138367\n",
      "---------------------\n",
      "Iteration Number: 900\n",
      "Loss: 61.89392153537356\n",
      "l2 norm of gradients: 0.7155602387461003\n",
      "l2 norm of weights: 5.542591469565013\n",
      "---------------------\n",
      "Iteration Number: 901\n",
      "Loss: 61.85861292265253\n",
      "l2 norm of gradients: 0.7153083684135363\n",
      "l2 norm of weights: 5.541064491345124\n",
      "---------------------\n",
      "Iteration Number: 902\n",
      "Loss: 61.82332651359854\n",
      "l2 norm of gradients: 0.7150565342553984\n",
      "l2 norm of weights: 5.539538112333168\n",
      "---------------------\n",
      "Iteration Number: 903\n",
      "Loss: 61.78806229481012\n",
      "l2 norm of gradients: 0.714804736244838\n",
      "l2 norm of weights: 5.538012332383752\n",
      "---------------------\n",
      "Iteration Number: 904\n",
      "Loss: 61.75282025290389\n",
      "l2 norm of gradients: 0.7145529743549992\n",
      "l2 norm of weights: 5.536487151351624\n",
      "---------------------\n",
      "Iteration Number: 905\n",
      "Loss: 61.71760037450275\n",
      "l2 norm of gradients: 0.71430124855902\n",
      "l2 norm of weights: 5.534962569091674\n",
      "---------------------\n",
      "Iteration Number: 906\n",
      "Loss: 61.68240264623488\n",
      "l2 norm of gradients: 0.7140495588300318\n",
      "l2 norm of weights: 5.533438585458932\n",
      "---------------------\n",
      "Iteration Number: 907\n",
      "Loss: 61.64722705474213\n",
      "l2 norm of gradients: 0.7137979051411606\n",
      "l2 norm of weights: 5.531915200308572\n",
      "---------------------\n",
      "Iteration Number: 908\n",
      "Loss: 61.61207358667299\n",
      "l2 norm of gradients: 0.713546287465526\n",
      "l2 norm of weights: 5.530392413495907\n",
      "---------------------\n",
      "Iteration Number: 909\n",
      "Loss: 61.57694222868711\n",
      "l2 norm of gradients: 0.7132947057762422\n",
      "l2 norm of weights: 5.528870224876394\n",
      "---------------------\n",
      "Iteration Number: 910\n",
      "Loss: 61.54183296745077\n",
      "l2 norm of gradients: 0.7130431600464184\n",
      "l2 norm of weights: 5.527348634305632\n",
      "---------------------\n",
      "Iteration Number: 911\n",
      "Loss: 61.50674578964012\n",
      "l2 norm of gradients: 0.7127916502491587\n",
      "l2 norm of weights: 5.525827641639364\n",
      "---------------------\n",
      "Iteration Number: 912\n",
      "Loss: 61.47168068194101\n",
      "l2 norm of gradients: 0.7125401763575621\n",
      "l2 norm of weights: 5.524307246733478\n",
      "---------------------\n",
      "Iteration Number: 913\n",
      "Loss: 61.43663763104566\n",
      "l2 norm of gradients: 0.7122887383447234\n",
      "l2 norm of weights: 5.522787449444004\n",
      "---------------------\n",
      "Iteration Number: 914\n",
      "Loss: 61.401616623661454\n",
      "l2 norm of gradients: 0.7120373361837331\n",
      "l2 norm of weights: 5.5212682496271155\n",
      "---------------------\n",
      "Iteration Number: 915\n",
      "Loss: 61.3666176464969\n",
      "l2 norm of gradients: 0.711785969847678\n",
      "l2 norm of weights: 5.519749647139134\n",
      "---------------------\n",
      "Iteration Number: 916\n",
      "Loss: 61.331640686275776\n",
      "l2 norm of gradients: 0.7115346393096406\n",
      "l2 norm of weights: 5.518231641836524\n",
      "---------------------\n",
      "Iteration Number: 917\n",
      "Loss: 61.296685729726214\n",
      "l2 norm of gradients: 0.7112833445427007\n",
      "l2 norm of weights: 5.516714233575896\n",
      "---------------------\n",
      "Iteration Number: 918\n",
      "Loss: 61.26175276359221\n",
      "l2 norm of gradients: 0.7110320855199345\n",
      "l2 norm of weights: 5.5151974222140066\n",
      "---------------------\n",
      "Iteration Number: 919\n",
      "Loss: 61.22684177461727\n",
      "l2 norm of gradients: 0.7107808622144158\n",
      "l2 norm of weights: 5.51368120760776\n",
      "---------------------\n",
      "Iteration Number: 920\n",
      "Loss: 61.191952749562866\n",
      "l2 norm of gradients: 0.7105296745992158\n",
      "l2 norm of weights: 5.512165589614205\n",
      "---------------------\n",
      "Iteration Number: 921\n",
      "Loss: 61.157085675192974\n",
      "l2 norm of gradients: 0.7102785226474037\n",
      "l2 norm of weights: 5.51065056809054\n",
      "---------------------\n",
      "Iteration Number: 922\n",
      "Loss: 61.1222405382866\n",
      "l2 norm of gradients: 0.7100274063320461\n",
      "l2 norm of weights: 5.509136142894108\n",
      "---------------------\n",
      "Iteration Number: 923\n",
      "Loss: 61.08741732562816\n",
      "l2 norm of gradients: 0.7097763256262093\n",
      "l2 norm of weights: 5.507622313882402\n",
      "---------------------\n",
      "Iteration Number: 924\n",
      "Loss: 61.05261602400909\n",
      "l2 norm of gradients: 0.7095252805029575\n",
      "l2 norm of weights: 5.506109080913063\n",
      "---------------------\n",
      "Iteration Number: 925\n",
      "Loss: 61.017836620235116\n",
      "l2 norm of gradients: 0.7092742709353542\n",
      "l2 norm of weights: 5.504596443843881\n",
      "---------------------\n",
      "Iteration Number: 926\n",
      "Loss: 60.98307910111839\n",
      "l2 norm of gradients: 0.7090232968964628\n",
      "l2 norm of weights: 5.503084402532793\n",
      "---------------------\n",
      "Iteration Number: 927\n",
      "Loss: 60.948343453480035\n",
      "l2 norm of gradients: 0.7087723583593465\n",
      "l2 norm of weights: 5.501572956837889\n",
      "---------------------\n",
      "Iteration Number: 928\n",
      "Loss: 60.913629664148885\n",
      "l2 norm of gradients: 0.7085214552970678\n",
      "l2 norm of weights: 5.5000621066174045\n",
      "---------------------\n",
      "Iteration Number: 929\n",
      "Loss: 60.87893771996841\n",
      "l2 norm of gradients: 0.7082705876826909\n",
      "l2 norm of weights: 5.4985518517297285\n",
      "---------------------\n",
      "Iteration Number: 930\n",
      "Loss: 60.84426760778304\n",
      "l2 norm of gradients: 0.7080197554892809\n",
      "l2 norm of weights: 5.4970421920333985\n",
      "---------------------\n",
      "Iteration Number: 931\n",
      "Loss: 60.80961931445475\n",
      "l2 norm of gradients: 0.7077689586899032\n",
      "l2 norm of weights: 5.4955331273871035\n",
      "---------------------\n",
      "Iteration Number: 932\n",
      "Loss: 60.77499282684736\n",
      "l2 norm of gradients: 0.7075181972576259\n",
      "l2 norm of weights: 5.494024657649684\n",
      "---------------------\n",
      "Iteration Number: 933\n",
      "Loss: 60.740388131837754\n",
      "l2 norm of gradients: 0.7072674711655188\n",
      "l2 norm of weights: 5.492516782680131\n",
      "---------------------\n",
      "Iteration Number: 934\n",
      "Loss: 60.70580521631405\n",
      "l2 norm of gradients: 0.7070167803866542\n",
      "l2 norm of weights: 5.491009502337588\n",
      "---------------------\n",
      "Iteration Number: 935\n",
      "Loss: 60.671244067166874\n",
      "l2 norm of gradients: 0.7067661248941073\n",
      "l2 norm of weights: 5.489502816481352\n",
      "---------------------\n",
      "Iteration Number: 936\n",
      "Loss: 60.636704671300826\n",
      "l2 norm of gradients: 0.7065155046609568\n",
      "l2 norm of weights: 5.487996724970868\n",
      "---------------------\n",
      "Iteration Number: 937\n",
      "Loss: 60.602187015630086\n",
      "l2 norm of gradients: 0.7062649196602846\n",
      "l2 norm of weights: 5.4864912276657405\n",
      "---------------------\n",
      "Iteration Number: 938\n",
      "Loss: 60.567691087077854\n",
      "l2 norm of gradients: 0.7060143698651775\n",
      "l2 norm of weights: 5.484986324425721\n",
      "---------------------\n",
      "Iteration Number: 939\n",
      "Loss: 60.533216872570925\n",
      "l2 norm of gradients: 0.7057638552487263\n",
      "l2 norm of weights: 5.4834820151107175\n",
      "---------------------\n",
      "Iteration Number: 940\n",
      "Loss: 60.49876435905219\n",
      "l2 norm of gradients: 0.7055133757840273\n",
      "l2 norm of weights: 5.481978299580793\n",
      "---------------------\n",
      "Iteration Number: 941\n",
      "Loss: 60.464333533470985\n",
      "l2 norm of gradients: 0.7052629314441812\n",
      "l2 norm of weights: 5.48047517769616\n",
      "---------------------\n",
      "Iteration Number: 942\n",
      "Loss: 60.429924382786986\n",
      "l2 norm of gradients: 0.7050125222022962\n",
      "l2 norm of weights: 5.47897264931719\n",
      "---------------------\n",
      "Iteration Number: 943\n",
      "Loss: 60.395536893965016\n",
      "l2 norm of gradients: 0.7047621480314854\n",
      "l2 norm of weights: 5.477470714304406\n",
      "---------------------\n",
      "Iteration Number: 944\n",
      "Loss: 60.36117105398616\n",
      "l2 norm of gradients: 0.7045118089048696\n",
      "l2 norm of weights: 5.47596937251849\n",
      "---------------------\n",
      "Iteration Number: 945\n",
      "Loss: 60.32682684983141\n",
      "l2 norm of gradients: 0.7042615047955768\n",
      "l2 norm of weights: 5.4744686238202735\n",
      "---------------------\n",
      "Iteration Number: 946\n",
      "Loss: 60.29250426849928\n",
      "l2 norm of gradients: 0.7040112356767421\n",
      "l2 norm of weights: 5.472968468070749\n",
      "---------------------\n",
      "Iteration Number: 947\n",
      "Loss: 60.2582032969942\n",
      "l2 norm of gradients: 0.7037610015215097\n",
      "l2 norm of weights: 5.471468905131061\n",
      "---------------------\n",
      "Iteration Number: 948\n",
      "Loss: 60.22392392233054\n",
      "l2 norm of gradients: 0.7035108023030321\n",
      "l2 norm of weights: 5.469969934862513\n",
      "---------------------\n",
      "Iteration Number: 949\n",
      "Loss: 60.189666131527446\n",
      "l2 norm of gradients: 0.7032606379944712\n",
      "l2 norm of weights: 5.468471557126562\n",
      "---------------------\n",
      "Iteration Number: 950\n",
      "Loss: 60.15542991162187\n",
      "l2 norm of gradients: 0.7030105085689986\n",
      "l2 norm of weights: 5.466973771784824\n",
      "---------------------\n",
      "Iteration Number: 951\n",
      "Loss: 60.12121524965076\n",
      "l2 norm of gradients: 0.7027604139997963\n",
      "l2 norm of weights: 5.46547657869907\n",
      "---------------------\n",
      "Iteration Number: 952\n",
      "Loss: 60.08702213266763\n",
      "l2 norm of gradients: 0.7025103542600566\n",
      "l2 norm of weights: 5.4639799777312295\n",
      "---------------------\n",
      "Iteration Number: 953\n",
      "Loss: 60.052850547731985\n",
      "l2 norm of gradients: 0.7022603293229837\n",
      "l2 norm of weights: 5.462483968743388\n",
      "---------------------\n",
      "Iteration Number: 954\n",
      "Loss: 60.018700481909555\n",
      "l2 norm of gradients: 0.7020103391617931\n",
      "l2 norm of weights: 5.460988551597793\n",
      "---------------------\n",
      "Iteration Number: 955\n",
      "Loss: 59.98457192228216\n",
      "l2 norm of gradients: 0.7017603837497132\n",
      "l2 norm of weights: 5.459493726156842\n",
      "---------------------\n",
      "Iteration Number: 956\n",
      "Loss: 59.95046485593508\n",
      "l2 norm of gradients: 0.7015104630599849\n",
      "l2 norm of weights: 5.457999492283098\n",
      "---------------------\n",
      "Iteration Number: 957\n",
      "Loss: 59.91637926996703\n",
      "l2 norm of gradients: 0.7012605770658629\n",
      "l2 norm of weights: 5.456505849839277\n",
      "---------------------\n",
      "Iteration Number: 958\n",
      "Loss: 59.882315151480206\n",
      "l2 norm of gradients: 0.7010107257406157\n",
      "l2 norm of weights: 5.455012798688259\n",
      "---------------------\n",
      "Iteration Number: 959\n",
      "Loss: 59.848272487591885\n",
      "l2 norm of gradients: 0.7007609090575263\n",
      "l2 norm of weights: 5.4535203386930755\n",
      "---------------------\n",
      "Iteration Number: 960\n",
      "Loss: 59.81425126542489\n",
      "l2 norm of gradients: 0.7005111269898934\n",
      "l2 norm of weights: 5.4520284697169235\n",
      "---------------------\n",
      "Iteration Number: 961\n",
      "Loss: 59.78025147211339\n",
      "l2 norm of gradients: 0.7002613795110306\n",
      "l2 norm of weights: 5.450537191623157\n",
      "---------------------\n",
      "Iteration Number: 962\n",
      "Loss: 59.7462730948006\n",
      "l2 norm of gradients: 0.7000116665942686\n",
      "l2 norm of weights: 5.449046504275287\n",
      "---------------------\n",
      "Iteration Number: 963\n",
      "Loss: 59.71231612063672\n",
      "l2 norm of gradients: 0.6997619882129545\n",
      "l2 norm of weights: 5.447556407536987\n",
      "---------------------\n",
      "Iteration Number: 964\n",
      "Loss: 59.67838053678388\n",
      "l2 norm of gradients: 0.6995123443404534\n",
      "l2 norm of weights: 5.44606690127209\n",
      "---------------------\n",
      "Iteration Number: 965\n",
      "Loss: 59.64446633041252\n",
      "l2 norm of gradients: 0.699262734950148\n",
      "l2 norm of weights: 5.444577985344586\n",
      "---------------------\n",
      "Iteration Number: 966\n",
      "Loss: 59.61057348870171\n",
      "l2 norm of gradients: 0.6990131600154404\n",
      "l2 norm of weights: 5.44308965961863\n",
      "---------------------\n",
      "Iteration Number: 967\n",
      "Loss: 59.576701998838864\n",
      "l2 norm of gradients: 0.6987636195097514\n",
      "l2 norm of weights: 5.4416019239585305\n",
      "---------------------\n",
      "Iteration Number: 968\n",
      "Loss: 59.542851848025244\n",
      "l2 norm of gradients: 0.698514113406522\n",
      "l2 norm of weights: 5.440114778228763\n",
      "---------------------\n",
      "Iteration Number: 969\n",
      "Loss: 59.509023023462596\n",
      "l2 norm of gradients: 0.6982646416792141\n",
      "l2 norm of weights: 5.43862822229396\n",
      "---------------------\n",
      "Iteration Number: 970\n",
      "Loss: 59.47521551237182\n",
      "l2 norm of gradients: 0.6980152043013103\n",
      "l2 norm of weights: 5.437142256018913\n",
      "---------------------\n",
      "Iteration Number: 971\n",
      "Loss: 59.441429301975916\n",
      "l2 norm of gradients: 0.6977658012463158\n",
      "l2 norm of weights: 5.435656879268581\n",
      "---------------------\n",
      "Iteration Number: 972\n",
      "Loss: 59.407664379511964\n",
      "l2 norm of gradients: 0.6975164324877579\n",
      "l2 norm of weights: 5.434172091908075\n",
      "---------------------\n",
      "Iteration Number: 973\n",
      "Loss: 59.37392073222157\n",
      "l2 norm of gradients: 0.6972670979991871\n",
      "l2 norm of weights: 5.432687893802675\n",
      "---------------------\n",
      "Iteration Number: 974\n",
      "Loss: 59.34019834735779\n",
      "l2 norm of gradients: 0.6970177977541776\n",
      "l2 norm of weights: 5.4312042848178175\n",
      "---------------------\n",
      "Iteration Number: 975\n",
      "Loss: 59.30649721218549\n",
      "l2 norm of gradients: 0.6967685317263288\n",
      "l2 norm of weights: 5.4297212648191\n",
      "---------------------\n",
      "Iteration Number: 976\n",
      "Loss: 59.27281731397458\n",
      "l2 norm of gradients: 0.6965192998892652\n",
      "l2 norm of weights: 5.428238833672284\n",
      "---------------------\n",
      "Iteration Number: 977\n",
      "Loss: 59.239158640006885\n",
      "l2 norm of gradients: 0.6962701022166363\n",
      "l2 norm of weights: 5.426756991243293\n",
      "---------------------\n",
      "Iteration Number: 978\n",
      "Loss: 59.205521177572756\n",
      "l2 norm of gradients: 0.6960209386821192\n",
      "l2 norm of weights: 5.4252757373982075\n",
      "---------------------\n",
      "Iteration Number: 979\n",
      "Loss: 59.17190491396927\n",
      "l2 norm of gradients: 0.6957718092594178\n",
      "l2 norm of weights: 5.423795072003274\n",
      "---------------------\n",
      "Iteration Number: 980\n",
      "Loss: 59.13830983650525\n",
      "l2 norm of gradients: 0.6955227139222643\n",
      "l2 norm of weights: 5.4223149949249\n",
      "---------------------\n",
      "Iteration Number: 981\n",
      "Loss: 59.10473593250013\n",
      "l2 norm of gradients: 0.6952736526444193\n",
      "l2 norm of weights: 5.420835506029652\n",
      "---------------------\n",
      "Iteration Number: 982\n",
      "Loss: 59.07118318927879\n",
      "l2 norm of gradients: 0.6950246253996729\n",
      "l2 norm of weights: 5.419356605184262\n",
      "---------------------\n",
      "Iteration Number: 983\n",
      "Loss: 59.037651594179266\n",
      "l2 norm of gradients: 0.6947756321618455\n",
      "l2 norm of weights: 5.417878292255623\n",
      "---------------------\n",
      "Iteration Number: 984\n",
      "Loss: 59.004141134545215\n",
      "l2 norm of gradients: 0.694526672904788\n",
      "l2 norm of weights: 5.416400567110788\n",
      "---------------------\n",
      "Iteration Number: 985\n",
      "Loss: 58.97065179773332\n",
      "l2 norm of gradients: 0.6942777476023835\n",
      "l2 norm of weights: 5.414923429616975\n",
      "---------------------\n",
      "Iteration Number: 986\n",
      "Loss: 58.93718357110585\n",
      "l2 norm of gradients: 0.6940288562285467\n",
      "l2 norm of weights: 5.413446879641561\n",
      "---------------------\n",
      "Iteration Number: 987\n",
      "Loss: 58.90373644203454\n",
      "l2 norm of gradients: 0.6937799987572257\n",
      "l2 norm of weights: 5.411970917052087\n",
      "---------------------\n",
      "Iteration Number: 988\n",
      "Loss: 58.87031039790395\n",
      "l2 norm of gradients: 0.6935311751624029\n",
      "l2 norm of weights: 5.410495541716256\n",
      "---------------------\n",
      "Iteration Number: 989\n",
      "Loss: 58.83690542610283\n",
      "l2 norm of gradients: 0.6932823854180945\n",
      "l2 norm of weights: 5.409020753501933\n",
      "---------------------\n",
      "Iteration Number: 990\n",
      "Loss: 58.8035215140342\n",
      "l2 norm of gradients: 0.6930336294983523\n",
      "l2 norm of weights: 5.407546552277145\n",
      "---------------------\n",
      "Iteration Number: 991\n",
      "Loss: 58.77015864910546\n",
      "l2 norm of gradients: 0.6927849073772646\n",
      "l2 norm of weights: 5.406072937910082\n",
      "---------------------\n",
      "Iteration Number: 992\n",
      "Loss: 58.73681681873642\n",
      "l2 norm of gradients: 0.6925362190289559\n",
      "l2 norm of weights: 5.404599910269094\n",
      "---------------------\n",
      "Iteration Number: 993\n",
      "Loss: 58.70349601035503\n",
      "l2 norm of gradients: 0.6922875644275891\n",
      "l2 norm of weights: 5.403127469222695\n",
      "---------------------\n",
      "Iteration Number: 994\n",
      "Loss: 58.67019621139801\n",
      "l2 norm of gradients: 0.6920389435473648\n",
      "l2 norm of weights: 5.401655614639562\n",
      "---------------------\n",
      "Iteration Number: 995\n",
      "Loss: 58.63691740931349\n",
      "l2 norm of gradients: 0.6917903563625234\n",
      "l2 norm of weights: 5.400184346388532\n",
      "---------------------\n",
      "Iteration Number: 996\n",
      "Loss: 58.60365959155347\n",
      "l2 norm of gradients: 0.6915418028473449\n",
      "l2 norm of weights: 5.398713664338606\n",
      "---------------------\n",
      "Iteration Number: 997\n",
      "Loss: 58.570422745586136\n",
      "l2 norm of gradients: 0.6912932829761504\n",
      "l2 norm of weights: 5.397243568358944\n",
      "---------------------\n",
      "Iteration Number: 998\n",
      "Loss: 58.53720685888407\n",
      "l2 norm of gradients: 0.6910447967233028\n",
      "l2 norm of weights: 5.395774058318874\n",
      "---------------------\n",
      "Iteration Number: 999\n",
      "Loss: 58.50401191892882\n",
      "l2 norm of gradients: 0.6907963440632067\n",
      "l2 norm of weights: 5.394305134087878\n",
      "---------------------\n",
      "Iteration Number: 1000\n",
      "Loss: 58.47083791321478\n",
      "l2 norm of gradients: 0.6905479249703107\n",
      "l2 norm of weights: 5.392836795535607\n",
      "---------------------\n",
      "Iteration Number: 1001\n",
      "Loss: 58.43768482924223\n",
      "l2 norm of gradients: 0.690299539419107\n",
      "l2 norm of weights: 5.39136904253187\n",
      "---------------------\n",
      "Iteration Number: 1002\n",
      "Loss: 58.404552654520614\n",
      "l2 norm of gradients: 0.6900511873841333\n",
      "l2 norm of weights: 5.389901874946639\n",
      "---------------------\n",
      "Iteration Number: 1003\n",
      "Loss: 58.37144137657218\n",
      "l2 norm of gradients: 0.6898028688399718\n",
      "l2 norm of weights: 5.3884352926500485\n",
      "---------------------\n",
      "Iteration Number: 1004\n",
      "Loss: 58.33835098292239\n",
      "l2 norm of gradients: 0.6895545837612528\n",
      "l2 norm of weights: 5.386969295512394\n",
      "---------------------\n",
      "Iteration Number: 1005\n",
      "Loss: 58.30528146110923\n",
      "l2 norm of gradients: 0.6893063321226529\n",
      "l2 norm of weights: 5.385503883404131\n",
      "---------------------\n",
      "Iteration Number: 1006\n",
      "Loss: 58.2722327986829\n",
      "l2 norm of gradients: 0.6890581138988972\n",
      "l2 norm of weights: 5.3840390561958795\n",
      "---------------------\n",
      "Iteration Number: 1007\n",
      "Loss: 58.239204983196345\n",
      "l2 norm of gradients: 0.6888099290647599\n",
      "l2 norm of weights: 5.38257481375842\n",
      "---------------------\n",
      "Iteration Number: 1008\n",
      "Loss: 58.206198002215494\n",
      "l2 norm of gradients: 0.6885617775950653\n",
      "l2 norm of weights: 5.381111155962691\n",
      "---------------------\n",
      "Iteration Number: 1009\n",
      "Loss: 58.173211843314675\n",
      "l2 norm of gradients: 0.6883136594646883\n",
      "l2 norm of weights: 5.379648082679799\n",
      "---------------------\n",
      "Iteration Number: 1010\n",
      "Loss: 58.140246494077246\n",
      "l2 norm of gradients: 0.6880655746485553\n",
      "l2 norm of weights: 5.378185593781006\n",
      "---------------------\n",
      "Iteration Number: 1011\n",
      "Loss: 58.107301942094054\n",
      "l2 norm of gradients: 0.6878175231216456\n",
      "l2 norm of weights: 5.376723689137736\n",
      "---------------------\n",
      "Iteration Number: 1012\n",
      "Loss: 58.074378174969794\n",
      "l2 norm of gradients: 0.6875695048589917\n",
      "l2 norm of weights: 5.375262368621577\n",
      "---------------------\n",
      "Iteration Number: 1013\n",
      "Loss: 58.041475180313626\n",
      "l2 norm of gradients: 0.68732151983568\n",
      "l2 norm of weights: 5.373801632104273\n",
      "---------------------\n",
      "Iteration Number: 1014\n",
      "Loss: 58.00859294574256\n",
      "l2 norm of gradients: 0.6870735680268528\n",
      "l2 norm of weights: 5.372341479457733\n",
      "---------------------\n",
      "Iteration Number: 1015\n",
      "Loss: 57.975731458889285\n",
      "l2 norm of gradients: 0.6868256494077077\n",
      "l2 norm of weights: 5.370881910554023\n",
      "---------------------\n",
      "Iteration Number: 1016\n",
      "Loss: 57.942890707386596\n",
      "l2 norm of gradients: 0.6865777639534995\n",
      "l2 norm of weights: 5.369422925265371\n",
      "---------------------\n",
      "Iteration Number: 1017\n",
      "Loss: 57.910070678886534\n",
      "l2 norm of gradients: 0.6863299116395407\n",
      "l2 norm of weights: 5.367964523464167\n",
      "---------------------\n",
      "Iteration Number: 1018\n",
      "Loss: 57.877271361041046\n",
      "l2 norm of gradients: 0.6860820924412024\n",
      "l2 norm of weights: 5.366506705022958\n",
      "---------------------\n",
      "Iteration Number: 1019\n",
      "Loss: 57.84449274151794\n",
      "l2 norm of gradients: 0.6858343063339158\n",
      "l2 norm of weights: 5.365049469814451\n",
      "---------------------\n",
      "Iteration Number: 1020\n",
      "Loss: 57.81173480798985\n",
      "l2 norm of gradients: 0.6855865532931719\n",
      "l2 norm of weights: 5.363592817711515\n",
      "---------------------\n",
      "Iteration Number: 1021\n",
      "Loss: 57.77899754813705\n",
      "l2 norm of gradients: 0.6853388332945229\n",
      "l2 norm of weights: 5.362136748587178\n",
      "---------------------\n",
      "Iteration Number: 1022\n",
      "Loss: 57.746280949654846\n",
      "l2 norm of gradients: 0.6850911463135845\n",
      "l2 norm of weights: 5.360681262314625\n",
      "---------------------\n",
      "Iteration Number: 1023\n",
      "Loss: 57.713585000240805\n",
      "l2 norm of gradients: 0.684843492326034\n",
      "l2 norm of weights: 5.359226358767203\n",
      "---------------------\n",
      "Iteration Number: 1024\n",
      "Loss: 57.68090968760844\n",
      "l2 norm of gradients: 0.6845958713076141\n",
      "l2 norm of weights: 5.357772037818417\n",
      "---------------------\n",
      "Iteration Number: 1025\n",
      "Loss: 57.64825499947443\n",
      "l2 norm of gradients: 0.6843482832341319\n",
      "l2 norm of weights: 5.356318299341929\n",
      "---------------------\n",
      "Iteration Number: 1026\n",
      "Loss: 57.61562092356711\n",
      "l2 norm of gradients: 0.6841007280814603\n",
      "l2 norm of weights: 5.354865143211564\n",
      "---------------------\n",
      "Iteration Number: 1027\n",
      "Loss: 57.583007447622094\n",
      "l2 norm of gradients: 0.6838532058255393\n",
      "l2 norm of weights: 5.3534125693013\n",
      "---------------------\n",
      "Iteration Number: 1028\n",
      "Loss: 57.55041455938621\n",
      "l2 norm of gradients: 0.6836057164423769\n",
      "l2 norm of weights: 5.351960577485277\n",
      "---------------------\n",
      "Iteration Number: 1029\n",
      "Loss: 57.51784224661301\n",
      "l2 norm of gradients: 0.6833582599080495\n",
      "l2 norm of weights: 5.350509167637792\n",
      "---------------------\n",
      "Iteration Number: 1030\n",
      "Loss: 57.48529049706815\n",
      "l2 norm of gradients: 0.683110836198703\n",
      "l2 norm of weights: 5.3490583396333\n",
      "---------------------\n",
      "Iteration Number: 1031\n",
      "Loss: 57.45275929852106\n",
      "l2 norm of gradients: 0.682863445290554\n",
      "l2 norm of weights: 5.347608093346412\n",
      "---------------------\n",
      "Iteration Number: 1032\n",
      "Loss: 57.42024863875621\n",
      "l2 norm of gradients: 0.682616087159891\n",
      "l2 norm of weights: 5.346158428651898\n",
      "---------------------\n",
      "Iteration Number: 1033\n",
      "Loss: 57.38775850556279\n",
      "l2 norm of gradients: 0.682368761783074\n",
      "l2 norm of weights: 5.344709345424685\n",
      "---------------------\n",
      "Iteration Number: 1034\n",
      "Loss: 57.355288886738364\n",
      "l2 norm of gradients: 0.6821214691365375\n",
      "l2 norm of weights: 5.343260843539856\n",
      "---------------------\n",
      "Iteration Number: 1035\n",
      "Loss: 57.3228397700927\n",
      "l2 norm of gradients: 0.6818742091967893\n",
      "l2 norm of weights: 5.34181292287265\n",
      "---------------------\n",
      "Iteration Number: 1036\n",
      "Loss: 57.29041114344127\n",
      "l2 norm of gradients: 0.6816269819404134\n",
      "l2 norm of weights: 5.340365583298466\n",
      "---------------------\n",
      "Iteration Number: 1037\n",
      "Loss: 57.258002994611815\n",
      "l2 norm of gradients: 0.6813797873440691\n",
      "l2 norm of weights: 5.338918824692853\n",
      "---------------------\n",
      "Iteration Number: 1038\n",
      "Loss: 57.22561531143792\n",
      "l2 norm of gradients: 0.6811326253844933\n",
      "l2 norm of weights: 5.337472646931522\n",
      "---------------------\n",
      "Iteration Number: 1039\n",
      "Loss: 57.193248081762846\n",
      "l2 norm of gradients: 0.6808854960385012\n",
      "l2 norm of weights: 5.336027049890336\n",
      "---------------------\n",
      "Iteration Number: 1040\n",
      "Loss: 57.16090129343966\n",
      "l2 norm of gradients: 0.6806383992829865\n",
      "l2 norm of weights: 5.334582033445314\n",
      "---------------------\n",
      "Iteration Number: 1041\n",
      "Loss: 57.12857493432867\n",
      "l2 norm of gradients: 0.6803913350949234\n",
      "l2 norm of weights: 5.33313759747263\n",
      "---------------------\n",
      "Iteration Number: 1042\n",
      "Loss: 57.09626899230093\n",
      "l2 norm of gradients: 0.6801443034513668\n",
      "l2 norm of weights: 5.331693741848615\n",
      "---------------------\n",
      "Iteration Number: 1043\n",
      "Loss: 57.0639834552346\n",
      "l2 norm of gradients: 0.6798973043294534\n",
      "l2 norm of weights: 5.33025046644975\n",
      "---------------------\n",
      "Iteration Number: 1044\n",
      "Loss: 57.03171831101771\n",
      "l2 norm of gradients: 0.6796503377064033\n",
      "l2 norm of weights: 5.328807771152675\n",
      "---------------------\n",
      "Iteration Number: 1045\n",
      "Loss: 56.999473547544774\n",
      "l2 norm of gradients: 0.6794034035595193\n",
      "l2 norm of weights: 5.327365655834181\n",
      "---------------------\n",
      "Iteration Number: 1046\n",
      "Loss: 56.967249152722275\n",
      "l2 norm of gradients: 0.6791565018661905\n",
      "l2 norm of weights: 5.325924120371214\n",
      "---------------------\n",
      "Iteration Number: 1047\n",
      "Loss: 56.93504511446424\n",
      "l2 norm of gradients: 0.6789096326038905\n",
      "l2 norm of weights: 5.324483164640872\n",
      "---------------------\n",
      "Iteration Number: 1048\n",
      "Loss: 56.902861420694855\n",
      "l2 norm of gradients: 0.6786627957501802\n",
      "l2 norm of weights: 5.323042788520408\n",
      "---------------------\n",
      "Iteration Number: 1049\n",
      "Loss: 56.87069805934102\n",
      "l2 norm of gradients: 0.6784159912827076\n",
      "l2 norm of weights: 5.321602991887227\n",
      "---------------------\n",
      "Iteration Number: 1050\n",
      "Loss: 56.838555018346355\n",
      "l2 norm of gradients: 0.6781692191792102\n",
      "l2 norm of weights: 5.3201637746188855\n",
      "---------------------\n",
      "Iteration Number: 1051\n",
      "Loss: 56.806432285659376\n",
      "l2 norm of gradients: 0.6779224794175145\n",
      "l2 norm of weights: 5.318725136593093\n",
      "---------------------\n",
      "Iteration Number: 1052\n",
      "Loss: 56.774329849236494\n",
      "l2 norm of gradients: 0.6776757719755374\n",
      "l2 norm of weights: 5.317287077687713\n",
      "---------------------\n",
      "Iteration Number: 1053\n",
      "Loss: 56.74224769704316\n",
      "l2 norm of gradients: 0.6774290968312877\n",
      "l2 norm of weights: 5.3158495977807565\n",
      "---------------------\n",
      "Iteration Number: 1054\n",
      "Loss: 56.710185817057535\n",
      "l2 norm of gradients: 0.6771824539628665\n",
      "l2 norm of weights: 5.314412696750389\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 1055\n",
      "Loss: 56.67814419725923\n",
      "l2 norm of gradients: 0.6769358433484681\n",
      "l2 norm of weights: 5.312976374474926\n",
      "---------------------\n",
      "Iteration Number: 1056\n",
      "Loss: 56.64612282564316\n",
      "l2 norm of gradients: 0.6766892649663817\n",
      "l2 norm of weights: 5.311540630832831\n",
      "---------------------\n",
      "Iteration Number: 1057\n",
      "Loss: 56.61412169020668\n",
      "l2 norm of gradients: 0.6764427187949917\n",
      "l2 norm of weights: 5.3101054657027245\n",
      "---------------------\n",
      "Iteration Number: 1058\n",
      "Loss: 56.582140778963726\n",
      "l2 norm of gradients: 0.6761962048127784\n",
      "l2 norm of weights: 5.30867087896337\n",
      "---------------------\n",
      "Iteration Number: 1059\n",
      "Loss: 56.55018007992993\n",
      "l2 norm of gradients: 0.6759497229983198\n",
      "l2 norm of weights: 5.307236870493684\n",
      "---------------------\n",
      "Iteration Number: 1060\n",
      "Loss: 56.51823958113027\n",
      "l2 norm of gradients: 0.6757032733302921\n",
      "l2 norm of weights: 5.305803440172733\n",
      "---------------------\n",
      "Iteration Number: 1061\n",
      "Loss: 56.48631927060348\n",
      "l2 norm of gradients: 0.6754568557874709\n",
      "l2 norm of weights: 5.304370587879731\n",
      "---------------------\n",
      "Iteration Number: 1062\n",
      "Loss: 56.45441913639118\n",
      "l2 norm of gradients: 0.6752104703487312\n",
      "l2 norm of weights: 5.302938313494041\n",
      "---------------------\n",
      "Iteration Number: 1063\n",
      "Loss: 56.42253916654463\n",
      "l2 norm of gradients: 0.6749641169930504\n",
      "l2 norm of weights: 5.301506616895174\n",
      "---------------------\n",
      "Iteration Number: 1064\n",
      "Loss: 56.390679349126884\n",
      "l2 norm of gradients: 0.6747177956995064\n",
      "l2 norm of weights: 5.3000754979627915\n",
      "---------------------\n",
      "Iteration Number: 1065\n",
      "Loss: 56.35883967220649\n",
      "l2 norm of gradients: 0.6744715064472819\n",
      "l2 norm of weights: 5.2986449565767\n",
      "---------------------\n",
      "Iteration Number: 1066\n",
      "Loss: 56.32702012386228\n",
      "l2 norm of gradients: 0.6742252492156624\n",
      "l2 norm of weights: 5.297214992616852\n",
      "---------------------\n",
      "Iteration Number: 1067\n",
      "Loss: 56.29522069217923\n",
      "l2 norm of gradients: 0.6739790239840389\n",
      "l2 norm of weights: 5.295785605963353\n",
      "---------------------\n",
      "Iteration Number: 1068\n",
      "Loss: 56.26344136525335\n",
      "l2 norm of gradients: 0.673732830731908\n",
      "l2 norm of weights: 5.294356796496448\n",
      "---------------------\n",
      "Iteration Number: 1069\n",
      "Loss: 56.231682131186226\n",
      "l2 norm of gradients: 0.6734866694388736\n",
      "l2 norm of weights: 5.292928564096534\n",
      "---------------------\n",
      "Iteration Number: 1070\n",
      "Loss: 56.19994297809339\n",
      "l2 norm of gradients: 0.6732405400846471\n",
      "l2 norm of weights: 5.291500908644151\n",
      "---------------------\n",
      "Iteration Number: 1071\n",
      "Loss: 56.168223894091966\n",
      "l2 norm of gradients: 0.6729944426490488\n",
      "l2 norm of weights: 5.290073830019984\n",
      "---------------------\n",
      "Iteration Number: 1072\n",
      "Loss: 56.13652486731085\n",
      "l2 norm of gradients: 0.672748377112009\n",
      "l2 norm of weights: 5.288647328104867\n",
      "---------------------\n",
      "Iteration Number: 1073\n",
      "Loss: 56.1048458858897\n",
      "l2 norm of gradients: 0.6725023434535683\n",
      "l2 norm of weights: 5.2872214027797755\n",
      "---------------------\n",
      "Iteration Number: 1074\n",
      "Loss: 56.07318693796979\n",
      "l2 norm of gradients: 0.6722563416538794\n",
      "l2 norm of weights: 5.285796053925831\n",
      "---------------------\n",
      "Iteration Number: 1075\n",
      "Loss: 56.04154801171022\n",
      "l2 norm of gradients: 0.6720103716932072\n",
      "l2 norm of weights: 5.284371281424298\n",
      "---------------------\n",
      "Iteration Number: 1076\n",
      "Loss: 56.00992909527158\n",
      "l2 norm of gradients: 0.6717644335519303\n",
      "l2 norm of weights: 5.282947085156587\n",
      "---------------------\n",
      "Iteration Number: 1077\n",
      "Loss: 55.978330176822226\n",
      "l2 norm of gradients: 0.6715185272105421\n",
      "l2 norm of weights: 5.281523465004249\n",
      "---------------------\n",
      "Iteration Number: 1078\n",
      "Loss: 55.946751244543684\n",
      "l2 norm of gradients: 0.6712726526496509\n",
      "l2 norm of weights: 5.280100420848982\n",
      "---------------------\n",
      "Iteration Number: 1079\n",
      "Loss: 55.915192286622855\n",
      "l2 norm of gradients: 0.6710268098499818\n",
      "l2 norm of weights: 5.2786779525726235\n",
      "---------------------\n",
      "Iteration Number: 1080\n",
      "Loss: 55.883653291255634\n",
      "l2 norm of gradients: 0.6707809987923768\n",
      "l2 norm of weights: 5.277256060057155\n",
      "---------------------\n",
      "Iteration Number: 1081\n",
      "Loss: 55.852134246646024\n",
      "l2 norm of gradients: 0.6705352194577967\n",
      "l2 norm of weights: 5.275834743184701\n",
      "---------------------\n",
      "Iteration Number: 1082\n",
      "Loss: 55.820635141006115\n",
      "l2 norm of gradients: 0.6702894718273212\n",
      "l2 norm of weights: 5.274414001837526\n",
      "---------------------\n",
      "Iteration Number: 1083\n",
      "Loss: 55.78915596255728\n",
      "l2 norm of gradients: 0.67004375588215\n",
      "l2 norm of weights: 5.272993835898035\n",
      "---------------------\n",
      "Iteration Number: 1084\n",
      "Loss: 55.757696699528445\n",
      "l2 norm of gradients: 0.669798071603604\n",
      "l2 norm of weights: 5.271574245248776\n",
      "---------------------\n",
      "Iteration Number: 1085\n",
      "Loss: 55.72625734015636\n",
      "l2 norm of gradients: 0.669552418973126\n",
      "l2 norm of weights: 5.270155229772439\n",
      "---------------------\n",
      "Iteration Number: 1086\n",
      "Loss: 55.694837872686726\n",
      "l2 norm of gradients: 0.6693067979722819\n",
      "l2 norm of weights: 5.268736789351851\n",
      "---------------------\n",
      "Iteration Number: 1087\n",
      "Loss: 55.66343828537367\n",
      "l2 norm of gradients: 0.669061208582761\n",
      "l2 norm of weights: 5.26731892386998\n",
      "---------------------\n",
      "Iteration Number: 1088\n",
      "Loss: 55.63205856647879\n",
      "l2 norm of gradients: 0.6688156507863778\n",
      "l2 norm of weights: 5.2659016332099355\n",
      "---------------------\n",
      "Iteration Number: 1089\n",
      "Loss: 55.60069870427157\n",
      "l2 norm of gradients: 0.668570124565072\n",
      "l2 norm of weights: 5.264484917254963\n",
      "---------------------\n",
      "Iteration Number: 1090\n",
      "Loss: 55.56935868703088\n",
      "l2 norm of gradients: 0.6683246299009103\n",
      "l2 norm of weights: 5.263068775888451\n",
      "---------------------\n",
      "Iteration Number: 1091\n",
      "Loss: 55.53803850304343\n",
      "l2 norm of gradients: 0.6680791667760865\n",
      "l2 norm of weights: 5.261653208993921\n",
      "---------------------\n",
      "Iteration Number: 1092\n",
      "Loss: 55.50673814060335\n",
      "l2 norm of gradients: 0.667833735172923\n",
      "l2 norm of weights: 5.260238216455037\n",
      "---------------------\n",
      "Iteration Number: 1093\n",
      "Loss: 55.47545758801387\n",
      "l2 norm of gradients: 0.6675883350738712\n",
      "l2 norm of weights: 5.258823798155601\n",
      "---------------------\n",
      "Iteration Number: 1094\n",
      "Loss: 55.44419683358413\n",
      "l2 norm of gradients: 0.6673429664615129\n",
      "l2 norm of weights: 5.257409953979548\n",
      "---------------------\n",
      "Iteration Number: 1095\n",
      "Loss: 55.412955865637585\n",
      "l2 norm of gradients: 0.6670976293185612\n",
      "l2 norm of weights: 5.255996683810956\n",
      "---------------------\n",
      "Iteration Number: 1096\n",
      "Loss: 55.38173467249612\n",
      "l2 norm of gradients: 0.6668523236278605\n",
      "l2 norm of weights: 5.254583987534034\n",
      "---------------------\n",
      "Iteration Number: 1097\n",
      "Loss: 55.350533242498045\n",
      "l2 norm of gradients: 0.6666070493723887\n",
      "l2 norm of weights: 5.253171865033131\n",
      "---------------------\n",
      "Iteration Number: 1098\n",
      "Loss: 55.31935156398621\n",
      "l2 norm of gradients: 0.6663618065352569\n",
      "l2 norm of weights: 5.251760316192733\n",
      "---------------------\n",
      "Iteration Number: 1099\n",
      "Loss: 55.28818962531079\n",
      "l2 norm of gradients: 0.6661165950997112\n",
      "l2 norm of weights: 5.250349340897458\n",
      "---------------------\n",
      "Iteration Number: 1100\n",
      "Loss: 55.257047414833124\n",
      "l2 norm of gradients: 0.6658714150491328\n",
      "l2 norm of weights: 5.24893893903206\n",
      "---------------------\n",
      "Iteration Number: 1101\n",
      "Loss: 55.225924920917606\n",
      "l2 norm of gradients: 0.6656262663670397\n",
      "l2 norm of weights: 5.247529110481431\n",
      "---------------------\n",
      "Iteration Number: 1102\n",
      "Loss: 55.194822131944136\n",
      "l2 norm of gradients: 0.6653811490370867\n",
      "l2 norm of weights: 5.246119855130594\n",
      "---------------------\n",
      "Iteration Number: 1103\n",
      "Loss: 55.16373903629102\n",
      "l2 norm of gradients: 0.6651360630430668\n",
      "l2 norm of weights: 5.244711172864709\n",
      "---------------------\n",
      "Iteration Number: 1104\n",
      "Loss: 55.13267562235213\n",
      "l2 norm of gradients: 0.6648910083689121\n",
      "l2 norm of weights: 5.243303063569067\n",
      "---------------------\n",
      "Iteration Number: 1105\n",
      "Loss: 55.101631878527165\n",
      "l2 norm of gradients: 0.6646459849986945\n",
      "l2 norm of weights: 5.241895527129096\n",
      "---------------------\n",
      "Iteration Number: 1106\n",
      "Loss: 55.070607793223274\n",
      "l2 norm of gradients: 0.6644009929166261\n",
      "l2 norm of weights: 5.240488563430356\n",
      "---------------------\n",
      "Iteration Number: 1107\n",
      "Loss: 55.03960335485408\n",
      "l2 norm of gradients: 0.6641560321070613\n",
      "l2 norm of weights: 5.239082172358535\n",
      "---------------------\n",
      "Iteration Number: 1108\n",
      "Loss: 55.008618551845004\n",
      "l2 norm of gradients: 0.6639111025544958\n",
      "l2 norm of weights: 5.237676353799462\n",
      "---------------------\n",
      "Iteration Number: 1109\n",
      "Loss: 54.97765337262646\n",
      "l2 norm of gradients: 0.6636662042435693\n",
      "l2 norm of weights: 5.236271107639093\n",
      "---------------------\n",
      "Iteration Number: 1110\n",
      "Loss: 54.94670780563628\n",
      "l2 norm of gradients: 0.663421337159065\n",
      "l2 norm of weights: 5.234866433763515\n",
      "---------------------\n",
      "Iteration Number: 1111\n",
      "Loss: 54.915781839321625\n",
      "l2 norm of gradients: 0.6631765012859114\n",
      "l2 norm of weights: 5.23346233205895\n",
      "---------------------\n",
      "Iteration Number: 1112\n",
      "Loss: 54.88487546213693\n",
      "l2 norm of gradients: 0.662931696609182\n",
      "l2 norm of weights: 5.23205880241175\n",
      "---------------------\n",
      "Iteration Number: 1113\n",
      "Loss: 54.85398866254622\n",
      "l2 norm of gradients: 0.6626869231140975\n",
      "l2 norm of weights: 5.230655844708394\n",
      "---------------------\n",
      "Iteration Number: 1114\n",
      "Loss: 54.82312142901916\n",
      "l2 norm of gradients: 0.6624421807860252\n",
      "l2 norm of weights: 5.229253458835498\n",
      "---------------------\n",
      "Iteration Number: 1115\n",
      "Loss: 54.792273750031846\n",
      "l2 norm of gradients: 0.662197469610481\n",
      "l2 norm of weights: 5.227851644679804\n",
      "---------------------\n",
      "Iteration Number: 1116\n",
      "Loss: 54.76144561407355\n",
      "l2 norm of gradients: 0.6619527895731292\n",
      "l2 norm of weights: 5.226450402128184\n",
      "---------------------\n",
      "Iteration Number: 1117\n",
      "Loss: 54.73063700963691\n",
      "l2 norm of gradients: 0.661708140659784\n",
      "l2 norm of weights: 5.2250497310676405\n",
      "---------------------\n",
      "Iteration Number: 1118\n",
      "Loss: 54.699847925221896\n",
      "l2 norm of gradients: 0.6614635228564104\n",
      "l2 norm of weights: 5.223649631385306\n",
      "---------------------\n",
      "Iteration Number: 1119\n",
      "Loss: 54.66907834933991\n",
      "l2 norm of gradients: 0.6612189361491239\n",
      "l2 norm of weights: 5.222250102968438\n",
      "---------------------\n",
      "Iteration Number: 1120\n",
      "Loss: 54.6383282705058\n",
      "l2 norm of gradients: 0.6609743805241927\n",
      "l2 norm of weights: 5.220851145704428\n",
      "---------------------\n",
      "Iteration Number: 1121\n",
      "Loss: 54.6075976772454\n",
      "l2 norm of gradients: 0.6607298559680376\n",
      "l2 norm of weights: 5.2194527594807925\n",
      "---------------------\n",
      "Iteration Number: 1122\n",
      "Loss: 54.57688655809356\n",
      "l2 norm of gradients: 0.6604853624672326\n",
      "l2 norm of weights: 5.218054944185175\n",
      "---------------------\n",
      "Iteration Number: 1123\n",
      "Loss: 54.54619490158586\n",
      "l2 norm of gradients: 0.6602409000085065\n",
      "l2 norm of weights: 5.21665769970535\n",
      "---------------------\n",
      "Iteration Number: 1124\n",
      "Loss: 54.51552269627372\n",
      "l2 norm of gradients: 0.6599964685787434\n",
      "l2 norm of weights: 5.215261025929216\n",
      "---------------------\n",
      "Iteration Number: 1125\n",
      "Loss: 54.48486993071105\n",
      "l2 norm of gradients: 0.6597520681649823\n",
      "l2 norm of weights: 5.213864922744801\n",
      "---------------------\n",
      "Iteration Number: 1126\n",
      "Loss: 54.454236593460536\n",
      "l2 norm of gradients: 0.6595076987544197\n",
      "l2 norm of weights: 5.212469390040257\n",
      "---------------------\n",
      "Iteration Number: 1127\n",
      "Loss: 54.42362267309594\n",
      "l2 norm of gradients: 0.6592633603344091\n",
      "l2 norm of weights: 5.211074427703865\n",
      "---------------------\n",
      "Iteration Number: 1128\n",
      "Loss: 54.39302815819199\n",
      "l2 norm of gradients: 0.6590190528924618\n",
      "l2 norm of weights: 5.209680035624031\n",
      "---------------------\n",
      "Iteration Number: 1129\n",
      "Loss: 54.36245303733718\n",
      "l2 norm of gradients: 0.658774776416248\n",
      "l2 norm of weights: 5.208286213689289\n",
      "---------------------\n",
      "Iteration Number: 1130\n",
      "Loss: 54.3318972991244\n",
      "l2 norm of gradients: 0.6585305308935981\n",
      "l2 norm of weights: 5.206892961788292\n",
      "---------------------\n",
      "Iteration Number: 1131\n",
      "Loss: 54.301360932154935\n",
      "l2 norm of gradients: 0.6582863163125017\n",
      "l2 norm of weights: 5.205500279809826\n",
      "---------------------\n",
      "Iteration Number: 1132\n",
      "Loss: 54.27084392503763\n",
      "l2 norm of gradients: 0.6580421326611098\n",
      "l2 norm of weights: 5.204108167642798\n",
      "---------------------\n",
      "Iteration Number: 1133\n",
      "Loss: 54.24034626638922\n",
      "l2 norm of gradients: 0.6577979799277349\n",
      "l2 norm of weights: 5.202716625176239\n",
      "---------------------\n",
      "Iteration Number: 1134\n",
      "Loss: 54.2098679448332\n",
      "l2 norm of gradients: 0.6575538581008521\n",
      "l2 norm of weights: 5.2013256522993085\n",
      "---------------------\n",
      "Iteration Number: 1135\n",
      "Loss: 54.1794089490008\n",
      "l2 norm of gradients: 0.6573097671690993\n",
      "l2 norm of weights: 5.1999352489012844\n",
      "---------------------\n",
      "Iteration Number: 1136\n",
      "Loss: 54.148969267530994\n",
      "l2 norm of gradients: 0.6570657071212779\n",
      "l2 norm of weights: 5.198545414871573\n",
      "---------------------\n",
      "Iteration Number: 1137\n",
      "Loss: 54.11854888907012\n",
      "l2 norm of gradients: 0.656821677946354\n",
      "l2 norm of weights: 5.197156150099702\n",
      "---------------------\n",
      "Iteration Number: 1138\n",
      "Loss: 54.08814780227305\n",
      "l2 norm of gradients: 0.6565776796334587\n",
      "l2 norm of weights: 5.195767454475324\n",
      "---------------------\n",
      "Iteration Number: 1139\n",
      "Loss: 54.057765995801454\n",
      "l2 norm of gradients: 0.6563337121718885\n",
      "l2 norm of weights: 5.194379327888213\n",
      "---------------------\n",
      "Iteration Number: 1140\n",
      "Loss: 54.027403458321785\n",
      "l2 norm of gradients: 0.6560897755511064\n",
      "l2 norm of weights: 5.192991770228266\n",
      "---------------------\n",
      "Iteration Number: 1141\n",
      "Loss: 53.99706017851372\n",
      "l2 norm of gradients: 0.6558458697607427\n",
      "l2 norm of weights: 5.191604781385504\n",
      "---------------------\n",
      "Iteration Number: 1142\n",
      "Loss: 53.9667361450581\n",
      "l2 norm of gradients: 0.655601994790595\n",
      "l2 norm of weights: 5.19021836125007\n",
      "---------------------\n",
      "Iteration Number: 1143\n",
      "Loss: 53.93643134664614\n",
      "l2 norm of gradients: 0.6553581506306292\n",
      "l2 norm of weights: 5.188832509712227\n",
      "---------------------\n",
      "Iteration Number: 1144\n",
      "Loss: 53.90614577197938\n",
      "l2 norm of gradients: 0.6551143372709802\n",
      "l2 norm of weights: 5.187447226662363\n",
      "---------------------\n",
      "Iteration Number: 1145\n",
      "Loss: 53.87587940976205\n",
      "l2 norm of gradients: 0.6548705547019523\n",
      "l2 norm of weights: 5.1860625119909844\n",
      "---------------------\n",
      "Iteration Number: 1146\n",
      "Loss: 53.845632248705726\n",
      "l2 norm of gradients: 0.6546268029140203\n",
      "l2 norm of weights: 5.184678365588723\n",
      "---------------------\n",
      "Iteration Number: 1147\n",
      "Loss: 53.81540427753379\n",
      "l2 norm of gradients: 0.6543830818978293\n",
      "l2 norm of weights: 5.183294787346328\n",
      "---------------------\n",
      "Iteration Number: 1148\n",
      "Loss: 53.78519548497283\n",
      "l2 norm of gradients: 0.6541393916441961\n",
      "l2 norm of weights: 5.18191177715467\n",
      "---------------------\n",
      "Iteration Number: 1149\n",
      "Loss: 53.75500585975972\n",
      "l2 norm of gradients: 0.6538957321441091\n",
      "l2 norm of weights: 5.180529334904743\n",
      "---------------------\n",
      "Iteration Number: 1150\n",
      "Loss: 53.72483539063465\n",
      "l2 norm of gradients: 0.6536521033887294\n",
      "l2 norm of weights: 5.179147460487658\n",
      "---------------------\n",
      "Iteration Number: 1151\n",
      "Loss: 53.69468406634855\n",
      "l2 norm of gradients: 0.6534085053693913\n",
      "l2 norm of weights: 5.177766153794651\n",
      "---------------------\n",
      "Iteration Number: 1152\n",
      "Loss: 53.664551875660685\n",
      "l2 norm of gradients: 0.6531649380776026\n",
      "l2 norm of weights: 5.176385414717072\n",
      "---------------------\n",
      "Iteration Number: 1153\n",
      "Loss: 53.63443880733266\n",
      "l2 norm of gradients: 0.6529214015050456\n",
      "l2 norm of weights: 5.175005243146396\n",
      "---------------------\n",
      "Iteration Number: 1154\n",
      "Loss: 53.60434485013853\n",
      "l2 norm of gradients: 0.6526778956435767\n",
      "l2 norm of weights: 5.173625638974214\n",
      "---------------------\n",
      "Iteration Number: 1155\n",
      "Loss: 53.574269992856706\n",
      "l2 norm of gradients: 0.6524344204852286\n",
      "l2 norm of weights: 5.17224660209224\n",
      "---------------------\n",
      "Iteration Number: 1156\n",
      "Loss: 53.54421422427457\n",
      "l2 norm of gradients: 0.6521909760222092\n",
      "l2 norm of weights: 5.1708681323923065\n",
      "---------------------\n",
      "Iteration Number: 1157\n",
      "Loss: 53.51417753318263\n",
      "l2 norm of gradients: 0.6519475622469031\n",
      "l2 norm of weights: 5.169490229766362\n",
      "---------------------\n",
      "Iteration Number: 1158\n",
      "Loss: 53.48415990838497\n",
      "l2 norm of gradients: 0.6517041791518715\n",
      "l2 norm of weights: 5.168112894106478\n",
      "---------------------\n",
      "Iteration Number: 1159\n",
      "Loss: 53.45416133868809\n",
      "l2 norm of gradients: 0.6514608267298537\n",
      "l2 norm of weights: 5.166736125304841\n",
      "---------------------\n",
      "Iteration Number: 1160\n",
      "Loss: 53.424181812907364\n",
      "l2 norm of gradients: 0.6512175049737663\n",
      "l2 norm of weights: 5.16535992325376\n",
      "---------------------\n",
      "Iteration Number: 1161\n",
      "Loss: 53.39422131986454\n",
      "l2 norm of gradients: 0.6509742138767047\n",
      "l2 norm of weights: 5.16398428784566\n",
      "---------------------\n",
      "Iteration Number: 1162\n",
      "Loss: 53.3642798483909\n",
      "l2 norm of gradients: 0.6507309534319432\n",
      "l2 norm of weights: 5.162609218973086\n",
      "---------------------\n",
      "Iteration Number: 1163\n",
      "Loss: 53.33435738732086\n",
      "l2 norm of gradients: 0.6504877236329354\n",
      "l2 norm of weights: 5.161234716528699\n",
      "---------------------\n",
      "Iteration Number: 1164\n",
      "Loss: 53.30445392550081\n",
      "l2 norm of gradients: 0.6502445244733149\n",
      "l2 norm of weights: 5.159860780405278\n",
      "---------------------\n",
      "Iteration Number: 1165\n",
      "Loss: 53.27456945178037\n",
      "l2 norm of gradients: 0.6500013559468961\n",
      "l2 norm of weights: 5.158487410495723\n",
      "---------------------\n",
      "Iteration Number: 1166\n",
      "Loss: 53.244703955017116\n",
      "l2 norm of gradients: 0.6497582180476738\n",
      "l2 norm of weights: 5.1571146066930496\n",
      "---------------------\n",
      "Iteration Number: 1167\n",
      "Loss: 53.21485742407771\n",
      "l2 norm of gradients: 0.649515110769824\n",
      "l2 norm of weights: 5.155742368890389\n",
      "---------------------\n",
      "Iteration Number: 1168\n",
      "Loss: 53.18502984783294\n",
      "l2 norm of gradients: 0.6492720341077048\n",
      "l2 norm of weights: 5.154370696980995\n",
      "---------------------\n",
      "Iteration Number: 1169\n",
      "Loss: 53.15522121516256\n",
      "l2 norm of gradients: 0.6490289880558564\n",
      "l2 norm of weights: 5.152999590858233\n",
      "---------------------\n",
      "Iteration Number: 1170\n",
      "Loss: 53.12543151495429\n",
      "l2 norm of gradients: 0.6487859726090017\n",
      "l2 norm of weights: 5.15162905041559\n",
      "---------------------\n",
      "Iteration Number: 1171\n",
      "Loss: 53.09566073610103\n",
      "l2 norm of gradients: 0.6485429877620464\n",
      "l2 norm of weights: 5.150259075546667\n",
      "---------------------\n",
      "Iteration Number: 1172\n",
      "Loss: 53.065908867501534\n",
      "l2 norm of gradients: 0.64830003351008\n",
      "l2 norm of weights: 5.148889666145185\n",
      "---------------------\n",
      "Iteration Number: 1173\n",
      "Loss: 53.03617589806635\n",
      "l2 norm of gradients: 0.6480571098483757\n",
      "l2 norm of weights: 5.147520822104979\n",
      "---------------------\n",
      "Iteration Number: 1174\n",
      "Loss: 53.00646181670777\n",
      "l2 norm of gradients: 0.6478142167723909\n",
      "l2 norm of weights: 5.146152543320002\n",
      "---------------------\n",
      "Iteration Number: 1175\n",
      "Loss: 52.97676661234874\n",
      "l2 norm of gradients: 0.6475713542777678\n",
      "l2 norm of weights: 5.144784829684325\n",
      "---------------------\n",
      "Iteration Number: 1176\n",
      "Loss: 52.94709027391909\n",
      "l2 norm of gradients: 0.6473285223603336\n",
      "l2 norm of weights: 5.143417681092134\n",
      "---------------------\n",
      "Iteration Number: 1177\n",
      "Loss: 52.91743279035135\n",
      "l2 norm of gradients: 0.6470857210161011\n",
      "l2 norm of weights: 5.142051097437732\n",
      "---------------------\n",
      "Iteration Number: 1178\n",
      "Loss: 52.887794150589556\n",
      "l2 norm of gradients: 0.6468429502412683\n",
      "l2 norm of weights: 5.140685078615538\n",
      "---------------------\n",
      "Iteration Number: 1179\n",
      "Loss: 52.85817434358447\n",
      "l2 norm of gradients: 0.6466002100322201\n",
      "l2 norm of weights: 5.139319624520087\n",
      "---------------------\n",
      "Iteration Number: 1180\n",
      "Loss: 52.82857335829099\n",
      "l2 norm of gradients: 0.6463575003855276\n",
      "l2 norm of weights: 5.137954735046032\n",
      "---------------------\n",
      "Iteration Number: 1181\n",
      "Loss: 52.79899118367393\n",
      "l2 norm of gradients: 0.6461148212979486\n",
      "l2 norm of weights: 5.136590410088141\n",
      "---------------------\n",
      "Iteration Number: 1182\n",
      "Loss: 52.76942780870302\n",
      "l2 norm of gradients: 0.645872172766428\n",
      "l2 norm of weights: 5.1352266495413\n",
      "---------------------\n",
      "Iteration Number: 1183\n",
      "Loss: 52.73988322235589\n",
      "l2 norm of gradients: 0.6456295547880989\n",
      "l2 norm of weights: 5.133863453300508\n",
      "---------------------\n",
      "Iteration Number: 1184\n",
      "Loss: 52.71035741361773\n",
      "l2 norm of gradients: 0.6453869673602812\n",
      "l2 norm of weights: 5.132500821260883\n",
      "---------------------\n",
      "Iteration Number: 1185\n",
      "Loss: 52.68085037147881\n",
      "l2 norm of gradients: 0.6451444104804835\n",
      "l2 norm of weights: 5.131138753317656\n",
      "---------------------\n",
      "Iteration Number: 1186\n",
      "Loss: 52.65136208493578\n",
      "l2 norm of gradients: 0.6449018841464028\n",
      "l2 norm of weights: 5.129777249366178\n",
      "---------------------\n",
      "Iteration Number: 1187\n",
      "Loss: 52.621892542996896\n",
      "l2 norm of gradients: 0.644659388355925\n",
      "l2 norm of weights: 5.128416309301913\n",
      "---------------------\n",
      "Iteration Number: 1188\n",
      "Loss: 52.59244173467084\n",
      "l2 norm of gradients: 0.6444169231071244\n",
      "l2 norm of weights: 5.127055933020441\n",
      "---------------------\n",
      "Iteration Number: 1189\n",
      "Loss: 52.563009648979815\n",
      "l2 norm of gradients: 0.6441744883982649\n",
      "l2 norm of weights: 5.12569612041746\n",
      "---------------------\n",
      "Iteration Number: 1190\n",
      "Loss: 52.533596274945225\n",
      "l2 norm of gradients: 0.6439320842278\n",
      "l2 norm of weights: 5.124336871388782\n",
      "---------------------\n",
      "Iteration Number: 1191\n",
      "Loss: 52.50420160160451\n",
      "l2 norm of gradients: 0.6436897105943732\n",
      "l2 norm of weights: 5.122978185830337\n",
      "---------------------\n",
      "Iteration Number: 1192\n",
      "Loss: 52.47482561799167\n",
      "l2 norm of gradients: 0.6434473674968174\n",
      "l2 norm of weights: 5.121620063638168\n",
      "---------------------\n",
      "Iteration Number: 1193\n",
      "Loss: 52.445468313158024\n",
      "l2 norm of gradients: 0.6432050549341566\n",
      "l2 norm of weights: 5.120262504708435\n",
      "---------------------\n",
      "Iteration Number: 1194\n",
      "Loss: 52.41612967615156\n",
      "l2 norm of gradients: 0.6429627729056047\n",
      "l2 norm of weights: 5.118905508937418\n",
      "---------------------\n",
      "Iteration Number: 1195\n",
      "Loss: 52.38680969603555\n",
      "l2 norm of gradients: 0.6427205214105662\n",
      "l2 norm of weights: 5.117549076221506\n",
      "---------------------\n",
      "Iteration Number: 1196\n",
      "Loss: 52.35750836187623\n",
      "l2 norm of gradients: 0.6424783004486375\n",
      "l2 norm of weights: 5.116193206457209\n",
      "---------------------\n",
      "Iteration Number: 1197\n",
      "Loss: 52.32822566274616\n",
      "l2 norm of gradients: 0.6422361100196051\n",
      "l2 norm of weights: 5.1148378995411505\n",
      "---------------------\n",
      "Iteration Number: 1198\n",
      "Loss: 52.29896158772554\n",
      "l2 norm of gradients: 0.6419939501234476\n",
      "l2 norm of weights: 5.113483155370072\n",
      "---------------------\n",
      "Iteration Number: 1199\n",
      "Loss: 52.26971612590139\n",
      "l2 norm of gradients: 0.6417518207603349\n",
      "l2 norm of weights: 5.1121289738408295\n",
      "---------------------\n",
      "Iteration Number: 1200\n",
      "Loss: 52.24048926636692\n",
      "l2 norm of gradients: 0.6415097219306284\n",
      "l2 norm of weights: 5.1107753548503965\n",
      "---------------------\n",
      "Iteration Number: 1201\n",
      "Loss: 52.211280998224225\n",
      "l2 norm of gradients: 0.6412676536348818\n",
      "l2 norm of weights: 5.10942229829586\n",
      "---------------------\n",
      "Iteration Number: 1202\n",
      "Loss: 52.18209131057841\n",
      "l2 norm of gradients: 0.6410256158738405\n",
      "l2 norm of weights: 5.108069804074428\n",
      "---------------------\n",
      "Iteration Number: 1203\n",
      "Loss: 52.152920192544606\n",
      "l2 norm of gradients: 0.6407836086484426\n",
      "l2 norm of weights: 5.106717872083419\n",
      "---------------------\n",
      "Iteration Number: 1204\n",
      "Loss: 52.12376763324419\n",
      "l2 norm of gradients: 0.6405416319598182\n",
      "l2 norm of weights: 5.1053665022202726\n",
      "---------------------\n",
      "Iteration Number: 1205\n",
      "Loss: 52.09463362180409\n",
      "l2 norm of gradients: 0.6402996858092901\n",
      "l2 norm of weights: 5.104015694382542\n",
      "---------------------\n",
      "Iteration Number: 1206\n",
      "Loss: 52.06551814735747\n",
      "l2 norm of gradients: 0.6400577701983734\n",
      "l2 norm of weights: 5.102665448467898\n",
      "---------------------\n",
      "Iteration Number: 1207\n",
      "Loss: 52.03642119904684\n",
      "l2 norm of gradients: 0.6398158851287766\n",
      "l2 norm of weights: 5.101315764374128\n",
      "---------------------\n",
      "Iteration Number: 1208\n",
      "Loss: 52.00734276601773\n",
      "l2 norm of gradients: 0.6395740306024005\n",
      "l2 norm of weights: 5.099966641999136\n",
      "---------------------\n",
      "Iteration Number: 1209\n",
      "Loss: 51.978282837426256\n",
      "l2 norm of gradients: 0.6393322066213392\n",
      "l2 norm of weights: 5.098618081240941\n",
      "---------------------\n",
      "Iteration Number: 1210\n",
      "Loss: 51.949241402432854\n",
      "l2 norm of gradients: 0.6390904131878797\n",
      "l2 norm of weights: 5.097270081997683\n",
      "---------------------\n",
      "Iteration Number: 1211\n",
      "Loss: 51.92021845020472\n",
      "l2 norm of gradients: 0.6388486503045026\n",
      "l2 norm of weights: 5.095922644167615\n",
      "---------------------\n",
      "Iteration Number: 1212\n",
      "Loss: 51.891213969916556\n",
      "l2 norm of gradients: 0.638606917973881\n",
      "l2 norm of weights: 5.094575767649109\n",
      "---------------------\n",
      "Iteration Number: 1213\n",
      "Loss: 51.86222795074844\n",
      "l2 norm of gradients: 0.638365216198882\n",
      "l2 norm of weights: 5.093229452340652\n",
      "---------------------\n",
      "Iteration Number: 1214\n",
      "Loss: 51.83326038189046\n",
      "l2 norm of gradients: 0.6381235449825658\n",
      "l2 norm of weights: 5.091883698140852\n",
      "---------------------\n",
      "Iteration Number: 1215\n",
      "Loss: 51.80431125253391\n",
      "l2 norm of gradients: 0.6378819043281864\n",
      "l2 norm of weights: 5.090538504948431\n",
      "---------------------\n",
      "Iteration Number: 1216\n",
      "Loss: 51.77538055188236\n",
      "l2 norm of gradients: 0.6376402942391906\n",
      "l2 norm of weights: 5.089193872662231\n",
      "---------------------\n",
      "Iteration Number: 1217\n",
      "Loss: 51.746468269141126\n",
      "l2 norm of gradients: 0.6373987147192192\n",
      "l2 norm of weights: 5.0878498011812106\n",
      "---------------------\n",
      "Iteration Number: 1218\n",
      "Loss: 51.7175743935261\n",
      "l2 norm of gradients: 0.6371571657721062\n",
      "l2 norm of weights: 5.086506290404445\n",
      "---------------------\n",
      "Iteration Number: 1219\n",
      "Loss: 51.68869891425742\n",
      "l2 norm of gradients: 0.6369156474018801\n",
      "l2 norm of weights: 5.08516334023113\n",
      "---------------------\n",
      "Iteration Number: 1220\n",
      "Loss: 51.659841820563145\n",
      "l2 norm of gradients: 0.6366741596127617\n",
      "l2 norm of weights: 5.083820950560577\n",
      "---------------------\n",
      "Iteration Number: 1221\n",
      "Loss: 51.63100310167561\n",
      "l2 norm of gradients: 0.6364327024091665\n",
      "l2 norm of weights: 5.082479121292217\n",
      "---------------------\n",
      "Iteration Number: 1222\n",
      "Loss: 51.602182746837606\n",
      "l2 norm of gradients: 0.6361912757957027\n",
      "l2 norm of weights: 5.081137852325601\n",
      "---------------------\n",
      "Iteration Number: 1223\n",
      "Loss: 51.573380745296156\n",
      "l2 norm of gradients: 0.6359498797771727\n",
      "l2 norm of weights: 5.079797143560396\n",
      "---------------------\n",
      "Iteration Number: 1224\n",
      "Loss: 51.5445970863044\n",
      "l2 norm of gradients: 0.6357085143585721\n",
      "l2 norm of weights: 5.078456994896388\n",
      "---------------------\n",
      "Iteration Number: 1225\n",
      "Loss: 51.51583175912151\n",
      "l2 norm of gradients: 0.6354671795450902\n",
      "l2 norm of weights: 5.077117406233482\n",
      "---------------------\n",
      "Iteration Number: 1226\n",
      "Loss: 51.48708475301836\n",
      "l2 norm of gradients: 0.63522587534211\n",
      "l2 norm of weights: 5.075778377471704\n",
      "---------------------\n",
      "Iteration Number: 1227\n",
      "Loss: 51.458356057265696\n",
      "l2 norm of gradients: 0.6349846017552073\n",
      "l2 norm of weights: 5.0744399085111995\n",
      "---------------------\n",
      "Iteration Number: 1228\n",
      "Loss: 51.42964566114411\n",
      "l2 norm of gradients: 0.634743358790152\n",
      "l2 norm of weights: 5.0731019992522315\n",
      "---------------------\n",
      "Iteration Number: 1229\n",
      "Loss: 51.400953553940134\n",
      "l2 norm of gradients: 0.6345021464529066\n",
      "l2 norm of weights: 5.071764649595183\n",
      "---------------------\n",
      "Iteration Number: 1230\n",
      "Loss: 51.37227972494829\n",
      "l2 norm of gradients: 0.6342609647496276\n",
      "l2 norm of weights: 5.070427859440557\n",
      "---------------------\n",
      "Iteration Number: 1231\n",
      "Loss: 51.343624163468554\n",
      "l2 norm of gradients: 0.6340198136866638\n",
      "l2 norm of weights: 5.0690916286889784\n",
      "---------------------\n",
      "Iteration Number: 1232\n",
      "Loss: 51.31498685880631\n",
      "l2 norm of gradients: 0.6337786932705584\n",
      "l2 norm of weights: 5.067755957241191\n",
      "---------------------\n",
      "Iteration Number: 1233\n",
      "Loss: 51.28636780027429\n",
      "l2 norm of gradients: 0.6335376035080459\n",
      "l2 norm of weights: 5.066420844998061\n",
      "---------------------\n",
      "Iteration Number: 1234\n",
      "Loss: 51.25776697719481\n",
      "l2 norm of gradients: 0.6332965444060551\n",
      "l2 norm of weights: 5.06508629186057\n",
      "---------------------\n",
      "Iteration Number: 1235\n",
      "Loss: 51.229184378890324\n",
      "l2 norm of gradients: 0.6330555159717068\n",
      "l2 norm of weights: 5.063752297729829\n",
      "---------------------\n",
      "Iteration Number: 1236\n",
      "Loss: 51.20061999469579\n",
      "l2 norm of gradients: 0.6328145182123147\n",
      "l2 norm of weights: 5.062418862507065\n",
      "---------------------\n",
      "Iteration Number: 1237\n",
      "Loss: 51.17207381394999\n",
      "l2 norm of gradients: 0.632573551135385\n",
      "l2 norm of weights: 5.061085986093626\n",
      "---------------------\n",
      "Iteration Number: 1238\n",
      "Loss: 51.143545825998785\n",
      "l2 norm of gradients: 0.6323326147486165\n",
      "l2 norm of weights: 5.059753668390984\n",
      "---------------------\n",
      "Iteration Number: 1239\n",
      "Loss: 51.11503602019367\n",
      "l2 norm of gradients: 0.6320917090598994\n",
      "l2 norm of weights: 5.058421909300733\n",
      "---------------------\n",
      "Iteration Number: 1240\n",
      "Loss: 51.08654438589434\n",
      "l2 norm of gradients: 0.6318508340773172\n",
      "l2 norm of weights: 5.05709070872459\n",
      "---------------------\n",
      "Iteration Number: 1241\n",
      "Loss: 51.05807091246432\n",
      "l2 norm of gradients: 0.6316099898091445\n",
      "l2 norm of weights: 5.055760066564391\n",
      "---------------------\n",
      "Iteration Number: 1242\n",
      "Loss: 51.02961558927838\n",
      "l2 norm of gradients: 0.6313691762638476\n",
      "l2 norm of weights: 5.054429982722099\n",
      "---------------------\n",
      "Iteration Number: 1243\n",
      "Loss: 51.00117840571211\n",
      "l2 norm of gradients: 0.6311283934500849\n",
      "l2 norm of weights: 5.053100457099797\n",
      "---------------------\n",
      "Iteration Number: 1244\n",
      "Loss: 50.97275935115232\n",
      "l2 norm of gradients: 0.6308876413767058\n",
      "l2 norm of weights: 5.051771489599695\n",
      "---------------------\n",
      "Iteration Number: 1245\n",
      "Loss: 50.94435841498845\n",
      "l2 norm of gradients: 0.6306469200527512\n",
      "l2 norm of weights: 5.050443080124122\n",
      "---------------------\n",
      "Iteration Number: 1246\n",
      "Loss: 50.915975586619886\n",
      "l2 norm of gradients: 0.6304062294874525\n",
      "l2 norm of weights: 5.049115228575536\n",
      "---------------------\n",
      "Iteration Number: 1247\n",
      "Loss: 50.8876108554499\n",
      "l2 norm of gradients: 0.6301655696902324\n",
      "l2 norm of weights: 5.047787934856515\n",
      "---------------------\n",
      "Iteration Number: 1248\n",
      "Loss: 50.85926421088942\n",
      "l2 norm of gradients: 0.6299249406707039\n",
      "l2 norm of weights: 5.046461198869765\n",
      "---------------------\n",
      "Iteration Number: 1249\n",
      "Loss: 50.83093564235656\n",
      "l2 norm of gradients: 0.6296843424386704\n",
      "l2 norm of weights: 5.0451350205181145\n",
      "---------------------\n",
      "Iteration Number: 1250\n",
      "Loss: 50.802625139274326\n",
      "l2 norm of gradients: 0.6294437750041256\n",
      "l2 norm of weights: 5.043809399704519\n",
      "---------------------\n",
      "Iteration Number: 1251\n",
      "Loss: 50.77433269107342\n",
      "l2 norm of gradients: 0.6292032383772523\n",
      "l2 norm of weights: 5.042484336332057\n",
      "---------------------\n",
      "Iteration Number: 1252\n",
      "Loss: 50.746058287190046\n",
      "l2 norm of gradients: 0.6289627325684242\n",
      "l2 norm of weights: 5.041159830303936\n",
      "---------------------\n",
      "Iteration Number: 1253\n",
      "Loss: 50.717801917068705\n",
      "l2 norm of gradients: 0.6287222575882033\n",
      "l2 norm of weights: 5.039835881523487\n",
      "---------------------\n",
      "Iteration Number: 1254\n",
      "Loss: 50.689563570156416\n",
      "l2 norm of gradients: 0.628481813447341\n",
      "l2 norm of weights: 5.03851248989417\n",
      "---------------------\n",
      "Iteration Number: 1255\n",
      "Loss: 50.66134323591259\n",
      "l2 norm of gradients: 0.6282414001567777\n",
      "l2 norm of weights: 5.03718965531957\n",
      "---------------------\n",
      "Iteration Number: 1256\n",
      "Loss: 50.63314090379746\n",
      "l2 norm of gradients: 0.628001017727642\n",
      "l2 norm of weights: 5.0358673777034\n",
      "---------------------\n",
      "Iteration Number: 1257\n",
      "Loss: 50.60495656328125\n",
      "l2 norm of gradients: 0.6277606661712514\n",
      "l2 norm of weights: 5.034545656949499\n",
      "---------------------\n",
      "Iteration Number: 1258\n",
      "Loss: 50.57679020383967\n",
      "l2 norm of gradients: 0.6275203454991108\n",
      "l2 norm of weights: 5.0332244929618355\n",
      "---------------------\n",
      "Iteration Number: 1259\n",
      "Loss: 50.548641814953776\n",
      "l2 norm of gradients: 0.6272800557229127\n",
      "l2 norm of weights: 5.031903885644507\n",
      "---------------------\n",
      "Iteration Number: 1260\n",
      "Loss: 50.52051138611358\n",
      "l2 norm of gradients: 0.6270397968545374\n",
      "l2 norm of weights: 5.030583834901737\n",
      "---------------------\n",
      "Iteration Number: 1261\n",
      "Loss: 50.492398906813406\n",
      "l2 norm of gradients: 0.6267995689060518\n",
      "l2 norm of weights: 5.029264340637881\n",
      "---------------------\n",
      "Iteration Number: 1262\n",
      "Loss: 50.46430436655396\n",
      "l2 norm of gradients: 0.62655937188971\n",
      "l2 norm of weights: 5.02794540275742\n",
      "---------------------\n",
      "Iteration Number: 1263\n",
      "Loss: 50.436227754845\n",
      "l2 norm of gradients: 0.6263192058179519\n",
      "l2 norm of weights: 5.026627021164968\n",
      "---------------------\n",
      "Iteration Number: 1264\n",
      "Loss: 50.408169061198805\n",
      "l2 norm of gradients: 0.6260790707034036\n",
      "l2 norm of weights: 5.025309195765268\n",
      "---------------------\n",
      "Iteration Number: 1265\n",
      "Loss: 50.380128275137004\n",
      "l2 norm of gradients: 0.6258389665588775\n",
      "l2 norm of weights: 5.023991926463193\n",
      "---------------------\n",
      "Iteration Number: 1266\n",
      "Loss: 50.35210538618734\n",
      "l2 norm of gradients: 0.6255988933973704\n",
      "l2 norm of weights: 5.0226752131637475\n",
      "---------------------\n",
      "Iteration Number: 1267\n",
      "Loss: 50.324100383882275\n",
      "l2 norm of gradients: 0.6253588512320649\n",
      "l2 norm of weights: 5.0213590557720655\n",
      "---------------------\n",
      "Iteration Number: 1268\n",
      "Loss: 50.29611325776324\n",
      "l2 norm of gradients: 0.6251188400763275\n",
      "l2 norm of weights: 5.0200434541934165\n",
      "---------------------\n",
      "Iteration Number: 1269\n",
      "Loss: 50.26814399737705\n",
      "l2 norm of gradients: 0.6248788599437093\n",
      "l2 norm of weights: 5.018728408333198\n",
      "---------------------\n",
      "Iteration Number: 1270\n",
      "Loss: 50.24019259227588\n",
      "l2 norm of gradients: 0.6246389108479452\n",
      "l2 norm of weights: 5.017413918096941\n",
      "---------------------\n",
      "Iteration Number: 1271\n",
      "Loss: 50.21225903202089\n",
      "l2 norm of gradients: 0.6243989928029536\n",
      "l2 norm of weights: 5.01609998339031\n",
      "---------------------\n",
      "Iteration Number: 1272\n",
      "Loss: 50.18434330617565\n",
      "l2 norm of gradients: 0.6241591058228358\n",
      "l2 norm of weights: 5.014786604119103\n",
      "---------------------\n",
      "Iteration Number: 1273\n",
      "Loss: 50.15644540431441\n",
      "l2 norm of gradients: 0.6239192499218761\n",
      "l2 norm of weights: 5.013473780189251\n",
      "---------------------\n",
      "Iteration Number: 1274\n",
      "Loss: 50.128565316015056\n",
      "l2 norm of gradients: 0.6236794251145403\n",
      "l2 norm of weights: 5.012161511506819\n",
      "---------------------\n",
      "Iteration Number: 1275\n",
      "Loss: 50.10070303086442\n",
      "l2 norm of gradients: 0.6234396314154769\n",
      "l2 norm of weights: 5.010849797978007\n",
      "---------------------\n",
      "Iteration Number: 1276\n",
      "Loss: 50.07285853845282\n",
      "l2 norm of gradients: 0.6231998688395152\n",
      "l2 norm of weights: 5.009538639509147\n",
      "---------------------\n",
      "Iteration Number: 1277\n",
      "Loss: 50.04503182838029\n",
      "l2 norm of gradients: 0.6229601374016653\n",
      "l2 norm of weights: 5.008228036006712\n",
      "---------------------\n",
      "Iteration Number: 1278\n",
      "Loss: 50.01722289024983\n",
      "l2 norm of gradients: 0.6227204371171189\n",
      "l2 norm of weights: 5.006917987377306\n",
      "---------------------\n",
      "Iteration Number: 1279\n",
      "Loss: 49.98943171367387\n",
      "l2 norm of gradients: 0.6224807680012463\n",
      "l2 norm of weights: 5.005608493527672\n",
      "---------------------\n",
      "Iteration Number: 1280\n",
      "Loss: 49.961658288268\n",
      "l2 norm of gradients: 0.6222411300695985\n",
      "l2 norm of weights: 5.004299554364687\n",
      "---------------------\n",
      "Iteration Number: 1281\n",
      "Loss: 49.93390260365948\n",
      "l2 norm of gradients: 0.622001523337905\n",
      "l2 norm of weights: 5.002991169795368\n",
      "---------------------\n",
      "Iteration Number: 1282\n",
      "Loss: 49.906164649476494\n",
      "l2 norm of gradients: 0.6217619478220745\n",
      "l2 norm of weights: 5.001683339726867\n",
      "---------------------\n",
      "Iteration Number: 1283\n",
      "Loss: 49.87844441535665\n",
      "l2 norm of gradients: 0.6215224035381937\n",
      "l2 norm of weights: 5.000376064066476\n",
      "---------------------\n",
      "Iteration Number: 1284\n",
      "Loss: 49.85074189094347\n",
      "l2 norm of gradients: 0.6212828905025265\n",
      "l2 norm of weights: 4.999069342721624\n",
      "---------------------\n",
      "Iteration Number: 1285\n",
      "Loss: 49.823057065887056\n",
      "l2 norm of gradients: 0.6210434087315149\n",
      "l2 norm of weights: 4.9977631755998795\n",
      "---------------------\n",
      "Iteration Number: 1286\n",
      "Loss: 49.795389929843665\n",
      "l2 norm of gradients: 0.6208039582417773\n",
      "l2 norm of weights: 4.996457562608951\n",
      "---------------------\n",
      "Iteration Number: 1287\n",
      "Loss: 49.76774047247758\n",
      "l2 norm of gradients: 0.6205645390501082\n",
      "l2 norm of weights: 4.995152503656686\n",
      "---------------------\n",
      "Iteration Number: 1288\n",
      "Loss: 49.740108683455325\n",
      "l2 norm of gradients: 0.6203251511734779\n",
      "l2 norm of weights: 4.993847998651073\n",
      "---------------------\n",
      "Iteration Number: 1289\n",
      "Loss: 49.712494552454466\n",
      "l2 norm of gradients: 0.620085794629032\n",
      "l2 norm of weights: 4.99254404750024\n",
      "---------------------\n",
      "Iteration Number: 1290\n",
      "Loss: 49.6848980691574\n",
      "l2 norm of gradients: 0.6198464694340908\n",
      "l2 norm of weights: 4.9912406501124575\n",
      "---------------------\n",
      "Iteration Number: 1291\n",
      "Loss: 49.657319223251996\n",
      "l2 norm of gradients: 0.6196071756061485\n",
      "l2 norm of weights: 4.989937806396138\n",
      "---------------------\n",
      "Iteration Number: 1292\n",
      "Loss: 49.62975800443386\n",
      "l2 norm of gradients: 0.6193679131628735\n",
      "l2 norm of weights: 4.988635516259835\n",
      "---------------------\n",
      "Iteration Number: 1293\n",
      "Loss: 49.60221440240463\n",
      "l2 norm of gradients: 0.6191286821221067\n",
      "l2 norm of weights: 4.987333779612246\n",
      "---------------------\n",
      "Iteration Number: 1294\n",
      "Loss: 49.57468840687295\n",
      "l2 norm of gradients: 0.6188894825018619\n",
      "l2 norm of weights: 4.986032596362212\n",
      "---------------------\n",
      "Iteration Number: 1295\n",
      "Loss: 49.54718000755258\n",
      "l2 norm of gradients: 0.618650314320325\n",
      "l2 norm of weights: 4.984731966418715\n",
      "---------------------\n",
      "Iteration Number: 1296\n",
      "Loss: 49.51968919416482\n",
      "l2 norm of gradients: 0.6184111775958527\n",
      "l2 norm of weights: 4.983431889690885\n",
      "---------------------\n",
      "Iteration Number: 1297\n",
      "Loss: 49.49221595643704\n",
      "l2 norm of gradients: 0.6181720723469734\n",
      "l2 norm of weights: 4.982132366087994\n",
      "---------------------\n",
      "Iteration Number: 1298\n",
      "Loss: 49.4647602841038\n",
      "l2 norm of gradients: 0.6179329985923854\n",
      "l2 norm of weights: 4.9808333955194595\n",
      "---------------------\n",
      "Iteration Number: 1299\n",
      "Loss: 49.437322166905055\n",
      "l2 norm of gradients: 0.6176939563509565\n",
      "l2 norm of weights: 4.979534977894847\n",
      "---------------------\n",
      "Iteration Number: 1300\n",
      "Loss: 49.40990159458801\n",
      "l2 norm of gradients: 0.6174549456417239\n",
      "l2 norm of weights: 4.978237113123865\n",
      "---------------------\n",
      "Iteration Number: 1301\n",
      "Loss: 49.38249855690691\n",
      "l2 norm of gradients: 0.6172159664838937\n",
      "l2 norm of weights: 4.976939801116372\n",
      "---------------------\n",
      "Iteration Number: 1302\n",
      "Loss: 49.35511304362103\n",
      "l2 norm of gradients: 0.6169770188968392\n",
      "l2 norm of weights: 4.975643041782372\n",
      "---------------------\n",
      "Iteration Number: 1303\n",
      "Loss: 49.327745044497846\n",
      "l2 norm of gradients: 0.6167381029001018\n",
      "l2 norm of weights: 4.974346835032016\n",
      "---------------------\n",
      "Iteration Number: 1304\n",
      "Loss: 49.30039454930763\n",
      "l2 norm of gradients: 0.6164992185133888\n",
      "l2 norm of weights: 4.973051180775605\n",
      "---------------------\n",
      "Iteration Number: 1305\n",
      "Loss: 49.273061547832704\n",
      "l2 norm of gradients: 0.616260365756575\n",
      "l2 norm of weights: 4.971756078923589\n",
      "---------------------\n",
      "Iteration Number: 1306\n",
      "Loss: 49.245746029857116\n",
      "l2 norm of gradients: 0.6160215446496988\n",
      "l2 norm of weights: 4.970461529386564\n",
      "---------------------\n",
      "Iteration Number: 1307\n",
      "Loss: 49.2184479851753\n",
      "l2 norm of gradients: 0.6157827552129651\n",
      "l2 norm of weights: 4.969167532075282\n",
      "---------------------\n",
      "Iteration Number: 1308\n",
      "Loss: 49.191167403584494\n",
      "l2 norm of gradients: 0.6155439974667426\n",
      "l2 norm of weights: 4.9678740869006415\n",
      "---------------------\n",
      "Iteration Number: 1309\n",
      "Loss: 49.16390427489024\n",
      "l2 norm of gradients: 0.6153052714315633\n",
      "l2 norm of weights: 4.966581193773691\n",
      "---------------------\n",
      "Iteration Number: 1310\n",
      "Loss: 49.13665858890627\n",
      "l2 norm of gradients: 0.6150665771281224\n",
      "l2 norm of weights: 4.9652888526056325\n",
      "---------------------\n",
      "Iteration Number: 1311\n",
      "Loss: 49.109430335448614\n",
      "l2 norm of gradients: 0.6148279145772776\n",
      "l2 norm of weights: 4.96399706330782\n",
      "---------------------\n",
      "Iteration Number: 1312\n",
      "Loss: 49.08221950434534\n",
      "l2 norm of gradients: 0.614589283800048\n",
      "l2 norm of weights: 4.96270582579176\n",
      "---------------------\n",
      "Iteration Number: 1313\n",
      "Loss: 49.05502608542473\n",
      "l2 norm of gradients: 0.6143506848176138\n",
      "l2 norm of weights: 4.961415139969112\n",
      "---------------------\n",
      "Iteration Number: 1314\n",
      "Loss: 49.02785006852695\n",
      "l2 norm of gradients: 0.6141121176513158\n",
      "l2 norm of weights: 4.9601250057516895\n",
      "---------------------\n",
      "Iteration Number: 1315\n",
      "Loss: 49.0006914434968\n",
      "l2 norm of gradients: 0.6138735823226541\n",
      "l2 norm of weights: 4.958835423051458\n",
      "---------------------\n",
      "Iteration Number: 1316\n",
      "Loss: 48.97355020018402\n",
      "l2 norm of gradients: 0.6136350788532882\n",
      "l2 norm of weights: 4.957546391780543\n",
      "---------------------\n",
      "Iteration Number: 1317\n",
      "Loss: 48.94642632844715\n",
      "l2 norm of gradients: 0.6133966072650355\n",
      "l2 norm of weights: 4.956257911851219\n",
      "---------------------\n",
      "Iteration Number: 1318\n",
      "Loss: 48.919319818149795\n",
      "l2 norm of gradients: 0.6131581675798716\n",
      "l2 norm of weights: 4.954969983175923\n",
      "---------------------\n",
      "Iteration Number: 1319\n",
      "Loss: 48.8922306591638\n",
      "l2 norm of gradients: 0.6129197598199286\n",
      "l2 norm of weights: 4.953682605667243\n",
      "---------------------\n",
      "Iteration Number: 1320\n",
      "Loss: 48.86515884136656\n",
      "l2 norm of gradients: 0.6126813840074952\n",
      "l2 norm of weights: 4.952395779237927\n",
      "---------------------\n",
      "Iteration Number: 1321\n",
      "Loss: 48.83810435464063\n",
      "l2 norm of gradients: 0.6124430401650152\n",
      "l2 norm of weights: 4.951109503800879\n",
      "---------------------\n",
      "Iteration Number: 1322\n",
      "Loss: 48.81106718887676\n",
      "l2 norm of gradients: 0.6122047283150877\n",
      "l2 norm of weights: 4.949823779269163\n",
      "---------------------\n",
      "Iteration Number: 1323\n",
      "Loss: 48.78404733397288\n",
      "l2 norm of gradients: 0.611966448480466\n",
      "l2 norm of weights: 4.948538605556002\n",
      "---------------------\n",
      "Iteration Number: 1324\n",
      "Loss: 48.757044779832036\n",
      "l2 norm of gradients: 0.6117282006840562\n",
      "l2 norm of weights: 4.947253982574774\n",
      "---------------------\n",
      "Iteration Number: 1325\n",
      "Loss: 48.73005951636443\n",
      "l2 norm of gradients: 0.6114899849489178\n",
      "l2 norm of weights: 4.945969910239024\n",
      "---------------------\n",
      "Iteration Number: 1326\n",
      "Loss: 48.70309153348701\n",
      "l2 norm of gradients: 0.6112518012982623\n",
      "l2 norm of weights: 4.9446863884624515\n",
      "---------------------\n",
      "Iteration Number: 1327\n",
      "Loss: 48.67614082112238\n",
      "l2 norm of gradients: 0.6110136497554518\n",
      "l2 norm of weights: 4.94340341715892\n",
      "---------------------\n",
      "Iteration Number: 1328\n",
      "Loss: 48.64920736920126\n",
      "l2 norm of gradients: 0.6107755303439993\n",
      "l2 norm of weights: 4.942120996242454\n",
      "---------------------\n",
      "Iteration Number: 1329\n",
      "Loss: 48.62229116765912\n",
      "l2 norm of gradients: 0.6105374430875677\n",
      "l2 norm of weights: 4.94083912562724\n",
      "---------------------\n",
      "Iteration Number: 1330\n",
      "Loss: 48.59539220643965\n",
      "l2 norm of gradients: 0.6102993880099691\n",
      "l2 norm of weights: 4.939557805227627\n",
      "---------------------\n",
      "Iteration Number: 1331\n",
      "Loss: 48.56851047549321\n",
      "l2 norm of gradients: 0.6100613651351631\n",
      "l2 norm of weights: 4.938277034958129\n",
      "---------------------\n",
      "Iteration Number: 1332\n",
      "Loss: 48.54164596477534\n",
      "l2 norm of gradients: 0.6098233744872575\n",
      "l2 norm of weights: 4.936996814733423\n",
      "---------------------\n",
      "Iteration Number: 1333\n",
      "Loss: 48.51479866424825\n",
      "l2 norm of gradients: 0.6095854160905069\n",
      "l2 norm of weights: 4.935717144468349\n",
      "---------------------\n",
      "Iteration Number: 1334\n",
      "Loss: 48.48796856388247\n",
      "l2 norm of gradients: 0.6093474899693115\n",
      "l2 norm of weights: 4.934438024077916\n",
      "---------------------\n",
      "Iteration Number: 1335\n",
      "Loss: 48.46115565365369\n",
      "l2 norm of gradients: 0.6091095961482172\n",
      "l2 norm of weights: 4.933159453477295\n",
      "---------------------\n",
      "Iteration Number: 1336\n",
      "Loss: 48.43435992354486\n",
      "l2 norm of gradients: 0.6088717346519138\n",
      "l2 norm of weights: 4.931881432581824\n",
      "---------------------\n",
      "Iteration Number: 1337\n",
      "Loss: 48.40758136354456\n",
      "l2 norm of gradients: 0.6086339055052353\n",
      "l2 norm of weights: 4.9306039613070105\n",
      "---------------------\n",
      "Iteration Number: 1338\n",
      "Loss: 48.380819963650225\n",
      "l2 norm of gradients: 0.6083961087331584\n",
      "l2 norm of weights: 4.929327039568528\n",
      "---------------------\n",
      "Iteration Number: 1339\n",
      "Loss: 48.35407571386397\n",
      "l2 norm of gradients: 0.6081583443608017\n",
      "l2 norm of weights: 4.928050667282217\n",
      "---------------------\n",
      "Iteration Number: 1340\n",
      "Loss: 48.327348604193624\n",
      "l2 norm of gradients: 0.6079206124134259\n",
      "l2 norm of weights: 4.926774844364089\n",
      "---------------------\n",
      "Iteration Number: 1341\n",
      "Loss: 48.300638624657395\n",
      "l2 norm of gradients: 0.6076829129164314\n",
      "l2 norm of weights: 4.925499570730321\n",
      "---------------------\n",
      "Iteration Number: 1342\n",
      "Loss: 48.27394576527654\n",
      "l2 norm of gradients: 0.6074452458953584\n",
      "l2 norm of weights: 4.924224846297266\n",
      "---------------------\n",
      "Iteration Number: 1343\n",
      "Loss: 48.24727001608128\n",
      "l2 norm of gradients: 0.6072076113758869\n",
      "l2 norm of weights: 4.922950670981442\n",
      "---------------------\n",
      "Iteration Number: 1344\n",
      "Loss: 48.22061136710598\n",
      "l2 norm of gradients: 0.6069700093838338\n",
      "l2 norm of weights: 4.921677044699542\n",
      "---------------------\n",
      "Iteration Number: 1345\n",
      "Loss: 48.193969808394826\n",
      "l2 norm of gradients: 0.6067324399451546\n",
      "l2 norm of weights: 4.920403967368426\n",
      "---------------------\n",
      "Iteration Number: 1346\n",
      "Loss: 48.16734532999527\n",
      "l2 norm of gradients: 0.60649490308594\n",
      "l2 norm of weights: 4.919131438905132\n",
      "---------------------\n",
      "Iteration Number: 1347\n",
      "Loss: 48.140737921966036\n",
      "l2 norm of gradients: 0.6062573988324175\n",
      "l2 norm of weights: 4.917859459226868\n",
      "---------------------\n",
      "Iteration Number: 1348\n",
      "Loss: 48.11414757436769\n",
      "l2 norm of gradients: 0.6060199272109488\n",
      "l2 norm of weights: 4.916588028251015\n",
      "---------------------\n",
      "Iteration Number: 1349\n",
      "Loss: 48.08757427726903\n",
      "l2 norm of gradients: 0.6057824882480302\n",
      "l2 norm of weights: 4.91531714589513\n",
      "---------------------\n",
      "Iteration Number: 1350\n",
      "Loss: 48.061018020748676\n",
      "l2 norm of gradients: 0.6055450819702904\n",
      "l2 norm of weights: 4.914046812076944\n",
      "---------------------\n",
      "Iteration Number: 1351\n",
      "Loss: 48.03447879488708\n",
      "l2 norm of gradients: 0.6053077084044912\n",
      "l2 norm of weights: 4.912777026714363\n",
      "---------------------\n",
      "Iteration Number: 1352\n",
      "Loss: 48.007956589774494\n",
      "l2 norm of gradients: 0.6050703675775256\n",
      "l2 norm of weights: 4.911507789725469\n",
      "---------------------\n",
      "Iteration Number: 1353\n",
      "Loss: 47.98145139550749\n",
      "l2 norm of gradients: 0.6048330595164176\n",
      "l2 norm of weights: 4.910239101028522\n",
      "---------------------\n",
      "Iteration Number: 1354\n",
      "Loss: 47.95496320218797\n",
      "l2 norm of gradients: 0.6045957842483205\n",
      "l2 norm of weights: 4.908970960541957\n",
      "---------------------\n",
      "Iteration Number: 1355\n",
      "Loss: 47.928491999927154\n",
      "l2 norm of gradients: 0.6043585418005172\n",
      "l2 norm of weights: 4.9077033681843885\n",
      "---------------------\n",
      "Iteration Number: 1356\n",
      "Loss: 47.90203777883963\n",
      "l2 norm of gradients: 0.6041213322004184\n",
      "l2 norm of weights: 4.906436323874609\n",
      "---------------------\n",
      "Iteration Number: 1357\n",
      "Loss: 47.87560052905078\n",
      "l2 norm of gradients: 0.603884155475562\n",
      "l2 norm of weights: 4.905169827531591\n",
      "---------------------\n",
      "Iteration Number: 1358\n",
      "Loss: 47.8491802406884\n",
      "l2 norm of gradients: 0.6036470116536129\n",
      "l2 norm of weights: 4.903903879074484\n",
      "---------------------\n",
      "Iteration Number: 1359\n",
      "Loss: 47.82277690388965\n",
      "l2 norm of gradients: 0.6034099007623609\n",
      "l2 norm of weights: 4.902638478422621\n",
      "---------------------\n",
      "Iteration Number: 1360\n",
      "Loss: 47.79639050879897\n",
      "l2 norm of gradients: 0.6031728228297208\n",
      "l2 norm of weights: 4.901373625495514\n",
      "---------------------\n",
      "Iteration Number: 1361\n",
      "Loss: 47.77002104556529\n",
      "l2 norm of gradients: 0.6029357778837315\n",
      "l2 norm of weights: 4.900109320212856\n",
      "---------------------\n",
      "Iteration Number: 1362\n",
      "Loss: 47.743668504346275\n",
      "l2 norm of gradients: 0.6026987659525544\n",
      "l2 norm of weights: 4.898845562494525\n",
      "---------------------\n",
      "Iteration Number: 1363\n",
      "Loss: 47.71733287530609\n",
      "l2 norm of gradients: 0.6024617870644734\n",
      "l2 norm of weights: 4.897582352260579\n",
      "---------------------\n",
      "Iteration Number: 1364\n",
      "Loss: 47.69101414861428\n",
      "l2 norm of gradients: 0.6022248412478932\n",
      "l2 norm of weights: 4.89631968943126\n",
      "---------------------\n",
      "Iteration Number: 1365\n",
      "Loss: 47.66471231444968\n",
      "l2 norm of gradients: 0.6019879285313392\n",
      "l2 norm of weights: 4.895057573926995\n",
      "---------------------\n",
      "Iteration Number: 1366\n",
      "Loss: 47.63842736299427\n",
      "l2 norm of gradients: 0.6017510489434561\n",
      "l2 norm of weights: 4.893796005668394\n",
      "---------------------\n",
      "Iteration Number: 1367\n",
      "Loss: 47.61215928443968\n",
      "l2 norm of gradients: 0.6015142025130071\n",
      "l2 norm of weights: 4.892534984576254\n",
      "---------------------\n",
      "Iteration Number: 1368\n",
      "Loss: 47.585908068985546\n",
      "l2 norm of gradients: 0.6012773892688731\n",
      "l2 norm of weights: 4.891274510571556\n",
      "---------------------\n",
      "Iteration Number: 1369\n",
      "Loss: 47.559673706834516\n",
      "l2 norm of gradients: 0.6010406092400516\n",
      "l2 norm of weights: 4.890014583575469\n",
      "---------------------\n",
      "Iteration Number: 1370\n",
      "Loss: 47.53345618819858\n",
      "l2 norm of gradients: 0.6008038624556566\n",
      "l2 norm of weights: 4.888755203509349\n",
      "---------------------\n",
      "Iteration Number: 1371\n",
      "Loss: 47.50725550329711\n",
      "l2 norm of gradients: 0.6005671489449163\n",
      "l2 norm of weights: 4.8874963702947385\n",
      "---------------------\n",
      "Iteration Number: 1372\n",
      "Loss: 47.481071642353136\n",
      "l2 norm of gradients: 0.6003304687371734\n",
      "l2 norm of weights: 4.886238083853368\n",
      "---------------------\n",
      "Iteration Number: 1373\n",
      "Loss: 47.45490459559998\n",
      "l2 norm of gradients: 0.6000938218618835\n",
      "l2 norm of weights: 4.88498034410716\n",
      "---------------------\n",
      "Iteration Number: 1374\n",
      "Loss: 47.42875435327592\n",
      "l2 norm of gradients: 0.5998572083486151\n",
      "l2 norm of weights: 4.883723150978223\n",
      "---------------------\n",
      "Iteration Number: 1375\n",
      "Loss: 47.402620905627685\n",
      "l2 norm of gradients: 0.5996206282270471\n",
      "l2 norm of weights: 4.882466504388858\n",
      "---------------------\n",
      "Iteration Number: 1376\n",
      "Loss: 47.376504242906194\n",
      "l2 norm of gradients: 0.5993840815269695\n",
      "l2 norm of weights: 4.881210404261555\n",
      "---------------------\n",
      "Iteration Number: 1377\n",
      "Loss: 47.350404355373485\n",
      "l2 norm of gradients: 0.5991475682782818\n",
      "l2 norm of weights: 4.879954850518996\n",
      "---------------------\n",
      "Iteration Number: 1378\n",
      "Loss: 47.324321233292984\n",
      "l2 norm of gradients: 0.5989110885109915\n",
      "l2 norm of weights: 4.878699843084058\n",
      "---------------------\n",
      "Iteration Number: 1379\n",
      "Loss: 47.29825486693929\n",
      "l2 norm of gradients: 0.5986746422552146\n",
      "l2 norm of weights: 4.877445381879803\n",
      "---------------------\n",
      "Iteration Number: 1380\n",
      "Loss: 47.27220524659269\n",
      "l2 norm of gradients: 0.5984382295411731\n",
      "l2 norm of weights: 4.8761914668294954\n",
      "---------------------\n",
      "Iteration Number: 1381\n",
      "Loss: 47.24617236254037\n",
      "l2 norm of gradients: 0.5982018503991952\n",
      "l2 norm of weights: 4.874938097856587\n",
      "---------------------\n",
      "Iteration Number: 1382\n",
      "Loss: 47.220156205076584\n",
      "l2 norm of gradients: 0.5979655048597141\n",
      "l2 norm of weights: 4.873685274884727\n",
      "---------------------\n",
      "Iteration Number: 1383\n",
      "Loss: 47.19415676450025\n",
      "l2 norm of gradients: 0.5977291929532668\n",
      "l2 norm of weights: 4.872432997837757\n",
      "---------------------\n",
      "Iteration Number: 1384\n",
      "Loss: 47.16817403112246\n",
      "l2 norm of gradients: 0.5974929147104935\n",
      "l2 norm of weights: 4.871181266639719\n",
      "---------------------\n",
      "Iteration Number: 1385\n",
      "Loss: 47.14220799525477\n",
      "l2 norm of gradients: 0.5972566701621359\n",
      "l2 norm of weights: 4.8699300812148465\n",
      "---------------------\n",
      "Iteration Number: 1386\n",
      "Loss: 47.116258647221\n",
      "l2 norm of gradients: 0.5970204593390378\n",
      "l2 norm of weights: 4.868679441487573\n",
      "---------------------\n",
      "Iteration Number: 1387\n",
      "Loss: 47.09032597734867\n",
      "l2 norm of gradients: 0.5967842822721424\n",
      "l2 norm of weights: 4.867429347382526\n",
      "---------------------\n",
      "Iteration Number: 1388\n",
      "Loss: 47.06440997597423\n",
      "l2 norm of gradients: 0.5965481389924927\n",
      "l2 norm of weights: 4.866179798824535\n",
      "---------------------\n",
      "Iteration Number: 1389\n",
      "Loss: 47.038510633439834\n",
      "l2 norm of gradients: 0.5963120295312295\n",
      "l2 norm of weights: 4.864930795738628\n",
      "---------------------\n",
      "Iteration Number: 1390\n",
      "Loss: 47.012627940095115\n",
      "l2 norm of gradients: 0.5960759539195915\n",
      "l2 norm of weights: 4.863682338050028\n",
      "---------------------\n",
      "Iteration Number: 1391\n",
      "Loss: 46.98676188629684\n",
      "l2 norm of gradients: 0.5958399121889135\n",
      "l2 norm of weights: 4.862434425684163\n",
      "---------------------\n",
      "Iteration Number: 1392\n",
      "Loss: 46.96091246240885\n",
      "l2 norm of gradients: 0.595603904370626\n",
      "l2 norm of weights: 4.861187058566658\n",
      "---------------------\n",
      "Iteration Number: 1393\n",
      "Loss: 46.935079658801136\n",
      "l2 norm of gradients: 0.5953679304962539\n",
      "l2 norm of weights: 4.859940236623343\n",
      "---------------------\n",
      "Iteration Number: 1394\n",
      "Loss: 46.90926346585129\n",
      "l2 norm of gradients: 0.5951319905974156\n",
      "l2 norm of weights: 4.858693959780244\n",
      "---------------------\n",
      "Iteration Number: 1395\n",
      "Loss: 46.883463873944386\n",
      "l2 norm of gradients: 0.5948960847058223\n",
      "l2 norm of weights: 4.857448227963595\n",
      "---------------------\n",
      "Iteration Number: 1396\n",
      "Loss: 46.8576808734718\n",
      "l2 norm of gradients: 0.5946602128532769\n",
      "l2 norm of weights: 4.85620304109983\n",
      "---------------------\n",
      "Iteration Number: 1397\n",
      "Loss: 46.831914454832884\n",
      "l2 norm of gradients: 0.5944243750716727\n",
      "l2 norm of weights: 4.8549583991155885\n",
      "---------------------\n",
      "Iteration Number: 1398\n",
      "Loss: 46.8061646084314\n",
      "l2 norm of gradients: 0.5941885713929929\n",
      "l2 norm of weights: 4.85371430193771\n",
      "---------------------\n",
      "Iteration Number: 1399\n",
      "Loss: 46.78043132468188\n",
      "l2 norm of gradients: 0.5939528018493094\n",
      "l2 norm of weights: 4.852470749493244\n",
      "---------------------\n",
      "Iteration Number: 1400\n",
      "Loss: 46.75471459400373\n",
      "l2 norm of gradients: 0.5937170664727821\n",
      "l2 norm of weights: 4.851227741709441\n",
      "---------------------\n",
      "Iteration Number: 1401\n",
      "Loss: 46.7290144068231\n",
      "l2 norm of gradients: 0.5934813652956576\n",
      "l2 norm of weights: 4.849985278513762\n",
      "---------------------\n",
      "Iteration Number: 1402\n",
      "Loss: 46.7033307535746\n",
      "l2 norm of gradients: 0.5932456983502684\n",
      "l2 norm of weights: 4.84874335983387\n",
      "---------------------\n",
      "Iteration Number: 1403\n",
      "Loss: 46.677663624699136\n",
      "l2 norm of gradients: 0.5930100656690318\n",
      "l2 norm of weights: 4.847501985597637\n",
      "---------------------\n",
      "Iteration Number: 1404\n",
      "Loss: 46.652013010645234\n",
      "l2 norm of gradients: 0.5927744672844492\n",
      "l2 norm of weights: 4.846261155733143\n",
      "---------------------\n",
      "Iteration Number: 1405\n",
      "Loss: 46.626378901866794\n",
      "l2 norm of gradients: 0.5925389032291046\n",
      "l2 norm of weights: 4.845020870168677\n",
      "---------------------\n",
      "Iteration Number: 1406\n",
      "Loss: 46.60076128882668\n",
      "l2 norm of gradients: 0.5923033735356645\n",
      "l2 norm of weights: 4.843781128832736\n",
      "---------------------\n",
      "Iteration Number: 1407\n",
      "Loss: 46.57516016199552\n",
      "l2 norm of gradients: 0.5920678782368761\n",
      "l2 norm of weights: 4.842541931654026\n",
      "---------------------\n",
      "Iteration Number: 1408\n",
      "Loss: 46.54957551184745\n",
      "l2 norm of gradients: 0.5918324173655668\n",
      "l2 norm of weights: 4.841303278561463\n",
      "---------------------\n",
      "Iteration Number: 1409\n",
      "Loss: 46.52400732886784\n",
      "l2 norm of gradients: 0.5915969909546426\n",
      "l2 norm of weights: 4.840065169484174\n",
      "---------------------\n",
      "Iteration Number: 1410\n",
      "Loss: 46.498455603546674\n",
      "l2 norm of gradients: 0.591361599037088\n",
      "l2 norm of weights: 4.8388276043515\n",
      "---------------------\n",
      "Iteration Number: 1411\n",
      "Loss: 46.47292032638295\n",
      "l2 norm of gradients: 0.5911262416459646\n",
      "l2 norm of weights: 4.837590583092989\n",
      "---------------------\n",
      "Iteration Number: 1412\n",
      "Loss: 46.44740148788016\n",
      "l2 norm of gradients: 0.5908909188144098\n",
      "l2 norm of weights: 4.836354105638404\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 1413\n",
      "Loss: 46.42189907855173\n",
      "l2 norm of gradients: 0.5906556305756365\n",
      "l2 norm of weights: 4.8351181719177205\n",
      "---------------------\n",
      "Iteration Number: 1414\n",
      "Loss: 46.39641308891713\n",
      "l2 norm of gradients: 0.5904203769629313\n",
      "l2 norm of weights: 4.83388278186113\n",
      "---------------------\n",
      "Iteration Number: 1415\n",
      "Loss: 46.37094350950112\n",
      "l2 norm of gradients: 0.5901851580096541\n",
      "l2 norm of weights: 4.8326479353990335\n",
      "---------------------\n",
      "Iteration Number: 1416\n",
      "Loss: 46.34549033083983\n",
      "l2 norm of gradients: 0.5899499737492374\n",
      "l2 norm of weights: 4.831413632462049\n",
      "---------------------\n",
      "Iteration Number: 1417\n",
      "Loss: 46.32005354347268\n",
      "l2 norm of gradients: 0.5897148242151842\n",
      "l2 norm of weights: 4.830179872981012\n",
      "---------------------\n",
      "Iteration Number: 1418\n",
      "Loss: 46.294633137947734\n",
      "l2 norm of gradients: 0.5894797094410681\n",
      "l2 norm of weights: 4.82894665688697\n",
      "---------------------\n",
      "Iteration Number: 1419\n",
      "Loss: 46.26922910482113\n",
      "l2 norm of gradients: 0.5892446294605316\n",
      "l2 norm of weights: 4.82771398411119\n",
      "---------------------\n",
      "Iteration Number: 1420\n",
      "Loss: 46.243841434654634\n",
      "l2 norm of gradients: 0.5890095843072858\n",
      "l2 norm of weights: 4.826481854585153\n",
      "---------------------\n",
      "Iteration Number: 1421\n",
      "Loss: 46.218470118018146\n",
      "l2 norm of gradients: 0.5887745740151089\n",
      "l2 norm of weights: 4.82525026824056\n",
      "---------------------\n",
      "Iteration Number: 1422\n",
      "Loss: 46.19311514548806\n",
      "l2 norm of gradients: 0.588539598617845\n",
      "l2 norm of weights: 4.824019225009331\n",
      "---------------------\n",
      "Iteration Number: 1423\n",
      "Loss: 46.16777650765114\n",
      "l2 norm of gradients: 0.5883046581494037\n",
      "l2 norm of weights: 4.822788724823599\n",
      "---------------------\n",
      "Iteration Number: 1424\n",
      "Loss: 46.14245419509574\n",
      "l2 norm of gradients: 0.5880697526437588\n",
      "l2 norm of weights: 4.821558767615725\n",
      "---------------------\n",
      "Iteration Number: 1425\n",
      "Loss: 46.11714819842199\n",
      "l2 norm of gradients: 0.5878348821349473\n",
      "l2 norm of weights: 4.820329353318282\n",
      "---------------------\n",
      "Iteration Number: 1426\n",
      "Loss: 46.09185850823607\n",
      "l2 norm of gradients: 0.5876000466570684\n",
      "l2 norm of weights: 4.8191004818640675\n",
      "---------------------\n",
      "Iteration Number: 1427\n",
      "Loss: 46.066585115149714\n",
      "l2 norm of gradients: 0.5873652462442828\n",
      "l2 norm of weights: 4.817872153186097\n",
      "---------------------\n",
      "Iteration Number: 1428\n",
      "Loss: 46.041328009785076\n",
      "l2 norm of gradients: 0.587130480930811\n",
      "l2 norm of weights: 4.816644367217611\n",
      "---------------------\n",
      "Iteration Number: 1429\n",
      "Loss: 46.01608718277049\n",
      "l2 norm of gradients: 0.5868957507509331\n",
      "l2 norm of weights: 4.81541712389207\n",
      "---------------------\n",
      "Iteration Number: 1430\n",
      "Loss: 45.99086262473884\n",
      "l2 norm of gradients: 0.5866610557389874\n",
      "l2 norm of weights: 4.814190423143157\n",
      "---------------------\n",
      "Iteration Number: 1431\n",
      "Loss: 45.965654326333684\n",
      "l2 norm of gradients: 0.5864263959293693\n",
      "l2 norm of weights: 4.812964264904778\n",
      "---------------------\n",
      "Iteration Number: 1432\n",
      "Loss: 45.94046227820579\n",
      "l2 norm of gradients: 0.5861917713565308\n",
      "l2 norm of weights: 4.811738649111063\n",
      "---------------------\n",
      "Iteration Number: 1433\n",
      "Loss: 45.91528647100998\n",
      "l2 norm of gradients: 0.5859571820549787\n",
      "l2 norm of weights: 4.810513575696366\n",
      "---------------------\n",
      "Iteration Number: 1434\n",
      "Loss: 45.89012689541186\n",
      "l2 norm of gradients: 0.5857226280592743\n",
      "l2 norm of weights: 4.809289044595266\n",
      "---------------------\n",
      "Iteration Number: 1435\n",
      "Loss: 45.86498354208239\n",
      "l2 norm of gradients: 0.5854881094040325\n",
      "l2 norm of weights: 4.808065055742566\n",
      "---------------------\n",
      "Iteration Number: 1436\n",
      "Loss: 45.83985640170249\n",
      "l2 norm of gradients: 0.5852536261239195\n",
      "l2 norm of weights: 4.806841609073298\n",
      "---------------------\n",
      "Iteration Number: 1437\n",
      "Loss: 45.814745464955735\n",
      "l2 norm of gradients: 0.5850191782536537\n",
      "l2 norm of weights: 4.805618704522715\n",
      "---------------------\n",
      "Iteration Number: 1438\n",
      "Loss: 45.789650722538425\n",
      "l2 norm of gradients: 0.5847847658280034\n",
      "l2 norm of weights: 4.804396342026302\n",
      "---------------------\n",
      "Iteration Number: 1439\n",
      "Loss: 45.76457216514947\n",
      "l2 norm of gradients: 0.584550388881786\n",
      "l2 norm of weights: 4.803174521519769\n",
      "---------------------\n",
      "Iteration Number: 1440\n",
      "Loss: 45.739509783498306\n",
      "l2 norm of gradients: 0.5843160474498675\n",
      "l2 norm of weights: 4.801953242939052\n",
      "---------------------\n",
      "Iteration Number: 1441\n",
      "Loss: 45.714463568300964\n",
      "l2 norm of gradients: 0.584081741567161\n",
      "l2 norm of weights: 4.80073250622032\n",
      "---------------------\n",
      "Iteration Number: 1442\n",
      "Loss: 45.689433510281376\n",
      "l2 norm of gradients: 0.5838474712686255\n",
      "l2 norm of weights: 4.799512311299966\n",
      "---------------------\n",
      "Iteration Number: 1443\n",
      "Loss: 45.66441960016794\n",
      "l2 norm of gradients: 0.5836132365892659\n",
      "l2 norm of weights: 4.798292658114618\n",
      "---------------------\n",
      "Iteration Number: 1444\n",
      "Loss: 45.63942182870133\n",
      "l2 norm of gradients: 0.5833790375641308\n",
      "l2 norm of weights: 4.797073546601127\n",
      "---------------------\n",
      "Iteration Number: 1445\n",
      "Loss: 45.61444018662448\n",
      "l2 norm of gradients: 0.5831448742283124\n",
      "l2 norm of weights: 4.79585497669658\n",
      "---------------------\n",
      "Iteration Number: 1446\n",
      "Loss: 45.58947466469133\n",
      "l2 norm of gradients: 0.5829107466169448\n",
      "l2 norm of weights: 4.794636948338293\n",
      "---------------------\n",
      "Iteration Number: 1447\n",
      "Loss: 45.564525253663305\n",
      "l2 norm of gradients: 0.5826766547652037\n",
      "l2 norm of weights: 4.793419461463812\n",
      "---------------------\n",
      "Iteration Number: 1448\n",
      "Loss: 45.53959194430595\n",
      "l2 norm of gradients: 0.5824425987083047\n",
      "l2 norm of weights: 4.792202516010918\n",
      "---------------------\n",
      "Iteration Number: 1449\n",
      "Loss: 45.514674727396574\n",
      "l2 norm of gradients: 0.582208578481503\n",
      "l2 norm of weights: 4.790986111917622\n",
      "---------------------\n",
      "Iteration Number: 1450\n",
      "Loss: 45.48977359371665\n",
      "l2 norm of gradients: 0.5819745941200919\n",
      "l2 norm of weights: 4.789770249122169\n",
      "---------------------\n",
      "Iteration Number: 1451\n",
      "Loss: 45.46488853405647\n",
      "l2 norm of gradients: 0.5817406456594015\n",
      "l2 norm of weights: 4.788554927563037\n",
      "---------------------\n",
      "Iteration Number: 1452\n",
      "Loss: 45.44001953921302\n",
      "l2 norm of gradients: 0.581506733134799\n",
      "l2 norm of weights: 4.787340147178937\n",
      "---------------------\n",
      "Iteration Number: 1453\n",
      "Loss: 45.415166599992126\n",
      "l2 norm of gradients: 0.5812728565816858\n",
      "l2 norm of weights: 4.786125907908818\n",
      "---------------------\n",
      "Iteration Number: 1454\n",
      "Loss: 45.390329707205495\n",
      "l2 norm of gradients: 0.5810390160354985\n",
      "l2 norm of weights: 4.784912209691859\n",
      "---------------------\n",
      "Iteration Number: 1455\n",
      "Loss: 45.36550885167369\n",
      "l2 norm of gradients: 0.5808052115317062\n",
      "l2 norm of weights: 4.783699052467479\n",
      "---------------------\n",
      "Iteration Number: 1456\n",
      "Loss: 45.34070402422522\n",
      "l2 norm of gradients: 0.5805714431058109\n",
      "l2 norm of weights: 4.782486436175327\n",
      "---------------------\n",
      "Iteration Number: 1457\n",
      "Loss: 45.315915215692925\n",
      "l2 norm of gradients: 0.5803377107933451\n",
      "l2 norm of weights: 4.7812743607552965\n",
      "---------------------\n",
      "Iteration Number: 1458\n",
      "Loss: 45.29114241692092\n",
      "l2 norm of gradients: 0.5801040146298719\n",
      "l2 norm of weights: 4.78006282614751\n",
      "---------------------\n",
      "Iteration Number: 1459\n",
      "Loss: 45.26638561875891\n",
      "l2 norm of gradients: 0.5798703546509839\n",
      "l2 norm of weights: 4.7788518322923315\n",
      "---------------------\n",
      "Iteration Number: 1460\n",
      "Loss: 45.241644812064166\n",
      "l2 norm of gradients: 0.5796367308923014\n",
      "l2 norm of weights: 4.777641379130362\n",
      "---------------------\n",
      "Iteration Number: 1461\n",
      "Loss: 45.21691998770122\n",
      "l2 norm of gradients: 0.5794031433894724\n",
      "l2 norm of weights: 4.776431466602439\n",
      "---------------------\n",
      "Iteration Number: 1462\n",
      "Loss: 45.192211136545055\n",
      "l2 norm of gradients: 0.5791695921781705\n",
      "l2 norm of weights: 4.775222094649642\n",
      "---------------------\n",
      "Iteration Number: 1463\n",
      "Loss: 45.16751824947342\n",
      "l2 norm of gradients: 0.5789360772940955\n",
      "l2 norm of weights: 4.774013263213287\n",
      "---------------------\n",
      "Iteration Number: 1464\n",
      "Loss: 45.14284131737496\n",
      "l2 norm of gradients: 0.5787025987729706\n",
      "l2 norm of weights: 4.772804972234929\n",
      "---------------------\n",
      "Iteration Number: 1465\n",
      "Loss: 45.118180331145986\n",
      "l2 norm of gradients: 0.5784691566505427\n",
      "l2 norm of weights: 4.771597221656365\n",
      "---------------------\n",
      "Iteration Number: 1466\n",
      "Loss: 45.09353528168758\n",
      "l2 norm of gradients: 0.5782357509625805\n",
      "l2 norm of weights: 4.770390011419633\n",
      "---------------------\n",
      "Iteration Number: 1467\n",
      "Loss: 45.06890615991225\n",
      "l2 norm of gradients: 0.5780023817448747\n",
      "l2 norm of weights: 4.769183341467009\n",
      "---------------------\n",
      "Iteration Number: 1468\n",
      "Loss: 45.04429295673596\n",
      "l2 norm of gradients: 0.5777690490332356\n",
      "l2 norm of weights: 4.767977211741013\n",
      "---------------------\n",
      "Iteration Number: 1469\n",
      "Loss: 45.01969566308655\n",
      "l2 norm of gradients: 0.5775357528634929\n",
      "l2 norm of weights: 4.766771622184406\n",
      "---------------------\n",
      "Iteration Number: 1470\n",
      "Loss: 44.99511426989628\n",
      "l2 norm of gradients: 0.577302493271495\n",
      "l2 norm of weights: 4.765566572740189\n",
      "---------------------\n",
      "Iteration Number: 1471\n",
      "Loss: 44.970548768105864\n",
      "l2 norm of gradients: 0.5770692702931068\n",
      "l2 norm of weights: 4.7643620633516095\n",
      "---------------------\n",
      "Iteration Number: 1472\n",
      "Loss: 44.945999148664576\n",
      "l2 norm of gradients: 0.5768360839642106\n",
      "l2 norm of weights: 4.763158093962156\n",
      "---------------------\n",
      "Iteration Number: 1473\n",
      "Loss: 44.921465402528625\n",
      "l2 norm of gradients: 0.5766029343207029\n",
      "l2 norm of weights: 4.761954664515561\n",
      "---------------------\n",
      "Iteration Number: 1474\n",
      "Loss: 44.89694752066106\n",
      "l2 norm of gradients: 0.5763698213984949\n",
      "l2 norm of weights: 4.7607517749558\n",
      "---------------------\n",
      "Iteration Number: 1475\n",
      "Loss: 44.87244549403308\n",
      "l2 norm of gradients: 0.5761367452335115\n",
      "l2 norm of weights: 4.759549425227094\n",
      "---------------------\n",
      "Iteration Number: 1476\n",
      "Loss: 44.847959313625715\n",
      "l2 norm of gradients: 0.5759037058616899\n",
      "l2 norm of weights: 4.758347615273908\n",
      "---------------------\n",
      "Iteration Number: 1477\n",
      "Loss: 44.823488970424\n",
      "l2 norm of gradients: 0.575670703318978\n",
      "l2 norm of weights: 4.757146345040954\n",
      "---------------------\n",
      "Iteration Number: 1478\n",
      "Loss: 44.79903445542362\n",
      "l2 norm of gradients: 0.5754377376413343\n",
      "l2 norm of weights: 4.755945614473185\n",
      "---------------------\n",
      "Iteration Number: 1479\n",
      "Loss: 44.774595759625555\n",
      "l2 norm of gradients: 0.5752048088647276\n",
      "l2 norm of weights: 4.754745423515806\n",
      "---------------------\n",
      "Iteration Number: 1480\n",
      "Loss: 44.75017287404021\n",
      "l2 norm of gradients: 0.5749719170251337\n",
      "l2 norm of weights: 4.753545772114265\n",
      "---------------------\n",
      "Iteration Number: 1481\n",
      "Loss: 44.72576578968532\n",
      "l2 norm of gradients: 0.5747390621585368\n",
      "l2 norm of weights: 4.752346660214256\n",
      "---------------------\n",
      "Iteration Number: 1482\n",
      "Loss: 44.70137449758638\n",
      "l2 norm of gradients: 0.5745062443009271\n",
      "l2 norm of weights: 4.751148087761723\n",
      "---------------------\n",
      "Iteration Number: 1483\n",
      "Loss: 44.67699898877555\n",
      "l2 norm of gradients: 0.5742734634883006\n",
      "l2 norm of weights: 4.749950054702855\n",
      "---------------------\n",
      "Iteration Number: 1484\n",
      "Loss: 44.6526392542935\n",
      "l2 norm of gradients: 0.5740407197566574\n",
      "l2 norm of weights: 4.748752560984091\n",
      "---------------------\n",
      "Iteration Number: 1485\n",
      "Loss: 44.62829528518917\n",
      "l2 norm of gradients: 0.5738080131420012\n",
      "l2 norm of weights: 4.747555606552116\n",
      "---------------------\n",
      "Iteration Number: 1486\n",
      "Loss: 44.603967072518465\n",
      "l2 norm of gradients: 0.5735753436803386\n",
      "l2 norm of weights: 4.746359191353867\n",
      "---------------------\n",
      "Iteration Number: 1487\n",
      "Loss: 44.579654607345546\n",
      "l2 norm of gradients: 0.5733427114076773\n",
      "l2 norm of weights: 4.745163315336527\n",
      "---------------------\n",
      "Iteration Number: 1488\n",
      "Loss: 44.55535788074113\n",
      "l2 norm of gradients: 0.5731101163600256\n",
      "l2 norm of weights: 4.743967978447531\n",
      "---------------------\n",
      "Iteration Number: 1489\n",
      "Loss: 44.53107688378507\n",
      "l2 norm of gradients: 0.5728775585733917\n",
      "l2 norm of weights: 4.74277318063456\n",
      "---------------------\n",
      "Iteration Number: 1490\n",
      "Loss: 44.50681160756494\n",
      "l2 norm of gradients: 0.5726450380837822\n",
      "l2 norm of weights: 4.7415789218455515\n",
      "---------------------\n",
      "Iteration Number: 1491\n",
      "Loss: 44.48256204317465\n",
      "l2 norm of gradients: 0.5724125549272013\n",
      "l2 norm of weights: 4.740385202028687\n",
      "---------------------\n",
      "Iteration Number: 1492\n",
      "Loss: 44.458328181717135\n",
      "l2 norm of gradients: 0.5721801091396502\n",
      "l2 norm of weights: 4.7391920211324035\n",
      "---------------------\n",
      "Iteration Number: 1493\n",
      "Loss: 44.434110014302796\n",
      "l2 norm of gradients: 0.5719477007571258\n",
      "l2 norm of weights: 4.7379993791053865\n",
      "---------------------\n",
      "Iteration Number: 1494\n",
      "Loss: 44.40990753205067\n",
      "l2 norm of gradients: 0.5717153298156193\n",
      "l2 norm of weights: 4.736807275896576\n",
      "---------------------\n",
      "Iteration Number: 1495\n",
      "Loss: 44.385720726086454\n",
      "l2 norm of gradients: 0.5714829963511161\n",
      "l2 norm of weights: 4.73561571145516\n",
      "---------------------\n",
      "Iteration Number: 1496\n",
      "Loss: 44.361549587542264\n",
      "l2 norm of gradients: 0.5712507003995944\n",
      "l2 norm of weights: 4.734424685730583\n",
      "---------------------\n",
      "Iteration Number: 1497\n",
      "Loss: 44.337394107561934\n",
      "l2 norm of gradients: 0.571018441997024\n",
      "l2 norm of weights: 4.733234198672539\n",
      "---------------------\n",
      "Iteration Number: 1498\n",
      "Loss: 44.313254277293275\n",
      "l2 norm of gradients: 0.5707862211793662\n",
      "l2 norm of weights: 4.732044250230979\n",
      "---------------------\n",
      "Iteration Number: 1499\n",
      "Loss: 44.28913008789479\n",
      "l2 norm of gradients: 0.5705540379825714\n",
      "l2 norm of weights: 4.730854840356102\n",
      "---------------------\n",
      "Iteration Number: 1500\n",
      "Loss: 44.2650215305305\n",
      "l2 norm of gradients: 0.5703218924425799\n",
      "l2 norm of weights: 4.729665968998365\n",
      "---------------------\n",
      "Iteration Number: 1501\n",
      "Loss: 44.24092859637467\n",
      "l2 norm of gradients: 0.5700897845953193\n",
      "l2 norm of weights: 4.728477636108476\n",
      "---------------------\n",
      "Iteration Number: 1502\n",
      "Loss: 44.216851276606064\n",
      "l2 norm of gradients: 0.5698577144767049\n",
      "l2 norm of weights: 4.7272898416374\n",
      "---------------------\n",
      "Iteration Number: 1503\n",
      "Loss: 44.19278956241517\n",
      "l2 norm of gradients: 0.5696256821226374\n",
      "l2 norm of weights: 4.7261025855363545\n",
      "---------------------\n",
      "Iteration Number: 1504\n",
      "Loss: 44.168743444996934\n",
      "l2 norm of gradients: 0.5693936875690033\n",
      "l2 norm of weights: 4.724915867756814\n",
      "---------------------\n",
      "Iteration Number: 1505\n",
      "Loss: 44.14471291555665\n",
      "l2 norm of gradients: 0.5691617308516734\n",
      "l2 norm of weights: 4.723729688250508\n",
      "---------------------\n",
      "Iteration Number: 1506\n",
      "Loss: 44.12069796530628\n",
      "l2 norm of gradients: 0.5689298120065014\n",
      "l2 norm of weights: 4.722544046969418\n",
      "---------------------\n",
      "Iteration Number: 1507\n",
      "Loss: 44.09669858546619\n",
      "l2 norm of gradients: 0.5686979310693233\n",
      "l2 norm of weights: 4.721358943865785\n",
      "---------------------\n",
      "Iteration Number: 1508\n",
      "Loss: 44.07271476726355\n",
      "l2 norm of gradients: 0.5684660880759569\n",
      "l2 norm of weights: 4.720174378892107\n",
      "---------------------\n",
      "Iteration Number: 1509\n",
      "Loss: 44.04874650193503\n",
      "l2 norm of gradients: 0.5682342830621999\n",
      "l2 norm of weights: 4.718990352001136\n",
      "---------------------\n",
      "Iteration Number: 1510\n",
      "Loss: 44.02479378072332\n",
      "l2 norm of gradients: 0.5680025160638302\n",
      "l2 norm of weights: 4.71780686314588\n",
      "---------------------\n",
      "Iteration Number: 1511\n",
      "Loss: 44.00085659488109\n",
      "l2 norm of gradients: 0.5677707871166038\n",
      "l2 norm of weights: 4.716623912279607\n",
      "---------------------\n",
      "Iteration Number: 1512\n",
      "Loss: 43.976934935667\n",
      "l2 norm of gradients: 0.5675390962562546\n",
      "l2 norm of weights: 4.7154414993558404\n",
      "---------------------\n",
      "Iteration Number: 1513\n",
      "Loss: 43.95302879435004\n",
      "l2 norm of gradients: 0.567307443518493\n",
      "l2 norm of weights: 4.714259624328363\n",
      "---------------------\n",
      "Iteration Number: 1514\n",
      "Loss: 43.929138162204126\n",
      "l2 norm of gradients: 0.5670758289390054\n",
      "l2 norm of weights: 4.7130782871512125\n",
      "---------------------\n",
      "Iteration Number: 1515\n",
      "Loss: 43.9052630305123\n",
      "l2 norm of gradients: 0.5668442525534527\n",
      "l2 norm of weights: 4.711897487778688\n",
      "---------------------\n",
      "Iteration Number: 1516\n",
      "Loss: 43.881403390567236\n",
      "l2 norm of gradients: 0.5666127143974701\n",
      "l2 norm of weights: 4.710717226165344\n",
      "---------------------\n",
      "Iteration Number: 1517\n",
      "Loss: 43.85755923366724\n",
      "l2 norm of gradients: 0.5663812145066657\n",
      "l2 norm of weights: 4.709537502265996\n",
      "---------------------\n",
      "Iteration Number: 1518\n",
      "Loss: 43.8337305511198\n",
      "l2 norm of gradients: 0.5661497529166194\n",
      "l2 norm of weights: 4.708358316035717\n",
      "---------------------\n",
      "Iteration Number: 1519\n",
      "Loss: 43.80991733423972\n",
      "l2 norm of gradients: 0.5659183296628827\n",
      "l2 norm of weights: 4.707179667429841\n",
      "---------------------\n",
      "Iteration Number: 1520\n",
      "Loss: 43.786119574349804\n",
      "l2 norm of gradients: 0.5656869447809769\n",
      "l2 norm of weights: 4.706001556403959\n",
      "---------------------\n",
      "Iteration Number: 1521\n",
      "Loss: 43.76233726278134\n",
      "l2 norm of gradients: 0.5654555983063928\n",
      "l2 norm of weights: 4.704823982913925\n",
      "---------------------\n",
      "Iteration Number: 1522\n",
      "Loss: 43.738570390874294\n",
      "l2 norm of gradients: 0.5652242902745899\n",
      "l2 norm of weights: 4.703646946915847\n",
      "---------------------\n",
      "Iteration Number: 1523\n",
      "Loss: 43.71481894997453\n",
      "l2 norm of gradients: 0.5649930207209944\n",
      "l2 norm of weights: 4.702470448366101\n",
      "---------------------\n",
      "Iteration Number: 1524\n",
      "Loss: 43.69108293143671\n",
      "l2 norm of gradients: 0.5647617896809999\n",
      "l2 norm of weights: 4.701294487221317\n",
      "---------------------\n",
      "Iteration Number: 1525\n",
      "Loss: 43.66736232662505\n",
      "l2 norm of gradients: 0.5645305971899651\n",
      "l2 norm of weights: 4.70011906343839\n",
      "---------------------\n",
      "Iteration Number: 1526\n",
      "Loss: 43.643657126909574\n",
      "l2 norm of gradients: 0.5642994432832138\n",
      "l2 norm of weights: 4.6989441769744715\n",
      "---------------------\n",
      "Iteration Number: 1527\n",
      "Loss: 43.61996732366954\n",
      "l2 norm of gradients: 0.5640683279960335\n",
      "l2 norm of weights: 4.697769827786979\n",
      "---------------------\n",
      "Iteration Number: 1528\n",
      "Loss: 43.59629290829188\n",
      "l2 norm of gradients: 0.5638372513636746\n",
      "l2 norm of weights: 4.696596015833586\n",
      "---------------------\n",
      "Iteration Number: 1529\n",
      "Loss: 43.5726338721713\n",
      "l2 norm of gradients: 0.5636062134213495\n",
      "l2 norm of weights: 4.695422741072233\n",
      "---------------------\n",
      "Iteration Number: 1530\n",
      "Loss: 43.548990206711274\n",
      "l2 norm of gradients: 0.563375214204232\n",
      "l2 norm of weights: 4.694250003461118\n",
      "---------------------\n",
      "Iteration Number: 1531\n",
      "Loss: 43.52536190332305\n",
      "l2 norm of gradients: 0.563144253747456\n",
      "l2 norm of weights: 4.693077802958702\n",
      "---------------------\n",
      "Iteration Number: 1532\n",
      "Loss: 43.50174895342452\n",
      "l2 norm of gradients: 0.5629133320861147\n",
      "l2 norm of weights: 4.69190613952371\n",
      "---------------------\n",
      "Iteration Number: 1533\n",
      "Loss: 43.478151348443866\n",
      "l2 norm of gradients: 0.5626824492552598\n",
      "l2 norm of weights: 4.690735013115127\n",
      "---------------------\n",
      "Iteration Number: 1534\n",
      "Loss: 43.45456907981592\n",
      "l2 norm of gradients: 0.5624516052899006\n",
      "l2 norm of weights: 4.6895644236922\n",
      "---------------------\n",
      "Iteration Number: 1535\n",
      "Loss: 43.43100213898436\n",
      "l2 norm of gradients: 0.562220800225003\n",
      "l2 norm of weights: 4.688394371214443\n",
      "---------------------\n",
      "Iteration Number: 1536\n",
      "Loss: 43.40745051739887\n",
      "l2 norm of gradients: 0.561990034095489\n",
      "l2 norm of weights: 4.687224855641627\n",
      "---------------------\n",
      "Iteration Number: 1537\n",
      "Loss: 43.38391420652084\n",
      "l2 norm of gradients: 0.5617593069362352\n",
      "l2 norm of weights: 4.686055876933791\n",
      "---------------------\n",
      "Iteration Number: 1538\n",
      "Loss: 43.360393197816556\n",
      "l2 norm of gradients: 0.5615286187820726\n",
      "l2 norm of weights: 4.684887435051233\n",
      "---------------------\n",
      "Iteration Number: 1539\n",
      "Loss: 43.33688748276113\n",
      "l2 norm of gradients: 0.5612979696677849\n",
      "l2 norm of weights: 4.6837195299545185\n",
      "---------------------\n",
      "Iteration Number: 1540\n",
      "Loss: 43.313397052839726\n",
      "l2 norm of gradients: 0.5610673596281084\n",
      "l2 norm of weights: 4.682552161604472\n",
      "---------------------\n",
      "Iteration Number: 1541\n",
      "Loss: 43.28992189954235\n",
      "l2 norm of gradients: 0.560836788697731\n",
      "l2 norm of weights: 4.681385329962188\n",
      "---------------------\n",
      "Iteration Number: 1542\n",
      "Loss: 43.26646201436893\n",
      "l2 norm of gradients: 0.5606062569112905\n",
      "l2 norm of weights: 4.680219034989018\n",
      "---------------------\n",
      "Iteration Number: 1543\n",
      "Loss: 43.24301738882767\n",
      "l2 norm of gradients: 0.5603757643033753\n",
      "l2 norm of weights: 4.679053276646581\n",
      "---------------------\n",
      "Iteration Number: 1544\n",
      "Loss: 43.21958801443414\n",
      "l2 norm of gradients: 0.5601453109085222\n",
      "l2 norm of weights: 4.67788805489676\n",
      "---------------------\n",
      "Iteration Number: 1545\n",
      "Loss: 43.19617388271322\n",
      "l2 norm of gradients: 0.5599148967612154\n",
      "l2 norm of weights: 4.676723369701703\n",
      "---------------------\n",
      "Iteration Number: 1546\n",
      "Loss: 43.1727749851961\n",
      "l2 norm of gradients: 0.5596845218958872\n",
      "l2 norm of weights: 4.67555922102382\n",
      "---------------------\n",
      "Iteration Number: 1547\n",
      "Loss: 43.14939131342296\n",
      "l2 norm of gradients: 0.5594541863469158\n",
      "l2 norm of weights: 4.6743956088257885\n",
      "---------------------\n",
      "Iteration Number: 1548\n",
      "Loss: 43.12602285894284\n",
      "l2 norm of gradients: 0.5592238901486245\n",
      "l2 norm of weights: 4.673232533070548\n",
      "---------------------\n",
      "Iteration Number: 1549\n",
      "Loss: 43.10266961331225\n",
      "l2 norm of gradients: 0.5589936333352815\n",
      "l2 norm of weights: 4.672069993721306\n",
      "---------------------\n",
      "Iteration Number: 1550\n",
      "Loss: 43.079331568094894\n",
      "l2 norm of gradients: 0.5587634159410986\n",
      "l2 norm of weights: 4.67090799074153\n",
      "---------------------\n",
      "Iteration Number: 1551\n",
      "Loss: 43.0560087148644\n",
      "l2 norm of gradients: 0.5585332380002302\n",
      "l2 norm of weights: 4.6697465240949585\n",
      "---------------------\n",
      "Iteration Number: 1552\n",
      "Loss: 43.032701045202266\n",
      "l2 norm of gradients: 0.5583030995467735\n",
      "l2 norm of weights: 4.668585593745592\n",
      "---------------------\n",
      "Iteration Number: 1553\n",
      "Loss: 43.00940855069503\n",
      "l2 norm of gradients: 0.558073000614766\n",
      "l2 norm of weights: 4.667425199657694\n",
      "---------------------\n",
      "Iteration Number: 1554\n",
      "Loss: 42.9861312229431\n",
      "l2 norm of gradients: 0.557842941238186\n",
      "l2 norm of weights: 4.666265341795797\n",
      "---------------------\n",
      "Iteration Number: 1555\n",
      "Loss: 42.96286905354911\n",
      "l2 norm of gradients: 0.5576129214509513\n",
      "l2 norm of weights: 4.665106020124699\n",
      "---------------------\n",
      "Iteration Number: 1556\n",
      "Loss: 42.93962203412779\n",
      "l2 norm of gradients: 0.5573829412869185\n",
      "l2 norm of weights: 4.663947234609462\n",
      "---------------------\n",
      "Iteration Number: 1557\n",
      "Loss: 42.91639015630179\n",
      "l2 norm of gradients: 0.5571530007798816\n",
      "l2 norm of weights: 4.662788985215414\n",
      "---------------------\n",
      "Iteration Number: 1558\n",
      "Loss: 42.89317341169912\n",
      "l2 norm of gradients: 0.5569230999635723\n",
      "l2 norm of weights: 4.661631271908148\n",
      "---------------------\n",
      "Iteration Number: 1559\n",
      "Loss: 42.86997179195854\n",
      "l2 norm of gradients: 0.5566932388716578\n",
      "l2 norm of weights: 4.660474094653526\n",
      "---------------------\n",
      "Iteration Number: 1560\n",
      "Loss: 42.84678528872638\n",
      "l2 norm of gradients: 0.5564634175377412\n",
      "l2 norm of weights: 4.65931745341767\n",
      "---------------------\n",
      "Iteration Number: 1561\n",
      "Loss: 42.82361389365679\n",
      "l2 norm of gradients: 0.5562336359953602\n",
      "l2 norm of weights: 4.658161348166975\n",
      "---------------------\n",
      "Iteration Number: 1562\n",
      "Loss: 42.80045759841189\n",
      "l2 norm of gradients: 0.5560038942779861\n",
      "l2 norm of weights: 4.657005778868096\n",
      "---------------------\n",
      "Iteration Number: 1563\n",
      "Loss: 42.7773163946645\n",
      "l2 norm of gradients: 0.555774192419023\n",
      "l2 norm of weights: 4.6558507454879585\n",
      "---------------------\n",
      "Iteration Number: 1564\n",
      "Loss: 42.754190274090824\n",
      "l2 norm of gradients: 0.5555445304518076\n",
      "l2 norm of weights: 4.6546962479937495\n",
      "---------------------\n",
      "Iteration Number: 1565\n",
      "Loss: 42.73107922837983\n",
      "l2 norm of gradients: 0.5553149084096077\n",
      "l2 norm of weights: 4.653542286352928\n",
      "---------------------\n",
      "Iteration Number: 1566\n",
      "Loss: 42.707983249226345\n",
      "l2 norm of gradients: 0.5550853263256216\n",
      "l2 norm of weights: 4.652388860533215\n",
      "---------------------\n",
      "Iteration Number: 1567\n",
      "Loss: 42.68490232833382\n",
      "l2 norm of gradients: 0.5548557842329773\n",
      "l2 norm of weights: 4.651235970502597\n",
      "---------------------\n",
      "Iteration Number: 1568\n",
      "Loss: 42.66183645741363\n",
      "l2 norm of gradients: 0.5546262821647323\n",
      "l2 norm of weights: 4.650083616229332\n",
      "---------------------\n",
      "Iteration Number: 1569\n",
      "Loss: 42.63878562818687\n",
      "l2 norm of gradients: 0.5543968201538716\n",
      "l2 norm of weights: 4.648931797681939\n",
      "---------------------\n",
      "Iteration Number: 1570\n",
      "Loss: 42.61574983238146\n",
      "l2 norm of gradients: 0.5541673982333079\n",
      "l2 norm of weights: 4.647780514829206\n",
      "---------------------\n",
      "Iteration Number: 1571\n",
      "Loss: 42.592729061733706\n",
      "l2 norm of gradients: 0.5539380164358807\n",
      "l2 norm of weights: 4.646629767640188\n",
      "---------------------\n",
      "Iteration Number: 1572\n",
      "Loss: 42.56972330798707\n",
      "l2 norm of gradients: 0.553708674794355\n",
      "l2 norm of weights: 4.645479556084203\n",
      "---------------------\n",
      "Iteration Number: 1573\n",
      "Loss: 42.54673256289651\n",
      "l2 norm of gradients: 0.5534793733414213\n",
      "l2 norm of weights: 4.644329880130838\n",
      "---------------------\n",
      "Iteration Number: 1574\n",
      "Loss: 42.523756818222495\n",
      "l2 norm of gradients: 0.5532501121096934\n",
      "l2 norm of weights: 4.6431807397499485\n",
      "---------------------\n",
      "Iteration Number: 1575\n",
      "Loss: 42.500796065733766\n",
      "l2 norm of gradients: 0.55302089113171\n",
      "l2 norm of weights: 4.642032134911651\n",
      "---------------------\n",
      "Iteration Number: 1576\n",
      "Loss: 42.477850297208136\n",
      "l2 norm of gradients: 0.5527917104399317\n",
      "l2 norm of weights: 4.640884065586334\n",
      "---------------------\n",
      "Iteration Number: 1577\n",
      "Loss: 42.45491950443135\n",
      "l2 norm of gradients: 0.552562570066741\n",
      "l2 norm of weights: 4.639736531744648\n",
      "---------------------\n",
      "Iteration Number: 1578\n",
      "Loss: 42.4320036791985\n",
      "l2 norm of gradients: 0.5523334700444418\n",
      "l2 norm of weights: 4.638589533357514\n",
      "---------------------\n",
      "Iteration Number: 1579\n",
      "Loss: 42.40910281331198\n",
      "l2 norm of gradients: 0.5521044104052588\n",
      "l2 norm of weights: 4.637443070396115\n",
      "---------------------\n",
      "Iteration Number: 1580\n",
      "Loss: 42.3862168985808\n",
      "l2 norm of gradients: 0.551875391181336\n",
      "l2 norm of weights: 4.6362971428319035\n",
      "---------------------\n",
      "Iteration Number: 1581\n",
      "Loss: 42.36334592682536\n",
      "l2 norm of gradients: 0.5516464124047367\n",
      "l2 norm of weights: 4.635151750636598\n",
      "---------------------\n",
      "Iteration Number: 1582\n",
      "Loss: 42.34048988987271\n",
      "l2 norm of gradients: 0.5514174741074417\n",
      "l2 norm of weights: 4.634006893782181\n",
      "---------------------\n",
      "Iteration Number: 1583\n",
      "Loss: 42.31764877955755\n",
      "l2 norm of gradients: 0.5511885763213503\n",
      "l2 norm of weights: 4.632862572240905\n",
      "---------------------\n",
      "Iteration Number: 1584\n",
      "Loss: 42.29482258772426\n",
      "l2 norm of gradients: 0.5509597190782779\n",
      "l2 norm of weights: 4.631718785985286\n",
      "---------------------\n",
      "Iteration Number: 1585\n",
      "Loss: 42.27201130622455\n",
      "l2 norm of gradients: 0.5507309024099558\n",
      "l2 norm of weights: 4.6305755349881075\n",
      "---------------------\n",
      "Iteration Number: 1586\n",
      "Loss: 42.249214926918825\n",
      "l2 norm of gradients: 0.5505021263480312\n",
      "l2 norm of weights: 4.6294328192224175\n",
      "---------------------\n",
      "Iteration Number: 1587\n",
      "Loss: 42.22643344167585\n",
      "l2 norm of gradients: 0.5502733909240654\n",
      "l2 norm of weights: 4.628290638661533\n",
      "---------------------\n",
      "Iteration Number: 1588\n",
      "Loss: 42.20366684237203\n",
      "l2 norm of gradients: 0.5500446961695332\n",
      "l2 norm of weights: 4.627148993279033\n",
      "---------------------\n",
      "Iteration Number: 1589\n",
      "Loss: 42.18091512089281\n",
      "l2 norm of gradients: 0.5498160421158234\n",
      "l2 norm of weights: 4.626007883048768\n",
      "---------------------\n",
      "Iteration Number: 1590\n",
      "Loss: 42.15817826913133\n",
      "l2 norm of gradients: 0.549587428794236\n",
      "l2 norm of weights: 4.624867307944848\n",
      "---------------------\n",
      "Iteration Number: 1591\n",
      "Loss: 42.13545627898976\n",
      "l2 norm of gradients: 0.549358856235984\n",
      "l2 norm of weights: 4.623727267941654\n",
      "---------------------\n",
      "Iteration Number: 1592\n",
      "Loss: 42.112749142378284\n",
      "l2 norm of gradients: 0.5491303244721902\n",
      "l2 norm of weights: 4.622587763013829\n",
      "---------------------\n",
      "Iteration Number: 1593\n",
      "Loss: 42.0900568512138\n",
      "l2 norm of gradients: 0.5489018335338884\n",
      "l2 norm of weights: 4.621448793136286\n",
      "---------------------\n",
      "Iteration Number: 1594\n",
      "Loss: 42.06737939742489\n",
      "l2 norm of gradients: 0.5486733834520215\n",
      "l2 norm of weights: 4.6203103582842\n",
      "---------------------\n",
      "Iteration Number: 1595\n",
      "Loss: 42.04471677294517\n",
      "l2 norm of gradients: 0.5484449742574417\n",
      "l2 norm of weights: 4.6191724584330105\n",
      "---------------------\n",
      "Iteration Number: 1596\n",
      "Loss: 42.02206896971832\n",
      "l2 norm of gradients: 0.5482166059809087\n",
      "l2 norm of weights: 4.618035093558427\n",
      "---------------------\n",
      "Iteration Number: 1597\n",
      "Loss: 41.999435979695704\n",
      "l2 norm of gradients: 0.5479882786530904\n",
      "l2 norm of weights: 4.616898263636419\n",
      "---------------------\n",
      "Iteration Number: 1598\n",
      "Loss: 41.97681779483706\n",
      "l2 norm of gradients: 0.5477599923045611\n",
      "l2 norm of weights: 4.615761968643228\n",
      "---------------------\n",
      "Iteration Number: 1599\n",
      "Loss: 41.95421440711154\n",
      "l2 norm of gradients: 0.5475317469658009\n",
      "l2 norm of weights: 4.614626208555353\n",
      "---------------------\n",
      "Iteration Number: 1600\n",
      "Loss: 41.93162580849432\n",
      "l2 norm of gradients: 0.547303542667196\n",
      "l2 norm of weights: 4.613490983349561\n",
      "---------------------\n",
      "Iteration Number: 1601\n",
      "Loss: 41.90905199097095\n",
      "l2 norm of gradients: 0.547075379439037\n",
      "l2 norm of weights: 4.612356293002887\n",
      "---------------------\n",
      "Iteration Number: 1602\n",
      "Loss: 41.88649294653469\n",
      "l2 norm of gradients: 0.5468472573115184\n",
      "l2 norm of weights: 4.611222137492625\n",
      "---------------------\n",
      "Iteration Number: 1603\n",
      "Loss: 41.86394866718692\n",
      "l2 norm of gradients: 0.5466191763147382\n",
      "l2 norm of weights: 4.610088516796338\n",
      "---------------------\n",
      "Iteration Number: 1604\n",
      "Loss: 41.84141914493669\n",
      "l2 norm of gradients: 0.5463911364786975\n",
      "l2 norm of weights: 4.608955430891852\n",
      "---------------------\n",
      "Iteration Number: 1605\n",
      "Loss: 41.81890437180307\n",
      "l2 norm of gradients: 0.546163137833299\n",
      "l2 norm of weights: 4.607822879757256\n",
      "---------------------\n",
      "Iteration Number: 1606\n",
      "Loss: 41.796404339812\n",
      "l2 norm of gradients: 0.5459351804083469\n",
      "l2 norm of weights: 4.606690863370905\n",
      "---------------------\n",
      "Iteration Number: 1607\n",
      "Loss: 41.77391904099865\n",
      "l2 norm of gradients: 0.5457072642335464\n",
      "l2 norm of weights: 4.605559381711417\n",
      "---------------------\n",
      "Iteration Number: 1608\n",
      "Loss: 41.75144846740537\n",
      "l2 norm of gradients: 0.5454793893385028\n",
      "l2 norm of weights: 4.604428434757673\n",
      "---------------------\n",
      "Iteration Number: 1609\n",
      "Loss: 41.72899261108449\n",
      "l2 norm of gradients: 0.5452515557527208\n",
      "l2 norm of weights: 4.60329802248882\n",
      "---------------------\n",
      "Iteration Number: 1610\n",
      "Loss: 41.706551464095526\n",
      "l2 norm of gradients: 0.5450237635056038\n",
      "l2 norm of weights: 4.602168144884266\n",
      "---------------------\n",
      "Iteration Number: 1611\n",
      "Loss: 41.68412501850658\n",
      "l2 norm of gradients: 0.5447960126264534\n",
      "l2 norm of weights: 4.601038801923685\n",
      "---------------------\n",
      "Iteration Number: 1612\n",
      "Loss: 41.66171326639396\n",
      "l2 norm of gradients: 0.5445683031444694\n",
      "l2 norm of weights: 4.59990999358701\n",
      "---------------------\n",
      "Iteration Number: 1613\n",
      "Loss: 41.639316199842334\n",
      "l2 norm of gradients: 0.5443406350887477\n",
      "l2 norm of weights: 4.598781719854442\n",
      "---------------------\n",
      "Iteration Number: 1614\n",
      "Loss: 41.616933810945405\n",
      "l2 norm of gradients: 0.5441130084882811\n",
      "l2 norm of weights: 4.59765398070644\n",
      "---------------------\n",
      "Iteration Number: 1615\n",
      "Loss: 41.59456609180432\n",
      "l2 norm of gradients: 0.543885423371958\n",
      "l2 norm of weights: 4.596526776123729\n",
      "---------------------\n",
      "Iteration Number: 1616\n",
      "Loss: 41.57221303452963\n",
      "l2 norm of gradients: 0.5436578797685615\n",
      "l2 norm of weights: 4.595400106087293\n",
      "---------------------\n",
      "Iteration Number: 1617\n",
      "Loss: 41.54987463123864\n",
      "l2 norm of gradients: 0.5434303777067698\n",
      "l2 norm of weights: 4.594273970578384\n",
      "---------------------\n",
      "Iteration Number: 1618\n",
      "Loss: 41.527550874057766\n",
      "l2 norm of gradients: 0.5432029172151547\n",
      "l2 norm of weights: 4.593148369578509\n",
      "---------------------\n",
      "Iteration Number: 1619\n",
      "Loss: 41.505241755122434\n",
      "l2 norm of gradients: 0.5429754983221812\n",
      "l2 norm of weights: 4.592023303069442\n",
      "---------------------\n",
      "Iteration Number: 1620\n",
      "Loss: 41.4829472665762\n",
      "l2 norm of gradients: 0.5427481210562068\n",
      "l2 norm of weights: 4.590898771033214\n",
      "---------------------\n",
      "Iteration Number: 1621\n",
      "Loss: 41.46066740057066\n",
      "l2 norm of gradients: 0.542520785445482\n",
      "l2 norm of weights: 4.589774773452121\n",
      "---------------------\n",
      "Iteration Number: 1622\n",
      "Loss: 41.43840214926522\n",
      "l2 norm of gradients: 0.5422934915181475\n",
      "l2 norm of weights: 4.588651310308719\n",
      "---------------------\n",
      "Iteration Number: 1623\n",
      "Loss: 41.41615150482802\n",
      "l2 norm of gradients: 0.542066239302236\n",
      "l2 norm of weights: 4.587528381585824\n",
      "---------------------\n",
      "Iteration Number: 1624\n",
      "Loss: 41.393915459436585\n",
      "l2 norm of gradients: 0.5418390288256699\n",
      "l2 norm of weights: 4.586405987266513\n",
      "---------------------\n",
      "Iteration Number: 1625\n",
      "Loss: 41.371694005275224\n",
      "l2 norm of gradients: 0.5416118601162616\n",
      "l2 norm of weights: 4.5852841273341225\n",
      "---------------------\n",
      "Iteration Number: 1626\n",
      "Loss: 41.349487134537654\n",
      "l2 norm of gradients: 0.541384733201713\n",
      "l2 norm of weights: 4.58416280177225\n",
      "---------------------\n",
      "Iteration Number: 1627\n",
      "Loss: 41.32729483942601\n",
      "l2 norm of gradients: 0.5411576481096141\n",
      "l2 norm of weights: 4.583042010564755\n",
      "---------------------\n",
      "Iteration Number: 1628\n",
      "Loss: 41.305117112149546\n",
      "l2 norm of gradients: 0.5409306048674432\n",
      "l2 norm of weights: 4.5819217536957515\n",
      "---------------------\n",
      "Iteration Number: 1629\n",
      "Loss: 41.282953944927876\n",
      "l2 norm of gradients: 0.540703603502566\n",
      "l2 norm of weights: 4.580802031149617\n",
      "---------------------\n",
      "Iteration Number: 1630\n",
      "Loss: 41.26080532998644\n",
      "l2 norm of gradients: 0.5404766440422353\n",
      "l2 norm of weights: 4.5796828429109855\n",
      "---------------------\n",
      "Iteration Number: 1631\n",
      "Loss: 41.23867125956109\n",
      "l2 norm of gradients: 0.5402497265135906\n",
      "l2 norm of weights: 4.5785641889647515\n",
      "---------------------\n",
      "Iteration Number: 1632\n",
      "Loss: 41.21655172589599\n",
      "l2 norm of gradients: 0.5400228509436568\n",
      "l2 norm of weights: 4.577446069296067\n",
      "---------------------\n",
      "Iteration Number: 1633\n",
      "Loss: 41.1944467212419\n",
      "l2 norm of gradients: 0.5397960173593441\n",
      "l2 norm of weights: 4.576328483890342\n",
      "---------------------\n",
      "Iteration Number: 1634\n",
      "Loss: 41.17235623786023\n",
      "l2 norm of gradients: 0.5395692257874478\n",
      "l2 norm of weights: 4.575211432733246\n",
      "---------------------\n",
      "Iteration Number: 1635\n",
      "Loss: 41.15028026801942\n",
      "l2 norm of gradients: 0.5393424762546474\n",
      "l2 norm of weights: 4.5740949158107025\n",
      "---------------------\n",
      "Iteration Number: 1636\n",
      "Loss: 41.128218803995544\n",
      "l2 norm of gradients: 0.539115768787506\n",
      "l2 norm of weights: 4.572978933108896\n",
      "---------------------\n",
      "Iteration Number: 1637\n",
      "Loss: 41.106171838074886\n",
      "l2 norm of gradients: 0.5388891034124702\n",
      "l2 norm of weights: 4.5718634846142665\n",
      "---------------------\n",
      "Iteration Number: 1638\n",
      "Loss: 41.084139362550296\n",
      "l2 norm of gradients: 0.5386624801558693\n",
      "l2 norm of weights: 4.570748570313509\n",
      "---------------------\n",
      "Iteration Number: 1639\n",
      "Loss: 41.06212136972431\n",
      "l2 norm of gradients: 0.5384358990439142\n",
      "l2 norm of weights: 4.569634190193579\n",
      "---------------------\n",
      "Iteration Number: 1640\n",
      "Loss: 41.04011785190812\n",
      "l2 norm of gradients: 0.5382093601026982\n",
      "l2 norm of weights: 4.568520344241683\n",
      "---------------------\n",
      "Iteration Number: 1641\n",
      "Loss: 41.01812880141955\n",
      "l2 norm of gradients: 0.5379828633581955\n",
      "l2 norm of weights: 4.567407032445287\n",
      "---------------------\n",
      "Iteration Number: 1642\n",
      "Loss: 40.996154210586376\n",
      "l2 norm of gradients: 0.537756408836261\n",
      "l2 norm of weights: 4.566294254792109\n",
      "---------------------\n",
      "Iteration Number: 1643\n",
      "Loss: 40.9741940717429\n",
      "l2 norm of gradients: 0.5375299965626299\n",
      "l2 norm of weights: 4.5651820112701245\n",
      "---------------------\n",
      "Iteration Number: 1644\n",
      "Loss: 40.952248377234646\n",
      "l2 norm of gradients: 0.537303626562917\n",
      "l2 norm of weights: 4.564070301867566\n",
      "---------------------\n",
      "Iteration Number: 1645\n",
      "Loss: 40.930317119412784\n",
      "l2 norm of gradients: 0.5370772988626162\n",
      "l2 norm of weights: 4.562959126572912\n",
      "---------------------\n",
      "Iteration Number: 1646\n",
      "Loss: 40.90840029063842\n",
      "l2 norm of gradients: 0.536851013487101\n",
      "l2 norm of weights: 4.561848485374904\n",
      "---------------------\n",
      "Iteration Number: 1647\n",
      "Loss: 40.88649788327983\n",
      "l2 norm of gradients: 0.5366247704616219\n",
      "l2 norm of weights: 4.560738378262531\n",
      "---------------------\n",
      "Iteration Number: 1648\n",
      "Loss: 40.86460988971528\n",
      "l2 norm of gradients: 0.5363985698113083\n",
      "l2 norm of weights: 4.5596288052250395\n",
      "---------------------\n",
      "Iteration Number: 1649\n",
      "Loss: 40.84273630232986\n",
      "l2 norm of gradients: 0.5361724115611662\n",
      "l2 norm of weights: 4.558519766251924\n",
      "---------------------\n",
      "Iteration Number: 1650\n",
      "Loss: 40.82087711351743\n",
      "l2 norm of gradients: 0.5359462957360792\n",
      "l2 norm of weights: 4.557411261332937\n",
      "---------------------\n",
      "Iteration Number: 1651\n",
      "Loss: 40.799032315680535\n",
      "l2 norm of gradients: 0.5357202223608069\n",
      "l2 norm of weights: 4.556303290458077\n",
      "---------------------\n",
      "Iteration Number: 1652\n",
      "Loss: 40.77720190123043\n",
      "l2 norm of gradients: 0.5354941914599849\n",
      "l2 norm of weights: 4.5551958536176\n",
      "---------------------\n",
      "Iteration Number: 1653\n",
      "Loss: 40.75538586258492\n",
      "l2 norm of gradients: 0.5352682030581245\n",
      "l2 norm of weights: 4.554088950802009\n",
      "---------------------\n",
      "Iteration Number: 1654\n",
      "Loss: 40.73358419217211\n",
      "l2 norm of gradients: 0.5350422571796122\n",
      "l2 norm of weights: 4.552982582002061\n",
      "---------------------\n",
      "Iteration Number: 1655\n",
      "Loss: 40.71179688242847\n",
      "l2 norm of gradients: 0.5348163538487092\n",
      "l2 norm of weights: 4.551876747208759\n",
      "---------------------\n",
      "Iteration Number: 1656\n",
      "Loss: 40.69002392579705\n",
      "l2 norm of gradients: 0.5345904930895508\n",
      "l2 norm of weights: 4.550771446413362\n",
      "---------------------\n",
      "Iteration Number: 1657\n",
      "Loss: 40.66826531473079\n",
      "l2 norm of gradients: 0.534364674926146\n",
      "l2 norm of weights: 4.549666679607373\n",
      "---------------------\n",
      "Iteration Number: 1658\n",
      "Loss: 40.64652104168986\n",
      "l2 norm of gradients: 0.534138899382378\n",
      "l2 norm of weights: 4.54856244678255\n",
      "---------------------\n",
      "Iteration Number: 1659\n",
      "Loss: 40.62479109914461\n",
      "l2 norm of gradients: 0.5339131664820023\n",
      "l2 norm of weights: 4.547458747930894\n",
      "---------------------\n",
      "Iteration Number: 1660\n",
      "Loss: 40.60307547957138\n",
      "l2 norm of gradients: 0.5336874762486469\n",
      "l2 norm of weights: 4.546355583044657\n",
      "---------------------\n",
      "Iteration Number: 1661\n",
      "Loss: 40.58137417545642\n",
      "l2 norm of gradients: 0.5334618287058126\n",
      "l2 norm of weights: 4.54525295211634\n",
      "---------------------\n",
      "Iteration Number: 1662\n",
      "Loss: 40.55968717929434\n",
      "l2 norm of gradients: 0.5332362238768721\n",
      "l2 norm of weights: 4.54415085513869\n",
      "---------------------\n",
      "Iteration Number: 1663\n",
      "Loss: 40.53801448358665\n",
      "l2 norm of gradients: 0.5330106617850684\n",
      "l2 norm of weights: 4.543049292104699\n",
      "---------------------\n",
      "Iteration Number: 1664\n",
      "Loss: 40.516356080844474\n",
      "l2 norm of gradients: 0.5327851424535172\n",
      "l2 norm of weights: 4.541948263007611\n",
      "---------------------\n",
      "Iteration Number: 1665\n",
      "Loss: 40.494711963587555\n",
      "l2 norm of gradients: 0.5325596659052034\n",
      "l2 norm of weights: 4.540847767840912\n",
      "---------------------\n",
      "Iteration Number: 1666\n",
      "Loss: 40.47308212434229\n",
      "l2 norm of gradients: 0.5323342321629833\n",
      "l2 norm of weights: 4.539747806598334\n",
      "---------------------\n",
      "Iteration Number: 1667\n",
      "Loss: 40.451466555645474\n",
      "l2 norm of gradients: 0.5321088412495822\n",
      "l2 norm of weights: 4.538648379273855\n",
      "---------------------\n",
      "Iteration Number: 1668\n",
      "Loss: 40.42986525004087\n",
      "l2 norm of gradients: 0.5318834931875955\n",
      "l2 norm of weights: 4.537549485861697\n",
      "---------------------\n",
      "Iteration Number: 1669\n",
      "Loss: 40.40827820008025\n",
      "l2 norm of gradients: 0.5316581879994879\n",
      "l2 norm of weights: 4.536451126356328\n",
      "---------------------\n",
      "Iteration Number: 1670\n",
      "Loss: 40.38670539832556\n",
      "l2 norm of gradients: 0.5314329257075927\n",
      "l2 norm of weights: 4.5353533007524565\n",
      "---------------------\n",
      "Iteration Number: 1671\n",
      "Loss: 40.36514683734473\n",
      "l2 norm of gradients: 0.5312077063341119\n",
      "l2 norm of weights: 4.534256009045037\n",
      "---------------------\n",
      "Iteration Number: 1672\n",
      "Loss: 40.343602509716135\n",
      "l2 norm of gradients: 0.5309825299011152\n",
      "l2 norm of weights: 4.533159251229265\n",
      "---------------------\n",
      "Iteration Number: 1673\n",
      "Loss: 40.32207240802445\n",
      "l2 norm of gradients: 0.5307573964305405\n",
      "l2 norm of weights: 4.532063027300581\n",
      "---------------------\n",
      "Iteration Number: 1674\n",
      "Loss: 40.300556524863886\n",
      "l2 norm of gradients: 0.5305323059441935\n",
      "l2 norm of weights: 4.530967337254663\n",
      "---------------------\n",
      "Iteration Number: 1675\n",
      "Loss: 40.279054852837305\n",
      "l2 norm of gradients: 0.5303072584637465\n",
      "l2 norm of weights: 4.529872181087434\n",
      "---------------------\n",
      "Iteration Number: 1676\n",
      "Loss: 40.25756738455502\n",
      "l2 norm of gradients: 0.5300822540107388\n",
      "l2 norm of weights: 4.528777558795055\n",
      "---------------------\n",
      "Iteration Number: 1677\n",
      "Loss: 40.2360941126357\n",
      "l2 norm of gradients: 0.5298572926065765\n",
      "l2 norm of weights: 4.527683470373929\n",
      "---------------------\n",
      "Iteration Number: 1678\n",
      "Loss: 40.21463502970667\n",
      "l2 norm of gradients: 0.5296323742725316\n",
      "l2 norm of weights: 4.526589915820697\n",
      "---------------------\n",
      "Iteration Number: 1679\n",
      "Loss: 40.19319012840326\n",
      "l2 norm of gradients: 0.5294074990297423\n",
      "l2 norm of weights: 4.52549689513224\n",
      "---------------------\n",
      "Iteration Number: 1680\n",
      "Loss: 40.1717594013695\n",
      "l2 norm of gradients: 0.5291826668992122\n",
      "l2 norm of weights: 4.524404408305679\n",
      "---------------------\n",
      "Iteration Number: 1681\n",
      "Loss: 40.150342841256524\n",
      "l2 norm of gradients: 0.5289578779018103\n",
      "l2 norm of weights: 4.523312455338369\n",
      "---------------------\n",
      "Iteration Number: 1682\n",
      "Loss: 40.12894044072586\n",
      "l2 norm of gradients: 0.5287331320582709\n",
      "l2 norm of weights: 4.522221036227907\n",
      "---------------------\n",
      "Iteration Number: 1683\n",
      "Loss: 40.10755219244591\n",
      "l2 norm of gradients: 0.5285084293891923\n",
      "l2 norm of weights: 4.521130150972123\n",
      "---------------------\n",
      "Iteration Number: 1684\n",
      "Loss: 40.08617808909253\n",
      "l2 norm of gradients: 0.5282837699150383\n",
      "l2 norm of weights: 4.520039799569086\n",
      "---------------------\n",
      "Iteration Number: 1685\n",
      "Loss: 40.06481812335188\n",
      "l2 norm of gradients: 0.5280591536561365\n",
      "l2 norm of weights: 4.5189499820171\n",
      "---------------------\n",
      "Iteration Number: 1686\n",
      "Loss: 40.04347228791678\n",
      "l2 norm of gradients: 0.5278345806326782\n",
      "l2 norm of weights: 4.517860698314704\n",
      "---------------------\n",
      "Iteration Number: 1687\n",
      "Loss: 40.02214057548857\n",
      "l2 norm of gradients: 0.5276100508647187\n",
      "l2 norm of weights: 4.51677194846067\n",
      "---------------------\n",
      "Iteration Number: 1688\n",
      "Loss: 40.00082297877812\n",
      "l2 norm of gradients: 0.5273855643721767\n",
      "l2 norm of weights: 4.515683732454008\n",
      "---------------------\n",
      "Iteration Number: 1689\n",
      "Loss: 39.9795194905031\n",
      "l2 norm of gradients: 0.5271611211748342\n",
      "l2 norm of weights: 4.514596050293957\n",
      "---------------------\n",
      "Iteration Number: 1690\n",
      "Loss: 39.9582301033898\n",
      "l2 norm of gradients: 0.5269367212923357\n",
      "l2 norm of weights: 4.51350890197999\n",
      "---------------------\n",
      "Iteration Number: 1691\n",
      "Loss: 39.936954810172196\n",
      "l2 norm of gradients: 0.526712364744189\n",
      "l2 norm of weights: 4.512422287511815\n",
      "---------------------\n",
      "Iteration Number: 1692\n",
      "Loss: 39.915693603594626\n",
      "l2 norm of gradients: 0.5264880515497641\n",
      "l2 norm of weights: 4.51133620688937\n",
      "---------------------\n",
      "Iteration Number: 1693\n",
      "Loss: 39.89444647640706\n",
      "l2 norm of gradients: 0.5262637817282935\n",
      "l2 norm of weights: 4.510250660112821\n",
      "---------------------\n",
      "Iteration Number: 1694\n",
      "Loss: 39.873213421369215\n",
      "l2 norm of gradients: 0.5260395552988715\n",
      "l2 norm of weights: 4.509165647182567\n",
      "---------------------\n",
      "Iteration Number: 1695\n",
      "Loss: 39.851994431249075\n",
      "l2 norm of gradients: 0.5258153722804543\n",
      "l2 norm of weights: 4.508081168099239\n",
      "---------------------\n",
      "Iteration Number: 1696\n",
      "Loss: 39.83078949882145\n",
      "l2 norm of gradients: 0.5255912326918599\n",
      "l2 norm of weights: 4.506997222863694\n",
      "---------------------\n",
      "Iteration Number: 1697\n",
      "Loss: 39.809598616870865\n",
      "l2 norm of gradients: 0.5253671365517675\n",
      "l2 norm of weights: 4.505913811477016\n",
      "---------------------\n",
      "Iteration Number: 1698\n",
      "Loss: 39.788421778189814\n",
      "l2 norm of gradients: 0.5251430838787177\n",
      "l2 norm of weights: 4.504830933940521\n",
      "---------------------\n",
      "Iteration Number: 1699\n",
      "Loss: 39.76725897557817\n",
      "l2 norm of gradients: 0.5249190746911122\n",
      "l2 norm of weights: 4.503748590255748\n",
      "---------------------\n",
      "Iteration Number: 1700\n",
      "Loss: 39.74611020184464\n",
      "l2 norm of gradients: 0.5246951090072137\n",
      "l2 norm of weights: 4.502666780424467\n",
      "---------------------\n",
      "Iteration Number: 1701\n",
      "Loss: 39.72497544980618\n",
      "l2 norm of gradients: 0.524471186845145\n",
      "l2 norm of weights: 4.50158550444867\n",
      "---------------------\n",
      "Iteration Number: 1702\n",
      "Loss: 39.703854712287395\n",
      "l2 norm of gradients: 0.5242473082228903\n",
      "l2 norm of weights: 4.500504762330576\n",
      "---------------------\n",
      "Iteration Number: 1703\n",
      "Loss: 39.68274798212255\n",
      "l2 norm of gradients: 0.5240234731582936\n",
      "l2 norm of weights: 4.499424554072628\n",
      "---------------------\n",
      "Iteration Number: 1704\n",
      "Loss: 39.66165525215159\n",
      "l2 norm of gradients: 0.523799681669059\n",
      "l2 norm of weights: 4.498344879677493\n",
      "---------------------\n",
      "Iteration Number: 1705\n",
      "Loss: 39.64057651522494\n",
      "l2 norm of gradients: 0.5235759337727512\n",
      "l2 norm of weights: 4.4972657391480615\n",
      "---------------------\n",
      "Iteration Number: 1706\n",
      "Loss: 39.61951176420039\n",
      "l2 norm of gradients: 0.5233522294867944\n",
      "l2 norm of weights: 4.496187132487446\n",
      "---------------------\n",
      "Iteration Number: 1707\n",
      "Loss: 39.598460991942524\n",
      "l2 norm of gradients: 0.5231285688284726\n",
      "l2 norm of weights: 4.49510905969898\n",
      "---------------------\n",
      "Iteration Number: 1708\n",
      "Loss: 39.5774241913264\n",
      "l2 norm of gradients: 0.5229049518149297\n",
      "l2 norm of weights: 4.494031520786221\n",
      "---------------------\n",
      "Iteration Number: 1709\n",
      "Loss: 39.55640135523412\n",
      "l2 norm of gradients: 0.5226813784631689\n",
      "l2 norm of weights: 4.492954515752943\n",
      "---------------------\n",
      "Iteration Number: 1710\n",
      "Loss: 39.53539247655629\n",
      "l2 norm of gradients: 0.5224578487900527\n",
      "l2 norm of weights: 4.491878044603141\n",
      "---------------------\n",
      "Iteration Number: 1711\n",
      "Loss: 39.51439754818997\n",
      "l2 norm of gradients: 0.5222343628123034\n",
      "l2 norm of weights: 4.490802107341032\n",
      "---------------------\n",
      "Iteration Number: 1712\n",
      "Loss: 39.49341656304298\n",
      "l2 norm of gradients: 0.5220109205465018\n",
      "l2 norm of weights: 4.489726703971047\n",
      "---------------------\n",
      "Iteration Number: 1713\n",
      "Loss: 39.472449514029556\n",
      "l2 norm of gradients: 0.521787522009088\n",
      "l2 norm of weights: 4.488651834497835\n",
      "---------------------\n",
      "Iteration Number: 1714\n",
      "Loss: 39.45149639407246\n",
      "l2 norm of gradients: 0.5215641672163611\n",
      "l2 norm of weights: 4.487577498926266\n",
      "---------------------\n",
      "Iteration Number: 1715\n",
      "Loss: 39.43055719610254\n",
      "l2 norm of gradients: 0.5213408561844795\n",
      "l2 norm of weights: 4.48650369726142\n",
      "---------------------\n",
      "Iteration Number: 1716\n",
      "Loss: 39.409631913059386\n",
      "l2 norm of gradients: 0.5211175889294597\n",
      "l2 norm of weights: 4.485430429508597\n",
      "---------------------\n",
      "Iteration Number: 1717\n",
      "Loss: 39.3887205378888\n",
      "l2 norm of gradients: 0.5208943654671772\n",
      "l2 norm of weights: 4.4843576956733076\n",
      "---------------------\n",
      "Iteration Number: 1718\n",
      "Loss: 39.36782306354772\n",
      "l2 norm of gradients: 0.5206711858133662\n",
      "l2 norm of weights: 4.483285495761282\n",
      "---------------------\n",
      "Iteration Number: 1719\n",
      "Loss: 39.346939482997946\n",
      "l2 norm of gradients: 0.5204480499836197\n",
      "l2 norm of weights: 4.482213829778456\n",
      "---------------------\n",
      "Iteration Number: 1720\n",
      "Loss: 39.326069789211296\n",
      "l2 norm of gradients: 0.5202249579933885\n",
      "l2 norm of weights: 4.481142697730985\n",
      "---------------------\n",
      "Iteration Number: 1721\n",
      "Loss: 39.30521397516747\n",
      "l2 norm of gradients: 0.520001909857983\n",
      "l2 norm of weights: 4.480072099625229\n",
      "---------------------\n",
      "Iteration Number: 1722\n",
      "Loss: 39.284372033853664\n",
      "l2 norm of gradients: 0.5197789055925709\n",
      "l2 norm of weights: 4.479002035467763\n",
      "---------------------\n",
      "Iteration Number: 1723\n",
      "Loss: 39.263543958265224\n",
      "l2 norm of gradients: 0.5195559452121787\n",
      "l2 norm of weights: 4.477932505265373\n",
      "---------------------\n",
      "Iteration Number: 1724\n",
      "Loss: 39.24272974140613\n",
      "l2 norm of gradients: 0.5193330287316916\n",
      "l2 norm of weights: 4.4768635090250495\n",
      "---------------------\n",
      "Iteration Number: 1725\n",
      "Loss: 39.22192937628732\n",
      "l2 norm of gradients: 0.5191101561658528\n",
      "l2 norm of weights: 4.475795046753994\n",
      "---------------------\n",
      "Iteration Number: 1726\n",
      "Loss: 39.201142855929476\n",
      "l2 norm of gradients: 0.5188873275292635\n",
      "l2 norm of weights: 4.474727118459618\n",
      "---------------------\n",
      "Iteration Number: 1727\n",
      "Loss: 39.180370173359705\n",
      "l2 norm of gradients: 0.518664542836384\n",
      "l2 norm of weights: 4.473659724149534\n",
      "---------------------\n",
      "Iteration Number: 1728\n",
      "Loss: 39.15961132161319\n",
      "l2 norm of gradients: 0.5184418021015321\n",
      "l2 norm of weights: 4.472592863831564\n",
      "---------------------\n",
      "Iteration Number: 1729\n",
      "Loss: 39.13886629373476\n",
      "l2 norm of gradients: 0.5182191053388842\n",
      "l2 norm of weights: 4.471526537513737\n",
      "---------------------\n",
      "Iteration Number: 1730\n",
      "Loss: 39.11813508277515\n",
      "l2 norm of gradients: 0.5179964525624751\n",
      "l2 norm of weights: 4.4704607452042815\n",
      "---------------------\n",
      "Iteration Number: 1731\n",
      "Loss: 39.097417681794205\n",
      "l2 norm of gradients: 0.5177738437861976\n",
      "l2 norm of weights: 4.469395486911633\n",
      "---------------------\n",
      "Iteration Number: 1732\n",
      "Loss: 39.0767140838598\n",
      "l2 norm of gradients: 0.5175512790238032\n",
      "l2 norm of weights: 4.468330762644428\n",
      "---------------------\n",
      "Iteration Number: 1733\n",
      "Loss: 39.05602428204755\n",
      "l2 norm of gradients: 0.5173287582889012\n",
      "l2 norm of weights: 4.467266572411507\n",
      "---------------------\n",
      "Iteration Number: 1734\n",
      "Loss: 39.03534826944082\n",
      "l2 norm of gradients: 0.5171062815949599\n",
      "l2 norm of weights: 4.466202916221907\n",
      "---------------------\n",
      "Iteration Number: 1735\n",
      "Loss: 39.01468603913206\n",
      "l2 norm of gradients: 0.5168838489553054\n",
      "l2 norm of weights: 4.46513979408487\n",
      "---------------------\n",
      "Iteration Number: 1736\n",
      "Loss: 38.99403758421927\n",
      "l2 norm of gradients: 0.5166614603831226\n",
      "l2 norm of weights: 4.464077206009833\n",
      "---------------------\n",
      "Iteration Number: 1737\n",
      "Loss: 38.97340289781141\n",
      "l2 norm of gradients: 0.516439115891455\n",
      "l2 norm of weights: 4.463015152006435\n",
      "---------------------\n",
      "Iteration Number: 1738\n",
      "Loss: 38.95278197302276\n",
      "l2 norm of gradients: 0.5162168154932042\n",
      "l2 norm of weights: 4.461953632084511\n",
      "---------------------\n",
      "Iteration Number: 1739\n",
      "Loss: 38.932174802976824\n",
      "l2 norm of gradients: 0.5159945592011309\n",
      "l2 norm of weights: 4.460892646254091\n",
      "---------------------\n",
      "Iteration Number: 1740\n",
      "Loss: 38.91158138080539\n",
      "l2 norm of gradients: 0.5157723470278542\n",
      "l2 norm of weights: 4.459832194525404\n",
      "---------------------\n",
      "Iteration Number: 1741\n",
      "Loss: 38.891001699646615\n",
      "l2 norm of gradients: 0.5155501789858521\n",
      "l2 norm of weights: 4.458772276908872\n",
      "---------------------\n",
      "Iteration Number: 1742\n",
      "Loss: 38.87043575264892\n",
      "l2 norm of gradients: 0.5153280550874615\n",
      "l2 norm of weights: 4.457712893415109\n",
      "---------------------\n",
      "Iteration Number: 1743\n",
      "Loss: 38.84988353296606\n",
      "l2 norm of gradients: 0.5151059753448778\n",
      "l2 norm of weights: 4.456654044054925\n",
      "---------------------\n",
      "Iteration Number: 1744\n",
      "Loss: 38.82934503376128\n",
      "l2 norm of gradients: 0.5148839397701561\n",
      "l2 norm of weights: 4.455595728839324\n",
      "---------------------\n",
      "Iteration Number: 1745\n",
      "Loss: 38.80882024820491\n",
      "l2 norm of gradients: 0.5146619483752101\n",
      "l2 norm of weights: 4.454537947779496\n",
      "---------------------\n",
      "Iteration Number: 1746\n",
      "Loss: 38.78830916947576\n",
      "l2 norm of gradients: 0.514440001171813\n",
      "l2 norm of weights: 4.453480700886824\n",
      "---------------------\n",
      "Iteration Number: 1747\n",
      "Loss: 38.76781179075998\n",
      "l2 norm of gradients: 0.5142180981715974\n",
      "l2 norm of weights: 4.452423988172882\n",
      "---------------------\n",
      "Iteration Number: 1748\n",
      "Loss: 38.747328105251846\n",
      "l2 norm of gradients: 0.513996239386055\n",
      "l2 norm of weights: 4.45136780964943\n",
      "---------------------\n",
      "Iteration Number: 1749\n",
      "Loss: 38.72685810615379\n",
      "l2 norm of gradients: 0.5137744248265378\n",
      "l2 norm of weights: 4.450312165328415\n",
      "---------------------\n",
      "Iteration Number: 1750\n",
      "Loss: 38.70640178667455\n",
      "l2 norm of gradients: 0.5135526545042568\n",
      "l2 norm of weights: 4.449257055221975\n",
      "---------------------\n",
      "Iteration Number: 1751\n",
      "Loss: 38.685959140032466\n",
      "l2 norm of gradients: 0.5133309284302834\n",
      "l2 norm of weights: 4.448202479342429\n",
      "---------------------\n",
      "Iteration Number: 1752\n",
      "Loss: 38.66553015945356\n",
      "l2 norm of gradients: 0.5131092466155489\n",
      "l2 norm of weights: 4.447148437702284\n",
      "---------------------\n",
      "Iteration Number: 1753\n",
      "Loss: 38.645114838170294\n",
      "l2 norm of gradients: 0.5128876090708449\n",
      "l2 norm of weights: 4.446094930314229\n",
      "---------------------\n",
      "Iteration Number: 1754\n",
      "Loss: 38.624713169423266\n",
      "l2 norm of gradients: 0.5126660158068228\n",
      "l2 norm of weights: 4.445041957191136\n",
      "---------------------\n",
      "Iteration Number: 1755\n",
      "Loss: 38.60432514646266\n",
      "l2 norm of gradients: 0.5124444668339954\n",
      "l2 norm of weights: 4.44398951834606\n",
      "---------------------\n",
      "Iteration Number: 1756\n",
      "Loss: 38.583950762543616\n",
      "l2 norm of gradients: 0.5122229621627354\n",
      "l2 norm of weights: 4.4429376137922345\n",
      "---------------------\n",
      "Iteration Number: 1757\n",
      "Loss: 38.56359001093141\n",
      "l2 norm of gradients: 0.5120015018032772\n",
      "l2 norm of weights: 4.441886243543077\n",
      "---------------------\n",
      "Iteration Number: 1758\n",
      "Loss: 38.54324288489673\n",
      "l2 norm of gradients: 0.5117800857657154\n",
      "l2 norm of weights: 4.440835407612181\n",
      "---------------------\n",
      "Iteration Number: 1759\n",
      "Loss: 38.52290937772089\n",
      "l2 norm of gradients: 0.5115587140600064\n",
      "l2 norm of weights: 4.439785106013318\n",
      "---------------------\n",
      "Iteration Number: 1760\n",
      "Loss: 38.50258948269014\n",
      "l2 norm of gradients: 0.5113373866959683\n",
      "l2 norm of weights: 4.438735338760437\n",
      "---------------------\n",
      "Iteration Number: 1761\n",
      "Loss: 38.48228319310022\n",
      "l2 norm of gradients: 0.51111610368328\n",
      "l2 norm of weights: 4.437686105867665\n",
      "---------------------\n",
      "Iteration Number: 1762\n",
      "Loss: 38.461990502253876\n",
      "l2 norm of gradients: 0.5108948650314832\n",
      "l2 norm of weights: 4.4366374073493\n",
      "---------------------\n",
      "Iteration Number: 1763\n",
      "Loss: 38.44171140346129\n",
      "l2 norm of gradients: 0.5106736707499814\n",
      "l2 norm of weights: 4.435589243219819\n",
      "---------------------\n",
      "Iteration Number: 1764\n",
      "Loss: 38.42144589004137\n",
      "l2 norm of gradients: 0.5104525208480403\n",
      "l2 norm of weights: 4.434541613493867\n",
      "---------------------\n",
      "Iteration Number: 1765\n",
      "Loss: 38.40119395531896\n",
      "l2 norm of gradients: 0.5102314153347882\n",
      "l2 norm of weights: 4.433494518186263\n",
      "---------------------\n",
      "Iteration Number: 1766\n",
      "Loss: 38.38095559262834\n",
      "l2 norm of gradients: 0.5100103542192167\n",
      "l2 norm of weights: 4.432447957311999\n",
      "---------------------\n",
      "Iteration Number: 1767\n",
      "Loss: 38.36073079530997\n",
      "l2 norm of gradients: 0.50978933751018\n",
      "l2 norm of weights: 4.431401930886235\n",
      "---------------------\n",
      "Iteration Number: 1768\n",
      "Loss: 38.34051955671293\n",
      "l2 norm of gradients: 0.5095683652163957\n",
      "l2 norm of weights: 4.430356438924298\n",
      "---------------------\n",
      "Iteration Number: 1769\n",
      "Loss: 38.320321870193226\n",
      "l2 norm of gradients: 0.509347437346445\n",
      "l2 norm of weights: 4.429311481441688\n",
      "---------------------\n",
      "Iteration Number: 1770\n",
      "Loss: 38.300137729115505\n",
      "l2 norm of gradients: 0.5091265539087734\n",
      "l2 norm of weights: 4.428267058454066\n",
      "---------------------\n",
      "Iteration Number: 1771\n",
      "Loss: 38.279967126850515\n",
      "l2 norm of gradients: 0.5089057149116899\n",
      "l2 norm of weights: 4.427223169977263\n",
      "---------------------\n",
      "Iteration Number: 1772\n",
      "Loss: 38.25981005677738\n",
      "l2 norm of gradients: 0.5086849203633684\n",
      "l2 norm of weights: 4.4261798160272745\n",
      "---------------------\n",
      "Iteration Number: 1773\n",
      "Loss: 38.23966651228331\n",
      "l2 norm of gradients: 0.5084641702718474\n",
      "l2 norm of weights: 4.425136996620256\n",
      "---------------------\n",
      "Iteration Number: 1774\n",
      "Loss: 38.21953648676209\n",
      "l2 norm of gradients: 0.5082434646450302\n",
      "l2 norm of weights: 4.424094711772529\n",
      "---------------------\n",
      "Iteration Number: 1775\n",
      "Loss: 38.199419973615335\n",
      "l2 norm of gradients: 0.5080228034906858\n",
      "l2 norm of weights: 4.423052961500576\n",
      "---------------------\n",
      "Iteration Number: 1776\n",
      "Loss: 38.17931696625248\n",
      "l2 norm of gradients: 0.5078021868164486\n",
      "l2 norm of weights: 4.42201174582104\n",
      "---------------------\n",
      "Iteration Number: 1777\n",
      "Loss: 38.15922745808974\n",
      "l2 norm of gradients: 0.5075816146298192\n",
      "l2 norm of weights: 4.420971064750721\n",
      "---------------------\n",
      "Iteration Number: 1778\n",
      "Loss: 38.139151442552354\n",
      "l2 norm of gradients: 0.5073610869381642\n",
      "l2 norm of weights: 4.419930918306582\n",
      "---------------------\n",
      "Iteration Number: 1779\n",
      "Loss: 38.119088913070875\n",
      "l2 norm of gradients: 0.5071406037487168\n",
      "l2 norm of weights: 4.418891306505738\n",
      "---------------------\n",
      "Iteration Number: 1780\n",
      "Loss: 38.09903986308505\n",
      "l2 norm of gradients: 0.5069201650685773\n",
      "l2 norm of weights: 4.417852229365463\n",
      "---------------------\n",
      "Iteration Number: 1781\n",
      "Loss: 38.0790042860419\n",
      "l2 norm of gradients: 0.5066997709047136\n",
      "l2 norm of weights: 4.416813686903187\n",
      "---------------------\n",
      "Iteration Number: 1782\n",
      "Loss: 38.05898217539468\n",
      "l2 norm of gradients: 0.5064794212639606\n",
      "l2 norm of weights: 4.415775679136491\n",
      "---------------------\n",
      "Iteration Number: 1783\n",
      "Loss: 38.038973524605346\n",
      "l2 norm of gradients: 0.5062591161530218\n",
      "l2 norm of weights: 4.41473820608311\n",
      "---------------------\n",
      "Iteration Number: 1784\n",
      "Loss: 38.01897832714306\n",
      "l2 norm of gradients: 0.5060388555784685\n",
      "l2 norm of weights: 4.413701267760932\n",
      "---------------------\n",
      "Iteration Number: 1785\n",
      "Loss: 37.998996576483684\n",
      "l2 norm of gradients: 0.5058186395467414\n",
      "l2 norm of weights: 4.412664864187994\n",
      "---------------------\n",
      "Iteration Number: 1786\n",
      "Loss: 37.97902826611101\n",
      "l2 norm of gradients: 0.5055984680641499\n",
      "l2 norm of weights: 4.4116289953824825\n",
      "---------------------\n",
      "Iteration Number: 1787\n",
      "Loss: 37.95907338951616\n",
      "l2 norm of gradients: 0.5053783411368729\n",
      "l2 norm of weights: 4.410593661362732\n",
      "---------------------\n",
      "Iteration Number: 1788\n",
      "Loss: 37.939131940197534\n",
      "l2 norm of gradients: 0.5051582587709593\n",
      "l2 norm of weights: 4.409558862147227\n",
      "---------------------\n",
      "Iteration Number: 1789\n",
      "Loss: 37.91920391166105\n",
      "l2 norm of gradients: 0.5049382209723284\n",
      "l2 norm of weights: 4.408524597754593\n",
      "---------------------\n",
      "Iteration Number: 1790\n",
      "Loss: 37.89928929741932\n",
      "l2 norm of gradients: 0.5047182277467703\n",
      "l2 norm of weights: 4.407490868203605\n",
      "---------------------\n",
      "Iteration Number: 1791\n",
      "Loss: 37.879388090993494\n",
      "l2 norm of gradients: 0.5044982790999459\n",
      "l2 norm of weights: 4.406457673513179\n",
      "---------------------\n",
      "Iteration Number: 1792\n",
      "Loss: 37.859500285910855\n",
      "l2 norm of gradients: 0.5042783750373879\n",
      "l2 norm of weights: 4.405425013702374\n",
      "---------------------\n",
      "Iteration Number: 1793\n",
      "Loss: 37.83962587570651\n",
      "l2 norm of gradients: 0.5040585155645011\n",
      "l2 norm of weights: 4.404392888790391\n",
      "---------------------\n",
      "Iteration Number: 1794\n",
      "Loss: 37.8197648539226\n",
      "l2 norm of gradients: 0.5038387006865619\n",
      "l2 norm of weights: 4.403361298796571\n",
      "---------------------\n",
      "Iteration Number: 1795\n",
      "Loss: 37.79991721410857\n",
      "l2 norm of gradients: 0.5036189304087209\n",
      "l2 norm of weights: 4.402330243740394\n",
      "---------------------\n",
      "Iteration Number: 1796\n",
      "Loss: 37.78008294982139\n",
      "l2 norm of gradients: 0.5033992047360005\n",
      "l2 norm of weights: 4.401299723641477\n",
      "---------------------\n",
      "Iteration Number: 1797\n",
      "Loss: 37.760262054625535\n",
      "l2 norm of gradients: 0.5031795236732983\n",
      "l2 norm of weights: 4.400269738519575\n",
      "---------------------\n",
      "Iteration Number: 1798\n",
      "Loss: 37.740454522091284\n",
      "l2 norm of gradients: 0.502959887225385\n",
      "l2 norm of weights: 4.399240288394578\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 1799\n",
      "Loss: 37.72066034579782\n",
      "l2 norm of gradients: 0.5027402953969067\n",
      "l2 norm of weights: 4.39821137328651\n",
      "---------------------\n",
      "Iteration Number: 1800\n",
      "Loss: 37.70087951932962\n",
      "l2 norm of gradients: 0.5025207481923841\n",
      "l2 norm of weights: 4.397182993215528\n",
      "---------------------\n",
      "Iteration Number: 1801\n",
      "Loss: 37.681112036281014\n",
      "l2 norm of gradients: 0.5023012456162139\n",
      "l2 norm of weights: 4.396155148201922\n",
      "---------------------\n",
      "Iteration Number: 1802\n",
      "Loss: 37.661357890250876\n",
      "l2 norm of gradients: 0.502081787672669\n",
      "l2 norm of weights: 4.395127838266111\n",
      "---------------------\n",
      "Iteration Number: 1803\n",
      "Loss: 37.6416170748462\n",
      "l2 norm of gradients: 0.5018623743658983\n",
      "l2 norm of weights: 4.394101063428644\n",
      "---------------------\n",
      "Iteration Number: 1804\n",
      "Loss: 37.621889583681856\n",
      "l2 norm of gradients: 0.5016430056999287\n",
      "l2 norm of weights: 4.393074823710197\n",
      "---------------------\n",
      "Iteration Number: 1805\n",
      "Loss: 37.602175410378656\n",
      "l2 norm of gradients: 0.5014236816786642\n",
      "l2 norm of weights: 4.3920491191315785\n",
      "---------------------\n",
      "Iteration Number: 1806\n",
      "Loss: 37.58247454856502\n",
      "l2 norm of gradients: 0.5012044023058869\n",
      "l2 norm of weights: 4.391023949713715\n",
      "---------------------\n",
      "Iteration Number: 1807\n",
      "Loss: 37.56278699187621\n",
      "l2 norm of gradients: 0.5009851675852579\n",
      "l2 norm of weights: 4.389999315477663\n",
      "---------------------\n",
      "Iteration Number: 1808\n",
      "Loss: 37.54311273395486\n",
      "l2 norm of gradients: 0.5007659775203173\n",
      "l2 norm of weights: 4.3889752164446\n",
      "---------------------\n",
      "Iteration Number: 1809\n",
      "Loss: 37.523451768450144\n",
      "l2 norm of gradients: 0.5005468321144847\n",
      "l2 norm of weights: 4.387951652635826\n",
      "---------------------\n",
      "Iteration Number: 1810\n",
      "Loss: 37.50380408901904\n",
      "l2 norm of gradients: 0.5003277313710607\n",
      "l2 norm of weights: 4.3869286240727625\n",
      "---------------------\n",
      "Iteration Number: 1811\n",
      "Loss: 37.48416968932532\n",
      "l2 norm of gradients: 0.5001086752932258\n",
      "l2 norm of weights: 4.38590613077695\n",
      "---------------------\n",
      "Iteration Number: 1812\n",
      "Loss: 37.464548563038605\n",
      "l2 norm of gradients: 0.49988966388404255\n",
      "l2 norm of weights: 4.384884172770046\n",
      "---------------------\n",
      "Iteration Number: 1813\n",
      "Loss: 37.444940703836934\n",
      "l2 norm of gradients: 0.49967069714645507\n",
      "l2 norm of weights: 4.383862750073827\n",
      "---------------------\n",
      "Iteration Number: 1814\n",
      "Loss: 37.425346105405076\n",
      "l2 norm of gradients: 0.49945177508329053\n",
      "l2 norm of weights: 4.382841862710184\n",
      "---------------------\n",
      "Iteration Number: 1815\n",
      "Loss: 37.4057647614341\n",
      "l2 norm of gradients: 0.49923289769725837\n",
      "l2 norm of weights: 4.381821510701125\n",
      "---------------------\n",
      "Iteration Number: 1816\n",
      "Loss: 37.386196665622464\n",
      "l2 norm of gradients: 0.49901406499095236\n",
      "l2 norm of weights: 4.380801694068767\n",
      "---------------------\n",
      "Iteration Number: 1817\n",
      "Loss: 37.3666418116749\n",
      "l2 norm of gradients: 0.4987952769668503\n",
      "l2 norm of weights: 4.379782412835342\n",
      "---------------------\n",
      "Iteration Number: 1818\n",
      "Loss: 37.34710019330397\n",
      "l2 norm of gradients: 0.49857653362731474\n",
      "l2 norm of weights: 4.378763667023192\n",
      "---------------------\n",
      "Iteration Number: 1819\n",
      "Loss: 37.32757180422844\n",
      "l2 norm of gradients: 0.49835783497459396\n",
      "l2 norm of weights: 4.377745456654768\n",
      "---------------------\n",
      "Iteration Number: 1820\n",
      "Loss: 37.30805663817445\n",
      "l2 norm of gradients: 0.49813918101082205\n",
      "l2 norm of weights: 4.3767277817526296\n",
      "---------------------\n",
      "Iteration Number: 1821\n",
      "Loss: 37.28855468887494\n",
      "l2 norm of gradients: 0.49792057173801996\n",
      "l2 norm of weights: 4.375710642339441\n",
      "---------------------\n",
      "Iteration Number: 1822\n",
      "Loss: 37.26906595006839\n",
      "l2 norm of gradients: 0.49770200715809565\n",
      "l2 norm of weights: 4.374694038437975\n",
      "---------------------\n",
      "Iteration Number: 1823\n",
      "Loss: 37.24959041550145\n",
      "l2 norm of gradients: 0.49748348727284547\n",
      "l2 norm of weights: 4.373677970071108\n",
      "---------------------\n",
      "Iteration Number: 1824\n",
      "Loss: 37.230128078927606\n",
      "l2 norm of gradients: 0.4972650120839541\n",
      "l2 norm of weights: 4.372662437261816\n",
      "---------------------\n",
      "Iteration Number: 1825\n",
      "Loss: 37.2106789341062\n",
      "l2 norm of gradients: 0.4970465815929952\n",
      "l2 norm of weights: 4.37164744003318\n",
      "---------------------\n",
      "Iteration Number: 1826\n",
      "Loss: 37.19124297480451\n",
      "l2 norm of gradients: 0.49682819580143284\n",
      "l2 norm of weights: 4.37063297840838\n",
      "---------------------\n",
      "Iteration Number: 1827\n",
      "Loss: 37.17182019479472\n",
      "l2 norm of gradients: 0.49660985471062097\n",
      "l2 norm of weights: 4.369619052410695\n",
      "---------------------\n",
      "Iteration Number: 1828\n",
      "Loss: 37.15241058785763\n",
      "l2 norm of gradients: 0.49639155832180526\n",
      "l2 norm of weights: 4.3686056620635005\n",
      "---------------------\n",
      "Iteration Number: 1829\n",
      "Loss: 37.133014147779804\n",
      "l2 norm of gradients: 0.4961733066361229\n",
      "l2 norm of weights: 4.36759280739027\n",
      "---------------------\n",
      "Iteration Number: 1830\n",
      "Loss: 37.11363086835498\n",
      "l2 norm of gradients: 0.4959550996546037\n",
      "l2 norm of weights: 4.36658048841457\n",
      "---------------------\n",
      "Iteration Number: 1831\n",
      "Loss: 37.0942607433822\n",
      "l2 norm of gradients: 0.4957369373781707\n",
      "l2 norm of weights: 4.3655687051600625\n",
      "---------------------\n",
      "Iteration Number: 1832\n",
      "Loss: 37.074903766669024\n",
      "l2 norm of gradients: 0.4955188198076408\n",
      "l2 norm of weights: 4.364557457650498\n",
      "---------------------\n",
      "Iteration Number: 1833\n",
      "Loss: 37.05555993202802\n",
      "l2 norm of gradients: 0.4953007469437255\n",
      "l2 norm of weights: 4.363546745909723\n",
      "---------------------\n",
      "Iteration Number: 1834\n",
      "Loss: 37.03622923327934\n",
      "l2 norm of gradients: 0.4950827187870314\n",
      "l2 norm of weights: 4.362536569961669\n",
      "---------------------\n",
      "Iteration Number: 1835\n",
      "Loss: 37.01691166424971\n",
      "l2 norm of gradients: 0.49486473533806147\n",
      "l2 norm of weights: 4.361526929830358\n",
      "---------------------\n",
      "Iteration Number: 1836\n",
      "Loss: 36.99760721877178\n",
      "l2 norm of gradients: 0.49464679659721517\n",
      "l2 norm of weights: 4.360517825539897\n",
      "---------------------\n",
      "Iteration Number: 1837\n",
      "Loss: 36.978315890685664\n",
      "l2 norm of gradients: 0.4944289025647891\n",
      "l2 norm of weights: 4.359509257114478\n",
      "---------------------\n",
      "Iteration Number: 1838\n",
      "Loss: 36.95903767383646\n",
      "l2 norm of gradients: 0.49421105324097864\n",
      "l2 norm of weights: 4.358501224578382\n",
      "---------------------\n",
      "Iteration Number: 1839\n",
      "Loss: 36.939772562077366\n",
      "l2 norm of gradients: 0.49399324862587746\n",
      "l2 norm of weights: 4.357493727955965\n",
      "---------------------\n",
      "Iteration Number: 1840\n",
      "Loss: 36.92052054926741\n",
      "l2 norm of gradients: 0.49377548871947935\n",
      "l2 norm of weights: 4.356486767271669\n",
      "---------------------\n",
      "Iteration Number: 1841\n",
      "Loss: 36.90128162927166\n",
      "l2 norm of gradients: 0.4935577735216779\n",
      "l2 norm of weights: 4.355480342550014\n",
      "---------------------\n",
      "Iteration Number: 1842\n",
      "Loss: 36.882055795962344\n",
      "l2 norm of gradients: 0.49334010303226844\n",
      "l2 norm of weights: 4.3544744538156\n",
      "---------------------\n",
      "Iteration Number: 1843\n",
      "Loss: 36.86284304321813\n",
      "l2 norm of gradients: 0.49312247725094766\n",
      "l2 norm of weights: 4.353469101093102\n",
      "---------------------\n",
      "Iteration Number: 1844\n",
      "Loss: 36.84364336492345\n",
      "l2 norm of gradients: 0.4929048961773151\n",
      "l2 norm of weights: 4.352464284407272\n",
      "---------------------\n",
      "Iteration Number: 1845\n",
      "Loss: 36.824456754969574\n",
      "l2 norm of gradients: 0.49268735981087364\n",
      "l2 norm of weights: 4.351460003782936\n",
      "---------------------\n",
      "Iteration Number: 1846\n",
      "Loss: 36.805283207253744\n",
      "l2 norm of gradients: 0.4924698681510306\n",
      "l2 norm of weights: 4.35045625924499\n",
      "---------------------\n",
      "Iteration Number: 1847\n",
      "Loss: 36.78612271568001\n",
      "l2 norm of gradients: 0.4922524211970981\n",
      "l2 norm of weights: 4.349453050818408\n",
      "---------------------\n",
      "Iteration Number: 1848\n",
      "Loss: 36.76697527415808\n",
      "l2 norm of gradients: 0.4920350189482939\n",
      "l2 norm of weights: 4.348450378528226\n",
      "---------------------\n",
      "Iteration Number: 1849\n",
      "Loss: 36.74784087660558\n",
      "l2 norm of gradients: 0.4918176614037424\n",
      "l2 norm of weights: 4.347448242399554\n",
      "---------------------\n",
      "Iteration Number: 1850\n",
      "Loss: 36.728719516944\n",
      "l2 norm of gradients: 0.4916003485624755\n",
      "l2 norm of weights: 4.346446642457566\n",
      "---------------------\n",
      "Iteration Number: 1851\n",
      "Loss: 36.70961118910329\n",
      "l2 norm of gradients: 0.4913830804234329\n",
      "l2 norm of weights: 4.345445578727504\n",
      "---------------------\n",
      "Iteration Number: 1852\n",
      "Loss: 36.690515887017746\n",
      "l2 norm of gradients: 0.4911658569854638\n",
      "l2 norm of weights: 4.344445051234672\n",
      "---------------------\n",
      "Iteration Number: 1853\n",
      "Loss: 36.671433604629826\n",
      "l2 norm of gradients: 0.4909486782473265\n",
      "l2 norm of weights: 4.34344506000444\n",
      "---------------------\n",
      "Iteration Number: 1854\n",
      "Loss: 36.65236433588605\n",
      "l2 norm of gradients: 0.49073154420769083\n",
      "l2 norm of weights: 4.342445605062235\n",
      "---------------------\n",
      "Iteration Number: 1855\n",
      "Loss: 36.63330807474155\n",
      "l2 norm of gradients: 0.4905144548651373\n",
      "l2 norm of weights: 4.341446686433547\n",
      "---------------------\n",
      "Iteration Number: 1856\n",
      "Loss: 36.61426481515523\n",
      "l2 norm of gradients: 0.4902974102181591\n",
      "l2 norm of weights: 4.340448304143924\n",
      "---------------------\n",
      "Iteration Number: 1857\n",
      "Loss: 36.5952345510939\n",
      "l2 norm of gradients: 0.49008041026516225\n",
      "l2 norm of weights: 4.339450458218969\n",
      "---------------------\n",
      "Iteration Number: 1858\n",
      "Loss: 36.57621727652941\n",
      "l2 norm of gradients: 0.48986345500446693\n",
      "l2 norm of weights: 4.338453148684343\n",
      "---------------------\n",
      "Iteration Number: 1859\n",
      "Loss: 36.5572129854402\n",
      "l2 norm of gradients: 0.4896465444343085\n",
      "l2 norm of weights: 4.337456375565761\n",
      "---------------------\n",
      "Iteration Number: 1860\n",
      "Loss: 36.53822167181031\n",
      "l2 norm of gradients: 0.48942967855283737\n",
      "l2 norm of weights: 4.336460138888989\n",
      "---------------------\n",
      "Iteration Number: 1861\n",
      "Loss: 36.51924332963076\n",
      "l2 norm of gradients: 0.48921285735812114\n",
      "l2 norm of weights: 4.3354644386798435\n",
      "---------------------\n",
      "Iteration Number: 1862\n",
      "Loss: 36.50027795289776\n",
      "l2 norm of gradients: 0.4889960808481443\n",
      "l2 norm of weights: 4.334469274964194\n",
      "---------------------\n",
      "Iteration Number: 1863\n",
      "Loss: 36.481325535613536\n",
      "l2 norm of gradients: 0.4887793490208099\n",
      "l2 norm of weights: 4.3334746477679555\n",
      "---------------------\n",
      "Iteration Number: 1864\n",
      "Loss: 36.46238607178676\n",
      "l2 norm of gradients: 0.48856266187394043\n",
      "l2 norm of weights: 4.332480557117091\n",
      "---------------------\n",
      "Iteration Number: 1865\n",
      "Loss: 36.44345955543207\n",
      "l2 norm of gradients: 0.4883460194052781\n",
      "l2 norm of weights: 4.331487003037606\n",
      "---------------------\n",
      "Iteration Number: 1866\n",
      "Loss: 36.42454598056923\n",
      "l2 norm of gradients: 0.48812942161248635\n",
      "l2 norm of weights: 4.330493985555553\n",
      "---------------------\n",
      "Iteration Number: 1867\n",
      "Loss: 36.40564534122481\n",
      "l2 norm of gradients: 0.4879128684931505\n",
      "l2 norm of weights: 4.329501504697028\n",
      "---------------------\n",
      "Iteration Number: 1868\n",
      "Loss: 36.38675763143083\n",
      "l2 norm of gradients: 0.4876963600447784\n",
      "l2 norm of weights: 4.328509560488161\n",
      "---------------------\n",
      "Iteration Number: 1869\n",
      "Loss: 36.36788284522548\n",
      "l2 norm of gradients: 0.48747989626480215\n",
      "l2 norm of weights: 4.327518152955128\n",
      "---------------------\n",
      "Iteration Number: 1870\n",
      "Loss: 36.34902097665254\n",
      "l2 norm of gradients: 0.48726347715057783\n",
      "l2 norm of weights: 4.326527282124141\n",
      "---------------------\n",
      "Iteration Number: 1871\n",
      "Loss: 36.330172019761484\n",
      "l2 norm of gradients: 0.4870471026993878\n",
      "l2 norm of weights: 4.325536948021447\n",
      "---------------------\n",
      "Iteration Number: 1872\n",
      "Loss: 36.31133596860779\n",
      "l2 norm of gradients: 0.4868307729084403\n",
      "l2 norm of weights: 4.324547150673327\n",
      "---------------------\n",
      "Iteration Number: 1873\n",
      "Loss: 36.292512817253424\n",
      "l2 norm of gradients: 0.4866144877748712\n",
      "l2 norm of weights: 4.323557890106098\n",
      "---------------------\n",
      "Iteration Number: 1874\n",
      "Loss: 36.27370255976415\n",
      "l2 norm of gradients: 0.48639824729574493\n",
      "l2 norm of weights: 4.322569166346107\n",
      "---------------------\n",
      "Iteration Number: 1875\n",
      "Loss: 36.254905190213904\n",
      "l2 norm of gradients: 0.4861820514680551\n",
      "l2 norm of weights: 4.321580979419731\n",
      "---------------------\n",
      "Iteration Number: 1876\n",
      "Loss: 36.23612070268039\n",
      "l2 norm of gradients: 0.4859659002887255\n",
      "l2 norm of weights: 4.320593329353377\n",
      "---------------------\n",
      "Iteration Number: 1877\n",
      "Loss: 36.217349091248316\n",
      "l2 norm of gradients: 0.48574979375461114\n",
      "l2 norm of weights: 4.3196062161734785\n",
      "---------------------\n",
      "Iteration Number: 1878\n",
      "Loss: 36.198590350006626\n",
      "l2 norm of gradients: 0.48553373186249943\n",
      "l2 norm of weights: 4.318619639906494\n",
      "---------------------\n",
      "Iteration Number: 1879\n",
      "Loss: 36.179844473051965\n",
      "l2 norm of gradients: 0.48531771460911094\n",
      "l2 norm of weights: 4.317633600578906\n",
      "---------------------\n",
      "Iteration Number: 1880\n",
      "Loss: 36.16111145448417\n",
      "l2 norm of gradients: 0.48510174199110007\n",
      "l2 norm of weights: 4.316648098217222\n",
      "---------------------\n",
      "Iteration Number: 1881\n",
      "Loss: 36.14239128841036\n",
      "l2 norm of gradients: 0.4848858140050565\n",
      "l2 norm of weights: 4.315663132847967\n",
      "---------------------\n",
      "Iteration Number: 1882\n",
      "Loss: 36.12368396894363\n",
      "l2 norm of gradients: 0.48466993064750613\n",
      "l2 norm of weights: 4.314678704497688\n",
      "---------------------\n",
      "Iteration Number: 1883\n",
      "Loss: 36.10498949020017\n",
      "l2 norm of gradients: 0.484454091914912\n",
      "l2 norm of weights: 4.313694813192949\n",
      "---------------------\n",
      "Iteration Number: 1884\n",
      "Loss: 36.08630784630463\n",
      "l2 norm of gradients: 0.48423829780367494\n",
      "l2 norm of weights: 4.312711458960331\n",
      "---------------------\n",
      "Iteration Number: 1885\n",
      "Loss: 36.06763903138509\n",
      "l2 norm of gradients: 0.48402254831013486\n",
      "l2 norm of weights: 4.31172864182643\n",
      "---------------------\n",
      "Iteration Number: 1886\n",
      "Loss: 36.048983039575965\n",
      "l2 norm of gradients: 0.4838068434305722\n",
      "l2 norm of weights: 4.310746361817854\n",
      "---------------------\n",
      "Iteration Number: 1887\n",
      "Loss: 36.03033986501676\n",
      "l2 norm of gradients: 0.4835911831612081\n",
      "l2 norm of weights: 4.309764618961224\n",
      "---------------------\n",
      "Iteration Number: 1888\n",
      "Loss: 36.0117095018532\n",
      "l2 norm of gradients: 0.48337556749820604\n",
      "l2 norm of weights: 4.30878341328317\n",
      "---------------------\n",
      "Iteration Number: 1889\n",
      "Loss: 35.99309194423501\n",
      "l2 norm of gradients: 0.4831599964376723\n",
      "l2 norm of weights: 4.307802744810333\n",
      "---------------------\n",
      "Iteration Number: 1890\n",
      "Loss: 35.97448718631881\n",
      "l2 norm of gradients: 0.4829444699756579\n",
      "l2 norm of weights: 4.306822613569359\n",
      "---------------------\n",
      "Iteration Number: 1891\n",
      "Loss: 35.95589522226549\n",
      "l2 norm of gradients: 0.48272898810815845\n",
      "l2 norm of weights: 4.3058430195868995\n",
      "---------------------\n",
      "Iteration Number: 1892\n",
      "Loss: 35.937316046241946\n",
      "l2 norm of gradients: 0.48251355083111624\n",
      "l2 norm of weights: 4.304863962889612\n",
      "---------------------\n",
      "Iteration Number: 1893\n",
      "Loss: 35.91874965241938\n",
      "l2 norm of gradients: 0.48229815814042054\n",
      "l2 norm of weights: 4.303885443504154\n",
      "---------------------\n",
      "Iteration Number: 1894\n",
      "Loss: 35.90019603497572\n",
      "l2 norm of gradients: 0.4820828100319092\n",
      "l2 norm of weights: 4.302907461457185\n",
      "---------------------\n",
      "Iteration Number: 1895\n",
      "Loss: 35.88165518809337\n",
      "l2 norm of gradients: 0.4818675065013693\n",
      "l2 norm of weights: 4.301930016775364\n",
      "---------------------\n",
      "Iteration Number: 1896\n",
      "Loss: 35.86312710595934\n",
      "l2 norm of gradients: 0.48165224754453817\n",
      "l2 norm of weights: 4.300953109485348\n",
      "---------------------\n",
      "Iteration Number: 1897\n",
      "Loss: 35.84461178276712\n",
      "l2 norm of gradients: 0.48143703315710507\n",
      "l2 norm of weights: 4.299976739613788\n",
      "---------------------\n",
      "Iteration Number: 1898\n",
      "Loss: 35.82610921271438\n",
      "l2 norm of gradients: 0.4812218633347114\n",
      "l2 norm of weights: 4.299000907187332\n",
      "---------------------\n",
      "Iteration Number: 1899\n",
      "Loss: 35.80761939000456\n",
      "l2 norm of gradients: 0.48100673807295236\n",
      "l2 norm of weights: 4.298025612232619\n",
      "---------------------\n",
      "Iteration Number: 1900\n",
      "Loss: 35.78914230884606\n",
      "l2 norm of gradients: 0.4807916573673778\n",
      "l2 norm of weights: 4.297050854776282\n",
      "---------------------\n",
      "Iteration Number: 1901\n",
      "Loss: 35.77067796345242\n",
      "l2 norm of gradients: 0.4805766212134934\n",
      "l2 norm of weights: 4.296076634844941\n",
      "---------------------\n",
      "Iteration Number: 1902\n",
      "Loss: 35.75222634804125\n",
      "l2 norm of gradients: 0.48036162960676176\n",
      "l2 norm of weights: 4.2951029524652045\n",
      "---------------------\n",
      "Iteration Number: 1903\n",
      "Loss: 35.73378745683686\n",
      "l2 norm of gradients: 0.4801466825426031\n",
      "l2 norm of weights: 4.29412980766367\n",
      "---------------------\n",
      "Iteration Number: 1904\n",
      "Loss: 35.715361284068\n",
      "l2 norm of gradients: 0.47993178001639714\n",
      "l2 norm of weights: 4.293157200466918\n",
      "---------------------\n",
      "Iteration Number: 1905\n",
      "Loss: 35.69694782396764\n",
      "l2 norm of gradients: 0.47971692202348337\n",
      "l2 norm of weights: 4.292185130901512\n",
      "---------------------\n",
      "Iteration Number: 1906\n",
      "Loss: 35.67854707077495\n",
      "l2 norm of gradients: 0.47950210855916275\n",
      "l2 norm of weights: 4.2912135989939975\n",
      "---------------------\n",
      "Iteration Number: 1907\n",
      "Loss: 35.660159018732884\n",
      "l2 norm of gradients: 0.4792873396186984\n",
      "l2 norm of weights: 4.290242604770903\n",
      "---------------------\n",
      "Iteration Number: 1908\n",
      "Loss: 35.64178366209035\n",
      "l2 norm of gradients: 0.479072615197317\n",
      "l2 norm of weights: 4.289272148258732\n",
      "---------------------\n",
      "Iteration Number: 1909\n",
      "Loss: 35.62342099510053\n",
      "l2 norm of gradients: 0.4788579352902098\n",
      "l2 norm of weights: 4.288302229483966\n",
      "---------------------\n",
      "Iteration Number: 1910\n",
      "Loss: 35.60507101202157\n",
      "l2 norm of gradients: 0.4786432998925337\n",
      "l2 norm of weights: 4.287332848473064\n",
      "---------------------\n",
      "Iteration Number: 1911\n",
      "Loss: 35.586733707116394\n",
      "l2 norm of gradients: 0.47842870899941253\n",
      "l2 norm of weights: 4.286364005252455\n",
      "---------------------\n",
      "Iteration Number: 1912\n",
      "Loss: 35.568409074653076\n",
      "l2 norm of gradients: 0.4782141626059378\n",
      "l2 norm of weights: 4.285395699848544\n",
      "---------------------\n",
      "Iteration Number: 1913\n",
      "Loss: 35.550097108904225\n",
      "l2 norm of gradients: 0.47799966070717026\n",
      "l2 norm of weights: 4.284427932287704\n",
      "---------------------\n",
      "Iteration Number: 1914\n",
      "Loss: 35.53179780414766\n",
      "l2 norm of gradients: 0.4777852032981409\n",
      "l2 norm of weights: 4.283460702596278\n",
      "---------------------\n",
      "Iteration Number: 1915\n",
      "Loss: 35.5135111546648\n",
      "l2 norm of gradients: 0.47757079037385214\n",
      "l2 norm of weights: 4.282494010800575\n",
      "---------------------\n",
      "Iteration Number: 1916\n",
      "Loss: 35.49523715474352\n",
      "l2 norm of gradients: 0.4773564219292784\n",
      "l2 norm of weights: 4.281527856926873\n",
      "---------------------\n",
      "Iteration Number: 1917\n",
      "Loss: 35.4769757986743\n",
      "l2 norm of gradients: 0.47714209795936835\n",
      "l2 norm of weights: 4.280562241001411\n",
      "---------------------\n",
      "Iteration Number: 1918\n",
      "Loss: 35.45872708075426\n",
      "l2 norm of gradients: 0.47692781845904525\n",
      "l2 norm of weights: 4.27959716305039\n",
      "---------------------\n",
      "Iteration Number: 1919\n",
      "Loss: 35.44049099528375\n",
      "l2 norm of gradients: 0.47671358342320796\n",
      "l2 norm of weights: 4.278632623099975\n",
      "---------------------\n",
      "Iteration Number: 1920\n",
      "Loss: 35.42226753656859\n",
      "l2 norm of gradients: 0.4764993928467329\n",
      "l2 norm of weights: 4.277668621176288\n",
      "---------------------\n",
      "Iteration Number: 1921\n",
      "Loss: 35.4040566989188\n",
      "l2 norm of gradients: 0.4762852467244746\n",
      "l2 norm of weights: 4.27670515730541\n",
      "---------------------\n",
      "Iteration Number: 1922\n",
      "Loss: 35.385858476648764\n",
      "l2 norm of gradients: 0.4760711450512669\n",
      "l2 norm of weights: 4.275742231513376\n",
      "---------------------\n",
      "Iteration Number: 1923\n",
      "Loss: 35.36767286407781\n",
      "l2 norm of gradients: 0.4758570878219243\n",
      "l2 norm of weights: 4.274779843826177\n",
      "---------------------\n",
      "Iteration Number: 1924\n",
      "Loss: 35.349499855529615\n",
      "l2 norm of gradients: 0.47564307503124315\n",
      "l2 norm of weights: 4.273817994269756\n",
      "---------------------\n",
      "Iteration Number: 1925\n",
      "Loss: 35.33133944533251\n",
      "l2 norm of gradients: 0.47542910667400273\n",
      "l2 norm of weights: 4.272856682870008\n",
      "---------------------\n",
      "Iteration Number: 1926\n",
      "Loss: 35.313191627818554\n",
      "l2 norm of gradients: 0.4752151827449663\n",
      "l2 norm of weights: 4.271895909652774\n",
      "---------------------\n",
      "Iteration Number: 1927\n",
      "Loss: 35.29505639732559\n",
      "l2 norm of gradients: 0.47500130323888273\n",
      "l2 norm of weights: 4.27093567464385\n",
      "---------------------\n",
      "Iteration Number: 1928\n",
      "Loss: 35.27693374819429\n",
      "l2 norm of gradients: 0.4747874681504872\n",
      "l2 norm of weights: 4.269975977868972\n",
      "---------------------\n",
      "Iteration Number: 1929\n",
      "Loss: 35.258823674770554\n",
      "l2 norm of gradients: 0.47457367747450246\n",
      "l2 norm of weights: 4.269016819353821\n",
      "---------------------\n",
      "Iteration Number: 1930\n",
      "Loss: 35.24072617140477\n",
      "l2 norm of gradients: 0.47435993120564063\n",
      "l2 norm of weights: 4.268058199124023\n",
      "---------------------\n",
      "Iteration Number: 1931\n",
      "Loss: 35.222641232451124\n",
      "l2 norm of gradients: 0.4741462293386033\n",
      "l2 norm of weights: 4.267100117205147\n",
      "---------------------\n",
      "Iteration Number: 1932\n",
      "Loss: 35.204568852268075\n",
      "l2 norm of gradients: 0.47393257186808374\n",
      "l2 norm of weights: 4.2661425736226954\n",
      "---------------------\n",
      "Iteration Number: 1933\n",
      "Loss: 35.18650902521914\n",
      "l2 norm of gradients: 0.4737189587887678\n",
      "l2 norm of weights: 4.265185568402116\n",
      "---------------------\n",
      "Iteration Number: 1934\n",
      "Loss: 35.16846174567094\n",
      "l2 norm of gradients: 0.47350539009533466\n",
      "l2 norm of weights: 4.264229101568788\n",
      "---------------------\n",
      "Iteration Number: 1935\n",
      "Loss: 35.15042700799564\n",
      "l2 norm of gradients: 0.47329186578245885\n",
      "l2 norm of weights: 4.2632731731480265\n",
      "---------------------\n",
      "Iteration Number: 1936\n",
      "Loss: 35.13240480656769\n",
      "l2 norm of gradients: 0.47307838584481055\n",
      "l2 norm of weights: 4.262317783165081\n",
      "---------------------\n",
      "Iteration Number: 1937\n",
      "Loss: 35.114395135767836\n",
      "l2 norm of gradients: 0.4728649502770578\n",
      "l2 norm of weights: 4.261362931645131\n",
      "---------------------\n",
      "Iteration Number: 1938\n",
      "Loss: 35.096397989978996\n",
      "l2 norm of gradients: 0.4726515590738668\n",
      "l2 norm of weights: 4.260408618613286\n",
      "---------------------\n",
      "Iteration Number: 1939\n",
      "Loss: 35.07841336358996\n",
      "l2 norm of gradients: 0.47243821222990384\n",
      "l2 norm of weights: 4.259454844094584\n",
      "---------------------\n",
      "Iteration Number: 1940\n",
      "Loss: 35.06044125099205\n",
      "l2 norm of gradients: 0.47222490973983594\n",
      "l2 norm of weights: 4.258501608113991\n",
      "---------------------\n",
      "Iteration Number: 1941\n",
      "Loss: 35.042481646581805\n",
      "l2 norm of gradients: 0.4720116515983328\n",
      "l2 norm of weights: 4.257548910696394\n",
      "---------------------\n",
      "Iteration Number: 1942\n",
      "Loss: 35.024534544758616\n",
      "l2 norm of gradients: 0.471798437800067\n",
      "l2 norm of weights: 4.256596751866607\n",
      "---------------------\n",
      "Iteration Number: 1943\n",
      "Loss: 35.006599939926836\n",
      "l2 norm of gradients: 0.4715852683397164\n",
      "l2 norm of weights: 4.2556451316493655\n",
      "---------------------\n",
      "Iteration Number: 1944\n",
      "Loss: 34.98867782649425\n",
      "l2 norm of gradients: 0.47137214321196447\n",
      "l2 norm of weights: 4.254694050069321\n",
      "---------------------\n",
      "Iteration Number: 1945\n",
      "Loss: 34.97076819887309\n",
      "l2 norm of gradients: 0.471159062411502\n",
      "l2 norm of weights: 4.253743507151047\n",
      "---------------------\n",
      "Iteration Number: 1946\n",
      "Loss: 34.9528710514786\n",
      "l2 norm of gradients: 0.47094602593302815\n",
      "l2 norm of weights: 4.252793502919033\n",
      "---------------------\n",
      "Iteration Number: 1947\n",
      "Loss: 34.934986378731026\n",
      "l2 norm of gradients: 0.4707330337712519\n",
      "l2 norm of weights: 4.251844037397683\n",
      "---------------------\n",
      "Iteration Number: 1948\n",
      "Loss: 34.917114175052944\n",
      "l2 norm of gradients: 0.47052008592089295\n",
      "l2 norm of weights: 4.250895110611315\n",
      "---------------------\n",
      "Iteration Number: 1949\n",
      "Loss: 34.899254434872674\n",
      "l2 norm of gradients: 0.47030718237668334\n",
      "l2 norm of weights: 4.2499467225841565\n",
      "---------------------\n",
      "Iteration Number: 1950\n",
      "Loss: 34.88140715262023\n",
      "l2 norm of gradients: 0.4700943231333685\n",
      "l2 norm of weights: 4.248998873340348\n",
      "---------------------\n",
      "Iteration Number: 1951\n",
      "Loss: 34.863572322731116\n",
      "l2 norm of gradients: 0.4698815081857086\n",
      "l2 norm of weights: 4.248051562903936\n",
      "---------------------\n",
      "Iteration Number: 1952\n",
      "Loss: 34.84574993964342\n",
      "l2 norm of gradients: 0.46966873752847965\n",
      "l2 norm of weights: 4.247104791298876\n",
      "---------------------\n",
      "Iteration Number: 1953\n",
      "Loss: 34.82793999779953\n",
      "l2 norm of gradients: 0.46945601115647473\n",
      "l2 norm of weights: 4.246158558549028\n",
      "---------------------\n",
      "Iteration Number: 1954\n",
      "Loss: 34.81014249164533\n",
      "l2 norm of gradients: 0.4692433290645059\n",
      "l2 norm of weights: 4.245212864678153\n",
      "---------------------\n",
      "Iteration Number: 1955\n",
      "Loss: 34.79235741563052\n",
      "l2 norm of gradients: 0.4690306912474043\n",
      "l2 norm of weights: 4.244267709709916\n",
      "---------------------\n",
      "Iteration Number: 1956\n",
      "Loss: 34.77458476420807\n",
      "l2 norm of gradients: 0.4688180977000224\n",
      "l2 norm of weights: 4.243323093667884\n",
      "---------------------\n",
      "Iteration Number: 1957\n",
      "Loss: 34.75682453183455\n",
      "l2 norm of gradients: 0.46860554841723523\n",
      "l2 norm of weights: 4.242379016575519\n",
      "---------------------\n",
      "Iteration Number: 1958\n",
      "Loss: 34.73907671297047\n",
      "l2 norm of gradients: 0.46839304339394056\n",
      "l2 norm of weights: 4.2414354784561805\n",
      "---------------------\n",
      "Iteration Number: 1959\n",
      "Loss: 34.7213413020793\n",
      "l2 norm of gradients: 0.4681805826250617\n",
      "l2 norm of weights: 4.240492479333125\n",
      "---------------------\n",
      "Iteration Number: 1960\n",
      "Loss: 34.70361829362824\n",
      "l2 norm of gradients: 0.4679681661055476\n",
      "l2 norm of weights: 4.239550019229502\n",
      "---------------------\n",
      "Iteration Number: 1961\n",
      "Loss: 34.68590768208831\n",
      "l2 norm of gradients: 0.4677557938303751\n",
      "l2 norm of weights: 4.238608098168351\n",
      "---------------------\n",
      "Iteration Number: 1962\n",
      "Loss: 34.668209461933614\n",
      "l2 norm of gradients: 0.46754346579454875\n",
      "l2 norm of weights: 4.237666716172604\n",
      "---------------------\n",
      "Iteration Number: 1963\n",
      "Loss: 34.65052362764191\n",
      "l2 norm of gradients: 0.4673311819931039\n",
      "l2 norm of weights: 4.2367258732650805\n",
      "---------------------\n",
      "Iteration Number: 1964\n",
      "Loss: 34.63285017369372\n",
      "l2 norm of gradients: 0.4671189424211067\n",
      "l2 norm of weights: 4.235785569468488\n",
      "---------------------\n",
      "Iteration Number: 1965\n",
      "Loss: 34.615189094573545\n",
      "l2 norm of gradients: 0.46690674707365576\n",
      "l2 norm of weights: 4.234845804805419\n",
      "---------------------\n",
      "Iteration Number: 1966\n",
      "Loss: 34.59754038476919\n",
      "l2 norm of gradients: 0.4666945959458833\n",
      "l2 norm of weights: 4.233906579298347\n",
      "---------------------\n",
      "Iteration Number: 1967\n",
      "Loss: 34.57990403877105\n",
      "l2 norm of gradients: 0.46648248903295725\n",
      "l2 norm of weights: 4.232967892969632\n",
      "---------------------\n",
      "Iteration Number: 1968\n",
      "Loss: 34.562280051073685\n",
      "l2 norm of gradients: 0.466270426330081\n",
      "l2 norm of weights: 4.232029745841512\n",
      "---------------------\n",
      "Iteration Number: 1969\n",
      "Loss: 34.544668416174396\n",
      "l2 norm of gradients: 0.46605840783249597\n",
      "l2 norm of weights: 4.231092137936104\n",
      "---------------------\n",
      "Iteration Number: 1970\n",
      "Loss: 34.527069128573835\n",
      "l2 norm of gradients: 0.46584643353548266\n",
      "l2 norm of weights: 4.230155069275401\n",
      "---------------------\n",
      "Iteration Number: 1971\n",
      "Loss: 34.50948218277553\n",
      "l2 norm of gradients: 0.4656345034343614\n",
      "l2 norm of weights: 4.229218539881274\n",
      "---------------------\n",
      "Iteration Number: 1972\n",
      "Loss: 34.49190757328683\n",
      "l2 norm of gradients: 0.4654226175244942\n",
      "l2 norm of weights: 4.228282549775465\n",
      "---------------------\n",
      "Iteration Number: 1973\n",
      "Loss: 34.47434529461737\n",
      "l2 norm of gradients: 0.4652107758012859\n",
      "l2 norm of weights: 4.227347098979592\n",
      "---------------------\n",
      "Iteration Number: 1974\n",
      "Loss: 34.456795341280056\n",
      "l2 norm of gradients: 0.46499897826018527\n",
      "l2 norm of weights: 4.2264121875151375\n",
      "---------------------\n",
      "Iteration Number: 1975\n",
      "Loss: 34.439257707791725\n",
      "l2 norm of gradients: 0.4647872248966866\n",
      "l2 norm of weights: 4.225477815403459\n",
      "---------------------\n",
      "Iteration Number: 1976\n",
      "Loss: 34.42173238867085\n",
      "l2 norm of gradients: 0.4645755157063306\n",
      "l2 norm of weights: 4.224543982665779\n",
      "---------------------\n",
      "Iteration Number: 1977\n",
      "Loss: 34.40421937844034\n",
      "l2 norm of gradients: 0.46436385068470615\n",
      "l2 norm of weights: 4.223610689323183\n",
      "---------------------\n",
      "Iteration Number: 1978\n",
      "Loss: 34.38671867162466\n",
      "l2 norm of gradients: 0.46415222982745136\n",
      "l2 norm of weights: 4.222677935396626\n",
      "---------------------\n",
      "Iteration Number: 1979\n",
      "Loss: 34.369230262751934\n",
      "l2 norm of gradients: 0.463940653130255\n",
      "l2 norm of weights: 4.22174572090692\n",
      "---------------------\n",
      "Iteration Number: 1980\n",
      "Loss: 34.35175414635358\n",
      "l2 norm of gradients: 0.4637291205888574\n",
      "l2 norm of weights: 4.220814045874741\n",
      "---------------------\n",
      "Iteration Number: 1981\n",
      "Loss: 34.33429031696353\n",
      "l2 norm of gradients: 0.46351763219905223\n",
      "l2 norm of weights: 4.219882910320623\n",
      "---------------------\n",
      "Iteration Number: 1982\n",
      "Loss: 34.31683876911806\n",
      "l2 norm of gradients: 0.46330618795668765\n",
      "l2 norm of weights: 4.218952314264956\n",
      "---------------------\n",
      "Iteration Number: 1983\n",
      "Loss: 34.29939949735699\n",
      "l2 norm of gradients: 0.46309478785766767\n",
      "l2 norm of weights: 4.2180222577279896\n",
      "---------------------\n",
      "Iteration Number: 1984\n",
      "Loss: 34.2819724962224\n",
      "l2 norm of gradients: 0.46288343189795295\n",
      "l2 norm of weights: 4.217092740729824\n",
      "---------------------\n",
      "Iteration Number: 1985\n",
      "Loss: 34.26455776025935\n",
      "l2 norm of gradients: 0.462672120073563\n",
      "l2 norm of weights: 4.216163763290414\n",
      "---------------------\n",
      "Iteration Number: 1986\n",
      "Loss: 34.24715528401632\n",
      "l2 norm of gradients: 0.46246085238057677\n",
      "l2 norm of weights: 4.215235325429563\n",
      "---------------------\n",
      "Iteration Number: 1987\n",
      "Loss: 34.22976506204275\n",
      "l2 norm of gradients: 0.46224962881513426\n",
      "l2 norm of weights: 4.214307427166927\n",
      "---------------------\n",
      "Iteration Number: 1988\n",
      "Loss: 34.212387088892534\n",
      "l2 norm of gradients: 0.46203844937343763\n",
      "l2 norm of weights: 4.213380068522008\n",
      "---------------------\n",
      "Iteration Number: 1989\n",
      "Loss: 34.19502135912132\n",
      "l2 norm of gradients: 0.4618273140517526\n",
      "l2 norm of weights: 4.212453249514153\n",
      "---------------------\n",
      "Iteration Number: 1990\n",
      "Loss: 34.17766786728741\n",
      "l2 norm of gradients: 0.46161622284640996\n",
      "l2 norm of weights: 4.211526970162556\n",
      "---------------------\n",
      "Iteration Number: 1991\n",
      "Loss: 34.160326607952214\n",
      "l2 norm of gradients: 0.4614051757538066\n",
      "l2 norm of weights: 4.210601230486251\n",
      "---------------------\n",
      "Iteration Number: 1992\n",
      "Loss: 34.14299757567917\n",
      "l2 norm of gradients: 0.4611941727704068\n",
      "l2 norm of weights: 4.209676030504116\n",
      "---------------------\n",
      "Iteration Number: 1993\n",
      "Loss: 34.12568076503392\n",
      "l2 norm of gradients: 0.4609832138927437\n",
      "l2 norm of weights: 4.208751370234865\n",
      "---------------------\n",
      "Iteration Number: 1994\n",
      "Loss: 34.10837617058513\n",
      "l2 norm of gradients: 0.46077229911742074\n",
      "l2 norm of weights: 4.207827249697055\n",
      "---------------------\n",
      "Iteration Number: 1995\n",
      "Loss: 34.09108378690445\n",
      "l2 norm of gradients: 0.4605614284411122\n",
      "l2 norm of weights: 4.206903668909075\n",
      "---------------------\n",
      "Iteration Number: 1996\n",
      "Loss: 34.07380360856478\n",
      "l2 norm of gradients: 0.4603506018605658\n",
      "l2 norm of weights: 4.205980627889151\n",
      "---------------------\n",
      "Iteration Number: 1997\n",
      "Loss: 34.05653563014213\n",
      "l2 norm of gradients: 0.46013981937260273\n",
      "l2 norm of weights: 4.205058126655341\n",
      "---------------------\n",
      "Iteration Number: 1998\n",
      "Loss: 34.03927984621468\n",
      "l2 norm of gradients: 0.4599290809741198\n",
      "l2 norm of weights: 4.204136165225536\n",
      "---------------------\n",
      "Iteration Number: 1999\n",
      "Loss: 34.02203625136304\n",
      "l2 norm of gradients: 0.45971838666209003\n",
      "l2 norm of weights: 4.203214743617455\n",
      "---------------------\n",
      "Iteration Number: 2000\n",
      "Loss: 34.00480484017025\n",
      "l2 norm of gradients: 0.4595077364335649\n",
      "l2 norm of weights: 4.202293861848647\n",
      "---------------------\n",
      "Iteration Number: 2001\n",
      "Loss: 33.987585607220986\n",
      "l2 norm of gradients: 0.4592971302856748\n",
      "l2 norm of weights: 4.201373519936486\n",
      "---------------------\n",
      "Iteration Number: 2002\n",
      "Loss: 33.9703785471034\n",
      "l2 norm of gradients: 0.45908656821563065\n",
      "l2 norm of weights: 4.200453717898175\n",
      "---------------------\n",
      "Iteration Number: 2003\n",
      "Loss: 33.95318365440685\n",
      "l2 norm of gradients: 0.45887605022072525\n",
      "l2 norm of weights: 4.199534455750736\n",
      "---------------------\n",
      "Iteration Number: 2004\n",
      "Loss: 33.936000923723206\n",
      "l2 norm of gradients: 0.4586655762983345\n",
      "l2 norm of weights: 4.198615733511016\n",
      "---------------------\n",
      "Iteration Number: 2005\n",
      "Loss: 33.91883034964588\n",
      "l2 norm of gradients: 0.4584551464459187\n",
      "l2 norm of weights: 4.197697551195679\n",
      "---------------------\n",
      "Iteration Number: 2006\n",
      "Loss: 33.90167192677211\n",
      "l2 norm of gradients: 0.458244760661024\n",
      "l2 norm of weights: 4.196779908821213\n",
      "---------------------\n",
      "Iteration Number: 2007\n",
      "Loss: 33.88452564969922\n",
      "l2 norm of gradients: 0.4580344189412833\n",
      "l2 norm of weights: 4.195862806403919\n",
      "---------------------\n",
      "Iteration Number: 2008\n",
      "Loss: 33.86739151302778\n",
      "l2 norm of gradients: 0.4578241212844181\n",
      "l2 norm of weights: 4.194946243959915\n",
      "---------------------\n",
      "Iteration Number: 2009\n",
      "Loss: 33.850269511360125\n",
      "l2 norm of gradients: 0.45761386768823936\n",
      "l2 norm of weights: 4.194030221505133\n",
      "---------------------\n",
      "Iteration Number: 2010\n",
      "Loss: 33.8331596393009\n",
      "l2 norm of gradients: 0.4574036581506491\n",
      "l2 norm of weights: 4.193114739055318\n",
      "---------------------\n",
      "Iteration Number: 2011\n",
      "Loss: 33.8160618914563\n",
      "l2 norm of gradients: 0.45719349266964127\n",
      "l2 norm of weights: 4.192199796626024\n",
      "---------------------\n",
      "Iteration Number: 2012\n",
      "Loss: 33.79897626243442\n",
      "l2 norm of gradients: 0.4569833712433035\n",
      "l2 norm of weights: 4.191285394232617\n",
      "---------------------\n",
      "Iteration Number: 2013\n",
      "Loss: 33.78190274684602\n",
      "l2 norm of gradients: 0.4567732938698183\n",
      "l2 norm of weights: 4.19037153189027\n",
      "---------------------\n",
      "Iteration Number: 2014\n",
      "Loss: 33.76484133930256\n",
      "l2 norm of gradients: 0.45656326054746393\n",
      "l2 norm of weights: 4.18945820961396\n",
      "---------------------\n",
      "Iteration Number: 2015\n",
      "Loss: 33.7477920344184\n",
      "l2 norm of gradients: 0.4563532712746166\n",
      "l2 norm of weights: 4.188545427418472\n",
      "---------------------\n",
      "Iteration Number: 2016\n",
      "Loss: 33.730754826808884\n",
      "l2 norm of gradients: 0.4561433260497504\n",
      "l2 norm of weights: 4.187633185318391\n",
      "---------------------\n",
      "Iteration Number: 2017\n",
      "Loss: 33.713729711092704\n",
      "l2 norm of gradients: 0.45593342487144\n",
      "l2 norm of weights: 4.186721483328105\n",
      "---------------------\n",
      "Iteration Number: 2018\n",
      "Loss: 33.696716681888\n",
      "l2 norm of gradients: 0.4557235677383611\n",
      "l2 norm of weights: 4.185810321461803\n",
      "---------------------\n",
      "Iteration Number: 2019\n",
      "Loss: 33.67971573381697\n",
      "l2 norm of gradients: 0.45551375464929167\n",
      "l2 norm of weights: 4.18489969973347\n",
      "---------------------\n",
      "Iteration Number: 2020\n",
      "Loss: 33.662726861501916\n",
      "l2 norm of gradients: 0.45530398560311375\n",
      "l2 norm of weights: 4.1839896181568905\n",
      "---------------------\n",
      "Iteration Number: 2021\n",
      "Loss: 33.64575005956748\n",
      "l2 norm of gradients: 0.4550942605988145\n",
      "l2 norm of weights: 4.183080076745642\n",
      "---------------------\n",
      "Iteration Number: 2022\n",
      "Loss: 33.62878532263964\n",
      "l2 norm of gradients: 0.4548845796354872\n",
      "l2 norm of weights: 4.1821710755130965\n",
      "---------------------\n",
      "Iteration Number: 2023\n",
      "Loss: 33.61183264534635\n",
      "l2 norm of gradients: 0.4546749427123329\n",
      "l2 norm of weights: 4.181262614472419\n",
      "---------------------\n",
      "Iteration Number: 2024\n",
      "Loss: 33.59489202231732\n",
      "l2 norm of gradients: 0.4544653498286615\n",
      "l2 norm of weights: 4.1803546936365645\n",
      "---------------------\n",
      "Iteration Number: 2025\n",
      "Loss: 33.57796344818336\n",
      "l2 norm of gradients: 0.4542558009838931\n",
      "l2 norm of weights: 4.179447313018278\n",
      "---------------------\n",
      "Iteration Number: 2026\n",
      "Loss: 33.56104691757684\n",
      "l2 norm of gradients: 0.4540462961775592\n",
      "l2 norm of weights: 4.17854047263009\n",
      "---------------------\n",
      "Iteration Number: 2027\n",
      "Loss: 33.54414242513193\n",
      "l2 norm of gradients: 0.45383683540930425\n",
      "l2 norm of weights: 4.177634172484319\n",
      "---------------------\n",
      "Iteration Number: 2028\n",
      "Loss: 33.527249965484174\n",
      "l2 norm of gradients: 0.4536274186788863\n",
      "l2 norm of weights: 4.176728412593069\n",
      "---------------------\n",
      "Iteration Number: 2029\n",
      "Loss: 33.51036953327021\n",
      "l2 norm of gradients: 0.45341804598617885\n",
      "l2 norm of weights: 4.175823192968223\n",
      "---------------------\n",
      "Iteration Number: 2030\n",
      "Loss: 33.49350112312878\n",
      "l2 norm of gradients: 0.4532087173311719\n",
      "l2 norm of weights: 4.174918513621451\n",
      "---------------------\n",
      "Iteration Number: 2031\n",
      "Loss: 33.47664472969937\n",
      "l2 norm of gradients: 0.4529994327139734\n",
      "l2 norm of weights: 4.174014374564198\n",
      "---------------------\n",
      "Iteration Number: 2032\n",
      "Loss: 33.45980034762332\n",
      "l2 norm of gradients: 0.45279019213480987\n",
      "l2 norm of weights: 4.173110775807691\n",
      "---------------------\n",
      "Iteration Number: 2033\n",
      "Loss: 33.44296797154283\n",
      "l2 norm of gradients: 0.45258099559402865\n",
      "l2 norm of weights: 4.172207717362934\n",
      "---------------------\n",
      "Iteration Number: 2034\n",
      "Loss: 33.4261475961017\n",
      "l2 norm of gradients: 0.45237184309209816\n",
      "l2 norm of weights: 4.171305199240702\n",
      "---------------------\n",
      "Iteration Number: 2035\n",
      "Loss: 33.40933921594506\n",
      "l2 norm of gradients: 0.4521627346296099\n",
      "l2 norm of weights: 4.170403221451551\n",
      "---------------------\n",
      "Iteration Number: 2036\n",
      "Loss: 33.392542825719104\n",
      "l2 norm of gradients: 0.4519536702072793\n",
      "l2 norm of weights: 4.169501784005803\n",
      "---------------------\n",
      "Iteration Number: 2037\n",
      "Loss: 33.375758420071406\n",
      "l2 norm of gradients: 0.45174464982594703\n",
      "l2 norm of weights: 4.168600886913556\n",
      "---------------------\n",
      "Iteration Number: 2038\n",
      "Loss: 33.358985993650414\n",
      "l2 norm of gradients: 0.4515356734865804\n",
      "l2 norm of weights: 4.167700530184672\n",
      "---------------------\n",
      "Iteration Number: 2039\n",
      "Loss: 33.34222554110573\n",
      "l2 norm of gradients: 0.4513267411902744\n",
      "l2 norm of weights: 4.166800713828789\n",
      "---------------------\n",
      "Iteration Number: 2040\n",
      "Loss: 33.32547705708843\n",
      "l2 norm of gradients: 0.451117852938253\n",
      "l2 norm of weights: 4.165901437855305\n",
      "---------------------\n",
      "Iteration Number: 2041\n",
      "Loss: 33.30874053625084\n",
      "l2 norm of gradients: 0.45090900873187056\n",
      "l2 norm of weights: 4.165002702273384\n",
      "---------------------\n",
      "Iteration Number: 2042\n",
      "Loss: 33.29201597324571\n",
      "l2 norm of gradients: 0.45070020857261256\n",
      "l2 norm of weights: 4.164104507091956\n",
      "---------------------\n",
      "Iteration Number: 2043\n",
      "Loss: 33.27530336272718\n",
      "l2 norm of gradients: 0.4504914524620978\n",
      "l2 norm of weights: 4.163206852319713\n",
      "---------------------\n",
      "Iteration Number: 2044\n",
      "Loss: 33.25860269935038\n",
      "l2 norm of gradients: 0.45028274040207816\n",
      "l2 norm of weights: 4.1623097379651055\n",
      "---------------------\n",
      "Iteration Number: 2045\n",
      "Loss: 33.24191397777127\n",
      "l2 norm of gradients: 0.45007407239444147\n",
      "l2 norm of weights: 4.161413164036344\n",
      "---------------------\n",
      "Iteration Number: 2046\n",
      "Loss: 33.22523719264728\n",
      "l2 norm of gradients: 0.44986544844121135\n",
      "l2 norm of weights: 4.160517130541399\n",
      "---------------------\n",
      "Iteration Number: 2047\n",
      "Loss: 33.20857233863574\n",
      "l2 norm of gradients: 0.4496568685445493\n",
      "l2 norm of weights: 4.159621637487993\n",
      "---------------------\n",
      "Iteration Number: 2048\n",
      "Loss: 33.19191941039591\n",
      "l2 norm of gradients: 0.4494483327067555\n",
      "l2 norm of weights: 4.158726684883608\n",
      "---------------------\n",
      "Iteration Number: 2049\n",
      "Loss: 33.17527840258723\n",
      "l2 norm of gradients: 0.4492398409302701\n",
      "l2 norm of weights: 4.157832272735477\n",
      "---------------------\n",
      "Iteration Number: 2050\n",
      "Loss: 33.15864930987031\n",
      "l2 norm of gradients: 0.44903139321767466\n",
      "l2 norm of weights: 4.156938401050585\n",
      "---------------------\n",
      "Iteration Number: 2051\n",
      "Loss: 33.14203212690644\n",
      "l2 norm of gradients: 0.44882298957169275\n",
      "l2 norm of weights: 4.156045069835669\n",
      "---------------------\n",
      "Iteration Number: 2052\n",
      "Loss: 33.12542684835782\n",
      "l2 norm of gradients: 0.44861462999519214\n",
      "l2 norm of weights: 4.155152279097212\n",
      "---------------------\n",
      "Iteration Number: 2053\n",
      "Loss: 33.10883346888756\n",
      "l2 norm of gradients: 0.4484063144911848\n",
      "l2 norm of weights: 4.154260028841449\n",
      "---------------------\n",
      "Iteration Number: 2054\n",
      "Loss: 33.09225198315856\n",
      "l2 norm of gradients: 0.448198043062829\n",
      "l2 norm of weights: 4.153368319074359\n",
      "---------------------\n",
      "Iteration Number: 2055\n",
      "Loss: 33.075682385835194\n",
      "l2 norm of gradients: 0.44798981571343016\n",
      "l2 norm of weights: 4.152477149801666\n",
      "---------------------\n",
      "Iteration Number: 2056\n",
      "Loss: 33.059124671582516\n",
      "l2 norm of gradients: 0.4477816324464421\n",
      "l2 norm of weights: 4.151586521028837\n",
      "---------------------\n",
      "Iteration Number: 2057\n",
      "Loss: 33.04257883506636\n",
      "l2 norm of gradients: 0.4475734932654681\n",
      "l2 norm of weights: 4.150696432761082\n",
      "---------------------\n",
      "Iteration Number: 2058\n",
      "Loss: 33.02604487095222\n",
      "l2 norm of gradients: 0.4473653981742622\n",
      "l2 norm of weights: 4.149806885003352\n",
      "---------------------\n",
      "Iteration Number: 2059\n",
      "Loss: 33.00952277390713\n",
      "l2 norm of gradients: 0.4471573471767303\n",
      "l2 norm of weights: 4.1489178777603355\n",
      "---------------------\n",
      "Iteration Number: 2060\n",
      "Loss: 32.99301253859804\n",
      "l2 norm of gradients: 0.44694934027693145\n",
      "l2 norm of weights: 4.148029411036461\n",
      "---------------------\n",
      "Iteration Number: 2061\n",
      "Loss: 32.97651415969304\n",
      "l2 norm of gradients: 0.44674137747907894\n",
      "l2 norm of weights: 4.147141484835892\n",
      "---------------------\n",
      "Iteration Number: 2062\n",
      "Loss: 32.9600276318601\n",
      "l2 norm of gradients: 0.4465334587875413\n",
      "l2 norm of weights: 4.146254099162527\n",
      "---------------------\n",
      "Iteration Number: 2063\n",
      "Loss: 32.94355294976828\n",
      "l2 norm of gradients: 0.4463255842068437\n",
      "l2 norm of weights: 4.145367254020001\n",
      "---------------------\n",
      "Iteration Number: 2064\n",
      "Loss: 32.92709010808598\n",
      "l2 norm of gradients: 0.4461177537416691\n",
      "l2 norm of weights: 4.144480949411678\n",
      "---------------------\n",
      "Iteration Number: 2065\n",
      "Loss: 32.9106391014834\n",
      "l2 norm of gradients: 0.4459099673968592\n",
      "l2 norm of weights: 4.1435951853406525\n",
      "---------------------\n",
      "Iteration Number: 2066\n",
      "Loss: 32.89419992463005\n",
      "l2 norm of gradients: 0.44570222517741565\n",
      "l2 norm of weights: 4.142709961809754\n",
      "---------------------\n",
      "Iteration Number: 2067\n",
      "Loss: 32.877772572196086\n",
      "l2 norm of gradients: 0.44549452708850124\n",
      "l2 norm of weights: 4.141825278821534\n",
      "---------------------\n",
      "Iteration Number: 2068\n",
      "Loss: 32.86135703885216\n",
      "l2 norm of gradients: 0.4452868731354412\n",
      "l2 norm of weights: 4.140941136378274\n",
      "---------------------\n",
      "Iteration Number: 2069\n",
      "Loss: 32.844953319268896\n",
      "l2 norm of gradients: 0.44507926332372405\n",
      "l2 norm of weights: 4.140057534481981\n",
      "---------------------\n",
      "Iteration Number: 2070\n",
      "Loss: 32.828561408117956\n",
      "l2 norm of gradients: 0.4448716976590027\n",
      "l2 norm of weights: 4.1391744731343865\n",
      "---------------------\n",
      "Iteration Number: 2071\n",
      "Loss: 32.81218130006987\n",
      "l2 norm of gradients: 0.44466417614709575\n",
      "l2 norm of weights: 4.138291952336943\n",
      "---------------------\n",
      "Iteration Number: 2072\n",
      "Loss: 32.79581298979658\n",
      "l2 norm of gradients: 0.44445669879398875\n",
      "l2 norm of weights: 4.137409972090827\n",
      "---------------------\n",
      "Iteration Number: 2073\n",
      "Loss: 32.77945647196978\n",
      "l2 norm of gradients: 0.44424926560583494\n",
      "l2 norm of weights: 4.136528532396935\n",
      "---------------------\n",
      "Iteration Number: 2074\n",
      "Loss: 32.76311174126115\n",
      "l2 norm of gradients: 0.4440418765889567\n",
      "l2 norm of weights: 4.1356476332558785\n",
      "---------------------\n",
      "Iteration Number: 2075\n",
      "Loss: 32.74677879234266\n",
      "l2 norm of gradients: 0.4438345317498462\n",
      "l2 norm of weights: 4.134767274667993\n",
      "---------------------\n",
      "Iteration Number: 2076\n",
      "Loss: 32.730457619886046\n",
      "l2 norm of gradients: 0.4436272310951669\n",
      "l2 norm of weights: 4.133887456633325\n",
      "---------------------\n",
      "Iteration Number: 2077\n",
      "Loss: 32.71414821856407\n",
      "l2 norm of gradients: 0.44341997463175487\n",
      "l2 norm of weights: 4.133008179151638\n",
      "---------------------\n",
      "Iteration Number: 2078\n",
      "Loss: 32.69785058304831\n",
      "l2 norm of gradients: 0.44321276236661905\n",
      "l2 norm of weights: 4.132129442222409\n",
      "---------------------\n",
      "Iteration Number: 2079\n",
      "Loss: 32.68156470801109\n",
      "l2 norm of gradients: 0.4430055943069429\n",
      "l2 norm of weights: 4.131251245844829\n",
      "---------------------\n",
      "Iteration Number: 2080\n",
      "Loss: 32.66529058812445\n",
      "l2 norm of gradients: 0.4427984704600857\n",
      "l2 norm of weights: 4.130373590017797\n",
      "---------------------\n",
      "Iteration Number: 2081\n",
      "Loss: 32.649028218060685\n",
      "l2 norm of gradients: 0.4425913908335831\n",
      "l2 norm of weights: 4.129496474739924\n",
      "---------------------\n",
      "Iteration Number: 2082\n",
      "Loss: 32.63277759249171\n",
      "l2 norm of gradients: 0.44238435543514837\n",
      "l2 norm of weights: 4.128619900009529\n",
      "---------------------\n",
      "Iteration Number: 2083\n",
      "Loss: 32.61653870608893\n",
      "l2 norm of gradients: 0.4421773642726736\n",
      "l2 norm of weights: 4.127743865824638\n",
      "---------------------\n",
      "Iteration Number: 2084\n",
      "Loss: 32.60031155352495\n",
      "l2 norm of gradients: 0.4419704173542306\n",
      "l2 norm of weights: 4.126868372182984\n",
      "---------------------\n",
      "Iteration Number: 2085\n",
      "Loss: 32.58409612947057\n",
      "l2 norm of gradients: 0.441763514688072\n",
      "l2 norm of weights: 4.125993419082004\n",
      "---------------------\n",
      "Iteration Number: 2086\n",
      "Loss: 32.5678924285978\n",
      "l2 norm of gradients: 0.44155665628263213\n",
      "l2 norm of weights: 4.125119006518837\n",
      "---------------------\n",
      "Iteration Number: 2087\n",
      "Loss: 32.551700445577666\n",
      "l2 norm of gradients: 0.44134984214652867\n",
      "l2 norm of weights: 4.124245134490327\n",
      "---------------------\n",
      "Iteration Number: 2088\n",
      "Loss: 32.535520175080975\n",
      "l2 norm of gradients: 0.4411430722885629\n",
      "l2 norm of weights: 4.123371802993017\n",
      "---------------------\n",
      "Iteration Number: 2089\n",
      "Loss: 32.51935161177858\n",
      "l2 norm of gradients: 0.44093634671772103\n",
      "l2 norm of weights: 4.122499012023151\n",
      "---------------------\n",
      "Iteration Number: 2090\n",
      "Loss: 32.50319475034106\n",
      "l2 norm of gradients: 0.4407296654431753\n",
      "l2 norm of weights: 4.121626761576669\n",
      "---------------------\n",
      "Iteration Number: 2091\n",
      "Loss: 32.48704958543837\n",
      "l2 norm of gradients: 0.4405230284742852\n",
      "l2 norm of weights: 4.120755051649215\n",
      "---------------------\n",
      "Iteration Number: 2092\n",
      "Loss: 32.47091611174028\n",
      "l2 norm of gradients: 0.440316435820598\n",
      "l2 norm of weights: 4.1198838822361195\n",
      "---------------------\n",
      "Iteration Number: 2093\n",
      "Loss: 32.454794323915905\n",
      "l2 norm of gradients: 0.44010988749185004\n",
      "l2 norm of weights: 4.119013253332415\n",
      "---------------------\n",
      "Iteration Number: 2094\n",
      "Loss: 32.43868421663457\n",
      "l2 norm of gradients: 0.43990338349796765\n",
      "l2 norm of weights: 4.118143164932825\n",
      "---------------------\n",
      "Iteration Number: 2095\n",
      "Loss: 32.42258578456473\n",
      "l2 norm of gradients: 0.43969692384906817\n",
      "l2 norm of weights: 4.117273617031766\n",
      "---------------------\n",
      "Iteration Number: 2096\n",
      "Loss: 32.406499022374845\n",
      "l2 norm of gradients: 0.4394905085554611\n",
      "l2 norm of weights: 4.116404609623345\n",
      "---------------------\n",
      "Iteration Number: 2097\n",
      "Loss: 32.390423924732175\n",
      "l2 norm of gradients: 0.439284137627649\n",
      "l2 norm of weights: 4.11553614270136\n",
      "---------------------\n",
      "Iteration Number: 2098\n",
      "Loss: 32.374360486303914\n",
      "l2 norm of gradients: 0.43907781107632793\n",
      "l2 norm of weights: 4.114668216259298\n",
      "---------------------\n",
      "Iteration Number: 2099\n",
      "Loss: 32.35830870175674\n",
      "l2 norm of gradients: 0.4388715289123894\n",
      "l2 norm of weights: 4.113800830290332\n",
      "---------------------\n",
      "Iteration Number: 2100\n",
      "Loss: 32.342268565756356\n",
      "l2 norm of gradients: 0.43866529114692027\n",
      "l2 norm of weights: 4.1129339847873245\n",
      "---------------------\n",
      "Iteration Number: 2101\n",
      "Loss: 32.32624007296905\n",
      "l2 norm of gradients: 0.438459097791205\n",
      "l2 norm of weights: 4.11206767974282\n",
      "---------------------\n",
      "Iteration Number: 2102\n",
      "Loss: 32.31022321805906\n",
      "l2 norm of gradients: 0.43825294885672517\n",
      "l2 norm of weights: 4.111201915149049\n",
      "---------------------\n",
      "Iteration Number: 2103\n",
      "Loss: 32.29421799569085\n",
      "l2 norm of gradients: 0.4380468443551614\n",
      "l2 norm of weights: 4.110336690997926\n",
      "---------------------\n",
      "Iteration Number: 2104\n",
      "Loss: 32.278224400527705\n",
      "l2 norm of gradients: 0.43784078429839396\n",
      "l2 norm of weights: 4.109472007281045\n",
      "---------------------\n",
      "Iteration Number: 2105\n",
      "Loss: 32.262242427232906\n",
      "l2 norm of gradients: 0.4376347686985038\n",
      "l2 norm of weights: 4.108607863989683\n",
      "---------------------\n",
      "Iteration Number: 2106\n",
      "Loss: 32.24627207046877\n",
      "l2 norm of gradients: 0.43742879756777336\n",
      "l2 norm of weights: 4.107744261114795\n",
      "---------------------\n",
      "Iteration Number: 2107\n",
      "Loss: 32.2303133248963\n",
      "l2 norm of gradients: 0.4372228709186875\n",
      "l2 norm of weights: 4.106881198647016\n",
      "---------------------\n",
      "Iteration Number: 2108\n",
      "Loss: 32.21436618517634\n",
      "l2 norm of gradients: 0.4370169887639346\n",
      "l2 norm of weights: 4.106018676576657\n",
      "---------------------\n",
      "Iteration Number: 2109\n",
      "Loss: 32.198430645969076\n",
      "l2 norm of gradients: 0.43681115111640706\n",
      "l2 norm of weights: 4.105156694893705\n",
      "---------------------\n",
      "Iteration Number: 2110\n",
      "Loss: 32.18250670193349\n",
      "l2 norm of gradients: 0.43660535798920286\n",
      "l2 norm of weights: 4.104295253587825\n",
      "---------------------\n",
      "Iteration Number: 2111\n",
      "Loss: 32.16659434772756\n",
      "l2 norm of gradients: 0.4363996093956256\n",
      "l2 norm of weights: 4.103434352648353\n",
      "---------------------\n",
      "Iteration Number: 2112\n",
      "Loss: 32.15069357800902\n",
      "l2 norm of gradients: 0.4361939053491861\n",
      "l2 norm of weights: 4.102573992064299\n",
      "---------------------\n",
      "Iteration Number: 2113\n",
      "Loss: 32.13480438743441\n",
      "l2 norm of gradients: 0.4359882458636031\n",
      "l2 norm of weights: 4.1017141718243435\n",
      "---------------------\n",
      "Iteration Number: 2114\n",
      "Loss: 32.11892677065925\n",
      "l2 norm of gradients: 0.43578263095280384\n",
      "l2 norm of weights: 4.100854891916842\n",
      "---------------------\n",
      "Iteration Number: 2115\n",
      "Loss: 32.10306072233826\n",
      "l2 norm of gradients: 0.43557706063092516\n",
      "l2 norm of weights: 4.0999961523298145\n",
      "---------------------\n",
      "Iteration Number: 2116\n",
      "Loss: 32.08720623712507\n",
      "l2 norm of gradients: 0.43537153491231456\n",
      "l2 norm of weights: 4.0991379530509535\n",
      "---------------------\n",
      "Iteration Number: 2117\n",
      "Loss: 32.07136330967249\n",
      "l2 norm of gradients: 0.4351660538115304\n",
      "l2 norm of weights: 4.098280294067618\n",
      "---------------------\n",
      "Iteration Number: 2118\n",
      "Loss: 32.055531934632754\n",
      "l2 norm of gradients: 0.43496061734334357\n",
      "l2 norm of weights: 4.097423175366831\n",
      "---------------------\n",
      "Iteration Number: 2119\n",
      "Loss: 32.0397121066559\n",
      "l2 norm of gradients: 0.4347552255227379\n",
      "l2 norm of weights: 4.0965665969352845\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 2120\n",
      "Loss: 32.02390382039166\n",
      "l2 norm of gradients: 0.4345498783649106\n",
      "l2 norm of weights: 4.095710558759334\n",
      "---------------------\n",
      "Iteration Number: 2121\n",
      "Loss: 32.00810707048913\n",
      "l2 norm of gradients: 0.43434457588527403\n",
      "l2 norm of weights: 4.094855060824997\n",
      "---------------------\n",
      "Iteration Number: 2122\n",
      "Loss: 31.992321851595307\n",
      "l2 norm of gradients: 0.4341393180994555\n",
      "l2 norm of weights: 4.094000103117955\n",
      "---------------------\n",
      "Iteration Number: 2123\n",
      "Loss: 31.97654815835689\n",
      "l2 norm of gradients: 0.4339341050232991\n",
      "l2 norm of weights: 4.093145685623551\n",
      "---------------------\n",
      "Iteration Number: 2124\n",
      "Loss: 31.960785985418763\n",
      "l2 norm of gradients: 0.4337289366728654\n",
      "l2 norm of weights: 4.092291808326785\n",
      "---------------------\n",
      "Iteration Number: 2125\n",
      "Loss: 31.945035327425053\n",
      "l2 norm of gradients: 0.43352381306443316\n",
      "l2 norm of weights: 4.091438471212322\n",
      "---------------------\n",
      "Iteration Number: 2126\n",
      "Loss: 31.929296179018838\n",
      "l2 norm of gradients: 0.43331873421449957\n",
      "l2 norm of weights: 4.090585674264481\n",
      "---------------------\n",
      "Iteration Number: 2127\n",
      "Loss: 31.913568534841403\n",
      "l2 norm of gradients: 0.43311370013978134\n",
      "l2 norm of weights: 4.089733417467237\n",
      "---------------------\n",
      "Iteration Number: 2128\n",
      "Loss: 31.89785238953353\n",
      "l2 norm of gradients: 0.43290871085721544\n",
      "l2 norm of weights: 4.088881700804227\n",
      "---------------------\n",
      "Iteration Number: 2129\n",
      "Loss: 31.88214773773386\n",
      "l2 norm of gradients: 0.43270376638395947\n",
      "l2 norm of weights: 4.088030524258739\n",
      "---------------------\n",
      "Iteration Number: 2130\n",
      "Loss: 31.866454574080485\n",
      "l2 norm of gradients: 0.43249886673739285\n",
      "l2 norm of weights: 4.087179887813718\n",
      "---------------------\n",
      "Iteration Number: 2131\n",
      "Loss: 31.85077289320974\n",
      "l2 norm of gradients: 0.43229401193511774\n",
      "l2 norm of weights: 4.086329791451759\n",
      "---------------------\n",
      "Iteration Number: 2132\n",
      "Loss: 31.835102689756944\n",
      "l2 norm of gradients: 0.4320892019949591\n",
      "l2 norm of weights: 4.085480235155114\n",
      "---------------------\n",
      "Iteration Number: 2133\n",
      "Loss: 31.819443958356032\n",
      "l2 norm of gradients: 0.4318844369349658\n",
      "l2 norm of weights: 4.084631218905681\n",
      "---------------------\n",
      "Iteration Number: 2134\n",
      "Loss: 31.80379669363873\n",
      "l2 norm of gradients: 0.43167971677341166\n",
      "l2 norm of weights: 4.083782742685013\n",
      "---------------------\n",
      "Iteration Number: 2135\n",
      "Loss: 31.78816089023705\n",
      "l2 norm of gradients: 0.4314750415287956\n",
      "l2 norm of weights: 4.082934806474313\n",
      "---------------------\n",
      "Iteration Number: 2136\n",
      "Loss: 31.772536542779918\n",
      "l2 norm of gradients: 0.43127041121984266\n",
      "l2 norm of weights: 4.082087410254428\n",
      "---------------------\n",
      "Iteration Number: 2137\n",
      "Loss: 31.75692364589576\n",
      "l2 norm of gradients: 0.43106582586550457\n",
      "l2 norm of weights: 4.081240554005857\n",
      "---------------------\n",
      "Iteration Number: 2138\n",
      "Loss: 31.741322194211428\n",
      "l2 norm of gradients: 0.4308612854849608\n",
      "l2 norm of weights: 4.080394237708745\n",
      "---------------------\n",
      "Iteration Number: 2139\n",
      "Loss: 31.72573218235145\n",
      "l2 norm of gradients: 0.43065679009761854\n",
      "l2 norm of weights: 4.079548461342882\n",
      "---------------------\n",
      "Iteration Number: 2140\n",
      "Loss: 31.7101536049402\n",
      "l2 norm of gradients: 0.43045233972311414\n",
      "l2 norm of weights: 4.078703224887704\n",
      "---------------------\n",
      "Iteration Number: 2141\n",
      "Loss: 31.694586456599176\n",
      "l2 norm of gradients: 0.4302479343813135\n",
      "l2 norm of weights: 4.077858528322291\n",
      "---------------------\n",
      "Iteration Number: 2142\n",
      "Loss: 31.679030731949418\n",
      "l2 norm of gradients: 0.4300435740923123\n",
      "l2 norm of weights: 4.0770143716253635\n",
      "---------------------\n",
      "Iteration Number: 2143\n",
      "Loss: 31.663486425609946\n",
      "l2 norm of gradients: 0.4298392588764372\n",
      "l2 norm of weights: 4.076170754775291\n",
      "---------------------\n",
      "Iteration Number: 2144\n",
      "Loss: 31.64795353219781\n",
      "l2 norm of gradients: 0.4296349887542467\n",
      "l2 norm of weights: 4.075327677750076\n",
      "---------------------\n",
      "Iteration Number: 2145\n",
      "Loss: 31.632432046328983\n",
      "l2 norm of gradients: 0.4294307637465309\n",
      "l2 norm of weights: 4.07448514052737\n",
      "---------------------\n",
      "Iteration Number: 2146\n",
      "Loss: 31.61692196261764\n",
      "l2 norm of gradients: 0.4292265838743126\n",
      "l2 norm of weights: 4.073643143084458\n",
      "---------------------\n",
      "Iteration Number: 2147\n",
      "Loss: 31.601423275676304\n",
      "l2 norm of gradients: 0.4290224491588483\n",
      "l2 norm of weights: 4.072801685398266\n",
      "---------------------\n",
      "Iteration Number: 2148\n",
      "Loss: 31.585935980115153\n",
      "l2 norm of gradients: 0.42881835962162834\n",
      "l2 norm of weights: 4.071960767445359\n",
      "---------------------\n",
      "Iteration Number: 2149\n",
      "Loss: 31.570460070544136\n",
      "l2 norm of gradients: 0.42861431528437755\n",
      "l2 norm of weights: 4.071120389201939\n",
      "---------------------\n",
      "Iteration Number: 2150\n",
      "Loss: 31.55499554157001\n",
      "l2 norm of gradients: 0.4284103161690558\n",
      "l2 norm of weights: 4.070280550643843\n",
      "---------------------\n",
      "Iteration Number: 2151\n",
      "Loss: 31.539542387798292\n",
      "l2 norm of gradients: 0.42820636229785913\n",
      "l2 norm of weights: 4.069441251746546\n",
      "---------------------\n",
      "Iteration Number: 2152\n",
      "Loss: 31.52410060383318\n",
      "l2 norm of gradients: 0.4280024536932195\n",
      "l2 norm of weights: 4.068602492485154\n",
      "---------------------\n",
      "Iteration Number: 2153\n",
      "Loss: 31.50867018427651\n",
      "l2 norm of gradients: 0.42779859037780593\n",
      "l2 norm of weights: 4.067764272834411\n",
      "---------------------\n",
      "Iteration Number: 2154\n",
      "Loss: 31.49325112372848\n",
      "l2 norm of gradients: 0.42759477237452476\n",
      "l2 norm of weights: 4.066926592768693\n",
      "---------------------\n",
      "Iteration Number: 2155\n",
      "Loss: 31.477843416787298\n",
      "l2 norm of gradients: 0.4273909997065207\n",
      "l2 norm of weights: 4.066089452262007\n",
      "---------------------\n",
      "Iteration Number: 2156\n",
      "Loss: 31.46244705804989\n",
      "l2 norm of gradients: 0.42718727239717663\n",
      "l2 norm of weights: 4.065252851287993\n",
      "---------------------\n",
      "Iteration Number: 2157\n",
      "Loss: 31.447062042111025\n",
      "l2 norm of gradients: 0.4269835904701148\n",
      "l2 norm of weights: 4.06441678981992\n",
      "---------------------\n",
      "Iteration Number: 2158\n",
      "Loss: 31.43168836356279\n",
      "l2 norm of gradients: 0.42677995394919693\n",
      "l2 norm of weights: 4.06358126783069\n",
      "---------------------\n",
      "Iteration Number: 2159\n",
      "Loss: 31.41632601699658\n",
      "l2 norm of gradients: 0.426576362858525\n",
      "l2 norm of weights: 4.062746285292833\n",
      "---------------------\n",
      "Iteration Number: 2160\n",
      "Loss: 31.40097499700104\n",
      "l2 norm of gradients: 0.4263728172224415\n",
      "l2 norm of weights: 4.061911842178504\n",
      "---------------------\n",
      "Iteration Number: 2161\n",
      "Loss: 31.38563529816343\n",
      "l2 norm of gradients: 0.4261693170655304\n",
      "l2 norm of weights: 4.061077938459492\n",
      "---------------------\n",
      "Iteration Number: 2162\n",
      "Loss: 31.37030691506849\n",
      "l2 norm of gradients: 0.4259658624126171\n",
      "l2 norm of weights: 4.06024457410721\n",
      "---------------------\n",
      "Iteration Number: 2163\n",
      "Loss: 31.354989842299524\n",
      "l2 norm of gradients: 0.4257624532887688\n",
      "l2 norm of weights: 4.059411749092695\n",
      "---------------------\n",
      "Iteration Number: 2164\n",
      "Loss: 31.339684074437432\n",
      "l2 norm of gradients: 0.4255590897192961\n",
      "l2 norm of weights: 4.058579463386614\n",
      "---------------------\n",
      "Iteration Number: 2165\n",
      "Loss: 31.324389606060944\n",
      "l2 norm of gradients: 0.4253557717297521\n",
      "l2 norm of weights: 4.057747716959256\n",
      "---------------------\n",
      "Iteration Number: 2166\n",
      "Loss: 31.3091064317473\n",
      "l2 norm of gradients: 0.42515249934593363\n",
      "l2 norm of weights: 4.056916509780535\n",
      "---------------------\n",
      "Iteration Number: 2167\n",
      "Loss: 31.293834546071537\n",
      "l2 norm of gradients: 0.42494927259388154\n",
      "l2 norm of weights: 4.056085841819989\n",
      "---------------------\n",
      "Iteration Number: 2168\n",
      "Loss: 31.278573943606084\n",
      "l2 norm of gradients: 0.42474609149988113\n",
      "l2 norm of weights: 4.055255713046776\n",
      "---------------------\n",
      "Iteration Number: 2169\n",
      "Loss: 31.26332461892168\n",
      "l2 norm of gradients: 0.42454295609046255\n",
      "l2 norm of weights: 4.054426123429681\n",
      "---------------------\n",
      "Iteration Number: 2170\n",
      "Loss: 31.24808656658721\n",
      "l2 norm of gradients: 0.4243398663924013\n",
      "l2 norm of weights: 4.053597072937107\n",
      "---------------------\n",
      "Iteration Number: 2171\n",
      "Loss: 31.232859781169026\n",
      "l2 norm of gradients: 0.42413682243271833\n",
      "l2 norm of weights: 4.052768561537076\n",
      "---------------------\n",
      "Iteration Number: 2172\n",
      "Loss: 31.217644257231182\n",
      "l2 norm of gradients: 0.42393382423868103\n",
      "l2 norm of weights: 4.051940589197235\n",
      "---------------------\n",
      "Iteration Number: 2173\n",
      "Loss: 31.202439989335858\n",
      "l2 norm of gradients: 0.4237308718378032\n",
      "l2 norm of weights: 4.051113155884847\n",
      "---------------------\n",
      "Iteration Number: 2174\n",
      "Loss: 31.18724697204329\n",
      "l2 norm of gradients: 0.42352796525784553\n",
      "l2 norm of weights: 4.050286261566795\n",
      "---------------------\n",
      "Iteration Number: 2175\n",
      "Loss: 31.172065199910858\n",
      "l2 norm of gradients: 0.423325104526816\n",
      "l2 norm of weights: 4.049459906209578\n",
      "---------------------\n",
      "Iteration Number: 2176\n",
      "Loss: 31.15689466749424\n",
      "l2 norm of gradients: 0.42312228967297016\n",
      "l2 norm of weights: 4.048634089779315\n",
      "---------------------\n",
      "Iteration Number: 2177\n",
      "Loss: 31.141735369346755\n",
      "l2 norm of gradients: 0.4229195207248117\n",
      "l2 norm of weights: 4.047808812241741\n",
      "---------------------\n",
      "Iteration Number: 2178\n",
      "Loss: 31.12658730001919\n",
      "l2 norm of gradients: 0.42271679771109255\n",
      "l2 norm of weights: 4.046984073562209\n",
      "---------------------\n",
      "Iteration Number: 2179\n",
      "Loss: 31.111450454059934\n",
      "l2 norm of gradients: 0.4225141206608135\n",
      "l2 norm of weights: 4.046159873705683\n",
      "---------------------\n",
      "Iteration Number: 2180\n",
      "Loss: 31.096324826016257\n",
      "l2 norm of gradients: 0.4223114896032241\n",
      "l2 norm of weights: 4.045336212636746\n",
      "---------------------\n",
      "Iteration Number: 2181\n",
      "Loss: 31.081210410431694\n",
      "l2 norm of gradients: 0.4221089045678238\n",
      "l2 norm of weights: 4.044513090319596\n",
      "---------------------\n",
      "Iteration Number: 2182\n",
      "Loss: 31.06610720184836\n",
      "l2 norm of gradients: 0.4219063655843612\n",
      "l2 norm of weights: 4.043690506718039\n",
      "---------------------\n",
      "Iteration Number: 2183\n",
      "Loss: 31.051015194805256\n",
      "l2 norm of gradients: 0.42170387268283477\n",
      "l2 norm of weights: 4.042868461795502\n",
      "---------------------\n",
      "Iteration Number: 2184\n",
      "Loss: 31.0359343838398\n",
      "l2 norm of gradients: 0.421501425893494\n",
      "l2 norm of weights: 4.042046955515019\n",
      "---------------------\n",
      "Iteration Number: 2185\n",
      "Loss: 31.020864763486635\n",
      "l2 norm of gradients: 0.4212990252468381\n",
      "l2 norm of weights: 4.0412259878392405\n",
      "---------------------\n",
      "Iteration Number: 2186\n",
      "Loss: 31.00580632827777\n",
      "l2 norm of gradients: 0.42109667077361773\n",
      "l2 norm of weights: 4.0404055587304235\n",
      "---------------------\n",
      "Iteration Number: 2187\n",
      "Loss: 30.990759072743465\n",
      "l2 norm of gradients: 0.4208943625048345\n",
      "l2 norm of weights: 4.0395856681504405\n",
      "---------------------\n",
      "Iteration Number: 2188\n",
      "Loss: 30.975722991411153\n",
      "l2 norm of gradients: 0.42069210047174116\n",
      "l2 norm of weights: 4.038766316060773\n",
      "---------------------\n",
      "Iteration Number: 2189\n",
      "Loss: 30.960698078805827\n",
      "l2 norm of gradients: 0.42048988470584236\n",
      "l2 norm of weights: 4.037947502422511\n",
      "---------------------\n",
      "Iteration Number: 2190\n",
      "Loss: 30.945684329450042\n",
      "l2 norm of gradients: 0.4202877152388944\n",
      "l2 norm of weights: 4.037129227196354\n",
      "---------------------\n",
      "Iteration Number: 2191\n",
      "Loss: 30.93068173786387\n",
      "l2 norm of gradients: 0.4200855921029059\n",
      "l2 norm of weights: 4.036311490342614\n",
      "---------------------\n",
      "Iteration Number: 2192\n",
      "Loss: 30.91569029856491\n",
      "l2 norm of gradients: 0.4198835153301377\n",
      "l2 norm of weights: 4.035494291821207\n",
      "---------------------\n",
      "Iteration Number: 2193\n",
      "Loss: 30.90071000606824\n",
      "l2 norm of gradients: 0.4196814849531029\n",
      "l2 norm of weights: 4.03467763159166\n",
      "---------------------\n",
      "Iteration Number: 2194\n",
      "Loss: 30.885740854886606\n",
      "l2 norm of gradients: 0.4194795010045678\n",
      "l2 norm of weights: 4.033861509613105\n",
      "---------------------\n",
      "Iteration Number: 2195\n",
      "Loss: 30.870782839530147\n",
      "l2 norm of gradients: 0.4192775635175513\n",
      "l2 norm of weights: 4.0330459258442835\n",
      "---------------------\n",
      "Iteration Number: 2196\n",
      "Loss: 30.85583595450614\n",
      "l2 norm of gradients: 0.4190756725253254\n",
      "l2 norm of weights: 4.0322308802435405\n",
      "---------------------\n",
      "Iteration Number: 2197\n",
      "Loss: 30.84090019431965\n",
      "l2 norm of gradients: 0.4188738280614156\n",
      "l2 norm of weights: 4.031416372768828\n",
      "---------------------\n",
      "Iteration Number: 2198\n",
      "Loss: 30.825975553472947\n",
      "l2 norm of gradients: 0.4186720301596008\n",
      "l2 norm of weights: 4.030602403377705\n",
      "---------------------\n",
      "Iteration Number: 2199\n",
      "Loss: 30.81106202646618\n",
      "l2 norm of gradients: 0.41847027885391325\n",
      "l2 norm of weights: 4.029788972027334\n",
      "---------------------\n",
      "Iteration Number: 2200\n",
      "Loss: 30.796159607796156\n",
      "l2 norm of gradients: 0.4182685741786391\n",
      "l2 norm of weights: 4.028976078674482\n",
      "---------------------\n",
      "Iteration Number: 2201\n",
      "Loss: 30.78126829195759\n",
      "l2 norm of gradients: 0.4180669161683186\n",
      "l2 norm of weights: 4.028163723275521\n",
      "---------------------\n",
      "Iteration Number: 2202\n",
      "Loss: 30.766388073442542\n",
      "l2 norm of gradients: 0.4178653048577457\n",
      "l2 norm of weights: 4.027351905786427\n",
      "---------------------\n",
      "Iteration Number: 2203\n",
      "Loss: 30.751518946740067\n",
      "l2 norm of gradients: 0.41766374028196845\n",
      "l2 norm of weights: 4.0265406261627765\n",
      "---------------------\n",
      "Iteration Number: 2204\n",
      "Loss: 30.73666090633692\n",
      "l2 norm of gradients: 0.41746222247628956\n",
      "l2 norm of weights: 4.0257298843597535\n",
      "---------------------\n",
      "Iteration Number: 2205\n",
      "Loss: 30.72181394671734\n",
      "l2 norm of gradients: 0.4172607514762656\n",
      "l2 norm of weights: 4.02491968033214\n",
      "---------------------\n",
      "Iteration Number: 2206\n",
      "Loss: 30.706978062362356\n",
      "l2 norm of gradients: 0.4170593273177079\n",
      "l2 norm of weights: 4.0241100140343224\n",
      "---------------------\n",
      "Iteration Number: 2207\n",
      "Loss: 30.69215324775044\n",
      "l2 norm of gradients: 0.416857950036682\n",
      "l2 norm of weights: 4.023300885420288\n",
      "---------------------\n",
      "Iteration Number: 2208\n",
      "Loss: 30.677339497357607\n",
      "l2 norm of gradients: 0.41665661966950823\n",
      "l2 norm of weights: 4.022492294443627\n",
      "---------------------\n",
      "Iteration Number: 2209\n",
      "Loss: 30.662536805656753\n",
      "l2 norm of gradients: 0.4164553362527616\n",
      "l2 norm of weights: 4.021684241057527\n",
      "---------------------\n",
      "Iteration Number: 2210\n",
      "Loss: 30.647745167118813\n",
      "l2 norm of gradients: 0.4162540998232716\n",
      "l2 norm of weights: 4.02087672521478\n",
      "---------------------\n",
      "Iteration Number: 2211\n",
      "Loss: 30.632964576210963\n",
      "l2 norm of gradients: 0.41605291041812253\n",
      "l2 norm of weights: 4.020069746867776\n",
      "---------------------\n",
      "Iteration Number: 2212\n",
      "Loss: 30.61819502739838\n",
      "l2 norm of gradients: 0.41585176807465357\n",
      "l2 norm of weights: 4.0192633059685035\n",
      "---------------------\n",
      "Iteration Number: 2213\n",
      "Loss: 30.60343651514307\n",
      "l2 norm of gradients: 0.4156506728304585\n",
      "l2 norm of weights: 4.018457402468555\n",
      "---------------------\n",
      "Iteration Number: 2214\n",
      "Loss: 30.588689033904267\n",
      "l2 norm of gradients: 0.415449624723386\n",
      "l2 norm of weights: 4.017652036319117\n",
      "---------------------\n",
      "Iteration Number: 2215\n",
      "Loss: 30.573952578138808\n",
      "l2 norm of gradients: 0.4152486237915393\n",
      "l2 norm of weights: 4.016847207470978\n",
      "---------------------\n",
      "Iteration Number: 2216\n",
      "Loss: 30.559227142300042\n",
      "l2 norm of gradients: 0.415047670073277\n",
      "l2 norm of weights: 4.016042915874523\n",
      "---------------------\n",
      "Iteration Number: 2217\n",
      "Loss: 30.544512720839425\n",
      "l2 norm of gradients: 0.4148467636072119\n",
      "l2 norm of weights: 4.0152391614797365\n",
      "---------------------\n",
      "Iteration Number: 2218\n",
      "Loss: 30.529809308204314\n",
      "l2 norm of gradients: 0.4146459044322117\n",
      "l2 norm of weights: 4.014435944236201\n",
      "---------------------\n",
      "Iteration Number: 2219\n",
      "Loss: 30.515116898840574\n",
      "l2 norm of gradients: 0.41444509258739903\n",
      "l2 norm of weights: 4.013633264093095\n",
      "---------------------\n",
      "Iteration Number: 2220\n",
      "Loss: 30.500435487190128\n",
      "l2 norm of gradients: 0.4142443281121509\n",
      "l2 norm of weights: 4.012831120999197\n",
      "---------------------\n",
      "Iteration Number: 2221\n",
      "Loss: 30.485765067692636\n",
      "l2 norm of gradients: 0.4140436110460989\n",
      "l2 norm of weights: 4.012029514902878\n",
      "---------------------\n",
      "Iteration Number: 2222\n",
      "Loss: 30.47110563478501\n",
      "l2 norm of gradients: 0.41384294142912953\n",
      "l2 norm of weights: 4.011228445752109\n",
      "---------------------\n",
      "Iteration Number: 2223\n",
      "Loss: 30.456457182900795\n",
      "l2 norm of gradients: 0.41364231930138345\n",
      "l2 norm of weights: 4.010427913494458\n",
      "---------------------\n",
      "Iteration Number: 2224\n",
      "Loss: 30.441819706470476\n",
      "l2 norm of gradients: 0.4134417447032557\n",
      "l2 norm of weights: 4.009627918077086\n",
      "---------------------\n",
      "Iteration Number: 2225\n",
      "Loss: 30.42719319992253\n",
      "l2 norm of gradients: 0.41324121767539596\n",
      "l2 norm of weights: 4.008828459446753\n",
      "---------------------\n",
      "Iteration Number: 2226\n",
      "Loss: 30.41257765768151\n",
      "l2 norm of gradients: 0.41304073825870735\n",
      "l2 norm of weights: 4.008029537549811\n",
      "---------------------\n",
      "Iteration Number: 2227\n",
      "Loss: 30.397973074170043\n",
      "l2 norm of gradients: 0.41284030649434805\n",
      "l2 norm of weights: 4.007231152332213\n",
      "---------------------\n",
      "Iteration Number: 2228\n",
      "Loss: 30.38337944380657\n",
      "l2 norm of gradients: 0.41263992242372954\n",
      "l2 norm of weights: 4.0064333037395\n",
      "---------------------\n",
      "Iteration Number: 2229\n",
      "Loss: 30.368796761008188\n",
      "l2 norm of gradients: 0.4124395860885172\n",
      "l2 norm of weights: 4.005635991716814\n",
      "---------------------\n",
      "Iteration Number: 2230\n",
      "Loss: 30.35422502018731\n",
      "l2 norm of gradients: 0.4122392975306302\n",
      "l2 norm of weights: 4.004839216208889\n",
      "---------------------\n",
      "Iteration Number: 2231\n",
      "Loss: 30.339664215754382\n",
      "l2 norm of gradients: 0.4120390567922413\n",
      "l2 norm of weights: 4.0040429771600525\n",
      "---------------------\n",
      "Iteration Number: 2232\n",
      "Loss: 30.325114342117093\n",
      "l2 norm of gradients: 0.4118388639157764\n",
      "l2 norm of weights: 4.003247274514228\n",
      "---------------------\n",
      "Iteration Number: 2233\n",
      "Loss: 30.310575393679304\n",
      "l2 norm of gradients: 0.4116387189439149\n",
      "l2 norm of weights: 4.002452108214935\n",
      "---------------------\n",
      "Iteration Number: 2234\n",
      "Loss: 30.296047364842607\n",
      "l2 norm of gradients: 0.4114386219195889\n",
      "l2 norm of weights: 4.0016574782052805\n",
      "---------------------\n",
      "Iteration Number: 2235\n",
      "Loss: 30.28153025000523\n",
      "l2 norm of gradients: 0.41123857288598364\n",
      "l2 norm of weights: 4.000863384427972\n",
      "---------------------\n",
      "Iteration Number: 2236\n",
      "Loss: 30.267024043562486\n",
      "l2 norm of gradients: 0.4110385718865367\n",
      "l2 norm of weights: 4.000069826825307\n",
      "---------------------\n",
      "Iteration Number: 2237\n",
      "Loss: 30.252528739906225\n",
      "l2 norm of gradients: 0.41083861896493806\n",
      "l2 norm of weights: 3.999276805339177\n",
      "---------------------\n",
      "Iteration Number: 2238\n",
      "Loss: 30.238044333426433\n",
      "l2 norm of gradients: 0.41063871416513037\n",
      "l2 norm of weights: 3.9984843199110673\n",
      "---------------------\n",
      "Iteration Number: 2239\n",
      "Loss: 30.223570818508634\n",
      "l2 norm of gradients: 0.4104388575313077\n",
      "l2 norm of weights: 3.997692370482055\n",
      "---------------------\n",
      "Iteration Number: 2240\n",
      "Loss: 30.209108189536376\n",
      "l2 norm of gradients: 0.41023904910791625\n",
      "l2 norm of weights: 3.9969009569928113\n",
      "---------------------\n",
      "Iteration Number: 2241\n",
      "Loss: 30.194656440889556\n",
      "l2 norm of gradients: 0.41003928893965325\n",
      "l2 norm of weights: 3.9961100793836\n",
      "---------------------\n",
      "Iteration Number: 2242\n",
      "Loss: 30.180215566945247\n",
      "l2 norm of gradients: 0.40983957707146784\n",
      "l2 norm of weights: 3.995319737594277\n",
      "---------------------\n",
      "Iteration Number: 2243\n",
      "Loss: 30.165785562077446\n",
      "l2 norm of gradients: 0.4096399135485594\n",
      "l2 norm of weights: 3.994529931564291\n",
      "---------------------\n",
      "Iteration Number: 2244\n",
      "Loss: 30.15136642065672\n",
      "l2 norm of gradients: 0.4094402984163786\n",
      "l2 norm of weights: 3.993740661232684\n",
      "---------------------\n",
      "Iteration Number: 2245\n",
      "Loss: 30.136958137051504\n",
      "l2 norm of gradients: 0.4092407317206261\n",
      "l2 norm of weights: 3.9929519265380877\n",
      "---------------------\n",
      "Iteration Number: 2246\n",
      "Loss: 30.122560705625936\n",
      "l2 norm of gradients: 0.4090412135072528\n",
      "l2 norm of weights: 3.9921637274187294\n",
      "---------------------\n",
      "Iteration Number: 2247\n",
      "Loss: 30.108174120741904\n",
      "l2 norm of gradients: 0.4088417438224594\n",
      "l2 norm of weights: 3.9913760638124267\n",
      "---------------------\n",
      "Iteration Number: 2248\n",
      "Loss: 30.093798376757707\n",
      "l2 norm of gradients: 0.40864232271269624\n",
      "l2 norm of weights: 3.9905889356565885\n",
      "---------------------\n",
      "Iteration Number: 2249\n",
      "Loss: 30.07943346802888\n",
      "l2 norm of gradients: 0.4084429502246626\n",
      "l2 norm of weights: 3.9898023428882166\n",
      "---------------------\n",
      "Iteration Number: 2250\n",
      "Loss: 30.0650793889076\n",
      "l2 norm of gradients: 0.4082436264053067\n",
      "l2 norm of weights: 3.9890162854439053\n",
      "---------------------\n",
      "Iteration Number: 2251\n",
      "Loss: 30.050736133742923\n",
      "l2 norm of gradients: 0.40804435130182515\n",
      "l2 norm of weights: 3.9882307632598395\n",
      "---------------------\n",
      "Iteration Number: 2252\n",
      "Loss: 30.036403696881006\n",
      "l2 norm of gradients: 0.4078451249616629\n",
      "l2 norm of weights: 3.9874457762717963\n",
      "---------------------\n",
      "Iteration Number: 2253\n",
      "Loss: 30.022082072664606\n",
      "l2 norm of gradients: 0.40764594743251253\n",
      "l2 norm of weights: 3.9866613244151448\n",
      "---------------------\n",
      "Iteration Number: 2254\n",
      "Loss: 30.00777125543327\n",
      "l2 norm of gradients: 0.4074468187623141\n",
      "l2 norm of weights: 3.9858774076248458\n",
      "---------------------\n",
      "Iteration Number: 2255\n",
      "Loss: 29.993471239523927\n",
      "l2 norm of gradients: 0.40724773899925454\n",
      "l2 norm of weights: 3.9850940258354504\n",
      "---------------------\n",
      "Iteration Number: 2256\n",
      "Loss: 29.979182019269672\n",
      "l2 norm of gradients: 0.4070487081917677\n",
      "l2 norm of weights: 3.9843111789811028\n",
      "---------------------\n",
      "Iteration Number: 2257\n",
      "Loss: 29.96490358900078\n",
      "l2 norm of gradients: 0.4068497263885335\n",
      "l2 norm of weights: 3.9835288669955395\n",
      "---------------------\n",
      "Iteration Number: 2258\n",
      "Loss: 29.95063594304435\n",
      "l2 norm of gradients: 0.4066507936384775\n",
      "l2 norm of weights: 3.982747089812086\n",
      "---------------------\n",
      "Iteration Number: 2259\n",
      "Loss: 29.93637907572432\n",
      "l2 norm of gradients: 0.4064519099907711\n",
      "l2 norm of weights: 3.9819658473636608\n",
      "---------------------\n",
      "Iteration Number: 2260\n",
      "Loss: 29.922132981361443\n",
      "l2 norm of gradients: 0.40625307549483025\n",
      "l2 norm of weights: 3.9811851395827738\n",
      "---------------------\n",
      "Iteration Number: 2261\n",
      "Loss: 29.907897654273047\n",
      "l2 norm of gradients: 0.4060542902003159\n",
      "l2 norm of weights: 3.9804049664015277\n",
      "---------------------\n",
      "Iteration Number: 2262\n",
      "Loss: 29.893673088773628\n",
      "l2 norm of gradients: 0.4058555541571327\n",
      "l2 norm of weights: 3.9796253277516143\n",
      "---------------------\n",
      "Iteration Number: 2263\n",
      "Loss: 29.879459279174373\n",
      "l2 norm of gradients: 0.40565686741542933\n",
      "l2 norm of weights: 3.9788462235643185\n",
      "---------------------\n",
      "Iteration Number: 2264\n",
      "Loss: 29.86525621978326\n",
      "l2 norm of gradients: 0.40545823002559717\n",
      "l2 norm of weights: 3.9780676537705175\n",
      "---------------------\n",
      "Iteration Number: 2265\n",
      "Loss: 29.851063904904727\n",
      "l2 norm of gradients: 0.4052596420382708\n",
      "l2 norm of weights: 3.9772896183006785\n",
      "---------------------\n",
      "Iteration Number: 2266\n",
      "Loss: 29.83688232884069\n",
      "l2 norm of gradients: 0.40506110350432684\n",
      "l2 norm of weights: 3.976512117084861\n",
      "---------------------\n",
      "Iteration Number: 2267\n",
      "Loss: 29.822711485889165\n",
      "l2 norm of gradients: 0.4048626144748834\n",
      "l2 norm of weights: 3.975735150052717\n",
      "---------------------\n",
      "Iteration Number: 2268\n",
      "Loss: 29.808551370345306\n",
      "l2 norm of gradients: 0.4046641750013004\n",
      "l2 norm of weights: 3.9749587171334895\n",
      "---------------------\n",
      "Iteration Number: 2269\n",
      "Loss: 29.79440197650137\n",
      "l2 norm of gradients: 0.4044657851351781\n",
      "l2 norm of weights: 3.974182818256014\n",
      "---------------------\n",
      "Iteration Number: 2270\n",
      "Loss: 29.780263298645732\n",
      "l2 norm of gradients: 0.40426744492835687\n",
      "l2 norm of weights: 3.973407453348717\n",
      "---------------------\n",
      "Iteration Number: 2271\n",
      "Loss: 29.766135331063968\n",
      "l2 norm of gradients: 0.4040691544329172\n",
      "l2 norm of weights: 3.9726326223396176\n",
      "---------------------\n",
      "Iteration Number: 2272\n",
      "Loss: 29.75201806803822\n",
      "l2 norm of gradients: 0.40387091370117817\n",
      "l2 norm of weights: 3.9718583251563273\n",
      "---------------------\n",
      "Iteration Number: 2273\n",
      "Loss: 29.73791150384727\n",
      "l2 norm of gradients: 0.4036727227856981\n",
      "l2 norm of weights: 3.9710845617260495\n",
      "---------------------\n",
      "Iteration Number: 2274\n",
      "Loss: 29.72381563276723\n",
      "l2 norm of gradients: 0.40347458173927275\n",
      "l2 norm of weights: 3.9703113319755796\n",
      "---------------------\n",
      "Iteration Number: 2275\n",
      "Loss: 29.709730449070424\n",
      "l2 norm of gradients: 0.40327649061493587\n",
      "l2 norm of weights: 3.9695386358313054\n",
      "---------------------\n",
      "Iteration Number: 2276\n",
      "Loss: 29.69565594702625\n",
      "l2 norm of gradients: 0.4030784494659576\n",
      "l2 norm of weights: 3.968766473219208\n",
      "---------------------\n",
      "Iteration Number: 2277\n",
      "Loss: 29.681592120900774\n",
      "l2 norm of gradients: 0.4028804583458451\n",
      "l2 norm of weights: 3.96799484406486\n",
      "---------------------\n",
      "Iteration Number: 2278\n",
      "Loss: 29.66753896495656\n",
      "l2 norm of gradients: 0.40268251730834054\n",
      "l2 norm of weights: 3.967223748293428\n",
      "---------------------\n",
      "Iteration Number: 2279\n",
      "Loss: 29.653496473453284\n",
      "l2 norm of gradients: 0.4024846264074217\n",
      "l2 norm of weights: 3.96645318582967\n",
      "---------------------\n",
      "Iteration Number: 2280\n",
      "Loss: 29.639464640647248\n",
      "l2 norm of gradients: 0.40228678569730086\n",
      "l2 norm of weights: 3.965683156597937\n",
      "---------------------\n",
      "Iteration Number: 2281\n",
      "Loss: 29.625443460791622\n",
      "l2 norm of gradients: 0.4020889952324241\n",
      "l2 norm of weights: 3.964913660522176\n",
      "---------------------\n",
      "Iteration Number: 2282\n",
      "Loss: 29.61143292813607\n",
      "l2 norm of gradients: 0.4018912550674705\n",
      "l2 norm of weights: 3.964144697525924\n",
      "---------------------\n",
      "Iteration Number: 2283\n",
      "Loss: 29.597433036926926\n",
      "l2 norm of gradients: 0.4016935652573525\n",
      "l2 norm of weights: 3.9633762675323125\n",
      "---------------------\n",
      "Iteration Number: 2284\n",
      "Loss: 29.58344378140792\n",
      "l2 norm of gradients: 0.4014959258572139\n",
      "l2 norm of weights: 3.9626083704640664\n",
      "---------------------\n",
      "Iteration Number: 2285\n",
      "Loss: 29.569465155818495\n",
      "l2 norm of gradients: 0.4012983369224304\n",
      "l2 norm of weights: 3.9618410062435054\n",
      "---------------------\n",
      "Iteration Number: 2286\n",
      "Loss: 29.555497154395987\n",
      "l2 norm of gradients: 0.4011007985086078\n",
      "l2 norm of weights: 3.961074174792543\n",
      "---------------------\n",
      "Iteration Number: 2287\n",
      "Loss: 29.54153977137366\n",
      "l2 norm of gradients: 0.4009033106715825\n",
      "l2 norm of weights: 3.960307876032685\n",
      "---------------------\n",
      "Iteration Number: 2288\n",
      "Loss: 29.527593000981632\n",
      "l2 norm of gradients: 0.40070587346741987\n",
      "l2 norm of weights: 3.959542109885034\n",
      "---------------------\n",
      "Iteration Number: 2289\n",
      "Loss: 29.513656837446888\n",
      "l2 norm of gradients: 0.4005084869524141\n",
      "l2 norm of weights: 3.9587768762702846\n",
      "---------------------\n",
      "Iteration Number: 2290\n",
      "Loss: 29.499731274993348\n",
      "l2 norm of gradients: 0.4003111511830874\n",
      "l2 norm of weights: 3.9580121751087294\n",
      "---------------------\n",
      "Iteration Number: 2291\n",
      "Loss: 29.485816307841443\n",
      "l2 norm of gradients: 0.40011386621618916\n",
      "l2 norm of weights: 3.957248006320252\n",
      "---------------------\n",
      "Iteration Number: 2292\n",
      "Loss: 29.471911930208364\n",
      "l2 norm of gradients: 0.3999166321086953\n",
      "l2 norm of weights: 3.956484369824335\n",
      "---------------------\n",
      "Iteration Number: 2293\n",
      "Loss: 29.458018136307782\n",
      "l2 norm of gradients: 0.3997194489178076\n",
      "l2 norm of weights: 3.955721265540053\n",
      "---------------------\n",
      "Iteration Number: 2294\n",
      "Loss: 29.444134920350685\n",
      "l2 norm of gradients: 0.399522316700953\n",
      "l2 norm of weights: 3.9549586933860787\n",
      "---------------------\n",
      "Iteration Number: 2295\n",
      "Loss: 29.430262276544216\n",
      "l2 norm of gradients: 0.3993252355157828\n",
      "l2 norm of weights: 3.95419665328068\n",
      "---------------------\n",
      "Iteration Number: 2296\n",
      "Loss: 29.416400199092717\n",
      "l2 norm of gradients: 0.3991282054201719\n",
      "l2 norm of weights: 3.953435145141719\n",
      "---------------------\n",
      "Iteration Number: 2297\n",
      "Loss: 29.40254868219703\n",
      "l2 norm of gradients: 0.39893122647221807\n",
      "l2 norm of weights: 3.9526741688866567\n",
      "---------------------\n",
      "Iteration Number: 2298\n",
      "Loss: 29.38870772005462\n",
      "l2 norm of gradients: 0.39873429873024113\n",
      "l2 norm of weights: 3.9519137244325493\n",
      "---------------------\n",
      "Iteration Number: 2299\n",
      "Loss: 29.374877306860025\n",
      "l2 norm of gradients: 0.3985374222527822\n",
      "l2 norm of weights: 3.9511538116960496\n",
      "---------------------\n",
      "Iteration Number: 2300\n",
      "Loss: 29.361057436804302\n",
      "l2 norm of gradients: 0.39834059709860326\n",
      "l2 norm of weights: 3.9503944305934082\n",
      "---------------------\n",
      "Iteration Number: 2301\n",
      "Loss: 29.34724810407513\n",
      "l2 norm of gradients: 0.39814382332668585\n",
      "l2 norm of weights: 3.9496355810404733\n",
      "---------------------\n",
      "Iteration Number: 2302\n",
      "Loss: 29.333449302857165\n",
      "l2 norm of gradients: 0.39794710099623026\n",
      "l2 norm of weights: 3.9488772629526894\n",
      "---------------------\n",
      "Iteration Number: 2303\n",
      "Loss: 29.319661027331968\n",
      "l2 norm of gradients: 0.3977504301666553\n",
      "l2 norm of weights: 3.9481194762451\n",
      "---------------------\n",
      "Iteration Number: 2304\n",
      "Loss: 29.305883271677256\n",
      "l2 norm of gradients: 0.3975538108975971\n",
      "l2 norm of weights: 3.9473622208323462\n",
      "---------------------\n",
      "Iteration Number: 2305\n",
      "Loss: 29.29211603006809\n",
      "l2 norm of gradients: 0.3973572432489082\n",
      "l2 norm of weights: 3.9466054966286683\n",
      "---------------------\n",
      "Iteration Number: 2306\n",
      "Loss: 29.278359296675855\n",
      "l2 norm of gradients: 0.3971607272806568\n",
      "l2 norm of weights: 3.945849303547904\n",
      "---------------------\n",
      "Iteration Number: 2307\n",
      "Loss: 29.264613065669028\n",
      "l2 norm of gradients: 0.3969642630531262\n",
      "l2 norm of weights: 3.9450936415034916\n",
      "---------------------\n",
      "Iteration Number: 2308\n",
      "Loss: 29.250877331212585\n",
      "l2 norm of gradients: 0.39676785062681375\n",
      "l2 norm of weights: 3.944338510408468\n",
      "---------------------\n",
      "Iteration Number: 2309\n",
      "Loss: 29.237152087468168\n",
      "l2 norm of gradients: 0.3965714900624296\n",
      "l2 norm of weights: 3.9435839101754695\n",
      "---------------------\n",
      "Iteration Number: 2310\n",
      "Loss: 29.223437328594702\n",
      "l2 norm of gradients: 0.39637518142089656\n",
      "l2 norm of weights: 3.942829840716733\n",
      "---------------------\n",
      "Iteration Number: 2311\n",
      "Loss: 29.20973304874711\n",
      "l2 norm of gradients: 0.396178924763349\n",
      "l2 norm of weights: 3.9420763019440956\n",
      "---------------------\n",
      "Iteration Number: 2312\n",
      "Loss: 29.196039242077934\n",
      "l2 norm of gradients: 0.39598272015113156\n",
      "l2 norm of weights: 3.9413232937689946\n",
      "---------------------\n",
      "Iteration Number: 2313\n",
      "Loss: 29.182355902735516\n",
      "l2 norm of gradients: 0.3957865676457988\n",
      "l2 norm of weights: 3.9405708161024693\n",
      "---------------------\n",
      "Iteration Number: 2314\n",
      "Loss: 29.168683024866013\n",
      "l2 norm of gradients: 0.39559046730911407\n",
      "l2 norm of weights: 3.9398188688551588\n",
      "---------------------\n",
      "Iteration Number: 2315\n",
      "Loss: 29.155020602611206\n",
      "l2 norm of gradients: 0.3953944192030486\n",
      "l2 norm of weights: 3.939067451937306\n",
      "---------------------\n",
      "Iteration Number: 2316\n",
      "Loss: 29.141368630110893\n",
      "l2 norm of gradients: 0.39519842338978056\n",
      "l2 norm of weights: 3.938316565258754\n",
      "---------------------\n",
      "Iteration Number: 2317\n",
      "Loss: 29.127727101500465\n",
      "l2 norm of gradients: 0.3950024799316943\n",
      "l2 norm of weights: 3.9375662087289487\n",
      "---------------------\n",
      "Iteration Number: 2318\n",
      "Loss: 29.114096010913094\n",
      "l2 norm of gradients: 0.3948065888913795\n",
      "l2 norm of weights: 3.93681638225694\n",
      "---------------------\n",
      "Iteration Number: 2319\n",
      "Loss: 29.100475352477932\n",
      "l2 norm of gradients: 0.39461075033162984\n",
      "l2 norm of weights: 3.9360670857513793\n",
      "---------------------\n",
      "Iteration Number: 2320\n",
      "Loss: 29.08686512032137\n",
      "l2 norm of gradients: 0.39441496431544254\n",
      "l2 norm of weights: 3.9353183191205225\n",
      "---------------------\n",
      "Iteration Number: 2321\n",
      "Loss: 29.073265308566565\n",
      "l2 norm of gradients: 0.394219230906017\n",
      "l2 norm of weights: 3.9345700822722285\n",
      "---------------------\n",
      "Iteration Number: 2322\n",
      "Loss: 29.05967591133338\n",
      "l2 norm of gradients: 0.3940235501667542\n",
      "l2 norm of weights: 3.933822375113962\n",
      "---------------------\n",
      "Iteration Number: 2323\n",
      "Loss: 29.046096922738343\n",
      "l2 norm of gradients: 0.39382792216125556\n",
      "l2 norm of weights: 3.9330751975527902\n",
      "---------------------\n",
      "Iteration Number: 2324\n",
      "Loss: 29.032528336895005\n",
      "l2 norm of gradients: 0.393632346953322\n",
      "l2 norm of weights: 3.9323285494953875\n",
      "---------------------\n",
      "Iteration Number: 2325\n",
      "Loss: 29.018970147913823\n",
      "l2 norm of gradients: 0.3934368246069529\n",
      "l2 norm of weights: 3.931582430848033\n",
      "---------------------\n",
      "Iteration Number: 2326\n",
      "Loss: 29.00542234990152\n",
      "l2 norm of gradients: 0.39324135518634556\n",
      "l2 norm of weights: 3.930836841516611\n",
      "---------------------\n",
      "Iteration Number: 2327\n",
      "Loss: 28.99188493696218\n",
      "l2 norm of gradients: 0.3930459387558931\n",
      "l2 norm of weights: 3.930091781406613\n",
      "---------------------\n",
      "Iteration Number: 2328\n",
      "Loss: 28.97835790319657\n",
      "l2 norm of gradients: 0.39285057538018514\n",
      "l2 norm of weights: 3.9293472504231373\n",
      "---------------------\n",
      "Iteration Number: 2329\n",
      "Loss: 28.964841242702196\n",
      "l2 norm of gradients: 0.3926552651240053\n",
      "l2 norm of weights: 3.9286032484708873\n",
      "---------------------\n",
      "Iteration Number: 2330\n",
      "Loss: 28.951334949573372\n",
      "l2 norm of gradients: 0.39246000805233094\n",
      "l2 norm of weights: 3.927859775454177\n",
      "---------------------\n",
      "Iteration Number: 2331\n",
      "Loss: 28.93783901790156\n",
      "l2 norm of gradients: 0.39226480423033194\n",
      "l2 norm of weights: 3.9271168312769267\n",
      "---------------------\n",
      "Iteration Number: 2332\n",
      "Loss: 28.92435344177455\n",
      "l2 norm of gradients: 0.39206965372337\n",
      "l2 norm of weights: 3.9263744158426648\n",
      "---------------------\n",
      "Iteration Number: 2333\n",
      "Loss: 28.91087821527716\n",
      "l2 norm of gradients: 0.3918745565969968\n",
      "l2 norm of weights: 3.9256325290545298\n",
      "---------------------\n",
      "Iteration Number: 2334\n",
      "Loss: 28.897413332491478\n",
      "l2 norm of gradients: 0.3916795129169542\n",
      "l2 norm of weights: 3.9248911708152674\n",
      "---------------------\n",
      "Iteration Number: 2335\n",
      "Loss: 28.883958787495935\n",
      "l2 norm of gradients: 0.3914845227491719\n",
      "l2 norm of weights: 3.9241503410272363\n",
      "---------------------\n",
      "Iteration Number: 2336\n",
      "Loss: 28.870514574366005\n",
      "l2 norm of gradients: 0.3912895861597674\n",
      "l2 norm of weights: 3.9234100395924014\n",
      "---------------------\n",
      "Iteration Number: 2337\n",
      "Loss: 28.85708068717394\n",
      "l2 norm of gradients: 0.39109470321504425\n",
      "l2 norm of weights: 3.922670266412342\n",
      "---------------------\n",
      "Iteration Number: 2338\n",
      "Loss: 28.843657119988965\n",
      "l2 norm of gradients: 0.39089987398149156\n",
      "l2 norm of weights: 3.921931021388247\n",
      "---------------------\n",
      "Iteration Number: 2339\n",
      "Loss: 28.830243866877364\n",
      "l2 norm of gradients: 0.39070509852578256\n",
      "l2 norm of weights: 3.921192304420916\n",
      "---------------------\n",
      "Iteration Number: 2340\n",
      "Loss: 28.816840921902024\n",
      "l2 norm of gradients: 0.3905103769147736\n",
      "l2 norm of weights: 3.920454115410762\n",
      "---------------------\n",
      "Iteration Number: 2341\n",
      "Loss: 28.8034482791227\n",
      "l2 norm of gradients: 0.39031570921550335\n",
      "l2 norm of weights: 3.9197164542578102\n",
      "---------------------\n",
      "Iteration Number: 2342\n",
      "Loss: 28.790065932596562\n",
      "l2 norm of gradients: 0.39012109549519103\n",
      "l2 norm of weights: 3.9189793208616983\n",
      "---------------------\n",
      "Iteration Number: 2343\n",
      "Loss: 28.776693876376754\n",
      "l2 norm of gradients: 0.3899265358212361\n",
      "l2 norm of weights: 3.918242715121678\n",
      "---------------------\n",
      "Iteration Number: 2344\n",
      "Loss: 28.763332104514042\n",
      "l2 norm of gradients: 0.3897320302612166\n",
      "l2 norm of weights: 3.917506636936615\n",
      "---------------------\n",
      "Iteration Number: 2345\n",
      "Loss: 28.749980611056113\n",
      "l2 norm of gradients: 0.38953757888288865\n",
      "l2 norm of weights: 3.9167710862049896\n",
      "---------------------\n",
      "Iteration Number: 2346\n",
      "Loss: 28.736639390047383\n",
      "l2 norm of gradients: 0.3893431817541845\n",
      "l2 norm of weights: 3.916036062824897\n",
      "---------------------\n",
      "Iteration Number: 2347\n",
      "Loss: 28.723308435529162\n",
      "l2 norm of gradients: 0.38914883894321217\n",
      "l2 norm of weights: 3.9153015666940476\n",
      "---------------------\n",
      "Iteration Number: 2348\n",
      "Loss: 28.709987741539727\n",
      "l2 norm of gradients: 0.38895455051825395\n",
      "l2 norm of weights: 3.9145675977097687\n",
      "---------------------\n",
      "Iteration Number: 2349\n",
      "Loss: 28.69667730211428\n",
      "l2 norm of gradients: 0.38876031654776516\n",
      "l2 norm of weights: 3.913834155769003\n",
      "---------------------\n",
      "Iteration Number: 2350\n",
      "Loss: 28.683377111285395\n",
      "l2 norm of gradients: 0.3885661371003738\n",
      "l2 norm of weights: 3.9131012407683117\n",
      "---------------------\n",
      "Iteration Number: 2351\n",
      "Loss: 28.670087163081988\n",
      "l2 norm of gradients: 0.38837201224487816\n",
      "l2 norm of weights: 3.9123688526038713\n",
      "---------------------\n",
      "Iteration Number: 2352\n",
      "Loss: 28.65680745153023\n",
      "l2 norm of gradients: 0.38817794205024664\n",
      "l2 norm of weights: 3.9116369911714797\n",
      "---------------------\n",
      "Iteration Number: 2353\n",
      "Loss: 28.64353797065329\n",
      "l2 norm of gradients: 0.3879839265856162\n",
      "l2 norm of weights: 3.9109056563665496\n",
      "---------------------\n",
      "Iteration Number: 2354\n",
      "Loss: 28.630278714471206\n",
      "l2 norm of gradients: 0.3877899659202915\n",
      "l2 norm of weights: 3.9101748480841167\n",
      "---------------------\n",
      "Iteration Number: 2355\n",
      "Loss: 28.61702967700151\n",
      "l2 norm of gradients: 0.3875960601237435\n",
      "l2 norm of weights: 3.909444566218833\n",
      "---------------------\n",
      "Iteration Number: 2356\n",
      "Loss: 28.60379085225783\n",
      "l2 norm of gradients: 0.38740220926560787\n",
      "l2 norm of weights: 3.908714810664973\n",
      "---------------------\n",
      "Iteration Number: 2357\n",
      "Loss: 28.59056223425168\n",
      "l2 norm of gradients: 0.38720841341568496\n",
      "l2 norm of weights: 3.907985581316432\n",
      "---------------------\n",
      "Iteration Number: 2358\n",
      "Loss: 28.577343816991036\n",
      "l2 norm of gradients: 0.38701467264393763\n",
      "l2 norm of weights: 3.9072568780667245\n",
      "---------------------\n",
      "Iteration Number: 2359\n",
      "Loss: 28.56413559448105\n",
      "l2 norm of gradients: 0.38682098702049017\n",
      "l2 norm of weights: 3.9065287008089893\n",
      "---------------------\n",
      "Iteration Number: 2360\n",
      "Loss: 28.550937560724027\n",
      "l2 norm of gradients: 0.38662735661562764\n",
      "l2 norm of weights: 3.905801049435987\n",
      "---------------------\n",
      "Iteration Number: 2361\n",
      "Loss: 28.537749709719602\n",
      "l2 norm of gradients: 0.3864337814997941\n",
      "l2 norm of weights: 3.9050739238401\n",
      "---------------------\n",
      "Iteration Number: 2362\n",
      "Loss: 28.524572035463574\n",
      "l2 norm of gradients: 0.38624026174359205\n",
      "l2 norm of weights: 3.904347323913336\n",
      "---------------------\n",
      "Iteration Number: 2363\n",
      "Loss: 28.511404531949502\n",
      "l2 norm of gradients: 0.38604679741778036\n",
      "l2 norm of weights: 3.903621249547326\n",
      "---------------------\n",
      "Iteration Number: 2364\n",
      "Loss: 28.498247193167973\n",
      "l2 norm of gradients: 0.3858533885932738\n",
      "l2 norm of weights: 3.902895700633326\n",
      "---------------------\n",
      "Iteration Number: 2365\n",
      "Loss: 28.485100013106507\n",
      "l2 norm of gradients: 0.38566003534114146\n",
      "l2 norm of weights: 3.902170677062216\n",
      "---------------------\n",
      "Iteration Number: 2366\n",
      "Loss: 28.471962985749773\n",
      "l2 norm of gradients: 0.38546673773260587\n",
      "l2 norm of weights: 3.901446178724505\n",
      "---------------------\n",
      "Iteration Number: 2367\n",
      "Loss: 28.458836105079634\n",
      "l2 norm of gradients: 0.3852734958390411\n",
      "l2 norm of weights: 3.9007222055103243\n",
      "---------------------\n",
      "Iteration Number: 2368\n",
      "Loss: 28.445719365074876\n",
      "l2 norm of gradients: 0.3850803097319724\n",
      "l2 norm of weights: 3.8999987573094357\n",
      "---------------------\n",
      "Iteration Number: 2369\n",
      "Loss: 28.43261275971129\n",
      "l2 norm of gradients: 0.3848871794830742\n",
      "l2 norm of weights: 3.8992758340112275\n",
      "---------------------\n",
      "Iteration Number: 2370\n",
      "Loss: 28.419516282962192\n",
      "l2 norm of gradients: 0.38469410516416946\n",
      "l2 norm of weights: 3.8985534355047156\n",
      "---------------------\n",
      "Iteration Number: 2371\n",
      "Loss: 28.406429928798023\n",
      "l2 norm of gradients: 0.38450108684722806\n",
      "l2 norm of weights: 3.897831561678545\n",
      "---------------------\n",
      "Iteration Number: 2372\n",
      "Loss: 28.393353691185883\n",
      "l2 norm of gradients: 0.38430812460436553\n",
      "l2 norm of weights: 3.8971102124209915\n",
      "---------------------\n",
      "Iteration Number: 2373\n",
      "Loss: 28.380287564090523\n",
      "l2 norm of gradients: 0.38411521850784214\n",
      "l2 norm of weights: 3.8963893876199585\n",
      "---------------------\n",
      "Iteration Number: 2374\n",
      "Loss: 28.367231541473735\n",
      "l2 norm of gradients: 0.3839223686300614\n",
      "l2 norm of weights: 3.8956690871629833\n",
      "---------------------\n",
      "Iteration Number: 2375\n",
      "Loss: 28.35418561729442\n",
      "l2 norm of gradients: 0.3837295750435686\n",
      "l2 norm of weights: 3.89494931093723\n",
      "---------------------\n",
      "Iteration Number: 2376\n",
      "Loss: 28.34114978550854\n",
      "l2 norm of gradients: 0.38353683782104997\n",
      "l2 norm of weights: 3.8942300588294994\n",
      "---------------------\n",
      "Iteration Number: 2377\n",
      "Loss: 28.32812404006985\n",
      "l2 norm of gradients: 0.38334415703533115\n",
      "l2 norm of weights: 3.893511330726221\n",
      "---------------------\n",
      "Iteration Number: 2378\n",
      "Loss: 28.31510837492844\n",
      "l2 norm of gradients: 0.3831515327593761\n",
      "l2 norm of weights: 3.8927931265134594\n",
      "---------------------\n",
      "Iteration Number: 2379\n",
      "Loss: 28.302102784032176\n",
      "l2 norm of gradients: 0.38295896506628535\n",
      "l2 norm of weights: 3.892075446076912\n",
      "---------------------\n",
      "Iteration Number: 2380\n",
      "Loss: 28.289107261326283\n",
      "l2 norm of gradients: 0.3827664540292955\n",
      "l2 norm of weights: 3.891358289301912\n",
      "---------------------\n",
      "Iteration Number: 2381\n",
      "Loss: 28.276121800752996\n",
      "l2 norm of gradients: 0.3825739997217771\n",
      "l2 norm of weights: 3.890641656073426\n",
      "---------------------\n",
      "Iteration Number: 2382\n",
      "Loss: 28.26314639625163\n",
      "l2 norm of gradients: 0.3823816022172341\n",
      "l2 norm of weights: 3.8899255462760562\n",
      "---------------------\n",
      "Iteration Number: 2383\n",
      "Loss: 28.250181041758946\n",
      "l2 norm of gradients: 0.3821892615893018\n",
      "l2 norm of weights: 3.889209959794042\n",
      "---------------------\n",
      "Iteration Number: 2384\n",
      "Loss: 28.237225731209442\n",
      "l2 norm of gradients: 0.3819969779117463\n",
      "l2 norm of weights: 3.88849489651126\n",
      "---------------------\n",
      "Iteration Number: 2385\n",
      "Loss: 28.224280458533972\n",
      "l2 norm of gradients: 0.3818047512584626\n",
      "l2 norm of weights: 3.887780356311223\n",
      "---------------------\n",
      "Iteration Number: 2386\n",
      "Loss: 28.21134521766132\n",
      "l2 norm of gradients: 0.38161258170347384\n",
      "l2 norm of weights: 3.8870663390770823\n",
      "---------------------\n",
      "Iteration Number: 2387\n",
      "Loss: 28.198420002517693\n",
      "l2 norm of gradients: 0.38142046932092927\n",
      "l2 norm of weights: 3.8863528446916287\n",
      "---------------------\n",
      "Iteration Number: 2388\n",
      "Loss: 28.185504807026234\n",
      "l2 norm of gradients: 0.3812284141851038\n",
      "l2 norm of weights: 3.885639873037292\n",
      "---------------------\n",
      "Iteration Number: 2389\n",
      "Loss: 28.1725996251074\n",
      "l2 norm of gradients: 0.38103641637039576\n",
      "l2 norm of weights: 3.884927423996142\n",
      "---------------------\n",
      "Iteration Number: 2390\n",
      "Loss: 28.159704450679474\n",
      "l2 norm of gradients: 0.38084447595132664\n",
      "l2 norm of weights: 3.8842154974498895\n",
      "---------------------\n",
      "Iteration Number: 2391\n",
      "Loss: 28.146819277657816\n",
      "l2 norm of gradients: 0.38065259300253873\n",
      "l2 norm of weights: 3.8835040932798868\n",
      "---------------------\n",
      "Iteration Number: 2392\n",
      "Loss: 28.133944099955027\n",
      "l2 norm of gradients: 0.38046076759879416\n",
      "l2 norm of weights: 3.8827932113671277\n",
      "---------------------\n",
      "Iteration Number: 2393\n",
      "Loss: 28.121078911480932\n",
      "l2 norm of gradients: 0.38026899981497403\n",
      "l2 norm of weights: 3.88208285159225\n",
      "---------------------\n",
      "Iteration Number: 2394\n",
      "Loss: 28.108223706143654\n",
      "l2 norm of gradients: 0.3800772897260765\n",
      "l2 norm of weights: 3.881373013835533\n",
      "---------------------\n",
      "Iteration Number: 2395\n",
      "Loss: 28.09537847784761\n",
      "l2 norm of gradients: 0.3798856374072157\n",
      "l2 norm of weights: 3.880663697976902\n",
      "---------------------\n",
      "Iteration Number: 2396\n",
      "Loss: 28.08254322049539\n",
      "l2 norm of gradients: 0.37969404293362014\n",
      "l2 norm of weights: 3.879954903895925\n",
      "---------------------\n",
      "Iteration Number: 2397\n",
      "Loss: 28.069717927986886\n",
      "l2 norm of gradients: 0.3795025063806319\n",
      "l2 norm of weights: 3.8792466314718177\n",
      "---------------------\n",
      "Iteration Number: 2398\n",
      "Loss: 28.056902594219075\n",
      "l2 norm of gradients: 0.3793110278237044\n",
      "l2 norm of weights: 3.8785388805834398\n",
      "---------------------\n",
      "Iteration Number: 2399\n",
      "Loss: 28.044097213086552\n",
      "l2 norm of gradients: 0.3791196073384021\n",
      "l2 norm of weights: 3.8778316511093\n",
      "---------------------\n",
      "Iteration Number: 2400\n",
      "Loss: 28.031301778481886\n",
      "l2 norm of gradients: 0.3789282450003985\n",
      "l2 norm of weights: 3.8771249429275514\n",
      "---------------------\n",
      "Iteration Number: 2401\n",
      "Loss: 28.018516284294424\n",
      "l2 norm of gradients: 0.37873694088547466\n",
      "l2 norm of weights: 3.8764187559159975\n",
      "---------------------\n",
      "Iteration Number: 2402\n",
      "Loss: 28.005740724411297\n",
      "l2 norm of gradients: 0.3785456950695187\n",
      "l2 norm of weights: 3.875713089952091\n",
      "---------------------\n",
      "Iteration Number: 2403\n",
      "Loss: 27.992975092717515\n",
      "l2 norm of gradients: 0.3783545076285229\n",
      "l2 norm of weights: 3.8750079449129315\n",
      "---------------------\n",
      "Iteration Number: 2404\n",
      "Loss: 27.980219383094774\n",
      "l2 norm of gradients: 0.37816337863858407\n",
      "l2 norm of weights: 3.874303320675271\n",
      "---------------------\n",
      "Iteration Number: 2405\n",
      "Loss: 27.967473589423197\n",
      "l2 norm of gradients: 0.37797230817590105\n",
      "l2 norm of weights: 3.873599217115512\n",
      "---------------------\n",
      "Iteration Number: 2406\n",
      "Loss: 27.954737705579927\n",
      "l2 norm of gradients: 0.37778129631677376\n",
      "l2 norm of weights: 3.8728956341097076\n",
      "---------------------\n",
      "Iteration Number: 2407\n",
      "Loss: 27.942011725439734\n",
      "l2 norm of gradients: 0.3775903431376016\n",
      "l2 norm of weights: 3.8721925715335637\n",
      "---------------------\n",
      "Iteration Number: 2408\n",
      "Loss: 27.92929564287515\n",
      "l2 norm of gradients: 0.3773994487148823\n",
      "l2 norm of weights: 3.8714900292624392\n",
      "---------------------\n",
      "Iteration Number: 2409\n",
      "Loss: 27.916589451756007\n",
      "l2 norm of gradients: 0.37720861312521053\n",
      "l2 norm of weights: 3.8707880071713467\n",
      "---------------------\n",
      "Iteration Number: 2410\n",
      "Loss: 27.903893145950192\n",
      "l2 norm of gradients: 0.3770178364452763\n",
      "l2 norm of weights: 3.8700865051349527\n",
      "---------------------\n",
      "Iteration Number: 2411\n",
      "Loss: 27.89120671932294\n",
      "l2 norm of gradients: 0.37682711875186375\n",
      "l2 norm of weights: 3.869385523027579\n",
      "---------------------\n",
      "Iteration Number: 2412\n",
      "Loss: 27.878530165736894\n",
      "l2 norm of gradients: 0.3766364601218498\n",
      "l2 norm of weights: 3.868685060723202\n",
      "---------------------\n",
      "Iteration Number: 2413\n",
      "Loss: 27.865863479052734\n",
      "l2 norm of gradients: 0.3764458606322027\n",
      "l2 norm of weights: 3.8679851180954574\n",
      "---------------------\n",
      "Iteration Number: 2414\n",
      "Loss: 27.853206653128854\n",
      "l2 norm of gradients: 0.37625532035998055\n",
      "l2 norm of weights: 3.8672856950176344\n",
      "---------------------\n",
      "Iteration Number: 2415\n",
      "Loss: 27.840559681820515\n",
      "l2 norm of gradients: 0.3760648393823301\n",
      "l2 norm of weights: 3.8665867913626824\n",
      "---------------------\n",
      "Iteration Number: 2416\n",
      "Loss: 27.827922558981914\n",
      "l2 norm of gradients: 0.3758744177764851\n",
      "l2 norm of weights: 3.865888407003209\n",
      "---------------------\n",
      "Iteration Number: 2417\n",
      "Loss: 27.815295278463942\n",
      "l2 norm of gradients: 0.3756840556197653\n",
      "l2 norm of weights: 3.86519054181148\n",
      "---------------------\n",
      "Iteration Number: 2418\n",
      "Loss: 27.80267783411566\n",
      "l2 norm of gradients: 0.37549375298957466\n",
      "l2 norm of weights: 3.8644931956594215\n",
      "---------------------\n",
      "Iteration Number: 2419\n",
      "Loss: 27.790070219783935\n",
      "l2 norm of gradients: 0.37530350996340006\n",
      "l2 norm of weights: 3.863796368418621\n",
      "---------------------\n",
      "Iteration Number: 2420\n",
      "Loss: 27.777472429312716\n",
      "l2 norm of gradients: 0.37511332661881014\n",
      "l2 norm of weights: 3.863100059960327\n",
      "---------------------\n",
      "Iteration Number: 2421\n",
      "Loss: 27.764884456544838\n",
      "l2 norm of gradients: 0.37492320303345356\n",
      "l2 norm of weights: 3.8624042701554493\n",
      "---------------------\n",
      "Iteration Number: 2422\n",
      "Loss: 27.75230629532011\n",
      "l2 norm of gradients: 0.37473313928505775\n",
      "l2 norm of weights: 3.861708998874562\n",
      "---------------------\n",
      "Iteration Number: 2423\n",
      "Loss: 27.73973793947599\n",
      "l2 norm of gradients: 0.3745431354514276\n",
      "l2 norm of weights: 3.8610142459879007\n",
      "---------------------\n",
      "Iteration Number: 2424\n",
      "Loss: 27.727179382848437\n",
      "l2 norm of gradients: 0.37435319161044384\n",
      "l2 norm of weights: 3.8603200113653666\n",
      "---------------------\n",
      "Iteration Number: 2425\n",
      "Loss: 27.71463061927066\n",
      "l2 norm of gradients: 0.37416330784006174\n",
      "l2 norm of weights: 3.8596262948765263\n",
      "---------------------\n",
      "Iteration Number: 2426\n",
      "Loss: 27.702091642574068\n",
      "l2 norm of gradients: 0.37397348421830995\n",
      "l2 norm of weights: 3.85893309639061\n",
      "---------------------\n",
      "Iteration Number: 2427\n",
      "Loss: 27.68956244658754\n",
      "l2 norm of gradients: 0.3737837208232885\n",
      "l2 norm of weights: 3.8582404157765167\n",
      "---------------------\n",
      "Iteration Number: 2428\n",
      "Loss: 27.677043025138172\n",
      "l2 norm of gradients: 0.37359401773316797\n",
      "l2 norm of weights: 3.8575482529028102\n",
      "---------------------\n",
      "Iteration Number: 2429\n",
      "Loss: 27.664533372050656\n",
      "l2 norm of gradients: 0.37340437502618784\n",
      "l2 norm of weights: 3.856856607637724\n",
      "---------------------\n",
      "Iteration Number: 2430\n",
      "Loss: 27.652033481148\n",
      "l2 norm of gradients: 0.37321479278065506\n",
      "l2 norm of weights: 3.856165479849159\n",
      "---------------------\n",
      "Iteration Number: 2431\n",
      "Loss: 27.639543346250505\n",
      "l2 norm of gradients: 0.3730252710749425\n",
      "l2 norm of weights: 3.855474869404686\n",
      "---------------------\n",
      "Iteration Number: 2432\n",
      "Loss: 27.627062961176893\n",
      "l2 norm of gradients: 0.37283580998748794\n",
      "l2 norm of weights: 3.854784776171545\n",
      "---------------------\n",
      "Iteration Number: 2433\n",
      "Loss: 27.61459231974358\n",
      "l2 norm of gradients: 0.3726464095967924\n",
      "l2 norm of weights: 3.8540952000166473\n",
      "---------------------\n",
      "Iteration Number: 2434\n",
      "Loss: 27.602131415765253\n",
      "l2 norm of gradients: 0.37245706998141853\n",
      "l2 norm of weights: 3.853406140806576\n",
      "---------------------\n",
      "Iteration Number: 2435\n",
      "Loss: 27.589680243054204\n",
      "l2 norm of gradients: 0.3722677912199895\n",
      "l2 norm of weights: 3.852717598407585\n",
      "---------------------\n",
      "Iteration Number: 2436\n",
      "Loss: 27.577238795420712\n",
      "l2 norm of gradients: 0.37207857339118766\n",
      "l2 norm of weights: 3.852029572685603\n",
      "---------------------\n",
      "Iteration Number: 2437\n",
      "Loss: 27.564807066673207\n",
      "l2 norm of gradients: 0.3718894165737526\n",
      "l2 norm of weights: 3.8513420635062303\n",
      "---------------------\n",
      "Iteration Number: 2438\n",
      "Loss: 27.55238505061831\n",
      "l2 norm of gradients: 0.3717003208464804\n",
      "l2 norm of weights: 3.8506550707347427\n",
      "---------------------\n",
      "Iteration Number: 2439\n",
      "Loss: 27.53997274106057\n",
      "l2 norm of gradients: 0.3715112862882217\n",
      "l2 norm of weights: 3.8499685942360915\n",
      "---------------------\n",
      "Iteration Number: 2440\n",
      "Loss: 27.52757013180203\n",
      "l2 norm of gradients: 0.3713223129778805\n",
      "l2 norm of weights: 3.8492826338749033\n",
      "---------------------\n",
      "Iteration Number: 2441\n",
      "Loss: 27.515177216643856\n",
      "l2 norm of gradients: 0.3711334009944128\n",
      "l2 norm of weights: 3.84859718951548\n",
      "---------------------\n",
      "Iteration Number: 2442\n",
      "Loss: 27.502793989384227\n",
      "l2 norm of gradients: 0.37094455041682484\n",
      "l2 norm of weights: 3.847912261021803\n",
      "---------------------\n",
      "Iteration Number: 2443\n",
      "Loss: 27.490420443820092\n",
      "l2 norm of gradients: 0.3707557613241722\n",
      "l2 norm of weights: 3.8472278482575297\n",
      "---------------------\n",
      "Iteration Number: 2444\n",
      "Loss: 27.47805657374615\n",
      "l2 norm of gradients: 0.3705670337955581\n",
      "l2 norm of weights: 3.8465439510859976\n",
      "---------------------\n",
      "Iteration Number: 2445\n",
      "Loss: 27.465702372955846\n",
      "l2 norm of gradients: 0.37037836791013173\n",
      "l2 norm of weights: 3.8458605693702235\n",
      "---------------------\n",
      "Iteration Number: 2446\n",
      "Loss: 27.45335783523985\n",
      "l2 norm of gradients: 0.3701897637470873\n",
      "l2 norm of weights: 3.8451777029729035\n",
      "---------------------\n",
      "Iteration Number: 2447\n",
      "Loss: 27.44102295438762\n",
      "l2 norm of gradients: 0.3700012213856624\n",
      "l2 norm of weights: 3.8444953517564153\n",
      "---------------------\n",
      "Iteration Number: 2448\n",
      "Loss: 27.42869772418657\n",
      "l2 norm of gradients: 0.3698127409051363\n",
      "l2 norm of weights: 3.8438135155828186\n",
      "---------------------\n",
      "Iteration Number: 2449\n",
      "Loss: 27.416382138422495\n",
      "l2 norm of gradients: 0.3696243223848292\n",
      "l2 norm of weights: 3.8431321943138546\n",
      "---------------------\n",
      "Iteration Number: 2450\n",
      "Loss: 27.404076190879096\n",
      "l2 norm of gradients: 0.36943596590410016\n",
      "l2 norm of weights: 3.8424513878109487\n",
      "---------------------\n",
      "Iteration Number: 2451\n",
      "Loss: 27.391779875338667\n",
      "l2 norm of gradients: 0.3692476715423461\n",
      "l2 norm of weights: 3.841771095935209\n",
      "---------------------\n",
      "Iteration Number: 2452\n",
      "Loss: 27.37949318558143\n",
      "l2 norm of gradients: 0.3690594393790001\n",
      "l2 norm of weights: 3.841091318547429\n",
      "---------------------\n",
      "Iteration Number: 2453\n",
      "Loss: 27.36721611538613\n",
      "l2 norm of gradients: 0.3688712694935301\n",
      "l2 norm of weights: 3.840412055508087\n",
      "---------------------\n",
      "Iteration Number: 2454\n",
      "Loss: 27.354948658529324\n",
      "l2 norm of gradients: 0.36868316196543754\n",
      "l2 norm of weights: 3.8397333066773487\n",
      "---------------------\n",
      "Iteration Number: 2455\n",
      "Loss: 27.34269080878639\n",
      "l2 norm of gradients: 0.3684951168742559\n",
      "l2 norm of weights: 3.8390550719150642\n",
      "---------------------\n",
      "Iteration Number: 2456\n",
      "Loss: 27.330442559930628\n",
      "l2 norm of gradients: 0.3683071342995492\n",
      "l2 norm of weights: 3.838377351080774\n",
      "---------------------\n",
      "Iteration Number: 2457\n",
      "Loss: 27.318203905734087\n",
      "l2 norm of gradients: 0.36811921432091066\n",
      "l2 norm of weights: 3.8377001440337044\n",
      "---------------------\n",
      "Iteration Number: 2458\n",
      "Loss: 27.305974839966705\n",
      "l2 norm of gradients: 0.3679313570179612\n",
      "l2 norm of weights: 3.8370234506327723\n",
      "---------------------\n",
      "Iteration Number: 2459\n",
      "Loss: 27.293755356396936\n",
      "l2 norm of gradients: 0.3677435624703484\n",
      "l2 norm of weights: 3.836347270736583\n",
      "---------------------\n",
      "Iteration Number: 2460\n",
      "Loss: 27.28154544879185\n",
      "l2 norm of gradients: 0.3675558307577442\n",
      "l2 norm of weights: 3.835671604203434\n",
      "---------------------\n",
      "Iteration Number: 2461\n",
      "Loss: 27.269345110916454\n",
      "l2 norm of gradients: 0.36736816195984456\n",
      "l2 norm of weights: 3.834996450891312\n",
      "---------------------\n",
      "Iteration Number: 2462\n",
      "Loss: 27.257154336534523\n",
      "l2 norm of gradients: 0.3671805561563675\n",
      "l2 norm of weights: 3.8343218106578982\n",
      "---------------------\n",
      "Iteration Number: 2463\n",
      "Loss: 27.244973119408186\n",
      "l2 norm of gradients: 0.3669930134270515\n",
      "l2 norm of weights: 3.8336476833605633\n",
      "---------------------\n",
      "Iteration Number: 2464\n",
      "Loss: 27.232801453297864\n",
      "l2 norm of gradients: 0.3668055338516545\n",
      "l2 norm of weights: 3.8329740688563745\n",
      "---------------------\n",
      "Iteration Number: 2465\n",
      "Loss: 27.220639331962413\n",
      "l2 norm of gradients: 0.36661811750995227\n",
      "l2 norm of weights: 3.8323009670020913\n",
      "---------------------\n",
      "Iteration Number: 2466\n",
      "Loss: 27.208486749159647\n",
      "l2 norm of gradients: 0.36643076448173695\n",
      "l2 norm of weights: 3.831628377654168\n",
      "---------------------\n",
      "Iteration Number: 2467\n",
      "Loss: 27.19634369864509\n",
      "l2 norm of gradients: 0.36624347484681585\n",
      "l2 norm of weights: 3.8309563006687553\n",
      "---------------------\n",
      "Iteration Number: 2468\n",
      "Loss: 27.184210174173455\n",
      "l2 norm of gradients: 0.36605624868500997\n",
      "l2 norm of weights: 3.8302847359017007\n",
      "---------------------\n",
      "Iteration Number: 2469\n",
      "Loss: 27.17208616949753\n",
      "l2 norm of gradients: 0.36586908607615237\n",
      "l2 norm of weights: 3.8296136832085477\n",
      "---------------------\n",
      "Iteration Number: 2470\n",
      "Loss: 27.15997167836912\n",
      "l2 norm of gradients: 0.3656819871000872\n",
      "l2 norm of weights: 3.8289431424445377\n",
      "---------------------\n",
      "Iteration Number: 2471\n",
      "Loss: 27.147866694538017\n",
      "l2 norm of gradients: 0.36549495183666764\n",
      "l2 norm of weights: 3.8282731134646104\n",
      "---------------------\n",
      "Iteration Number: 2472\n",
      "Loss: 27.135771211752875\n",
      "l2 norm of gradients: 0.3653079803657554\n",
      "l2 norm of weights: 3.827603596123407\n",
      "---------------------\n",
      "Iteration Number: 2473\n",
      "Loss: 27.123685223761278\n",
      "l2 norm of gradients: 0.3651210727672185\n",
      "l2 norm of weights: 3.8269345902752647\n",
      "---------------------\n",
      "Iteration Number: 2474\n",
      "Loss: 27.111608724308564\n",
      "l2 norm of gradients: 0.3649342291209301\n",
      "l2 norm of weights: 3.826266095774226\n",
      "---------------------\n",
      "Iteration Number: 2475\n",
      "Loss: 27.099541707139608\n",
      "l2 norm of gradients: 0.3647474495067677\n",
      "l2 norm of weights: 3.8255981124740313\n",
      "---------------------\n",
      "Iteration Number: 2476\n",
      "Loss: 27.087484165997235\n",
      "l2 norm of gradients: 0.36456073400461064\n",
      "l2 norm of weights: 3.824930640228124\n",
      "---------------------\n",
      "Iteration Number: 2477\n",
      "Loss: 27.07543609462342\n",
      "l2 norm of gradients: 0.36437408269433963\n",
      "l2 norm of weights: 3.8242636788896522\n",
      "---------------------\n",
      "Iteration Number: 2478\n",
      "Loss: 27.063397486758802\n",
      "l2 norm of gradients: 0.36418749565583514\n",
      "l2 norm of weights: 3.8235972283114656\n",
      "---------------------\n",
      "Iteration Number: 2479\n",
      "Loss: 27.051368336142282\n",
      "l2 norm of gradients: 0.3640009729689755\n",
      "l2 norm of weights: 3.822931288346118\n",
      "---------------------\n",
      "Iteration Number: 2480\n",
      "Loss: 27.03934863651171\n",
      "l2 norm of gradients: 0.36381451471363624\n",
      "l2 norm of weights: 3.8222658588458716\n",
      "---------------------\n",
      "Iteration Number: 2481\n",
      "Loss: 27.027338381603727\n",
      "l2 norm of gradients: 0.3636281209696884\n",
      "l2 norm of weights: 3.8216009396626895\n",
      "---------------------\n",
      "Iteration Number: 2482\n",
      "Loss: 27.015337565153974\n",
      "l2 norm of gradients: 0.36344179181699704\n",
      "l2 norm of weights: 3.8209365306482455\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 2483\n",
      "Loss: 27.003346180896067\n",
      "l2 norm of gradients: 0.3632555273354198\n",
      "l2 norm of weights: 3.820272631653918\n",
      "---------------------\n",
      "Iteration Number: 2484\n",
      "Loss: 26.99136422256349\n",
      "l2 norm of gradients: 0.3630693276048059\n",
      "l2 norm of weights: 3.8196092425307953\n",
      "---------------------\n",
      "Iteration Number: 2485\n",
      "Loss: 26.9793916838875\n",
      "l2 norm of gradients: 0.3628831927049943\n",
      "l2 norm of weights: 3.8189463631296725\n",
      "---------------------\n",
      "Iteration Number: 2486\n",
      "Loss: 26.967428558598826\n",
      "l2 norm of gradients: 0.3626971227158128\n",
      "l2 norm of weights: 3.818283993301056\n",
      "---------------------\n",
      "Iteration Number: 2487\n",
      "Loss: 26.95547484042669\n",
      "l2 norm of gradients: 0.3625111177170763\n",
      "l2 norm of weights: 3.8176221328951607\n",
      "---------------------\n",
      "Iteration Number: 2488\n",
      "Loss: 26.94353052309947\n",
      "l2 norm of gradients: 0.3623251777885854\n",
      "l2 norm of weights: 3.8169607817619133\n",
      "---------------------\n",
      "Iteration Number: 2489\n",
      "Loss: 26.931595600344238\n",
      "l2 norm of gradients: 0.36213930301012554\n",
      "l2 norm of weights: 3.8162999397509525\n",
      "---------------------\n",
      "Iteration Number: 2490\n",
      "Loss: 26.919670065887026\n",
      "l2 norm of gradients: 0.3619534934614648\n",
      "l2 norm of weights: 3.815639606711627\n",
      "---------------------\n",
      "Iteration Number: 2491\n",
      "Loss: 26.907753913452492\n",
      "l2 norm of gradients: 0.3617677492223535\n",
      "l2 norm of weights: 3.814979782493002\n",
      "---------------------\n",
      "Iteration Number: 2492\n",
      "Loss: 26.895847136764708\n",
      "l2 norm of gradients: 0.36158207037252194\n",
      "l2 norm of weights: 3.814320466943853\n",
      "---------------------\n",
      "Iteration Number: 2493\n",
      "Loss: 26.883949729546245\n",
      "l2 norm of gradients: 0.3613964569916797\n",
      "l2 norm of weights: 3.8136616599126723\n",
      "---------------------\n",
      "Iteration Number: 2494\n",
      "Loss: 26.8720616855188\n",
      "l2 norm of gradients: 0.361210909159514\n",
      "l2 norm of weights: 3.813003361247666\n",
      "---------------------\n",
      "Iteration Number: 2495\n",
      "Loss: 26.86018299840335\n",
      "l2 norm of gradients: 0.36102542695568834\n",
      "l2 norm of weights: 3.812345570796756\n",
      "---------------------\n",
      "Iteration Number: 2496\n",
      "Loss: 26.848313661919256\n",
      "l2 norm of gradients: 0.3608400104598411\n",
      "l2 norm of weights: 3.811688288407582\n",
      "---------------------\n",
      "Iteration Number: 2497\n",
      "Loss: 26.836453669785392\n",
      "l2 norm of gradients: 0.3606546597515845\n",
      "l2 norm of weights: 3.8110315139275\n",
      "---------------------\n",
      "Iteration Number: 2498\n",
      "Loss: 26.8246030157193\n",
      "l2 norm of gradients: 0.36046937491050274\n",
      "l2 norm of weights: 3.8103752472035834\n",
      "---------------------\n",
      "Iteration Number: 2499\n",
      "Loss: 26.812761693438123\n",
      "l2 norm of gradients: 0.3602841560161512\n",
      "l2 norm of weights: 3.8097194880826257\n",
      "---------------------\n",
      "Iteration Number: 2500\n",
      "Loss: 26.800929696657246\n",
      "l2 norm of gradients: 0.3600990031480548\n",
      "l2 norm of weights: 3.8090642364111384\n",
      "---------------------\n",
      "Iteration Number: 2501\n",
      "Loss: 26.78910701909193\n",
      "l2 norm of gradients: 0.3599139163857066\n",
      "l2 norm of weights: 3.8084094920353535\n",
      "---------------------\n",
      "Iteration Number: 2502\n",
      "Loss: 26.77729365445618\n",
      "l2 norm of gradients: 0.3597288958085668\n",
      "l2 norm of weights: 3.807755254801225\n",
      "---------------------\n",
      "Iteration Number: 2503\n",
      "Loss: 26.765489596462967\n",
      "l2 norm of gradients: 0.35954394149606106\n",
      "l2 norm of weights: 3.807101524554426\n",
      "---------------------\n",
      "Iteration Number: 2504\n",
      "Loss: 26.75369483882475\n",
      "l2 norm of gradients: 0.3593590535275793\n",
      "l2 norm of weights: 3.806448301140353\n",
      "---------------------\n",
      "Iteration Number: 2505\n",
      "Loss: 26.74190937525275\n",
      "l2 norm of gradients: 0.3591742319824744\n",
      "l2 norm of weights: 3.8057955844041262\n",
      "---------------------\n",
      "Iteration Number: 2506\n",
      "Loss: 26.73013319945789\n",
      "l2 norm of gradients: 0.35898947694006084\n",
      "l2 norm of weights: 3.8051433741905876\n",
      "---------------------\n",
      "Iteration Number: 2507\n",
      "Loss: 26.718366305149864\n",
      "l2 norm of gradients: 0.3588047884796136\n",
      "l2 norm of weights: 3.804491670344305\n",
      "---------------------\n",
      "Iteration Number: 2508\n",
      "Loss: 26.706608686037487\n",
      "l2 norm of gradients: 0.3586201666803664\n",
      "l2 norm of weights: 3.8038404727095694\n",
      "---------------------\n",
      "Iteration Number: 2509\n",
      "Loss: 26.69486033582932\n",
      "l2 norm of gradients: 0.3584356116215108\n",
      "l2 norm of weights: 3.803189781130399\n",
      "---------------------\n",
      "Iteration Number: 2510\n",
      "Loss: 26.68312124823252\n",
      "l2 norm of gradients: 0.35825112338219456\n",
      "l2 norm of weights: 3.802539595450538\n",
      "---------------------\n",
      "Iteration Number: 2511\n",
      "Loss: 26.671391416954048\n",
      "l2 norm of gradients: 0.35806670204152063\n",
      "l2 norm of weights: 3.8018899155134562\n",
      "---------------------\n",
      "Iteration Number: 2512\n",
      "Loss: 26.659670835699888\n",
      "l2 norm of gradients: 0.35788234767854593\n",
      "l2 norm of weights: 3.801240741162353\n",
      "---------------------\n",
      "Iteration Number: 2513\n",
      "Loss: 26.64795949817537\n",
      "l2 norm of gradients: 0.35769806037227947\n",
      "l2 norm of weights: 3.8005920722401543\n",
      "---------------------\n",
      "Iteration Number: 2514\n",
      "Loss: 26.636257398085135\n",
      "l2 norm of gradients: 0.35751384020168175\n",
      "l2 norm of weights: 3.799943908589517\n",
      "---------------------\n",
      "Iteration Number: 2515\n",
      "Loss: 26.624564529133043\n",
      "l2 norm of gradients: 0.357329687245663\n",
      "l2 norm of weights: 3.799296250052826\n",
      "---------------------\n",
      "Iteration Number: 2516\n",
      "Loss: 26.612880885022687\n",
      "l2 norm of gradients: 0.35714560158308195\n",
      "l2 norm of weights: 3.7986490964721966\n",
      "---------------------\n",
      "Iteration Number: 2517\n",
      "Loss: 26.601206459456385\n",
      "l2 norm of gradients: 0.35696158329274497\n",
      "l2 norm of weights: 3.7980024476894774\n",
      "---------------------\n",
      "Iteration Number: 2518\n",
      "Loss: 26.5895412461366\n",
      "l2 norm of gradients: 0.35677763245340427\n",
      "l2 norm of weights: 3.797356303546245\n",
      "---------------------\n",
      "Iteration Number: 2519\n",
      "Loss: 26.57788523876471\n",
      "l2 norm of gradients: 0.35659374914375686\n",
      "l2 norm of weights: 3.796710663883812\n",
      "---------------------\n",
      "Iteration Number: 2520\n",
      "Loss: 26.566238431041338\n",
      "l2 norm of gradients: 0.35640993344244304\n",
      "l2 norm of weights: 3.7960655285432217\n",
      "---------------------\n",
      "Iteration Number: 2521\n",
      "Loss: 26.55460081666715\n",
      "l2 norm of gradients: 0.3562261854280458\n",
      "l2 norm of weights: 3.795420897365252\n",
      "---------------------\n",
      "Iteration Number: 2522\n",
      "Loss: 26.542972389341955\n",
      "l2 norm of gradients: 0.3560425051790888\n",
      "l2 norm of weights: 3.794776770190415\n",
      "---------------------\n",
      "Iteration Number: 2523\n",
      "Loss: 26.531353142765003\n",
      "l2 norm of gradients: 0.3558588927740354\n",
      "l2 norm of weights: 3.7941331468589583\n",
      "---------------------\n",
      "Iteration Number: 2524\n",
      "Loss: 26.51974307063477\n",
      "l2 norm of gradients: 0.3556753482912874\n",
      "l2 norm of weights: 3.793490027210864\n",
      "---------------------\n",
      "Iteration Number: 2525\n",
      "Loss: 26.508142166649893\n",
      "l2 norm of gradients: 0.35549187180918407\n",
      "l2 norm of weights: 3.7928474110858525\n",
      "---------------------\n",
      "Iteration Number: 2526\n",
      "Loss: 26.49655042450822\n",
      "l2 norm of gradients: 0.3553084634060001\n",
      "l2 norm of weights: 3.792205298323379\n",
      "---------------------\n",
      "Iteration Number: 2527\n",
      "Loss: 26.484967837906954\n",
      "l2 norm of gradients: 0.3551251231599455\n",
      "l2 norm of weights: 3.7915636887626385\n",
      "---------------------\n",
      "Iteration Number: 2528\n",
      "Loss: 26.473394400543114\n",
      "l2 norm of gradients: 0.3549418511491633\n",
      "l2 norm of weights: 3.7909225822425623\n",
      "---------------------\n",
      "Iteration Number: 2529\n",
      "Loss: 26.461830106113066\n",
      "l2 norm of gradients: 0.3547586474517291\n",
      "l2 norm of weights: 3.7902819786018225\n",
      "---------------------\n",
      "Iteration Number: 2530\n",
      "Loss: 26.45027494831305\n",
      "l2 norm of gradients: 0.3545755121456491\n",
      "l2 norm of weights: 3.789641877678829\n",
      "---------------------\n",
      "Iteration Number: 2531\n",
      "Loss: 26.438728920838997\n",
      "l2 norm of gradients: 0.3543924453088596\n",
      "l2 norm of weights: 3.789002279311734\n",
      "---------------------\n",
      "Iteration Number: 2532\n",
      "Loss: 26.427192017385842\n",
      "l2 norm of gradients: 0.3542094470192255\n",
      "l2 norm of weights: 3.7883631833384293\n",
      "---------------------\n",
      "Iteration Number: 2533\n",
      "Loss: 26.41566423164913\n",
      "l2 norm of gradients: 0.35402651735453877\n",
      "l2 norm of weights: 3.7877245895965483\n",
      "---------------------\n",
      "Iteration Number: 2534\n",
      "Loss: 26.404145557323158\n",
      "l2 norm of gradients: 0.3538436563925177\n",
      "l2 norm of weights: 3.787086497923467\n",
      "---------------------\n",
      "Iteration Number: 2535\n",
      "Loss: 26.392635988102562\n",
      "l2 norm of gradients: 0.35366086421080534\n",
      "l2 norm of weights: 3.786448908156304\n",
      "---------------------\n",
      "Iteration Number: 2536\n",
      "Loss: 26.3811355176813\n",
      "l2 norm of gradients: 0.3534781408869686\n",
      "l2 norm of weights: 3.7858118201319213\n",
      "---------------------\n",
      "Iteration Number: 2537\n",
      "Loss: 26.369644139753337\n",
      "l2 norm of gradients: 0.35329548649849696\n",
      "l2 norm of weights: 3.7851752336869255\n",
      "---------------------\n",
      "Iteration Number: 2538\n",
      "Loss: 26.35816184801211\n",
      "l2 norm of gradients: 0.35311290112280097\n",
      "l2 norm of weights: 3.784539148657667\n",
      "---------------------\n",
      "Iteration Number: 2539\n",
      "Loss: 26.346688636150976\n",
      "l2 norm of gradients: 0.35293038483721156\n",
      "l2 norm of weights: 3.783903564880243\n",
      "---------------------\n",
      "Iteration Number: 2540\n",
      "Loss: 26.335224497863265\n",
      "l2 norm of gradients: 0.3527479377189782\n",
      "l2 norm of weights: 3.783268482190495\n",
      "---------------------\n",
      "Iteration Number: 2541\n",
      "Loss: 26.323769426841544\n",
      "l2 norm of gradients: 0.3525655598452687\n",
      "l2 norm of weights: 3.782633900424013\n",
      "---------------------\n",
      "Iteration Number: 2542\n",
      "Loss: 26.312323416778867\n",
      "l2 norm of gradients: 0.352383251293167\n",
      "l2 norm of weights: 3.7819998194161313\n",
      "---------------------\n",
      "Iteration Number: 2543\n",
      "Loss: 26.300886461367554\n",
      "l2 norm of gradients: 0.3522010121396726\n",
      "l2 norm of weights: 3.7813662390019354\n",
      "---------------------\n",
      "Iteration Number: 2544\n",
      "Loss: 26.289458554300072\n",
      "l2 norm of gradients: 0.3520188424616991\n",
      "l2 norm of weights: 3.780733159016257\n",
      "---------------------\n",
      "Iteration Number: 2545\n",
      "Loss: 26.27803968926879\n",
      "l2 norm of gradients: 0.35183674233607354\n",
      "l2 norm of weights: 3.7801005792936784\n",
      "---------------------\n",
      "Iteration Number: 2546\n",
      "Loss: 26.26662985996574\n",
      "l2 norm of gradients: 0.3516547118395344\n",
      "l2 norm of weights: 3.77946849966853\n",
      "---------------------\n",
      "Iteration Number: 2547\n",
      "Loss: 26.255229060083177\n",
      "l2 norm of gradients: 0.35147275104873127\n",
      "l2 norm of weights: 3.7788369199748937\n",
      "---------------------\n",
      "Iteration Number: 2548\n",
      "Loss: 26.24383728331282\n",
      "l2 norm of gradients: 0.35129086004022303\n",
      "l2 norm of weights: 3.778205840046602\n",
      "---------------------\n",
      "Iteration Number: 2549\n",
      "Loss: 26.232454523346828\n",
      "l2 norm of gradients: 0.35110903889047745\n",
      "l2 norm of weights: 3.777575259717239\n",
      "---------------------\n",
      "Iteration Number: 2550\n",
      "Loss: 26.221080773877006\n",
      "l2 norm of gradients: 0.35092728767586934\n",
      "l2 norm of weights: 3.77694517882014\n",
      "---------------------\n",
      "Iteration Number: 2551\n",
      "Loss: 26.209716028595235\n",
      "l2 norm of gradients: 0.3507456064726796\n",
      "l2 norm of weights: 3.776315597188395\n",
      "---------------------\n",
      "Iteration Number: 2552\n",
      "Loss: 26.198360281193228\n",
      "l2 norm of gradients: 0.3505639953570946\n",
      "l2 norm of weights: 3.775686514654845\n",
      "---------------------\n",
      "Iteration Number: 2553\n",
      "Loss: 26.1870135253629\n",
      "l2 norm of gradients: 0.3503824544052042\n",
      "l2 norm of weights: 3.775057931052086\n",
      "---------------------\n",
      "Iteration Number: 2554\n",
      "Loss: 26.17567575479632\n",
      "l2 norm of gradients: 0.35020098369300146\n",
      "l2 norm of weights: 3.774429846212469\n",
      "---------------------\n",
      "Iteration Number: 2555\n",
      "Loss: 26.164346963185057\n",
      "l2 norm of gradients: 0.35001958329638094\n",
      "l2 norm of weights: 3.7738022599680994\n",
      "---------------------\n",
      "Iteration Number: 2556\n",
      "Loss: 26.153027144221394\n",
      "l2 norm of gradients: 0.34983825329113777\n",
      "l2 norm of weights: 3.7731751721508386\n",
      "---------------------\n",
      "Iteration Number: 2557\n",
      "Loss: 26.141716291597284\n",
      "l2 norm of gradients: 0.34965699375296666\n",
      "l2 norm of weights: 3.7725485825923033\n",
      "---------------------\n",
      "Iteration Number: 2558\n",
      "Loss: 26.130414399004753\n",
      "l2 norm of gradients: 0.34947580475746076\n",
      "l2 norm of weights: 3.7719224911238682\n",
      "---------------------\n",
      "Iteration Number: 2559\n",
      "Loss: 26.11912146013638\n",
      "l2 norm of gradients: 0.34929468638011035\n",
      "l2 norm of weights: 3.771296897576666\n",
      "---------------------\n",
      "Iteration Number: 2560\n",
      "Loss: 26.107837468684295\n",
      "l2 norm of gradients: 0.34911363869630213\n",
      "l2 norm of weights: 3.7706718017815852\n",
      "---------------------\n",
      "Iteration Number: 2561\n",
      "Loss: 26.09656241834101\n",
      "l2 norm of gradients: 0.3489326617813178\n",
      "l2 norm of weights: 3.7700472035692743\n",
      "---------------------\n",
      "Iteration Number: 2562\n",
      "Loss: 26.085296302799524\n",
      "l2 norm of gradients: 0.348751755710333\n",
      "l2 norm of weights: 3.769423102770141\n",
      "---------------------\n",
      "Iteration Number: 2563\n",
      "Loss: 26.074039115752413\n",
      "l2 norm of gradients: 0.3485709205584167\n",
      "l2 norm of weights: 3.7687994992143525\n",
      "---------------------\n",
      "Iteration Number: 2564\n",
      "Loss: 26.062790850893023\n",
      "l2 norm of gradients: 0.34839015640052945\n",
      "l2 norm of weights: 3.7681763927318355\n",
      "---------------------\n",
      "Iteration Number: 2565\n",
      "Loss: 26.05155150191458\n",
      "l2 norm of gradients: 0.34820946331152275\n",
      "l2 norm of weights: 3.767553783152279\n",
      "---------------------\n",
      "Iteration Number: 2566\n",
      "Loss: 26.040321062510554\n",
      "l2 norm of gradients: 0.34802884136613793\n",
      "l2 norm of weights: 3.7669316703051328\n",
      "---------------------\n",
      "Iteration Number: 2567\n",
      "Loss: 26.029099526374733\n",
      "l2 norm of gradients: 0.34784829063900496\n",
      "l2 norm of weights: 3.7663100540196073\n",
      "---------------------\n",
      "Iteration Number: 2568\n",
      "Loss: 26.01788688720128\n",
      "l2 norm of gradients: 0.34766781120464185\n",
      "l2 norm of weights: 3.7656889341246775\n",
      "---------------------\n",
      "Iteration Number: 2569\n",
      "Loss: 26.006683138684505\n",
      "l2 norm of gradients: 0.3474874031374525\n",
      "l2 norm of weights: 3.76506831044908\n",
      "---------------------\n",
      "Iteration Number: 2570\n",
      "Loss: 25.99548827451908\n",
      "l2 norm of gradients: 0.34730706651172727\n",
      "l2 norm of weights: 3.7644481828213165\n",
      "---------------------\n",
      "Iteration Number: 2571\n",
      "Loss: 25.984302288399764\n",
      "l2 norm of gradients: 0.3471268014016406\n",
      "l2 norm of weights: 3.763828551069651\n",
      "---------------------\n",
      "Iteration Number: 2572\n",
      "Loss: 25.9731251740222\n",
      "l2 norm of gradients: 0.34694660788125065\n",
      "l2 norm of weights: 3.763209415022113\n",
      "---------------------\n",
      "Iteration Number: 2573\n",
      "Loss: 25.96195692508199\n",
      "l2 norm of gradients: 0.3467664860244982\n",
      "l2 norm of weights: 3.7625907745064975\n",
      "---------------------\n",
      "Iteration Number: 2574\n",
      "Loss: 25.950797535274802\n",
      "l2 norm of gradients: 0.3465864359052053\n",
      "l2 norm of weights: 3.7619726293503657\n",
      "---------------------\n",
      "Iteration Number: 2575\n",
      "Loss: 25.939646998297462\n",
      "l2 norm of gradients: 0.3464064575970747\n",
      "l2 norm of weights: 3.7613549793810437\n",
      "---------------------\n",
      "Iteration Number: 2576\n",
      "Loss: 25.928505307846518\n",
      "l2 norm of gradients: 0.3462265511736888\n",
      "l2 norm of weights: 3.7607378244256258\n",
      "---------------------\n",
      "Iteration Number: 2577\n",
      "Loss: 25.917372457619493\n",
      "l2 norm of gradients: 0.3460467167085081\n",
      "l2 norm of weights: 3.7601211643109727\n",
      "---------------------\n",
      "Iteration Number: 2578\n",
      "Loss: 25.906248441313743\n",
      "l2 norm of gradients: 0.34586695427487124\n",
      "l2 norm of weights: 3.7595049988637137\n",
      "---------------------\n",
      "Iteration Number: 2579\n",
      "Loss: 25.895133252627712\n",
      "l2 norm of gradients: 0.34568726394599275\n",
      "l2 norm of weights: 3.7588893279102464\n",
      "---------------------\n",
      "Iteration Number: 2580\n",
      "Loss: 25.8840268852597\n",
      "l2 norm of gradients: 0.3455076457949631\n",
      "l2 norm of weights: 3.7582741512767366\n",
      "---------------------\n",
      "Iteration Number: 2581\n",
      "Loss: 25.872929332909056\n",
      "l2 norm of gradients: 0.34532809989474744\n",
      "l2 norm of weights: 3.757659468789121\n",
      "---------------------\n",
      "Iteration Number: 2582\n",
      "Loss: 25.86184058927525\n",
      "l2 norm of gradients: 0.34514862631818416\n",
      "l2 norm of weights: 3.757045280273105\n",
      "---------------------\n",
      "Iteration Number: 2583\n",
      "Loss: 25.8507606480583\n",
      "l2 norm of gradients: 0.3449692251379845\n",
      "l2 norm of weights: 3.756431585554165\n",
      "---------------------\n",
      "Iteration Number: 2584\n",
      "Loss: 25.83968950295912\n",
      "l2 norm of gradients: 0.34478989642673163\n",
      "l2 norm of weights: 3.7558183844575486\n",
      "---------------------\n",
      "Iteration Number: 2585\n",
      "Loss: 25.828627147678848\n",
      "l2 norm of gradients: 0.344610640256879\n",
      "l2 norm of weights: 3.755205676808274\n",
      "---------------------\n",
      "Iteration Number: 2586\n",
      "Loss: 25.817573575919102\n",
      "l2 norm of gradients: 0.34443145670075026\n",
      "l2 norm of weights: 3.7545934624311332\n",
      "---------------------\n",
      "Iteration Number: 2587\n",
      "Loss: 25.80652878138236\n",
      "l2 norm of gradients: 0.3442523458305377\n",
      "l2 norm of weights: 3.753981741150689\n",
      "---------------------\n",
      "Iteration Number: 2588\n",
      "Loss: 25.795492757771576\n",
      "l2 norm of gradients: 0.34407330771830164\n",
      "l2 norm of weights: 3.7533705127912773\n",
      "---------------------\n",
      "Iteration Number: 2589\n",
      "Loss: 25.784465498790393\n",
      "l2 norm of gradients: 0.3438943424359693\n",
      "l2 norm of weights: 3.752759777177009\n",
      "---------------------\n",
      "Iteration Number: 2590\n",
      "Loss: 25.77344699814307\n",
      "l2 norm of gradients: 0.3437154500553341\n",
      "l2 norm of weights: 3.7521495341317674\n",
      "---------------------\n",
      "Iteration Number: 2591\n",
      "Loss: 25.76243724953432\n",
      "l2 norm of gradients: 0.3435366306480545\n",
      "l2 norm of weights: 3.7515397834792115\n",
      "---------------------\n",
      "Iteration Number: 2592\n",
      "Loss: 25.75143624666996\n",
      "l2 norm of gradients: 0.34335788428565317\n",
      "l2 norm of weights: 3.7509305250427745\n",
      "---------------------\n",
      "Iteration Number: 2593\n",
      "Loss: 25.74044398325602\n",
      "l2 norm of gradients: 0.34317921103951626\n",
      "l2 norm of weights: 3.7503217586456654\n",
      "---------------------\n",
      "Iteration Number: 2594\n",
      "Loss: 25.729460452999508\n",
      "l2 norm of gradients: 0.3430006109808923\n",
      "l2 norm of weights: 3.749713484110869\n",
      "---------------------\n",
      "Iteration Number: 2595\n",
      "Loss: 25.718485649607942\n",
      "l2 norm of gradients: 0.3428220841808913\n",
      "l2 norm of weights: 3.749105701261146\n",
      "---------------------\n",
      "Iteration Number: 2596\n",
      "Loss: 25.70751956679016\n",
      "l2 norm of gradients: 0.3426436307104839\n",
      "l2 norm of weights: 3.748498409919036\n",
      "---------------------\n",
      "Iteration Number: 2597\n",
      "Loss: 25.696562198254778\n",
      "l2 norm of gradients: 0.3424652506405008\n",
      "l2 norm of weights: 3.747891609906853\n",
      "---------------------\n",
      "Iteration Number: 2598\n",
      "Loss: 25.68561353771215\n",
      "l2 norm of gradients: 0.3422869440416312\n",
      "l2 norm of weights: 3.7472853010466918\n",
      "---------------------\n",
      "Iteration Number: 2599\n",
      "Loss: 25.674673578872607\n",
      "l2 norm of gradients: 0.3421087109844225\n",
      "l2 norm of weights: 3.746679483160423\n",
      "---------------------\n",
      "Iteration Number: 2600\n",
      "Loss: 25.66374231544795\n",
      "l2 norm of gradients: 0.34193055153927937\n",
      "l2 norm of weights: 3.746074156069698\n",
      "---------------------\n",
      "Iteration Number: 2601\n",
      "Loss: 25.652819741150363\n",
      "l2 norm of gradients: 0.3417524657764627\n",
      "l2 norm of weights: 3.745469319595947\n",
      "---------------------\n",
      "Iteration Number: 2602\n",
      "Loss: 25.64190584969295\n",
      "l2 norm of gradients: 0.3415744537660887\n",
      "l2 norm of weights: 3.7448649735603774\n",
      "---------------------\n",
      "Iteration Number: 2603\n",
      "Loss: 25.631000634789654\n",
      "l2 norm of gradients: 0.3413965155781283\n",
      "l2 norm of weights: 3.7442611177839806\n",
      "---------------------\n",
      "Iteration Number: 2604\n",
      "Loss: 25.620104090155454\n",
      "l2 norm of gradients: 0.34121865128240625\n",
      "l2 norm of weights: 3.7436577520875267\n",
      "---------------------\n",
      "Iteration Number: 2605\n",
      "Loss: 25.6092162095061\n",
      "l2 norm of gradients: 0.34104086094860003\n",
      "l2 norm of weights: 3.7430548762915663\n",
      "---------------------\n",
      "Iteration Number: 2606\n",
      "Loss: 25.598336986558095\n",
      "l2 norm of gradients: 0.3408631446462396\n",
      "l2 norm of weights: 3.7424524902164324\n",
      "---------------------\n",
      "Iteration Number: 2607\n",
      "Loss: 25.587466415029013\n",
      "l2 norm of gradients: 0.34068550244470575\n",
      "l2 norm of weights: 3.7418505936822397\n",
      "---------------------\n",
      "Iteration Number: 2608\n",
      "Loss: 25.576604488637397\n",
      "l2 norm of gradients: 0.34050793441323\n",
      "l2 norm of weights: 3.7412491865088855\n",
      "---------------------\n",
      "Iteration Number: 2609\n",
      "Loss: 25.565751201102564\n",
      "l2 norm of gradients: 0.3403304406208934\n",
      "l2 norm of weights: 3.7406482685160496\n",
      "---------------------\n",
      "Iteration Number: 2610\n",
      "Loss: 25.554906546144974\n",
      "l2 norm of gradients: 0.34015302113662593\n",
      "l2 norm of weights: 3.740047839523195\n",
      "---------------------\n",
      "Iteration Number: 2611\n",
      "Loss: 25.54407051748567\n",
      "l2 norm of gradients: 0.33997567602920553\n",
      "l2 norm of weights: 3.739447899349569\n",
      "---------------------\n",
      "Iteration Number: 2612\n",
      "Loss: 25.53324310884738\n",
      "l2 norm of gradients: 0.3397984053672574\n",
      "l2 norm of weights: 3.738848447814202\n",
      "---------------------\n",
      "Iteration Number: 2613\n",
      "Loss: 25.522424313953135\n",
      "l2 norm of gradients: 0.3396212092192534\n",
      "l2 norm of weights: 3.7382494847359093\n",
      "---------------------\n",
      "Iteration Number: 2614\n",
      "Loss: 25.51161412652728\n",
      "l2 norm of gradients: 0.33944408765351075\n",
      "l2 norm of weights: 3.7376510099332925\n",
      "---------------------\n",
      "Iteration Number: 2615\n",
      "Loss: 25.500812540295403\n",
      "l2 norm of gradients: 0.3392670407381918\n",
      "l2 norm of weights: 3.7370530232247368\n",
      "---------------------\n",
      "Iteration Number: 2616\n",
      "Loss: 25.490019548983728\n",
      "l2 norm of gradients: 0.3390900685413029\n",
      "l2 norm of weights: 3.7364555244284134\n",
      "---------------------\n",
      "Iteration Number: 2617\n",
      "Loss: 25.479235146319763\n",
      "l2 norm of gradients: 0.338913171130694\n",
      "l2 norm of weights: 3.735858513362281\n",
      "---------------------\n",
      "Iteration Number: 2618\n",
      "Loss: 25.468459326032168\n",
      "l2 norm of gradients: 0.33873634857405743\n",
      "l2 norm of weights: 3.735261989844084\n",
      "---------------------\n",
      "Iteration Number: 2619\n",
      "Loss: 25.45769208185057\n",
      "l2 norm of gradients: 0.3385596009389275\n",
      "l2 norm of weights: 3.734665953691353\n",
      "---------------------\n",
      "Iteration Number: 2620\n",
      "Loss: 25.446933407505696\n",
      "l2 norm of gradients: 0.3383829282926796\n",
      "l2 norm of weights: 3.734070404721408\n",
      "---------------------\n",
      "Iteration Number: 2621\n",
      "Loss: 25.43618329672935\n",
      "l2 norm of gradients: 0.3382063307025296\n",
      "l2 norm of weights: 3.733475342751355\n",
      "---------------------\n",
      "Iteration Number: 2622\n",
      "Loss: 25.425441743254808\n",
      "l2 norm of gradients: 0.3380298082355329\n",
      "l2 norm of weights: 3.7328807675980897\n",
      "---------------------\n",
      "Iteration Number: 2623\n",
      "Loss: 25.414708740816128\n",
      "l2 norm of gradients: 0.337853360958584\n",
      "l2 norm of weights: 3.7322866790782956\n",
      "---------------------\n",
      "Iteration Number: 2624\n",
      "Loss: 25.40398428314856\n",
      "l2 norm of gradients: 0.33767698893841536\n",
      "l2 norm of weights: 3.731693077008445\n",
      "---------------------\n",
      "Iteration Number: 2625\n",
      "Loss: 25.393268363988934\n",
      "l2 norm of gradients: 0.3375006922415971\n",
      "l2 norm of weights: 3.7310999612048\n",
      "---------------------\n",
      "Iteration Number: 2626\n",
      "Loss: 25.38256097707463\n",
      "l2 norm of gradients: 0.33732447093453627\n",
      "l2 norm of weights: 3.7305073314834134\n",
      "---------------------\n",
      "Iteration Number: 2627\n",
      "Loss: 25.371862116144953\n",
      "l2 norm of gradients: 0.33714832508347553\n",
      "l2 norm of weights: 3.729915187660126\n",
      "---------------------\n",
      "Iteration Number: 2628\n",
      "Loss: 25.361171774939823\n",
      "l2 norm of gradients: 0.3369722547544935\n",
      "l2 norm of weights: 3.729323529550572\n",
      "---------------------\n",
      "Iteration Number: 2629\n",
      "Loss: 25.35048994720082\n",
      "l2 norm of gradients: 0.3367962600135031\n",
      "l2 norm of weights: 3.7287323569701734\n",
      "---------------------\n",
      "Iteration Number: 2630\n",
      "Loss: 25.33981662667057\n",
      "l2 norm of gradients: 0.33662034092625137\n",
      "l2 norm of weights: 3.7281416697341463\n",
      "---------------------\n",
      "Iteration Number: 2631\n",
      "Loss: 25.329151807093055\n",
      "l2 norm of gradients: 0.3364444975583188\n",
      "l2 norm of weights: 3.7275514676574963\n",
      "---------------------\n",
      "Iteration Number: 2632\n",
      "Loss: 25.318495482213528\n",
      "l2 norm of gradients: 0.33626872997511836\n",
      "l2 norm of weights: 3.7269617505550228\n",
      "---------------------\n",
      "Iteration Number: 2633\n",
      "Loss: 25.30784764577838\n",
      "l2 norm of gradients: 0.3360930382418951\n",
      "l2 norm of weights: 3.726372518241317\n",
      "---------------------\n",
      "Iteration Number: 2634\n",
      "Loss: 25.297208291535892\n",
      "l2 norm of gradients: 0.33591742242372563\n",
      "l2 norm of weights: 3.725783770530763\n",
      "---------------------\n",
      "Iteration Number: 2635\n",
      "Loss: 25.286577413234696\n",
      "l2 norm of gradients: 0.3357418825855168\n",
      "l2 norm of weights: 3.725195507237537\n",
      "---------------------\n",
      "Iteration Number: 2636\n",
      "Loss: 25.27595500462582\n",
      "l2 norm of gradients: 0.33556641879200577\n",
      "l2 norm of weights: 3.72460772817561\n",
      "---------------------\n",
      "Iteration Number: 2637\n",
      "Loss: 25.26534105946088\n",
      "l2 norm of gradients: 0.33539103110775914\n",
      "l2 norm of weights: 3.724020433158747\n",
      "---------------------\n",
      "Iteration Number: 2638\n",
      "Loss: 25.254735571493313\n",
      "l2 norm of gradients: 0.3352157195971721\n",
      "l2 norm of weights: 3.723433622000506\n",
      "---------------------\n",
      "Iteration Number: 2639\n",
      "Loss: 25.244138534477564\n",
      "l2 norm of gradients: 0.3350404843244681\n",
      "l2 norm of weights: 3.7228472945142403\n",
      "---------------------\n",
      "Iteration Number: 2640\n",
      "Loss: 25.233549942169923\n",
      "l2 norm of gradients: 0.33486532535369806\n",
      "l2 norm of weights: 3.7222614505130984\n",
      "---------------------\n",
      "Iteration Number: 2641\n",
      "Loss: 25.22296978832773\n",
      "l2 norm of gradients: 0.3346902427487399\n",
      "l2 norm of weights: 3.721676089810024\n",
      "---------------------\n",
      "Iteration Number: 2642\n",
      "Loss: 25.212398066709863\n",
      "l2 norm of gradients: 0.3345152365732976\n",
      "l2 norm of weights: 3.7210912122177553\n",
      "---------------------\n",
      "Iteration Number: 2643\n",
      "Loss: 25.2018347710768\n",
      "l2 norm of gradients: 0.33434030689090094\n",
      "l2 norm of weights: 3.7205068175488276\n",
      "---------------------\n",
      "Iteration Number: 2644\n",
      "Loss: 25.191279895190185\n",
      "l2 norm of gradients: 0.33416545376490486\n",
      "l2 norm of weights: 3.7199229056155723\n",
      "---------------------\n",
      "Iteration Number: 2645\n",
      "Loss: 25.180733432813437\n",
      "l2 norm of gradients: 0.3339906772584886\n",
      "l2 norm of weights: 3.7193394762301173\n",
      "---------------------\n",
      "Iteration Number: 2646\n",
      "Loss: 25.17019537771123\n",
      "l2 norm of gradients: 0.33381597743465546\n",
      "l2 norm of weights: 3.718756529204387\n",
      "---------------------\n",
      "Iteration Number: 2647\n",
      "Loss: 25.159665723649827\n",
      "l2 norm of gradients: 0.33364135435623204\n",
      "l2 norm of weights: 3.718174064350104\n",
      "---------------------\n",
      "Iteration Number: 2648\n",
      "Loss: 25.14914446439699\n",
      "l2 norm of gradients: 0.33346680808586754\n",
      "l2 norm of weights: 3.7175920814787875\n",
      "---------------------\n",
      "Iteration Number: 2649\n",
      "Loss: 25.13863159372222\n",
      "l2 norm of gradients: 0.3332923386860335\n",
      "l2 norm of weights: 3.7170105804017552\n",
      "---------------------\n",
      "Iteration Number: 2650\n",
      "Loss: 25.12812710539594\n",
      "l2 norm of gradients: 0.3331179462190229\n",
      "l2 norm of weights: 3.7164295609301226\n",
      "---------------------\n",
      "Iteration Number: 2651\n",
      "Loss: 25.117630993190893\n",
      "l2 norm of gradients: 0.33294363074695005\n",
      "l2 norm of weights: 3.7158490228748042\n",
      "---------------------\n",
      "Iteration Number: 2652\n",
      "Loss: 25.10714325088081\n",
      "l2 norm of gradients: 0.3327693923317495\n",
      "l2 norm of weights: 3.7152689660465135\n",
      "---------------------\n",
      "Iteration Number: 2653\n",
      "Loss: 25.096663872241322\n",
      "l2 norm of gradients: 0.3325952310351758\n",
      "l2 norm of weights: 3.7146893902557623\n",
      "---------------------\n",
      "Iteration Number: 2654\n",
      "Loss: 25.086192851049606\n",
      "l2 norm of gradients: 0.33242114691880303\n",
      "l2 norm of weights: 3.714110295312863\n",
      "---------------------\n",
      "Iteration Number: 2655\n",
      "Loss: 25.07573018108422\n",
      "l2 norm of gradients: 0.3322471400440241\n",
      "l2 norm of weights: 3.713531681027926\n",
      "---------------------\n",
      "Iteration Number: 2656\n",
      "Loss: 25.065275856125705\n",
      "l2 norm of gradients: 0.33207321047205035\n",
      "l2 norm of weights: 3.7129535472108643\n",
      "---------------------\n",
      "Iteration Number: 2657\n",
      "Loss: 25.054829869955867\n",
      "l2 norm of gradients: 0.3318993582639108\n",
      "l2 norm of weights: 3.7123758936713895\n",
      "---------------------\n",
      "Iteration Number: 2658\n",
      "Loss: 25.044392216358467\n",
      "l2 norm of gradients: 0.331725583480452\n",
      "l2 norm of weights: 3.7117987202190137\n",
      "---------------------\n",
      "Iteration Number: 2659\n",
      "Loss: 25.033962889118616\n",
      "l2 norm of gradients: 0.33155188618233716\n",
      "l2 norm of weights: 3.711222026663051\n",
      "---------------------\n",
      "Iteration Number: 2660\n",
      "Loss: 25.02354188202352\n",
      "l2 norm of gradients: 0.33137826643004575\n",
      "l2 norm of weights: 3.7106458128126163\n",
      "---------------------\n",
      "Iteration Number: 2661\n",
      "Loss: 25.013129188861605\n",
      "l2 norm of gradients: 0.3312047242838733\n",
      "l2 norm of weights: 3.7100700784766256\n",
      "---------------------\n",
      "Iteration Number: 2662\n",
      "Loss: 25.002724803423387\n",
      "l2 norm of gradients: 0.33103125980393056\n",
      "l2 norm of weights: 3.7094948234637983\n",
      "---------------------\n",
      "Iteration Number: 2663\n",
      "Loss: 24.99232871950075\n",
      "l2 norm of gradients: 0.330857873050143\n",
      "l2 norm of weights: 3.7089200475826534\n",
      "---------------------\n",
      "Iteration Number: 2664\n",
      "Loss: 24.981940930887397\n",
      "l2 norm of gradients: 0.3306845640822506\n",
      "l2 norm of weights: 3.708345750641514\n",
      "---------------------\n",
      "Iteration Number: 2665\n",
      "Loss: 24.971561431379126\n",
      "l2 norm of gradients: 0.3305113329598072\n",
      "l2 norm of weights: 3.707771932448506\n",
      "---------------------\n",
      "Iteration Number: 2666\n",
      "Loss: 24.961190214772984\n",
      "l2 norm of gradients: 0.33033817974218005\n",
      "l2 norm of weights: 3.707198592811557\n",
      "---------------------\n",
      "Iteration Number: 2667\n",
      "Loss: 24.950827274867954\n",
      "l2 norm of gradients: 0.33016510448854924\n",
      "l2 norm of weights: 3.7066257315383995\n",
      "---------------------\n",
      "Iteration Number: 2668\n",
      "Loss: 24.94047260546482\n",
      "l2 norm of gradients: 0.32999210725790756\n",
      "l2 norm of weights: 3.706053348436568\n",
      "---------------------\n",
      "Iteration Number: 2669\n",
      "Loss: 24.930126200366246\n",
      "l2 norm of gradients: 0.32981918810905986\n",
      "l2 norm of weights: 3.7054814433134013\n",
      "---------------------\n",
      "Iteration Number: 2670\n",
      "Loss: 24.919788053376593\n",
      "l2 norm of gradients: 0.32964634710062235\n",
      "l2 norm of weights: 3.704910015976042\n",
      "---------------------\n",
      "Iteration Number: 2671\n",
      "Loss: 24.909458158302012\n",
      "l2 norm of gradients: 0.3294735842910227\n",
      "l2 norm of weights: 3.7043390662314377\n",
      "---------------------\n",
      "Iteration Number: 2672\n",
      "Loss: 24.899136508950487\n",
      "l2 norm of gradients: 0.32930089973849924\n",
      "l2 norm of weights: 3.70376859388634\n",
      "---------------------\n",
      "Iteration Number: 2673\n",
      "Loss: 24.88882309913176\n",
      "l2 norm of gradients: 0.32912829350110046\n",
      "l2 norm of weights: 3.7031985987473037\n",
      "---------------------\n",
      "Iteration Number: 2674\n",
      "Loss: 24.87851792265769\n",
      "l2 norm of gradients: 0.328955765636685\n",
      "l2 norm of weights: 3.702629080620692\n",
      "---------------------\n",
      "Iteration Number: 2675\n",
      "Loss: 24.868220973341735\n",
      "l2 norm of gradients: 0.3287833162029208\n",
      "l2 norm of weights: 3.7020600393126712\n",
      "---------------------\n",
      "Iteration Number: 2676\n",
      "Loss: 24.85793224499941\n",
      "l2 norm of gradients: 0.32861094525728485\n",
      "l2 norm of weights: 3.7014914746292127\n",
      "---------------------\n",
      "Iteration Number: 2677\n",
      "Loss: 24.847651731447716\n",
      "l2 norm of gradients: 0.32843865285706303\n",
      "l2 norm of weights: 3.700923386376095\n",
      "---------------------\n",
      "Iteration Number: 2678\n",
      "Loss: 24.837379426506146\n",
      "l2 norm of gradients: 0.3282664390593492\n",
      "l2 norm of weights: 3.700355774358903\n",
      "---------------------\n",
      "Iteration Number: 2679\n",
      "Loss: 24.82711532399563\n",
      "l2 norm of gradients: 0.32809430392104527\n",
      "l2 norm of weights: 3.6997886383830254\n",
      "---------------------\n",
      "Iteration Number: 2680\n",
      "Loss: 24.816859417739238\n",
      "l2 norm of gradients: 0.32792224749886073\n",
      "l2 norm of weights: 3.69922197825366\n",
      "---------------------\n",
      "Iteration Number: 2681\n",
      "Loss: 24.80661170156192\n",
      "l2 norm of gradients: 0.32775026984931205\n",
      "l2 norm of weights: 3.69865579377581\n",
      "---------------------\n",
      "Iteration Number: 2682\n",
      "Loss: 24.796372169290425\n",
      "l2 norm of gradients: 0.32757837102872234\n",
      "l2 norm of weights: 3.698090084754286\n",
      "---------------------\n",
      "Iteration Number: 2683\n",
      "Loss: 24.786140814753864\n",
      "l2 norm of gradients: 0.32740655109322125\n",
      "l2 norm of weights: 3.6975248509937053\n",
      "---------------------\n",
      "Iteration Number: 2684\n",
      "Loss: 24.775917631782825\n",
      "l2 norm of gradients: 0.3272348100987445\n",
      "l2 norm of weights: 3.696960092298494\n",
      "---------------------\n",
      "Iteration Number: 2685\n",
      "Loss: 24.7657026142101\n",
      "l2 norm of gradients: 0.32706314810103315\n",
      "l2 norm of weights: 3.696395808472883\n",
      "---------------------\n",
      "Iteration Number: 2686\n",
      "Loss: 24.755495755870495\n",
      "l2 norm of gradients: 0.32689156515563395\n",
      "l2 norm of weights: 3.6958319993209154\n",
      "---------------------\n",
      "Iteration Number: 2687\n",
      "Loss: 24.745297050600918\n",
      "l2 norm of gradients: 0.32672006131789827\n",
      "l2 norm of weights: 3.695268664646438\n",
      "---------------------\n",
      "Iteration Number: 2688\n",
      "Loss: 24.735106492239925\n",
      "l2 norm of gradients: 0.3265486366429824\n",
      "l2 norm of weights: 3.694705804253109\n",
      "---------------------\n",
      "Iteration Number: 2689\n",
      "Loss: 24.724924074628444\n",
      "l2 norm of gradients: 0.3263772911858465\n",
      "l2 norm of weights: 3.694143417944393\n",
      "---------------------\n",
      "Iteration Number: 2690\n",
      "Loss: 24.714749791609282\n",
      "l2 norm of gradients: 0.32620602500125523\n",
      "l2 norm of weights: 3.693581505523565\n",
      "---------------------\n",
      "Iteration Number: 2691\n",
      "Loss: 24.7045836370273\n",
      "l2 norm of gradients: 0.3260348381437762\n",
      "l2 norm of weights: 3.693020066793708\n",
      "---------------------\n",
      "Iteration Number: 2692\n",
      "Loss: 24.694425604729418\n",
      "l2 norm of gradients: 0.32586373066778096\n",
      "l2 norm of weights: 3.692459101557715\n",
      "---------------------\n",
      "Iteration Number: 2693\n",
      "Loss: 24.684275688564664\n",
      "l2 norm of gradients: 0.32569270262744354\n",
      "l2 norm of weights: 3.691898609618287\n",
      "---------------------\n",
      "Iteration Number: 2694\n",
      "Loss: 24.674133882384094\n",
      "l2 norm of gradients: 0.32552175407674105\n",
      "l2 norm of weights: 3.691338590777936\n",
      "---------------------\n",
      "Iteration Number: 2695\n",
      "Loss: 24.664000180040844\n",
      "l2 norm of gradients: 0.32535088506945287\n",
      "l2 norm of weights: 3.6907790448389832\n",
      "---------------------\n",
      "Iteration Number: 2696\n",
      "Loss: 24.653874575390326\n",
      "l2 norm of gradients: 0.3251800956591602\n",
      "l2 norm of weights: 3.6902199716035593\n",
      "---------------------\n",
      "Iteration Number: 2697\n",
      "Loss: 24.64375706228983\n",
      "l2 norm of gradients: 0.32500938589924644\n",
      "l2 norm of weights: 3.6896613708736057\n",
      "---------------------\n",
      "Iteration Number: 2698\n",
      "Loss: 24.63364763459879\n",
      "l2 norm of gradients: 0.3248387558428963\n",
      "l2 norm of weights: 3.689103242450875\n",
      "---------------------\n",
      "Iteration Number: 2699\n",
      "Loss: 24.623546286178815\n",
      "l2 norm of gradients: 0.32466820554309544\n",
      "l2 norm of weights: 3.6885455861369283\n",
      "---------------------\n",
      "Iteration Number: 2700\n",
      "Loss: 24.61345301089386\n",
      "l2 norm of gradients: 0.3244977350526312\n",
      "l2 norm of weights: 3.6879884017331395\n",
      "---------------------\n",
      "Iteration Number: 2701\n",
      "Loss: 24.603367802609636\n",
      "l2 norm of gradients: 0.32432734442409095\n",
      "l2 norm of weights: 3.687431689040692\n",
      "---------------------\n",
      "Iteration Number: 2702\n",
      "Loss: 24.59329065519438\n",
      "l2 norm of gradients: 0.32415703370986265\n",
      "l2 norm of weights: 3.6868754478605807\n",
      "---------------------\n",
      "Iteration Number: 2703\n",
      "Loss: 24.583221562518276\n",
      "l2 norm of gradients: 0.3239868029621346\n",
      "l2 norm of weights: 3.6863196779936125\n",
      "---------------------\n",
      "Iteration Number: 2704\n",
      "Loss: 24.57316051845373\n",
      "l2 norm of gradients: 0.3238166522328949\n",
      "l2 norm of weights: 3.6857643792404056\n",
      "---------------------\n",
      "Iteration Number: 2705\n",
      "Loss: 24.563107516875476\n",
      "l2 norm of gradients: 0.3236465815739313\n",
      "l2 norm of weights: 3.6852095514013885\n",
      "---------------------\n",
      "Iteration Number: 2706\n",
      "Loss: 24.55306255166031\n",
      "l2 norm of gradients: 0.32347659103683085\n",
      "l2 norm of weights: 3.6846551942768038\n",
      "---------------------\n",
      "Iteration Number: 2707\n",
      "Loss: 24.543025616687366\n",
      "l2 norm of gradients: 0.3233066806729801\n",
      "l2 norm of weights: 3.684101307666704\n",
      "---------------------\n",
      "Iteration Number: 2708\n",
      "Loss: 24.532996705837782\n",
      "l2 norm of gradients: 0.32313685053356417\n",
      "l2 norm of weights: 3.6835478913709556\n",
      "---------------------\n",
      "Iteration Number: 2709\n",
      "Loss: 24.52297581299511\n",
      "l2 norm of gradients: 0.32296710066956724\n",
      "l2 norm of weights: 3.6829949451892356\n",
      "---------------------\n",
      "Iteration Number: 2710\n",
      "Loss: 24.51296293204515\n",
      "l2 norm of gradients: 0.32279743113177195\n",
      "l2 norm of weights: 3.6824424689210358\n",
      "---------------------\n",
      "Iteration Number: 2711\n",
      "Loss: 24.50295805687595\n",
      "l2 norm of gradients: 0.3226278419707589\n",
      "l2 norm of weights: 3.6818904623656583\n",
      "---------------------\n",
      "Iteration Number: 2712\n",
      "Loss: 24.492961181377815\n",
      "l2 norm of gradients: 0.3224583332369072\n",
      "l2 norm of weights: 3.6813389253222204\n",
      "---------------------\n",
      "Iteration Number: 2713\n",
      "Loss: 24.482972299443215\n",
      "l2 norm of gradients: 0.32228890498039353\n",
      "l2 norm of weights: 3.6807878575896504\n",
      "---------------------\n",
      "Iteration Number: 2714\n",
      "Loss: 24.472991404966873\n",
      "l2 norm of gradients: 0.3221195572511924\n",
      "l2 norm of weights: 3.6802372589666907\n",
      "---------------------\n",
      "Iteration Number: 2715\n",
      "Loss: 24.46301849184625\n",
      "l2 norm of gradients: 0.3219502900990756\n",
      "l2 norm of weights: 3.6796871292518976\n",
      "---------------------\n",
      "Iteration Number: 2716\n",
      "Loss: 24.453053553980535\n",
      "l2 norm of gradients: 0.3217811035736125\n",
      "l2 norm of weights: 3.6791374682436397\n",
      "---------------------\n",
      "Iteration Number: 2717\n",
      "Loss: 24.44309658527157\n",
      "l2 norm of gradients: 0.3216119977241695\n",
      "l2 norm of weights: 3.678588275740101\n",
      "---------------------\n",
      "Iteration Number: 2718\n",
      "Loss: 24.433147579623274\n",
      "l2 norm of gradients: 0.3214429725999096\n",
      "l2 norm of weights: 3.6780395515392765\n",
      "---------------------\n",
      "Iteration Number: 2719\n",
      "Loss: 24.42320653094237\n",
      "l2 norm of gradients: 0.32127402824979295\n",
      "l2 norm of weights: 3.6774912954389785\n",
      "---------------------\n",
      "Iteration Number: 2720\n",
      "Loss: 24.41327343313734\n",
      "l2 norm of gradients: 0.3211051647225761\n",
      "l2 norm of weights: 3.6769435072368317\n",
      "---------------------\n",
      "Iteration Number: 2721\n",
      "Loss: 24.403348280119285\n",
      "l2 norm of gradients: 0.32093638206681196\n",
      "l2 norm of weights: 3.6763961867302744\n",
      "---------------------\n",
      "Iteration Number: 2722\n",
      "Loss: 24.393431065801863\n",
      "l2 norm of gradients: 0.32076768033084985\n",
      "l2 norm of weights: 3.6758493337165605\n",
      "---------------------\n",
      "Iteration Number: 2723\n",
      "Loss: 24.38352178410071\n",
      "l2 norm of gradients: 0.32059905956283496\n",
      "l2 norm of weights: 3.675302947992758\n",
      "---------------------\n",
      "Iteration Number: 2724\n",
      "Loss: 24.373620428934224\n",
      "l2 norm of gradients: 0.3204305198107087\n",
      "l2 norm of weights: 3.67475702935575\n",
      "---------------------\n",
      "Iteration Number: 2725\n",
      "Loss: 24.3637269942228\n",
      "l2 norm of gradients: 0.32026206112220795\n",
      "l2 norm of weights: 3.674211577602234\n",
      "---------------------\n",
      "Iteration Number: 2726\n",
      "Loss: 24.353841473889542\n",
      "l2 norm of gradients: 0.3200936835448655\n",
      "l2 norm of weights: 3.6736665925287224\n",
      "---------------------\n",
      "Iteration Number: 2727\n",
      "Loss: 24.343963861859763\n",
      "l2 norm of gradients: 0.31992538712600965\n",
      "l2 norm of weights: 3.673122073931543\n",
      "---------------------\n",
      "Iteration Number: 2728\n",
      "Loss: 24.33409415206144\n",
      "l2 norm of gradients: 0.319757171912764\n",
      "l2 norm of weights: 3.672578021606839\n",
      "---------------------\n",
      "Iteration Number: 2729\n",
      "Loss: 24.32423233842459\n",
      "l2 norm of gradients: 0.31958903795204746\n",
      "l2 norm of weights: 3.6720344353505694\n",
      "---------------------\n",
      "Iteration Number: 2730\n",
      "Loss: 24.31437841488214\n",
      "l2 norm of gradients: 0.319420985290574\n",
      "l2 norm of weights: 3.671491314958507\n",
      "---------------------\n",
      "Iteration Number: 2731\n",
      "Loss: 24.304532375369043\n",
      "l2 norm of gradients: 0.31925301397485284\n",
      "l2 norm of weights: 3.6709486602262413\n",
      "---------------------\n",
      "Iteration Number: 2732\n",
      "Loss: 24.29469421382287\n",
      "l2 norm of gradients: 0.31908512405118783\n",
      "l2 norm of weights: 3.6704064709491786\n",
      "---------------------\n",
      "Iteration Number: 2733\n",
      "Loss: 24.284863924183636\n",
      "l2 norm of gradients: 0.31891731556567793\n",
      "l2 norm of weights: 3.6698647469225394\n",
      "---------------------\n",
      "Iteration Number: 2734\n",
      "Loss: 24.275041500393836\n",
      "l2 norm of gradients: 0.31874958856421665\n",
      "l2 norm of weights: 3.669323487941361\n",
      "---------------------\n",
      "Iteration Number: 2735\n",
      "Loss: 24.265226936398577\n",
      "l2 norm of gradients: 0.31858194309249194\n",
      "l2 norm of weights: 3.668782693800497\n",
      "---------------------\n",
      "Iteration Number: 2736\n",
      "Loss: 24.25542022614509\n",
      "l2 norm of gradients: 0.31841437919598664\n",
      "l2 norm of weights: 3.668242364294617\n",
      "---------------------\n",
      "Iteration Number: 2737\n",
      "Loss: 24.245621363583425\n",
      "l2 norm of gradients: 0.3182468969199778\n",
      "l2 norm of weights: 3.667702499218207\n",
      "---------------------\n",
      "Iteration Number: 2738\n",
      "Loss: 24.235830342665974\n",
      "l2 norm of gradients: 0.3180794963095369\n",
      "l2 norm of weights: 3.66716309836557\n",
      "---------------------\n",
      "Iteration Number: 2739\n",
      "Loss: 24.22604715734786\n",
      "l2 norm of gradients: 0.31791217740952965\n",
      "l2 norm of weights: 3.6666241615308244\n",
      "---------------------\n",
      "Iteration Number: 2740\n",
      "Loss: 24.216271801586274\n",
      "l2 norm of gradients: 0.3177449402646158\n",
      "l2 norm of weights: 3.6660856885079065\n",
      "---------------------\n",
      "Iteration Number: 2741\n",
      "Loss: 24.206504269341345\n",
      "l2 norm of gradients: 0.3175777849192496\n",
      "l2 norm of weights: 3.6655476790905697\n",
      "---------------------\n",
      "Iteration Number: 2742\n",
      "Loss: 24.19674455457577\n",
      "l2 norm of gradients: 0.3174107114176792\n",
      "l2 norm of weights: 3.665010133072383\n",
      "---------------------\n",
      "Iteration Number: 2743\n",
      "Loss: 24.186992651254318\n",
      "l2 norm of gradients: 0.31724371980394644\n",
      "l2 norm of weights: 3.6644730502467335\n",
      "---------------------\n",
      "Iteration Number: 2744\n",
      "Loss: 24.177248553344786\n",
      "l2 norm of gradients: 0.31707681012188754\n",
      "l2 norm of weights: 3.6639364304068254\n",
      "---------------------\n",
      "Iteration Number: 2745\n",
      "Loss: 24.167512254817353\n",
      "l2 norm of gradients: 0.31690998241513246\n",
      "l2 norm of weights: 3.6634002733456796\n",
      "---------------------\n",
      "Iteration Number: 2746\n",
      "Loss: 24.15778374964478\n",
      "l2 norm of gradients: 0.316743236727105\n",
      "l2 norm of weights: 3.662864578856136\n",
      "---------------------\n",
      "Iteration Number: 2747\n",
      "Loss: 24.14806303180225\n",
      "l2 norm of gradients: 0.3165765731010228\n",
      "l2 norm of weights: 3.662329346730849\n",
      "---------------------\n",
      "Iteration Number: 2748\n",
      "Loss: 24.138350095267874\n",
      "l2 norm of gradients: 0.3164099915798972\n",
      "l2 norm of weights: 3.6617945767622944\n",
      "---------------------\n",
      "Iteration Number: 2749\n",
      "Loss: 24.1286449340219\n",
      "l2 norm of gradients: 0.31624349220653336\n",
      "l2 norm of weights: 3.661260268742762\n",
      "---------------------\n",
      "Iteration Number: 2750\n",
      "Loss: 24.1189475420476\n",
      "l2 norm of gradients: 0.31607707502353\n",
      "l2 norm of weights: 3.660726422464362\n",
      "---------------------\n",
      "Iteration Number: 2751\n",
      "Loss: 24.10925791333058\n",
      "l2 norm of gradients: 0.3159107400732798\n",
      "l2 norm of weights: 3.6601930377190226\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 2752\n",
      "Loss: 24.09957604185908\n",
      "l2 norm of gradients: 0.31574448739796884\n",
      "l2 norm of weights: 3.6596601142984877\n",
      "---------------------\n",
      "Iteration Number: 2753\n",
      "Loss: 24.089901921624207\n",
      "l2 norm of gradients: 0.31557831703957695\n",
      "l2 norm of weights: 3.659127651994321\n",
      "---------------------\n",
      "Iteration Number: 2754\n",
      "Loss: 24.080235546619328\n",
      "l2 norm of gradients: 0.31541222903987765\n",
      "l2 norm of weights: 3.6585956505979054\n",
      "---------------------\n",
      "Iteration Number: 2755\n",
      "Loss: 24.070576910840792\n",
      "l2 norm of gradients: 0.3152462234404379\n",
      "l2 norm of weights: 3.6580641099004394\n",
      "---------------------\n",
      "Iteration Number: 2756\n",
      "Loss: 24.060926008287222\n",
      "l2 norm of gradients: 0.31508030028261846\n",
      "l2 norm of weights: 3.6575330296929423\n",
      "---------------------\n",
      "Iteration Number: 2757\n",
      "Loss: 24.05128283296035\n",
      "l2 norm of gradients: 0.3149144596075736\n",
      "l2 norm of weights: 3.65700240976625\n",
      "---------------------\n",
      "Iteration Number: 2758\n",
      "Loss: 24.04164737886403\n",
      "l2 norm of gradients: 0.31474870145625145\n",
      "l2 norm of weights: 3.656472249911019\n",
      "---------------------\n",
      "Iteration Number: 2759\n",
      "Loss: 24.032019640005196\n",
      "l2 norm of gradients: 0.31458302586939346\n",
      "l2 norm of weights: 3.6559425499177225\n",
      "---------------------\n",
      "Iteration Number: 2760\n",
      "Loss: 24.02239961039329\n",
      "l2 norm of gradients: 0.3144174328875348\n",
      "l2 norm of weights: 3.655413309576654\n",
      "---------------------\n",
      "Iteration Number: 2761\n",
      "Loss: 24.012787284040467\n",
      "l2 norm of gradients: 0.31425192255100465\n",
      "l2 norm of weights: 3.6548845286779246\n",
      "---------------------\n",
      "Iteration Number: 2762\n",
      "Loss: 24.003182654961396\n",
      "l2 norm of gradients: 0.31408649489992535\n",
      "l2 norm of weights: 3.6543562070114657\n",
      "---------------------\n",
      "Iteration Number: 2763\n",
      "Loss: 23.99358571717393\n",
      "l2 norm of gradients: 0.31392114997421333\n",
      "l2 norm of weights: 3.653828344367026\n",
      "---------------------\n",
      "Iteration Number: 2764\n",
      "Loss: 23.983996464697952\n",
      "l2 norm of gradients: 0.31375588781357866\n",
      "l2 norm of weights: 3.6533009405341748\n",
      "---------------------\n",
      "Iteration Number: 2765\n",
      "Loss: 23.974414891556524\n",
      "l2 norm of gradients: 0.3135907084575251\n",
      "l2 norm of weights: 3.6527739953023004\n",
      "---------------------\n",
      "Iteration Number: 2766\n",
      "Loss: 23.964840991775155\n",
      "l2 norm of gradients: 0.3134256119453504\n",
      "l2 norm of weights: 3.6522475084606096\n",
      "---------------------\n",
      "Iteration Number: 2767\n",
      "Loss: 23.955274759382473\n",
      "l2 norm of gradients: 0.31326059831614605\n",
      "l2 norm of weights: 3.6517214797981286\n",
      "---------------------\n",
      "Iteration Number: 2768\n",
      "Loss: 23.945716188409204\n",
      "l2 norm of gradients: 0.3130956676087975\n",
      "l2 norm of weights: 3.6511959091037043\n",
      "---------------------\n",
      "Iteration Number: 2769\n",
      "Loss: 23.936165272889408\n",
      "l2 norm of gradients: 0.31293081986198407\n",
      "l2 norm of weights: 3.6506707961660014\n",
      "---------------------\n",
      "Iteration Number: 2770\n",
      "Loss: 23.926622006859372\n",
      "l2 norm of gradients: 0.3127660551141791\n",
      "l2 norm of weights: 3.650146140773505\n",
      "---------------------\n",
      "Iteration Number: 2771\n",
      "Loss: 23.917086384358626\n",
      "l2 norm of gradients: 0.3126013734036502\n",
      "l2 norm of weights: 3.6496219427145213\n",
      "---------------------\n",
      "Iteration Number: 2772\n",
      "Loss: 23.907558399429146\n",
      "l2 norm of gradients: 0.31243677476845855\n",
      "l2 norm of weights: 3.649098201777173\n",
      "---------------------\n",
      "Iteration Number: 2773\n",
      "Loss: 23.898038046115577\n",
      "l2 norm of gradients: 0.3122722592464603\n",
      "l2 norm of weights: 3.648574917749406\n",
      "---------------------\n",
      "Iteration Number: 2774\n",
      "Loss: 23.888525318465554\n",
      "l2 norm of gradients: 0.3121078268753052\n",
      "l2 norm of weights: 3.648052090418983\n",
      "---------------------\n",
      "Iteration Number: 2775\n",
      "Loss: 23.879020210529458\n",
      "l2 norm of gradients: 0.3119434776924377\n",
      "l2 norm of weights: 3.647529719573489\n",
      "---------------------\n",
      "Iteration Number: 2776\n",
      "Loss: 23.869522716360297\n",
      "l2 norm of gradients: 0.3117792117350965\n",
      "l2 norm of weights: 3.6470078050003285\n",
      "---------------------\n",
      "Iteration Number: 2777\n",
      "Loss: 23.860032830014024\n",
      "l2 norm of gradients: 0.3116150290403148\n",
      "l2 norm of weights: 3.646486346486726\n",
      "---------------------\n",
      "Iteration Number: 2778\n",
      "Loss: 23.850550545549268\n",
      "l2 norm of gradients: 0.3114509296449203\n",
      "l2 norm of weights: 3.645965343819725\n",
      "---------------------\n",
      "Iteration Number: 2779\n",
      "Loss: 23.84107585702755\n",
      "l2 norm of gradients: 0.3112869135855356\n",
      "l2 norm of weights: 3.645444796786192\n",
      "---------------------\n",
      "Iteration Number: 2780\n",
      "Loss: 23.83160875851317\n",
      "l2 norm of gradients: 0.3111229808985778\n",
      "l2 norm of weights: 3.6449247051728104\n",
      "---------------------\n",
      "Iteration Number: 2781\n",
      "Loss: 23.82214924407297\n",
      "l2 norm of gradients: 0.3109591316202589\n",
      "l2 norm of weights: 3.6444050687660874\n",
      "---------------------\n",
      "Iteration Number: 2782\n",
      "Loss: 23.81269730777706\n",
      "l2 norm of gradients: 0.31079536578658595\n",
      "l2 norm of weights: 3.643885887352348\n",
      "---------------------\n",
      "Iteration Number: 2783\n",
      "Loss: 23.803252943697956\n",
      "l2 norm of gradients: 0.31063168343336084\n",
      "l2 norm of weights: 3.64336716071774\n",
      "---------------------\n",
      "Iteration Number: 2784\n",
      "Loss: 23.793816145911343\n",
      "l2 norm of gradients: 0.3104680845961808\n",
      "l2 norm of weights: 3.6428488886482286\n",
      "---------------------\n",
      "Iteration Number: 2785\n",
      "Loss: 23.784386908495506\n",
      "l2 norm of gradients: 0.3103045693104381\n",
      "l2 norm of weights: 3.6423310709296035\n",
      "---------------------\n",
      "Iteration Number: 2786\n",
      "Loss: 23.774965225531624\n",
      "l2 norm of gradients: 0.31014113761132056\n",
      "l2 norm of weights: 3.6418137073474726\n",
      "---------------------\n",
      "Iteration Number: 2787\n",
      "Loss: 23.765551091103706\n",
      "l2 norm of gradients: 0.3099777895338114\n",
      "l2 norm of weights: 3.6412967976872657\n",
      "---------------------\n",
      "Iteration Number: 2788\n",
      "Loss: 23.756144499298685\n",
      "l2 norm of gradients: 0.3098145251126895\n",
      "l2 norm of weights: 3.6407803417342324\n",
      "---------------------\n",
      "Iteration Number: 2789\n",
      "Loss: 23.7467454442062\n",
      "l2 norm of gradients: 0.30965134438252934\n",
      "l2 norm of weights: 3.640264339273445\n",
      "---------------------\n",
      "Iteration Number: 2790\n",
      "Loss: 23.737353919918945\n",
      "l2 norm of gradients: 0.3094882473777013\n",
      "l2 norm of weights: 3.639748790089795\n",
      "---------------------\n",
      "Iteration Number: 2791\n",
      "Loss: 23.72796992053212\n",
      "l2 norm of gradients: 0.30932523413237195\n",
      "l2 norm of weights: 3.6392336939679955\n",
      "---------------------\n",
      "Iteration Number: 2792\n",
      "Loss: 23.718593440144407\n",
      "l2 norm of gradients: 0.30916230468050343\n",
      "l2 norm of weights: 3.6387190506925817\n",
      "---------------------\n",
      "Iteration Number: 2793\n",
      "Loss: 23.70922447285664\n",
      "l2 norm of gradients: 0.30899945905585474\n",
      "l2 norm of weights: 3.6382048600479093\n",
      "---------------------\n",
      "Iteration Number: 2794\n",
      "Loss: 23.6998630127731\n",
      "l2 norm of gradients: 0.3088366972919809\n",
      "l2 norm of weights: 3.637691121818155\n",
      "---------------------\n",
      "Iteration Number: 2795\n",
      "Loss: 23.690509054000575\n",
      "l2 norm of gradients: 0.3086740194222334\n",
      "l2 norm of weights: 3.6371778357873166\n",
      "---------------------\n",
      "Iteration Number: 2796\n",
      "Loss: 23.68116259064899\n",
      "l2 norm of gradients: 0.3085114254797607\n",
      "l2 norm of weights: 3.6366650017392144\n",
      "---------------------\n",
      "Iteration Number: 2797\n",
      "Loss: 23.671823616831137\n",
      "l2 norm of gradients: 0.30834891549750787\n",
      "l2 norm of weights: 3.63615261945749\n",
      "---------------------\n",
      "Iteration Number: 2798\n",
      "Loss: 23.662492126662425\n",
      "l2 norm of gradients: 0.308186489508217\n",
      "l2 norm of weights: 3.6356406887256045\n",
      "---------------------\n",
      "Iteration Number: 2799\n",
      "Loss: 23.653168114261522\n",
      "l2 norm of gradients: 0.3080241475444272\n",
      "l2 norm of weights: 3.635129209326843\n",
      "---------------------\n",
      "Iteration Number: 2800\n",
      "Loss: 23.64385157374997\n",
      "l2 norm of gradients: 0.3078618896384751\n",
      "l2 norm of weights: 3.6346181810443112\n",
      "---------------------\n",
      "Iteration Number: 2801\n",
      "Loss: 23.634542499251953\n",
      "l2 norm of gradients: 0.30769971582249456\n",
      "l2 norm of weights: 3.634107603660937\n",
      "---------------------\n",
      "Iteration Number: 2802\n",
      "Loss: 23.62524088489483\n",
      "l2 norm of gradients: 0.3075376261284171\n",
      "l2 norm of weights: 3.633597476959469\n",
      "---------------------\n",
      "Iteration Number: 2803\n",
      "Loss: 23.61594672480866\n",
      "l2 norm of gradients: 0.3073756205879721\n",
      "l2 norm of weights: 3.633087800722479\n",
      "---------------------\n",
      "Iteration Number: 2804\n",
      "Loss: 23.606660013126625\n",
      "l2 norm of gradients: 0.3072136992326867\n",
      "l2 norm of weights: 3.6325785747323596\n",
      "---------------------\n",
      "Iteration Number: 2805\n",
      "Loss: 23.597380743984857\n",
      "l2 norm of gradients: 0.30705186209388646\n",
      "l2 norm of weights: 3.6320697987713255\n",
      "---------------------\n",
      "Iteration Number: 2806\n",
      "Loss: 23.588108911522312\n",
      "l2 norm of gradients: 0.30689010920269494\n",
      "l2 norm of weights: 3.6315614726214136\n",
      "---------------------\n",
      "Iteration Number: 2807\n",
      "Loss: 23.578844509880916\n",
      "l2 norm of gradients: 0.30672844059003423\n",
      "l2 norm of weights: 3.6310535960644827\n",
      "---------------------\n",
      "Iteration Number: 2808\n",
      "Loss: 23.569587533205357\n",
      "l2 norm of gradients: 0.30656685628662533\n",
      "l2 norm of weights: 3.6305461688822147\n",
      "---------------------\n",
      "Iteration Number: 2809\n",
      "Loss: 23.560337975643744\n",
      "l2 norm of gradients: 0.30640535632298777\n",
      "l2 norm of weights: 3.6300391908561123\n",
      "---------------------\n",
      "Iteration Number: 2810\n",
      "Loss: 23.55109583134675\n",
      "l2 norm of gradients: 0.3062439407294403\n",
      "l2 norm of weights: 3.6295326617675\n",
      "---------------------\n",
      "Iteration Number: 2811\n",
      "Loss: 23.541861094468167\n",
      "l2 norm of gradients: 0.3060826095361007\n",
      "l2 norm of weights: 3.629026581397527\n",
      "---------------------\n",
      "Iteration Number: 2812\n",
      "Loss: 23.53263375916447\n",
      "l2 norm of gradients: 0.30592136277288645\n",
      "l2 norm of weights: 3.6285209495271626\n",
      "---------------------\n",
      "Iteration Number: 2813\n",
      "Loss: 23.523413819595568\n",
      "l2 norm of gradients: 0.3057602004695144\n",
      "l2 norm of weights: 3.6280157659372\n",
      "---------------------\n",
      "Iteration Number: 2814\n",
      "Loss: 23.514201269924104\n",
      "l2 norm of gradients: 0.3055991226555014\n",
      "l2 norm of weights: 3.627511030408253\n",
      "---------------------\n",
      "Iteration Number: 2815\n",
      "Loss: 23.504996104315566\n",
      "l2 norm of gradients: 0.30543812936016396\n",
      "l2 norm of weights: 3.62700674272076\n",
      "---------------------\n",
      "Iteration Number: 2816\n",
      "Loss: 23.49579831693868\n",
      "l2 norm of gradients: 0.30527722061261936\n",
      "l2 norm of weights: 3.6265029026549813\n",
      "---------------------\n",
      "Iteration Number: 2817\n",
      "Loss: 23.486607901964874\n",
      "l2 norm of gradients: 0.30511639644178484\n",
      "l2 norm of weights: 3.6259995099909985\n",
      "---------------------\n",
      "Iteration Number: 2818\n",
      "Loss: 23.477424853568937\n",
      "l2 norm of gradients: 0.3049556568763785\n",
      "l2 norm of weights: 3.625496564508718\n",
      "---------------------\n",
      "Iteration Number: 2819\n",
      "Loss: 23.46824916592821\n",
      "l2 norm of gradients: 0.3047950019449193\n",
      "l2 norm of weights: 3.6249940659878677\n",
      "---------------------\n",
      "Iteration Number: 2820\n",
      "Loss: 23.45908083322343\n",
      "l2 norm of gradients: 0.3046344316757271\n",
      "l2 norm of weights: 3.624492014207999\n",
      "---------------------\n",
      "Iteration Number: 2821\n",
      "Loss: 23.44991984963812\n",
      "l2 norm of gradients: 0.30447394609692324\n",
      "l2 norm of weights: 3.6239904089484845\n",
      "---------------------\n",
      "Iteration Number: 2822\n",
      "Loss: 23.440766209358866\n",
      "l2 norm of gradients: 0.30431354523643045\n",
      "l2 norm of weights: 3.623489249988522\n",
      "---------------------\n",
      "Iteration Number: 2823\n",
      "Loss: 23.43161990657518\n",
      "l2 norm of gradients: 0.3041532291219731\n",
      "l2 norm of weights: 3.6229885371071324\n",
      "---------------------\n",
      "Iteration Number: 2824\n",
      "Loss: 23.42248093547967\n",
      "l2 norm of gradients: 0.3039929977810778\n",
      "l2 norm of weights: 3.6224882700831564\n",
      "---------------------\n",
      "Iteration Number: 2825\n",
      "Loss: 23.413349290268023\n",
      "l2 norm of gradients: 0.30383285124107307\n",
      "l2 norm of weights: 3.621988448695262\n",
      "---------------------\n",
      "Iteration Number: 2826\n",
      "Loss: 23.404224965138773\n",
      "l2 norm of gradients: 0.30367278952909005\n",
      "l2 norm of weights: 3.6214890727219373\n",
      "---------------------\n",
      "Iteration Number: 2827\n",
      "Loss: 23.39510795429365\n",
      "l2 norm of gradients: 0.30351281267206226\n",
      "l2 norm of weights: 3.620990141941495\n",
      "---------------------\n",
      "Iteration Number: 2828\n",
      "Loss: 23.385998251937398\n",
      "l2 norm of gradients: 0.3033529206967265\n",
      "l2 norm of weights: 3.6204916561320712\n",
      "---------------------\n",
      "Iteration Number: 2829\n",
      "Loss: 23.376895852277396\n",
      "l2 norm of gradients: 0.30319311362962226\n",
      "l2 norm of weights: 3.6199936150716243\n",
      "---------------------\n",
      "Iteration Number: 2830\n",
      "Loss: 23.367800749524747\n",
      "l2 norm of gradients: 0.30303339149709274\n",
      "l2 norm of weights: 3.619496018537938\n",
      "---------------------\n",
      "Iteration Number: 2831\n",
      "Loss: 23.358712937893156\n",
      "l2 norm of gradients: 0.3028737543252845\n",
      "l2 norm of weights: 3.6189988663086177\n",
      "---------------------\n",
      "Iteration Number: 2832\n",
      "Loss: 23.34963241159938\n",
      "l2 norm of gradients: 0.3027142021401481\n",
      "l2 norm of weights: 3.6185021581610926\n",
      "---------------------\n",
      "Iteration Number: 2833\n",
      "Loss: 23.34055916486326\n",
      "l2 norm of gradients: 0.30255473496743823\n",
      "l2 norm of weights: 3.6180058938726174\n",
      "---------------------\n",
      "Iteration Number: 2834\n",
      "Loss: 23.33149319190773\n",
      "l2 norm of gradients: 0.30239535283271374\n",
      "l2 norm of weights: 3.617510073220268\n",
      "---------------------\n",
      "Iteration Number: 2835\n",
      "Loss: 23.322434486958752\n",
      "l2 norm of gradients: 0.3022360557613382\n",
      "l2 norm of weights: 3.6170146959809446\n",
      "---------------------\n",
      "Iteration Number: 2836\n",
      "Loss: 23.313383044245434\n",
      "l2 norm of gradients: 0.30207684377848015\n",
      "l2 norm of weights: 3.616519761931373\n",
      "---------------------\n",
      "Iteration Number: 2837\n",
      "Loss: 23.30433885799972\n",
      "l2 norm of gradients: 0.30191771690911307\n",
      "l2 norm of weights: 3.6160252708481004\n",
      "---------------------\n",
      "Iteration Number: 2838\n",
      "Loss: 23.295301922456893\n",
      "l2 norm of gradients: 0.3017586751780159\n",
      "l2 norm of weights: 3.6155312225075003\n",
      "---------------------\n",
      "Iteration Number: 2839\n",
      "Loss: 23.286272231855033\n",
      "l2 norm of gradients: 0.30159971860977314\n",
      "l2 norm of weights: 3.6150376166857687\n",
      "---------------------\n",
      "Iteration Number: 2840\n",
      "Loss: 23.2772497804354\n",
      "l2 norm of gradients: 0.30144084722877523\n",
      "l2 norm of weights: 3.614544453158925\n",
      "---------------------\n",
      "Iteration Number: 2841\n",
      "Loss: 23.26823456244247\n",
      "l2 norm of gradients: 0.30128206105921895\n",
      "l2 norm of weights: 3.614051731702815\n",
      "---------------------\n",
      "Iteration Number: 2842\n",
      "Loss: 23.259226572123474\n",
      "l2 norm of gradients: 0.30112336012510715\n",
      "l2 norm of weights: 3.6135594520931074\n",
      "---------------------\n",
      "Iteration Number: 2843\n",
      "Loss: 23.250225803729144\n",
      "l2 norm of gradients: 0.3009647444502496\n",
      "l2 norm of weights: 3.6130676141052946\n",
      "---------------------\n",
      "Iteration Number: 2844\n",
      "Loss: 23.24123225151275\n",
      "l2 norm of gradients: 0.3008062140582632\n",
      "l2 norm of weights: 3.612576217514695\n",
      "---------------------\n",
      "Iteration Number: 2845\n",
      "Loss: 23.232245909731212\n",
      "l2 norm of gradients: 0.3006477689725717\n",
      "l2 norm of weights: 3.6120852620964494\n",
      "---------------------\n",
      "Iteration Number: 2846\n",
      "Loss: 23.223266772644035\n",
      "l2 norm of gradients: 0.3004894092164068\n",
      "l2 norm of weights: 3.611594747625525\n",
      "---------------------\n",
      "Iteration Number: 2847\n",
      "Loss: 23.21429483451414\n",
      "l2 norm of gradients: 0.3003311348128077\n",
      "l2 norm of weights: 3.6111046738767114\n",
      "---------------------\n",
      "Iteration Number: 2848\n",
      "Loss: 23.205330089607443\n",
      "l2 norm of gradients: 0.3001729457846218\n",
      "l2 norm of weights: 3.610615040624626\n",
      "---------------------\n",
      "Iteration Number: 2849\n",
      "Loss: 23.196372532192964\n",
      "l2 norm of gradients: 0.30001484215450475\n",
      "l2 norm of weights: 3.610125847643708\n",
      "---------------------\n",
      "Iteration Number: 2850\n",
      "Loss: 23.18742215654278\n",
      "l2 norm of gradients: 0.299856823944921\n",
      "l2 norm of weights: 3.6096370947082224\n",
      "---------------------\n",
      "Iteration Number: 2851\n",
      "Loss: 23.178478956932025\n",
      "l2 norm of gradients: 0.29969889117814397\n",
      "l2 norm of weights: 3.609148781592259\n",
      "---------------------\n",
      "Iteration Number: 2852\n",
      "Loss: 23.16954292763902\n",
      "l2 norm of gradients: 0.29954104387625613\n",
      "l2 norm of weights: 3.6086609080697336\n",
      "---------------------\n",
      "Iteration Number: 2853\n",
      "Loss: 23.160614062945267\n",
      "l2 norm of gradients: 0.2993832820611495\n",
      "l2 norm of weights: 3.608173473914385\n",
      "---------------------\n",
      "Iteration Number: 2854\n",
      "Loss: 23.151692357135307\n",
      "l2 norm of gradients: 0.29922560575452606\n",
      "l2 norm of weights: 3.6076864788997782\n",
      "---------------------\n",
      "Iteration Number: 2855\n",
      "Loss: 23.142777804496507\n",
      "l2 norm of gradients: 0.2990680149778977\n",
      "l2 norm of weights: 3.6071999227993037\n",
      "---------------------\n",
      "Iteration Number: 2856\n",
      "Loss: 23.133870399319633\n",
      "l2 norm of gradients: 0.29891050975258665\n",
      "l2 norm of weights: 3.606713805386177\n",
      "---------------------\n",
      "Iteration Number: 2857\n",
      "Loss: 23.12497013589875\n",
      "l2 norm of gradients: 0.29875309009972617\n",
      "l2 norm of weights: 3.6062281264334377\n",
      "---------------------\n",
      "Iteration Number: 2858\n",
      "Loss: 23.116077008530638\n",
      "l2 norm of gradients: 0.2985957560402602\n",
      "l2 norm of weights: 3.6057428857139526\n",
      "---------------------\n",
      "Iteration Number: 2859\n",
      "Loss: 23.10719101151546\n",
      "l2 norm of gradients: 0.2984385075949439\n",
      "l2 norm of weights: 3.6052580830004137\n",
      "---------------------\n",
      "Iteration Number: 2860\n",
      "Loss: 23.09831213915632\n",
      "l2 norm of gradients: 0.2982813447843444\n",
      "l2 norm of weights: 3.6047737180653368\n",
      "---------------------\n",
      "Iteration Number: 2861\n",
      "Loss: 23.08944038575953\n",
      "l2 norm of gradients: 0.2981242676288404\n",
      "l2 norm of weights: 3.6042897906810656\n",
      "---------------------\n",
      "Iteration Number: 2862\n",
      "Loss: 23.08057574563457\n",
      "l2 norm of gradients: 0.29796727614862295\n",
      "l2 norm of weights: 3.603806300619768\n",
      "---------------------\n",
      "Iteration Number: 2863\n",
      "Loss: 23.071718213093938\n",
      "l2 norm of gradients: 0.29781037036369534\n",
      "l2 norm of weights: 3.6033232476534387\n",
      "---------------------\n",
      "Iteration Number: 2864\n",
      "Loss: 23.06286778245339\n",
      "l2 norm of gradients: 0.2976535502938741\n",
      "l2 norm of weights: 3.6028406315538977\n",
      "---------------------\n",
      "Iteration Number: 2865\n",
      "Loss: 23.054024448031758\n",
      "l2 norm of gradients: 0.2974968159587885\n",
      "l2 norm of weights: 3.602358452092791\n",
      "---------------------\n",
      "Iteration Number: 2866\n",
      "Loss: 23.045188204151014\n",
      "l2 norm of gradients: 0.29734016737788155\n",
      "l2 norm of weights: 3.601876709041591\n",
      "---------------------\n",
      "Iteration Number: 2867\n",
      "Loss: 23.036359045136155\n",
      "l2 norm of gradients: 0.2971836045704097\n",
      "l2 norm of weights: 3.601395402171596\n",
      "---------------------\n",
      "Iteration Number: 2868\n",
      "Loss: 23.027536965315456\n",
      "l2 norm of gradients: 0.29702712755544386\n",
      "l2 norm of weights: 3.600914531253931\n",
      "---------------------\n",
      "Iteration Number: 2869\n",
      "Loss: 23.01872195902029\n",
      "l2 norm of gradients: 0.29687073635186895\n",
      "l2 norm of weights: 3.6004340960595465\n",
      "---------------------\n",
      "Iteration Number: 2870\n",
      "Loss: 23.009914020585192\n",
      "l2 norm of gradients: 0.2967144309783847\n",
      "l2 norm of weights: 3.59995409635922\n",
      "---------------------\n",
      "Iteration Number: 2871\n",
      "Loss: 23.001113144347755\n",
      "l2 norm of gradients: 0.296558211453506\n",
      "l2 norm of weights: 3.5994745319235553\n",
      "---------------------\n",
      "Iteration Number: 2872\n",
      "Loss: 22.992319324648715\n",
      "l2 norm of gradients: 0.2964020777955629\n",
      "l2 norm of weights: 3.5989954025229833\n",
      "---------------------\n",
      "Iteration Number: 2873\n",
      "Loss: 22.98353255583222\n",
      "l2 norm of gradients: 0.2962460300227011\n",
      "l2 norm of weights: 3.5985167079277613\n",
      "---------------------\n",
      "Iteration Number: 2874\n",
      "Loss: 22.974752832245088\n",
      "l2 norm of gradients: 0.2960900681528825\n",
      "l2 norm of weights: 3.5980384479079737\n",
      "---------------------\n",
      "Iteration Number: 2875\n",
      "Loss: 22.96598014823773\n",
      "l2 norm of gradients: 0.29593419220388517\n",
      "l2 norm of weights: 3.597560622233531\n",
      "---------------------\n",
      "Iteration Number: 2876\n",
      "Loss: 22.95721449816354\n",
      "l2 norm of gradients: 0.29577840219330365\n",
      "l2 norm of weights: 3.5970832306741714\n",
      "---------------------\n",
      "Iteration Number: 2877\n",
      "Loss: 22.948455876379015\n",
      "l2 norm of gradients: 0.29562269813854974\n",
      "l2 norm of weights: 3.5966062729994603\n",
      "---------------------\n",
      "Iteration Number: 2878\n",
      "Loss: 22.939704277243866\n",
      "l2 norm of gradients: 0.29546708005685246\n",
      "l2 norm of weights: 3.59612974897879\n",
      "---------------------\n",
      "Iteration Number: 2879\n",
      "Loss: 22.93095969512089\n",
      "l2 norm of gradients: 0.29531154796525827\n",
      "l2 norm of weights: 3.59565365838138\n",
      "---------------------\n",
      "Iteration Number: 2880\n",
      "Loss: 22.922222124376123\n",
      "l2 norm of gradients: 0.2951561018806318\n",
      "l2 norm of weights: 3.5951780009762784\n",
      "---------------------\n",
      "Iteration Number: 2881\n",
      "Loss: 22.91349155937881\n",
      "l2 norm of gradients: 0.29500074181965574\n",
      "l2 norm of weights: 3.5947027765323587\n",
      "---------------------\n",
      "Iteration Number: 2882\n",
      "Loss: 22.90476799450122\n",
      "l2 norm of gradients: 0.2948454677988317\n",
      "l2 norm of weights: 3.594227984818324\n",
      "---------------------\n",
      "Iteration Number: 2883\n",
      "Loss: 22.89605142411879\n",
      "l2 norm of gradients: 0.29469027983448004\n",
      "l2 norm of weights: 3.593753625602704\n",
      "---------------------\n",
      "Iteration Number: 2884\n",
      "Loss: 22.88734184261023\n",
      "l2 norm of gradients: 0.2945351779427404\n",
      "l2 norm of weights: 3.593279698653857\n",
      "---------------------\n",
      "Iteration Number: 2885\n",
      "Loss: 22.87863924435738\n",
      "l2 norm of gradients: 0.2943801621395723\n",
      "l2 norm of weights: 3.5928062037399684\n",
      "---------------------\n",
      "Iteration Number: 2886\n",
      "Loss: 22.869943623745264\n",
      "l2 norm of gradients: 0.2942252324407549\n",
      "l2 norm of weights: 3.5923331406290533\n",
      "---------------------\n",
      "Iteration Number: 2887\n",
      "Loss: 22.86125497516195\n",
      "l2 norm of gradients: 0.2940703888618877\n",
      "l2 norm of weights: 3.591860509088953\n",
      "---------------------\n",
      "Iteration Number: 2888\n",
      "Loss: 22.8525732929988\n",
      "l2 norm of gradients: 0.2939156314183911\n",
      "l2 norm of weights: 3.591388308887338\n",
      "---------------------\n",
      "Iteration Number: 2889\n",
      "Loss: 22.84389857165036\n",
      "l2 norm of gradients: 0.2937609601255064\n",
      "l2 norm of weights: 3.590916539791708\n",
      "---------------------\n",
      "Iteration Number: 2890\n",
      "Loss: 22.8352308055142\n",
      "l2 norm of gradients: 0.29360637499829617\n",
      "l2 norm of weights: 3.5904452015693904\n",
      "---------------------\n",
      "Iteration Number: 2891\n",
      "Loss: 22.82656998899118\n",
      "l2 norm of gradients: 0.2934518760516447\n",
      "l2 norm of weights: 3.589974293987541\n",
      "---------------------\n",
      "Iteration Number: 2892\n",
      "Loss: 22.817916116485407\n",
      "l2 norm of gradients: 0.29329746330025835\n",
      "l2 norm of weights: 3.5895038168131452\n",
      "---------------------\n",
      "Iteration Number: 2893\n",
      "Loss: 22.809269182404034\n",
      "l2 norm of gradients: 0.29314313675866566\n",
      "l2 norm of weights: 3.5890337698130166\n",
      "---------------------\n",
      "Iteration Number: 2894\n",
      "Loss: 22.800629181157404\n",
      "l2 norm of gradients: 0.29298889644121817\n",
      "l2 norm of weights: 3.588564152753799\n",
      "---------------------\n",
      "Iteration Number: 2895\n",
      "Loss: 22.79199610715902\n",
      "l2 norm of gradients: 0.29283474236209034\n",
      "l2 norm of weights: 3.5880949654019636\n",
      "---------------------\n",
      "Iteration Number: 2896\n",
      "Loss: 22.78336995482567\n",
      "l2 norm of gradients: 0.29268067453527985\n",
      "l2 norm of weights: 3.5876262075238126\n",
      "---------------------\n",
      "Iteration Number: 2897\n",
      "Loss: 22.774750718577202\n",
      "l2 norm of gradients: 0.2925266929746087\n",
      "l2 norm of weights: 3.5871578788854763\n",
      "---------------------\n",
      "Iteration Number: 2898\n",
      "Loss: 22.76613839283678\n",
      "l2 norm of gradients: 0.2923727976937225\n",
      "l2 norm of weights: 3.5866899792529163\n",
      "---------------------\n",
      "Iteration Number: 2899\n",
      "Loss: 22.757532972030546\n",
      "l2 norm of gradients: 0.29221898870609153\n",
      "l2 norm of weights: 3.5862225083919212\n",
      "---------------------\n",
      "Iteration Number: 2900\n",
      "Loss: 22.748934450588145\n",
      "l2 norm of gradients: 0.2920652660250109\n",
      "l2 norm of weights: 3.5857554660681124\n",
      "---------------------\n",
      "Iteration Number: 2901\n",
      "Loss: 22.740342822941997\n",
      "l2 norm of gradients: 0.2919116296636009\n",
      "l2 norm of weights: 3.585288852046939\n",
      "---------------------\n",
      "Iteration Number: 2902\n",
      "Loss: 22.731758083528064\n",
      "l2 norm of gradients: 0.2917580796348072\n",
      "l2 norm of weights: 3.5848226660936824\n",
      "---------------------\n",
      "Iteration Number: 2903\n",
      "Loss: 22.72318022678519\n",
      "l2 norm of gradients: 0.29160461595140175\n",
      "l2 norm of weights: 3.584356907973451\n",
      "---------------------\n",
      "Iteration Number: 2904\n",
      "Loss: 22.714609247155764\n",
      "l2 norm of gradients: 0.2914512386259822\n",
      "l2 norm of weights: 3.5838915774511877\n",
      "---------------------\n",
      "Iteration Number: 2905\n",
      "Loss: 22.706045139084956\n",
      "l2 norm of gradients: 0.29129794767097333\n",
      "l2 norm of weights: 3.583426674291662\n",
      "---------------------\n",
      "Iteration Number: 2906\n",
      "Loss: 22.69748789702148\n",
      "l2 norm of gradients: 0.29114474309862665\n",
      "l2 norm of weights: 3.5829621982594766\n",
      "---------------------\n",
      "Iteration Number: 2907\n",
      "Loss: 22.688937515417102\n",
      "l2 norm of gradients: 0.2909916249210208\n",
      "l2 norm of weights: 3.5824981491190644\n",
      "---------------------\n",
      "Iteration Number: 2908\n",
      "Loss: 22.680393988726568\n",
      "l2 norm of gradients: 0.2908385931500625\n",
      "l2 norm of weights: 3.582034526634689\n",
      "---------------------\n",
      "Iteration Number: 2909\n",
      "Loss: 22.671857311408164\n",
      "l2 norm of gradients: 0.2906856477974861\n",
      "l2 norm of weights: 3.581571330570445\n",
      "---------------------\n",
      "Iteration Number: 2910\n",
      "Loss: 22.66332747792318\n",
      "l2 norm of gradients: 0.29053278887485484\n",
      "l2 norm of weights: 3.58110856069026\n",
      "---------------------\n",
      "Iteration Number: 2911\n",
      "Loss: 22.654804482736072\n",
      "l2 norm of gradients: 0.2903800163935602\n",
      "l2 norm of weights: 3.5806462167578896\n",
      "---------------------\n",
      "Iteration Number: 2912\n",
      "Loss: 22.646288320314678\n",
      "l2 norm of gradients: 0.29022733036482307\n",
      "l2 norm of weights: 3.5801842985369245\n",
      "---------------------\n",
      "Iteration Number: 2913\n",
      "Loss: 22.637778985129724\n",
      "l2 norm of gradients: 0.2900747307996939\n",
      "l2 norm of weights: 3.5797228057907846\n",
      "---------------------\n",
      "Iteration Number: 2914\n",
      "Loss: 22.629276471655437\n",
      "l2 norm of gradients: 0.2899222177090528\n",
      "l2 norm of weights: 3.5792617382827236\n",
      "---------------------\n",
      "Iteration Number: 2915\n",
      "Loss: 22.620780774368953\n",
      "l2 norm of gradients: 0.2897697911036103\n",
      "l2 norm of weights: 3.5788010957758263\n",
      "---------------------\n",
      "Iteration Number: 2916\n",
      "Loss: 22.61229188775088\n",
      "l2 norm of gradients: 0.2896174509939072\n",
      "l2 norm of weights: 3.5783408780330097\n",
      "---------------------\n",
      "Iteration Number: 2917\n",
      "Loss: 22.603809806284783\n",
      "l2 norm of gradients: 0.28946519739031557\n",
      "l2 norm of weights: 3.5778810848170233\n",
      "---------------------\n",
      "Iteration Number: 2918\n",
      "Loss: 22.595334524457684\n",
      "l2 norm of gradients: 0.2893130303030386\n",
      "l2 norm of weights: 3.5774217158904493\n",
      "---------------------\n",
      "Iteration Number: 2919\n",
      "Loss: 22.586866036759474\n",
      "l2 norm of gradients: 0.2891609497421114\n",
      "l2 norm of weights: 3.5769627710157024\n",
      "---------------------\n",
      "Iteration Number: 2920\n",
      "Loss: 22.57840433768335\n",
      "l2 norm of gradients: 0.2890089557174007\n",
      "l2 norm of weights: 3.5765042499550312\n",
      "---------------------\n",
      "Iteration Number: 2921\n",
      "Loss: 22.56994942172599\n",
      "l2 norm of gradients: 0.2888570482386061\n",
      "l2 norm of weights: 3.576046152470515\n",
      "---------------------\n",
      "Iteration Number: 2922\n",
      "Loss: 22.561501283386853\n",
      "l2 norm of gradients: 0.28870522731525966\n",
      "l2 norm of weights: 3.5755884783240695\n",
      "---------------------\n",
      "Iteration Number: 2923\n",
      "Loss: 22.55305991716889\n",
      "l2 norm of gradients: 0.2885534929567268\n",
      "l2 norm of weights: 3.575131227277441\n",
      "---------------------\n",
      "Iteration Number: 2924\n",
      "Loss: 22.544625317578177\n",
      "l2 norm of gradients: 0.2884018451722064\n",
      "l2 norm of weights: 3.574674399092212\n",
      "---------------------\n",
      "Iteration Number: 2925\n",
      "Loss: 22.536197479123754\n",
      "l2 norm of gradients: 0.28825028397073105\n",
      "l2 norm of weights: 3.5742179935297966\n",
      "---------------------\n",
      "Iteration Number: 2926\n",
      "Loss: 22.527776396318224\n",
      "l2 norm of gradients: 0.28809880936116794\n",
      "l2 norm of weights: 3.573762010351444\n",
      "---------------------\n",
      "Iteration Number: 2927\n",
      "Loss: 22.519362063677196\n",
      "l2 norm of gradients: 0.28794742135221857\n",
      "l2 norm of weights: 3.573306449318237\n",
      "---------------------\n",
      "Iteration Number: 2928\n",
      "Loss: 22.51095447571942\n",
      "l2 norm of gradients: 0.28779611995241966\n",
      "l2 norm of weights: 3.5728513101910933\n",
      "---------------------\n",
      "Iteration Number: 2929\n",
      "Loss: 22.502553626967096\n",
      "l2 norm of gradients: 0.28764490517014313\n",
      "l2 norm of weights: 3.572396592730765\n",
      "---------------------\n",
      "Iteration Number: 2930\n",
      "Loss: 22.494159511945256\n",
      "l2 norm of gradients: 0.2874937770135968\n",
      "l2 norm of weights: 3.5719422966978382\n",
      "---------------------\n",
      "Iteration Number: 2931\n",
      "Loss: 22.485772125182372\n",
      "l2 norm of gradients: 0.28734273549082456\n",
      "l2 norm of weights: 3.571488421852736\n",
      "---------------------\n",
      "Iteration Number: 2932\n",
      "Loss: 22.477391461210225\n",
      "l2 norm of gradients: 0.2871917806097067\n",
      "l2 norm of weights: 3.571034967955714\n",
      "---------------------\n",
      "Iteration Number: 2933\n",
      "Loss: 22.469017514563433\n",
      "l2 norm of gradients: 0.2870409123779605\n",
      "l2 norm of weights: 3.5705819347668646\n",
      "---------------------\n",
      "Iteration Number: 2934\n",
      "Loss: 22.460650279780122\n",
      "l2 norm of gradients: 0.28689013080314024\n",
      "l2 norm of weights: 3.570129322046115\n",
      "---------------------\n",
      "Iteration Number: 2935\n",
      "Loss: 22.452289751401523\n",
      "l2 norm of gradients: 0.2867394358926382\n",
      "l2 norm of weights: 3.56967712955323\n",
      "---------------------\n",
      "Iteration Number: 2936\n",
      "Loss: 22.443935923972017\n",
      "l2 norm of gradients: 0.28658882765368426\n",
      "l2 norm of weights: 3.5692253570478076\n",
      "---------------------\n",
      "Iteration Number: 2937\n",
      "Loss: 22.435588792039272\n",
      "l2 norm of gradients: 0.28643830609334686\n",
      "l2 norm of weights: 3.5687740042892835\n",
      "---------------------\n",
      "Iteration Number: 2938\n",
      "Loss: 22.4272483501541\n",
      "l2 norm of gradients: 0.28628787121853305\n",
      "l2 norm of weights: 3.5683230710369296\n",
      "---------------------\n",
      "Iteration Number: 2939\n",
      "Loss: 22.418914592870443\n",
      "l2 norm of gradients: 0.2861375230359891\n",
      "l2 norm of weights: 3.567872557049854\n",
      "---------------------\n",
      "Iteration Number: 2940\n",
      "Loss: 22.41058751474562\n",
      "l2 norm of gradients: 0.2859872615523007\n",
      "l2 norm of weights: 3.567422462087003\n",
      "---------------------\n",
      "Iteration Number: 2941\n",
      "Loss: 22.40226711033992\n",
      "l2 norm of gradients: 0.2858370867738933\n",
      "l2 norm of weights: 3.566972785907157\n",
      "---------------------\n",
      "Iteration Number: 2942\n",
      "Loss: 22.3939533742171\n",
      "l2 norm of gradients: 0.2856869987070328\n",
      "l2 norm of weights: 3.5665235282689367\n",
      "---------------------\n",
      "Iteration Number: 2943\n",
      "Loss: 22.385646300943957\n",
      "l2 norm of gradients: 0.2855369973578254\n",
      "l2 norm of weights: 3.5660746889307973\n",
      "---------------------\n",
      "Iteration Number: 2944\n",
      "Loss: 22.377345885090403\n",
      "l2 norm of gradients: 0.2853870827322184\n",
      "l2 norm of weights: 3.5656262676510346\n",
      "---------------------\n",
      "Iteration Number: 2945\n",
      "Loss: 22.369052121229785\n",
      "l2 norm of gradients: 0.28523725483600065\n",
      "l2 norm of weights: 3.5651782641877805\n",
      "---------------------\n",
      "Iteration Number: 2946\n",
      "Loss: 22.360765003938354\n",
      "l2 norm of gradients: 0.28508751367480245\n",
      "l2 norm of weights: 3.5647306782990054\n",
      "---------------------\n",
      "Iteration Number: 2947\n",
      "Loss: 22.35248452779589\n",
      "l2 norm of gradients: 0.28493785925409626\n",
      "l2 norm of weights: 3.5642835097425176\n",
      "---------------------\n",
      "Iteration Number: 2948\n",
      "Loss: 22.344210687385072\n",
      "l2 norm of gradients: 0.28478829157919705\n",
      "l2 norm of weights: 3.5638367582759645\n",
      "---------------------\n",
      "Iteration Number: 2949\n",
      "Loss: 22.33594347729193\n",
      "l2 norm of gradients: 0.2846388106552627\n",
      "l2 norm of weights: 3.563390423656832\n",
      "---------------------\n",
      "Iteration Number: 2950\n",
      "Loss: 22.327682892105717\n",
      "l2 norm of gradients: 0.28448941648729414\n",
      "l2 norm of weights: 3.562944505642446\n",
      "---------------------\n",
      "Iteration Number: 2951\n",
      "Loss: 22.319428926418798\n",
      "l2 norm of gradients: 0.2843401090801361\n",
      "l2 norm of weights: 3.56249900398997\n",
      "---------------------\n",
      "Iteration Number: 2952\n",
      "Loss: 22.311181574826822\n",
      "l2 norm of gradients: 0.2841908884384772\n",
      "l2 norm of weights: 3.5620539184564084\n",
      "---------------------\n",
      "Iteration Number: 2953\n",
      "Loss: 22.30294083192846\n",
      "l2 norm of gradients: 0.2840417545668503\n",
      "l2 norm of weights: 3.5616092487986046\n",
      "---------------------\n",
      "Iteration Number: 2954\n",
      "Loss: 22.294706692325896\n",
      "l2 norm of gradients: 0.28389270746963324\n",
      "l2 norm of weights: 3.561164994773242\n",
      "---------------------\n",
      "Iteration Number: 2955\n",
      "Loss: 22.286479150624114\n",
      "l2 norm of gradients: 0.2837437471510487\n",
      "l2 norm of weights: 3.5607211561368453\n",
      "---------------------\n",
      "Iteration Number: 2956\n",
      "Loss: 22.27825820143159\n",
      "l2 norm of gradients: 0.28359487361516494\n",
      "l2 norm of weights: 3.560277732645778\n",
      "---------------------\n",
      "Iteration Number: 2957\n",
      "Loss: 22.270043839359925\n",
      "l2 norm of gradients: 0.2834460868658962\n",
      "l2 norm of weights: 3.5598347240562456\n",
      "---------------------\n",
      "Iteration Number: 2958\n",
      "Loss: 22.26183605902388\n",
      "l2 norm of gradients: 0.2832973869070026\n",
      "l2 norm of weights: 3.559392130124294\n",
      "---------------------\n",
      "Iteration Number: 2959\n",
      "Loss: 22.253634855041433\n",
      "l2 norm of gradients: 0.28314877374209146\n",
      "l2 norm of weights: 3.558949950605811\n",
      "---------------------\n",
      "Iteration Number: 2960\n",
      "Loss: 22.24544022203374\n",
      "l2 norm of gradients: 0.28300024737461643\n",
      "l2 norm of weights: 3.5585081852565255\n",
      "---------------------\n",
      "Iteration Number: 2961\n",
      "Loss: 22.237252154625132\n",
      "l2 norm of gradients: 0.2828518078078789\n",
      "l2 norm of weights: 3.558066833832008\n",
      "---------------------\n",
      "Iteration Number: 2962\n",
      "Loss: 22.22907064744314\n",
      "l2 norm of gradients: 0.2827034550450278\n",
      "l2 norm of weights: 3.5576258960876714\n",
      "---------------------\n",
      "Iteration Number: 2963\n",
      "Loss: 22.22089569511864\n",
      "l2 norm of gradients: 0.28255518908906035\n",
      "l2 norm of weights: 3.557185371778771\n",
      "---------------------\n",
      "Iteration Number: 2964\n",
      "Loss: 22.212727292285383\n",
      "l2 norm of gradients: 0.28240700994282214\n",
      "l2 norm of weights: 3.556745260660404\n",
      "---------------------\n",
      "Iteration Number: 2965\n",
      "Loss: 22.204565433580594\n",
      "l2 norm of gradients: 0.2822589176090076\n",
      "l2 norm of weights: 3.556305562487512\n",
      "---------------------\n",
      "Iteration Number: 2966\n",
      "Loss: 22.196410113644756\n",
      "l2 norm of gradients: 0.2821109120901605\n",
      "l2 norm of weights: 3.5558662770148777\n",
      "---------------------\n",
      "Iteration Number: 2967\n",
      "Loss: 22.188261327121133\n",
      "l2 norm of gradients: 0.281962993388674\n",
      "l2 norm of weights: 3.5554274039971294\n",
      "---------------------\n",
      "Iteration Number: 2968\n",
      "Loss: 22.18011906865652\n",
      "l2 norm of gradients: 0.2818151615067915\n",
      "l2 norm of weights: 3.5549889431887363\n",
      "---------------------\n",
      "Iteration Number: 2969\n",
      "Loss: 22.171983332900886\n",
      "l2 norm of gradients: 0.2816674164466065\n",
      "l2 norm of weights: 3.5545508943440147\n",
      "---------------------\n",
      "Iteration Number: 2970\n",
      "Loss: 22.163854114507174\n",
      "l2 norm of gradients: 0.28151975821006336\n",
      "l2 norm of weights: 3.5541132572171232\n",
      "---------------------\n",
      "Iteration Number: 2971\n",
      "Loss: 22.155731408131786\n",
      "l2 norm of gradients: 0.28137218679895765\n",
      "l2 norm of weights: 3.553676031562065\n",
      "---------------------\n",
      "Iteration Number: 2972\n",
      "Loss: 22.147615208434136\n",
      "l2 norm of gradients: 0.2812247022149362\n",
      "l2 norm of weights: 3.553239217132689\n",
      "---------------------\n",
      "Iteration Number: 2973\n",
      "Loss: 22.13950551007687\n",
      "l2 norm of gradients: 0.2810773044594976\n",
      "l2 norm of weights: 3.5528028136826895\n",
      "---------------------\n",
      "Iteration Number: 2974\n",
      "Loss: 22.131402307725875\n",
      "l2 norm of gradients: 0.2809299935339932\n",
      "l2 norm of weights: 3.5523668209656045\n",
      "---------------------\n",
      "Iteration Number: 2975\n",
      "Loss: 22.1233055960501\n",
      "l2 norm of gradients: 0.28078276943962643\n",
      "l2 norm of weights: 3.5519312387348196\n",
      "---------------------\n",
      "Iteration Number: 2976\n",
      "Loss: 22.11521536972183\n",
      "l2 norm of gradients: 0.28063563217745363\n",
      "l2 norm of weights: 3.551496066743565\n",
      "---------------------\n",
      "Iteration Number: 2977\n",
      "Loss: 22.10713162341648\n",
      "l2 norm of gradients: 0.2804885817483849\n",
      "l2 norm of weights: 3.551061304744919\n",
      "---------------------\n",
      "Iteration Number: 2978\n",
      "Loss: 22.09905435181251\n",
      "l2 norm of gradients: 0.28034161815318376\n",
      "l2 norm of weights: 3.5506269524918044\n",
      "---------------------\n",
      "Iteration Number: 2979\n",
      "Loss: 22.090983549591787\n",
      "l2 norm of gradients: 0.28019474139246786\n",
      "l2 norm of weights: 3.550193009736992\n",
      "---------------------\n",
      "Iteration Number: 2980\n",
      "Loss: 22.082919211439254\n",
      "l2 norm of gradients: 0.2800479514667094\n",
      "l2 norm of weights: 3.5497594762331004\n",
      "---------------------\n",
      "Iteration Number: 2981\n",
      "Loss: 22.074861332043053\n",
      "l2 norm of gradients: 0.27990124837623526\n",
      "l2 norm of weights: 3.5493263517325953\n",
      "---------------------\n",
      "Iteration Number: 2982\n",
      "Loss: 22.066809906094438\n",
      "l2 norm of gradients: 0.2797546321212275\n",
      "l2 norm of weights: 3.5488936359877896\n",
      "---------------------\n",
      "Iteration Number: 2983\n",
      "Loss: 22.058764928288035\n",
      "l2 norm of gradients: 0.279608102701724\n",
      "l2 norm of weights: 3.5484613287508453\n",
      "---------------------\n",
      "Iteration Number: 2984\n",
      "Loss: 22.0507263933214\n",
      "l2 norm of gradients: 0.2794616601176181\n",
      "l2 norm of weights: 3.548029429773773\n",
      "---------------------\n",
      "Iteration Number: 2985\n",
      "Loss: 22.04269429589547\n",
      "l2 norm of gradients: 0.27931530436866003\n",
      "l2 norm of weights: 3.5475979388084316\n",
      "---------------------\n",
      "Iteration Number: 2986\n",
      "Loss: 22.034668630714208\n",
      "l2 norm of gradients: 0.2791690354544562\n",
      "l2 norm of weights: 3.5471668556065294\n",
      "---------------------\n",
      "Iteration Number: 2987\n",
      "Loss: 22.026649392485016\n",
      "l2 norm of gradients: 0.2790228533744703\n",
      "l2 norm of weights: 3.546736179919624\n",
      "---------------------\n",
      "Iteration Number: 2988\n",
      "Loss: 22.01863657591802\n",
      "l2 norm of gradients: 0.2788767581280234\n",
      "l2 norm of weights: 3.546305911499124\n",
      "---------------------\n",
      "Iteration Number: 2989\n",
      "Loss: 22.01063017572695\n",
      "l2 norm of gradients: 0.27873074971429435\n",
      "l2 norm of weights: 3.5458760500962856\n",
      "---------------------\n",
      "Iteration Number: 2990\n",
      "Loss: 22.002630186628647\n",
      "l2 norm of gradients: 0.27858482813232005\n",
      "l2 norm of weights: 3.5454465954622187\n",
      "---------------------\n",
      "Iteration Number: 2991\n",
      "Loss: 21.994636603342823\n",
      "l2 norm of gradients: 0.27843899338099604\n",
      "l2 norm of weights: 3.545017547347882\n",
      "---------------------\n",
      "Iteration Number: 2992\n",
      "Loss: 21.98664942059279\n",
      "l2 norm of gradients: 0.2782932454590767\n",
      "l2 norm of weights: 3.544588905504086\n",
      "---------------------\n",
      "Iteration Number: 2993\n",
      "Loss: 21.97866863310466\n",
      "l2 norm of gradients: 0.27814758436517556\n",
      "l2 norm of weights: 3.5441606696814922\n",
      "---------------------\n",
      "Iteration Number: 2994\n",
      "Loss: 21.970694235607972\n",
      "l2 norm of gradients: 0.2780020100977659\n",
      "l2 norm of weights: 3.5437328396306147\n",
      "---------------------\n",
      "Iteration Number: 2995\n",
      "Loss: 21.96272622283534\n",
      "l2 norm of gradients: 0.2778565226551808\n",
      "l2 norm of weights: 3.54330541510182\n",
      "---------------------\n",
      "Iteration Number: 2996\n",
      "Loss: 21.95476458952255\n",
      "l2 norm of gradients: 0.27771112203561393\n",
      "l2 norm of weights: 3.542878395845327\n",
      "---------------------\n",
      "Iteration Number: 2997\n",
      "Loss: 21.946809330408563\n",
      "l2 norm of gradients: 0.27756580823711946\n",
      "l2 norm of weights: 3.5424517816112067\n",
      "---------------------\n",
      "Iteration Number: 2998\n",
      "Loss: 21.938860440235565\n",
      "l2 norm of gradients: 0.27742058125761265\n",
      "l2 norm of weights: 3.542025572149384\n",
      "---------------------\n",
      "Iteration Number: 2999\n",
      "Loss: 21.93091791374875\n",
      "l2 norm of gradients: 0.27727544109487023\n",
      "l2 norm of weights: 3.541599767209638\n",
      "---------------------\n",
      "Iteration Number: 3000\n",
      "Loss: 21.922981745696717\n",
      "l2 norm of gradients: 0.27713038774653076\n",
      "l2 norm of weights: 3.541174366541602\n",
      "---------------------\n",
      "Iteration Number: 3001\n",
      "Loss: 21.915051930831037\n",
      "l2 norm of gradients: 0.27698542121009495\n",
      "l2 norm of weights: 3.5407493698947614\n",
      "---------------------\n",
      "Iteration Number: 3002\n",
      "Loss: 21.90712846390646\n",
      "l2 norm of gradients: 0.27684054148292586\n",
      "l2 norm of weights: 3.54032477701846\n",
      "---------------------\n",
      "Iteration Number: 3003\n",
      "Loss: 21.89921133968103\n",
      "l2 norm of gradients: 0.2766957485622497\n",
      "l2 norm of weights: 3.5399005876618928\n",
      "---------------------\n",
      "Iteration Number: 3004\n",
      "Loss: 21.89130055291598\n",
      "l2 norm of gradients: 0.2765510424451557\n",
      "l2 norm of weights: 3.5394768015741143\n",
      "---------------------\n",
      "Iteration Number: 3005\n",
      "Loss: 21.883396098375368\n",
      "l2 norm of gradients: 0.27640642312859687\n",
      "l2 norm of weights: 3.5390534185040314\n",
      "---------------------\n",
      "Iteration Number: 3006\n",
      "Loss: 21.87549797082691\n",
      "l2 norm of gradients: 0.27626189060939\n",
      "l2 norm of weights: 3.5386304382004097\n",
      "---------------------\n",
      "Iteration Number: 3007\n",
      "Loss: 21.86760616504109\n",
      "l2 norm of gradients: 0.2761174448842163\n",
      "l2 norm of weights: 3.5382078604118705\n",
      "---------------------\n",
      "Iteration Number: 3008\n",
      "Loss: 21.859720675791777\n",
      "l2 norm of gradients: 0.2759730859496216\n",
      "l2 norm of weights: 3.537785684886892\n",
      "---------------------\n",
      "Iteration Number: 3009\n",
      "Loss: 21.851841497855794\n",
      "l2 norm of gradients: 0.2758288138020168\n",
      "l2 norm of weights: 3.5373639113738102\n",
      "---------------------\n",
      "Iteration Number: 3010\n",
      "Loss: 21.843968626013442\n",
      "l2 norm of gradients: 0.27568462843767816\n",
      "l2 norm of weights: 3.5369425396208194\n",
      "---------------------\n",
      "Iteration Number: 3011\n",
      "Loss: 21.836102055047707\n",
      "l2 norm of gradients: 0.27554052985274785\n",
      "l2 norm of weights: 3.53652156937597\n",
      "---------------------\n",
      "Iteration Number: 3012\n",
      "Loss: 21.828241779745316\n",
      "l2 norm of gradients: 0.2753965180432338\n",
      "l2 norm of weights: 3.5361010003871742\n",
      "---------------------\n",
      "Iteration Number: 3013\n",
      "Loss: 21.820387794895655\n",
      "l2 norm of gradients: 0.2752525930050107\n",
      "l2 norm of weights: 3.5356808324022007\n",
      "---------------------\n",
      "Iteration Number: 3014\n",
      "Loss: 21.812540095291542\n",
      "l2 norm of gradients: 0.27510875473382007\n",
      "l2 norm of weights: 3.5352610651686787\n",
      "---------------------\n",
      "Iteration Number: 3015\n",
      "Loss: 21.80469867572887\n",
      "l2 norm of gradients: 0.2749650032252703\n",
      "l2 norm of weights: 3.5348416984340973\n",
      "---------------------\n",
      "Iteration Number: 3016\n",
      "Loss: 21.7968635310066\n",
      "l2 norm of gradients: 0.2748213384748376\n",
      "l2 norm of weights: 3.534422731945805\n",
      "---------------------\n",
      "Iteration Number: 3017\n",
      "Loss: 21.789034655927026\n",
      "l2 norm of gradients: 0.2746777604778657\n",
      "l2 norm of weights: 3.534004165451012\n",
      "---------------------\n",
      "Iteration Number: 3018\n",
      "Loss: 21.781212045295447\n",
      "l2 norm of gradients: 0.27453426922956703\n",
      "l2 norm of weights: 3.5335859986967892\n",
      "---------------------\n",
      "Iteration Number: 3019\n",
      "Loss: 21.773395693920325\n",
      "l2 norm of gradients: 0.27439086472502233\n",
      "l2 norm of weights: 3.5331682314300683\n",
      "---------------------\n",
      "Iteration Number: 3020\n",
      "Loss: 21.76558559661342\n",
      "l2 norm of gradients: 0.2742475469591812\n",
      "l2 norm of weights: 3.5327508633976437\n",
      "---------------------\n",
      "Iteration Number: 3021\n",
      "Loss: 21.757781748189434\n",
      "l2 norm of gradients: 0.2741043159268626\n",
      "l2 norm of weights: 3.532333894346172\n",
      "---------------------\n",
      "Iteration Number: 3022\n",
      "Loss: 21.749984143466254\n",
      "l2 norm of gradients: 0.2739611716227551\n",
      "l2 norm of weights: 3.5319173240221726\n",
      "---------------------\n",
      "Iteration Number: 3023\n",
      "Loss: 21.74219277726512\n",
      "l2 norm of gradients: 0.27381811404141726\n",
      "l2 norm of weights: 3.5315011521720274\n",
      "---------------------\n",
      "Iteration Number: 3024\n",
      "Loss: 21.734407644410307\n",
      "l2 norm of gradients: 0.2736751431772779\n",
      "l2 norm of weights: 3.531085378541983\n",
      "---------------------\n",
      "Iteration Number: 3025\n",
      "Loss: 21.726628739728945\n",
      "l2 norm of gradients: 0.2735322590246368\n",
      "l2 norm of weights: 3.530670002878149\n",
      "---------------------\n",
      "Iteration Number: 3026\n",
      "Loss: 21.718856058051806\n",
      "l2 norm of gradients: 0.2733894615776642\n",
      "l2 norm of weights: 3.5302550249265003\n",
      "---------------------\n",
      "Iteration Number: 3027\n",
      "Loss: 21.71108959421243\n",
      "l2 norm of gradients: 0.27324675083040223\n",
      "l2 norm of weights: 3.5298404444328755\n",
      "---------------------\n",
      "Iteration Number: 3028\n",
      "Loss: 21.703329343047628\n",
      "l2 norm of gradients: 0.2731041267767645\n",
      "l2 norm of weights: 3.5294262611429805\n",
      "---------------------\n",
      "Iteration Number: 3029\n",
      "Loss: 21.6955752993974\n",
      "l2 norm of gradients: 0.27296158941053644\n",
      "l2 norm of weights: 3.5290124748023852\n",
      "---------------------\n",
      "Iteration Number: 3030\n",
      "Loss: 21.687827458104817\n",
      "l2 norm of gradients: 0.27281913872537633\n",
      "l2 norm of weights: 3.528599085156526\n",
      "---------------------\n",
      "Iteration Number: 3031\n",
      "Loss: 21.680085814016167\n",
      "l2 norm of gradients: 0.27267677471481483\n",
      "l2 norm of weights: 3.528186091950707\n",
      "---------------------\n",
      "Iteration Number: 3032\n",
      "Loss: 21.67235036198066\n",
      "l2 norm of gradients: 0.2725344973722559\n",
      "l2 norm of weights: 3.527773494930099\n",
      "---------------------\n",
      "Iteration Number: 3033\n",
      "Loss: 21.66462109685092\n",
      "l2 norm of gradients: 0.2723923066909767\n",
      "l2 norm of weights: 3.5273612938397396\n",
      "---------------------\n",
      "Iteration Number: 3034\n",
      "Loss: 21.656898013482603\n",
      "l2 norm of gradients: 0.27225020266412836\n",
      "l2 norm of weights: 3.526949488424536\n",
      "---------------------\n",
      "Iteration Number: 3035\n",
      "Loss: 21.64918110673443\n",
      "l2 norm of gradients: 0.27210818528473607\n",
      "l2 norm of weights: 3.526538078429262\n",
      "---------------------\n",
      "Iteration Number: 3036\n",
      "Loss: 21.6414703714683\n",
      "l2 norm of gradients: 0.2719662545456992\n",
      "l2 norm of weights: 3.5261270635985626\n",
      "---------------------\n",
      "Iteration Number: 3037\n",
      "Loss: 21.633765802549217\n",
      "l2 norm of gradients: 0.2718244104397922\n",
      "l2 norm of weights: 3.5257164436769495\n",
      "---------------------\n",
      "Iteration Number: 3038\n",
      "Loss: 21.62606739484548\n",
      "l2 norm of gradients: 0.27168265295966454\n",
      "l2 norm of weights: 3.525306218408807\n",
      "---------------------\n",
      "Iteration Number: 3039\n",
      "Loss: 21.618375143228302\n",
      "l2 norm of gradients: 0.271540982097841\n",
      "l2 norm of weights: 3.5248963875383876\n",
      "---------------------\n",
      "Iteration Number: 3040\n",
      "Loss: 21.610689042572048\n",
      "l2 norm of gradients: 0.27139939784672235\n",
      "l2 norm of weights: 3.524486950809817\n",
      "---------------------\n",
      "Iteration Number: 3041\n",
      "Loss: 21.603009087754373\n",
      "l2 norm of gradients: 0.2712579001985853\n",
      "l2 norm of weights: 3.5240779079670905\n",
      "---------------------\n",
      "Iteration Number: 3042\n",
      "Loss: 21.595335273655976\n",
      "l2 norm of gradients: 0.2711164891455832\n",
      "l2 norm of weights: 3.5236692587540754\n",
      "---------------------\n",
      "Iteration Number: 3043\n",
      "Loss: 21.587667595160557\n",
      "l2 norm of gradients: 0.2709751646797459\n",
      "l2 norm of weights: 3.523261002914511\n",
      "---------------------\n",
      "Iteration Number: 3044\n",
      "Loss: 21.580006047155095\n",
      "l2 norm of gradients: 0.2708339267929806\n",
      "l2 norm of weights: 3.522853140192012\n",
      "---------------------\n",
      "Iteration Number: 3045\n",
      "Loss: 21.572350624529726\n",
      "l2 norm of gradients: 0.2706927754770718\n",
      "l2 norm of weights: 3.5224456703300615\n",
      "---------------------\n",
      "Iteration Number: 3046\n",
      "Loss: 21.56470132217756\n",
      "l2 norm of gradients: 0.27055171072368195\n",
      "l2 norm of weights: 3.522038593072021\n",
      "---------------------\n",
      "Iteration Number: 3047\n",
      "Loss: 21.55705813499493\n",
      "l2 norm of gradients: 0.2704107325243515\n",
      "l2 norm of weights: 3.521631908161124\n",
      "---------------------\n",
      "Iteration Number: 3048\n",
      "Loss: 21.549421057881208\n",
      "l2 norm of gradients: 0.27026984087049927\n",
      "l2 norm of weights: 3.5212256153404793\n",
      "---------------------\n",
      "Iteration Number: 3049\n",
      "Loss: 21.541790085738988\n",
      "l2 norm of gradients: 0.2701290357534229\n",
      "l2 norm of weights: 3.520819714353071\n",
      "---------------------\n",
      "Iteration Number: 3050\n",
      "Loss: 21.534165213473965\n",
      "l2 norm of gradients: 0.2699883171642991\n",
      "l2 norm of weights: 3.520414204941758\n",
      "---------------------\n",
      "Iteration Number: 3051\n",
      "Loss: 21.526546435994767\n",
      "l2 norm of gradients: 0.26984768509418405\n",
      "l2 norm of weights: 3.520009086849278\n",
      "---------------------\n",
      "Iteration Number: 3052\n",
      "Loss: 21.518933748213463\n",
      "l2 norm of gradients: 0.2697071395340137\n",
      "l2 norm of weights: 3.5196043598182416\n",
      "---------------------\n",
      "Iteration Number: 3053\n",
      "Loss: 21.511327145044994\n",
      "l2 norm of gradients: 0.2695666804746039\n",
      "l2 norm of weights: 3.5192000235911403\n",
      "---------------------\n",
      "Iteration Number: 3054\n",
      "Loss: 21.503726621407466\n",
      "l2 norm of gradients: 0.2694263079066509\n",
      "l2 norm of weights: 3.5187960779103413\n",
      "---------------------\n",
      "Iteration Number: 3055\n",
      "Loss: 21.496132172222165\n",
      "l2 norm of gradients: 0.26928602182073186\n",
      "l2 norm of weights: 3.5183925225180914\n",
      "---------------------\n",
      "Iteration Number: 3056\n",
      "Loss: 21.488543792413367\n",
      "l2 norm of gradients: 0.26914582220730493\n",
      "l2 norm of weights: 3.5179893571565146\n",
      "---------------------\n",
      "Iteration Number: 3057\n",
      "Loss: 21.480961476908686\n",
      "l2 norm of gradients: 0.2690057090567093\n",
      "l2 norm of weights: 3.517586581567616\n",
      "---------------------\n",
      "Iteration Number: 3058\n",
      "Loss: 21.47338522063852\n",
      "l2 norm of gradients: 0.26886568235916625\n",
      "l2 norm of weights: 3.517184195493279\n",
      "---------------------\n",
      "Iteration Number: 3059\n",
      "Loss: 21.465815018536624\n",
      "l2 norm of gradients: 0.26872574210477873\n",
      "l2 norm of weights: 3.5167821986752696\n",
      "---------------------\n",
      "Iteration Number: 3060\n",
      "Loss: 21.458250865539796\n",
      "l2 norm of gradients: 0.26858588828353225\n",
      "l2 norm of weights: 3.5163805908552317\n",
      "---------------------\n",
      "Iteration Number: 3061\n",
      "Loss: 21.450692756587873\n",
      "l2 norm of gradients: 0.2684461208852947\n",
      "l2 norm of weights: 3.515979371774693\n",
      "---------------------\n",
      "Iteration Number: 3062\n",
      "Loss: 21.443140686623956\n",
      "l2 norm of gradients: 0.26830643989981706\n",
      "l2 norm of weights: 3.5155785411750626\n",
      "---------------------\n",
      "Iteration Number: 3063\n",
      "Loss: 21.435594650594126\n",
      "l2 norm of gradients: 0.26816684531673357\n",
      "l2 norm of weights: 3.5151780987976315\n",
      "---------------------\n",
      "Iteration Number: 3064\n",
      "Loss: 21.42805464344748\n",
      "l2 norm of gradients: 0.2680273371255621\n",
      "l2 norm of weights: 3.5147780443835748\n",
      "---------------------\n",
      "Iteration Number: 3065\n",
      "Loss: 21.42052066013649\n",
      "l2 norm of gradients: 0.2678879153157041\n",
      "l2 norm of weights: 3.5143783776739506\n",
      "---------------------\n",
      "Iteration Number: 3066\n",
      "Loss: 21.412992695616513\n",
      "l2 norm of gradients: 0.26774857987644557\n",
      "l2 norm of weights: 3.5139790984097004\n",
      "---------------------\n",
      "Iteration Number: 3067\n",
      "Loss: 21.4054707448461\n",
      "l2 norm of gradients: 0.2676093307969568\n",
      "l2 norm of weights: 3.513580206331653\n",
      "---------------------\n",
      "Iteration Number: 3068\n",
      "Loss: 21.39795480278673\n",
      "l2 norm of gradients: 0.26747016806629303\n",
      "l2 norm of weights: 3.513181701180519\n",
      "---------------------\n",
      "Iteration Number: 3069\n",
      "Loss: 21.390444864403225\n",
      "l2 norm of gradients: 0.2673310916733947\n",
      "l2 norm of weights: 3.5127835826968967\n",
      "---------------------\n",
      "Iteration Number: 3070\n",
      "Loss: 21.382940924663437\n",
      "l2 norm of gradients: 0.2671921016070873\n",
      "l2 norm of weights: 3.5123858506212713\n",
      "---------------------\n",
      "Iteration Number: 3071\n",
      "Loss: 21.375442978538235\n",
      "l2 norm of gradients: 0.2670531978560823\n",
      "l2 norm of weights: 3.5119885046940142\n",
      "---------------------\n",
      "Iteration Number: 3072\n",
      "Loss: 21.367951021001545\n",
      "l2 norm of gradients: 0.26691438040897747\n",
      "l2 norm of weights: 3.511591544655384\n",
      "---------------------\n",
      "Iteration Number: 3073\n",
      "Loss: 21.360465047030555\n",
      "l2 norm of gradients: 0.2667756492542565\n",
      "l2 norm of weights: 3.5111949702455276\n",
      "---------------------\n",
      "Iteration Number: 3074\n",
      "Loss: 21.352985051605444\n",
      "l2 norm of gradients: 0.2666370043802901\n",
      "l2 norm of weights: 3.510798781204481\n",
      "---------------------\n",
      "Iteration Number: 3075\n",
      "Loss: 21.345511029709513\n",
      "l2 norm of gradients: 0.2664984457753357\n",
      "l2 norm of weights: 3.510402977272168\n",
      "---------------------\n",
      "Iteration Number: 3076\n",
      "Loss: 21.33804297632909\n",
      "l2 norm of gradients: 0.2663599734275382\n",
      "l2 norm of weights: 3.510007558188404\n",
      "---------------------\n",
      "Iteration Number: 3077\n",
      "Loss: 21.33058088645363\n",
      "l2 norm of gradients: 0.2662215873249298\n",
      "l2 norm of weights: 3.5096125236928932\n",
      "---------------------\n",
      "Iteration Number: 3078\n",
      "Loss: 21.32312475507573\n",
      "l2 norm of gradients: 0.26608328745543075\n",
      "l2 norm of weights: 3.509217873525232\n",
      "---------------------\n",
      "Iteration Number: 3079\n",
      "Loss: 21.315674577190993\n",
      "l2 norm of gradients: 0.26594507380684956\n",
      "l2 norm of weights: 3.5088236074249077\n",
      "---------------------\n",
      "Iteration Number: 3080\n",
      "Loss: 21.30823034779823\n",
      "l2 norm of gradients: 0.265806946366883\n",
      "l2 norm of weights: 3.5084297251312995\n",
      "---------------------\n",
      "Iteration Number: 3081\n",
      "Loss: 21.300792061899156\n",
      "l2 norm of gradients: 0.26566890512311675\n",
      "l2 norm of weights: 3.5080362263836786\n",
      "---------------------\n",
      "Iteration Number: 3082\n",
      "Loss: 21.293359714498827\n",
      "l2 norm of gradients: 0.2655309500630255\n",
      "l2 norm of weights: 3.507643110921211\n",
      "---------------------\n",
      "Iteration Number: 3083\n",
      "Loss: 21.285933300605045\n",
      "l2 norm of gradients: 0.26539308117397326\n",
      "l2 norm of weights: 3.5072503784829556\n",
      "---------------------\n",
      "Iteration Number: 3084\n",
      "Loss: 21.27851281522892\n",
      "l2 norm of gradients: 0.2652552984432138\n",
      "l2 norm of weights: 3.5068580288078666\n",
      "---------------------\n",
      "Iteration Number: 3085\n",
      "Loss: 21.271098253384704\n",
      "l2 norm of gradients: 0.2651176018578908\n",
      "l2 norm of weights: 3.506466061634791\n",
      "---------------------\n",
      "Iteration Number: 3086\n",
      "Loss: 21.26368961008956\n",
      "l2 norm of gradients: 0.26497999140503814\n",
      "l2 norm of weights: 3.506074476702474\n",
      "---------------------\n",
      "Iteration Number: 3087\n",
      "Loss: 21.25628688036384\n",
      "l2 norm of gradients: 0.2648424670715802\n",
      "l2 norm of weights: 3.5056832737495554\n",
      "---------------------\n",
      "Iteration Number: 3088\n",
      "Loss: 21.24889005923092\n",
      "l2 norm of gradients: 0.26470502884433234\n",
      "l2 norm of weights: 3.505292452514573\n",
      "---------------------\n",
      "Iteration Number: 3089\n",
      "Loss: 21.241499141717192\n",
      "l2 norm of gradients: 0.2645676767100009\n",
      "l2 norm of weights: 3.504902012735961\n",
      "---------------------\n",
      "Iteration Number: 3090\n",
      "Loss: 21.23411412285234\n",
      "l2 norm of gradients: 0.26443041065518363\n",
      "l2 norm of weights: 3.504511954152053\n",
      "---------------------\n",
      "Iteration Number: 3091\n",
      "Loss: 21.22673499766887\n",
      "l2 norm of gradients: 0.26429323066637017\n",
      "l2 norm of weights: 3.5041222765010787\n",
      "---------------------\n",
      "Iteration Number: 3092\n",
      "Loss: 21.219361761202574\n",
      "l2 norm of gradients: 0.2641561367299417\n",
      "l2 norm of weights: 3.5037329795211694\n",
      "---------------------\n",
      "Iteration Number: 3093\n",
      "Loss: 21.211994408492195\n",
      "l2 norm of gradients: 0.26401912883217227\n",
      "l2 norm of weights: 3.5033440629503554\n",
      "---------------------\n",
      "Iteration Number: 3094\n",
      "Loss: 21.204632934579543\n",
      "l2 norm of gradients: 0.263882206959228\n",
      "l2 norm of weights: 3.502955526526568\n",
      "---------------------\n",
      "Iteration Number: 3095\n",
      "Loss: 21.19727733450963\n",
      "l2 norm of gradients: 0.2637453710971679\n",
      "l2 norm of weights: 3.5025673699876396\n",
      "---------------------\n",
      "Iteration Number: 3096\n",
      "Loss: 21.189927603330375\n",
      "l2 norm of gradients: 0.26360862123194434\n",
      "l2 norm of weights: 3.5021795930713027\n",
      "---------------------\n",
      "Iteration Number: 3097\n",
      "Loss: 21.182583736092845\n",
      "l2 norm of gradients: 0.26347195734940293\n",
      "l2 norm of weights: 3.5017921955151947\n",
      "---------------------\n",
      "Iteration Number: 3098\n",
      "Loss: 21.175245727851216\n",
      "l2 norm of gradients: 0.263335379435283\n",
      "l2 norm of weights: 3.5014051770568537\n",
      "---------------------\n",
      "Iteration Number: 3099\n",
      "Loss: 21.167913573662695\n",
      "l2 norm of gradients: 0.26319888747521786\n",
      "l2 norm of weights: 3.501018537433723\n",
      "---------------------\n",
      "Iteration Number: 3100\n",
      "Loss: 21.160587268587467\n",
      "l2 norm of gradients: 0.263062481454735\n",
      "l2 norm of weights: 3.50063227638315\n",
      "---------------------\n",
      "Iteration Number: 3101\n",
      "Loss: 21.153266807688915\n",
      "l2 norm of gradients: 0.2629261613592564\n",
      "l2 norm of weights: 3.500246393642386\n",
      "---------------------\n",
      "Iteration Number: 3102\n",
      "Loss: 21.145952186033437\n",
      "l2 norm of gradients: 0.26278992717409894\n",
      "l2 norm of weights: 3.499860888948589\n",
      "---------------------\n",
      "Iteration Number: 3103\n",
      "Loss: 21.13864339869056\n",
      "l2 norm of gradients: 0.26265377888447455\n",
      "l2 norm of weights: 3.4994757620388213\n",
      "---------------------\n",
      "Iteration Number: 3104\n",
      "Loss: 21.131340440732767\n",
      "l2 norm of gradients: 0.26251771647549055\n",
      "l2 norm of weights: 3.4990910126500543\n",
      "---------------------\n",
      "Iteration Number: 3105\n",
      "Loss: 21.12404330723566\n",
      "l2 norm of gradients: 0.2623817399321496\n",
      "l2 norm of weights: 3.4987066405191665\n",
      "---------------------\n",
      "Iteration Number: 3106\n",
      "Loss: 21.116751993277926\n",
      "l2 norm of gradients: 0.2622458492393507\n",
      "l2 norm of weights: 3.498322645382942\n",
      "---------------------\n",
      "Iteration Number: 3107\n",
      "Loss: 21.109466493941255\n",
      "l2 norm of gradients: 0.2621100443818886\n",
      "l2 norm of weights: 3.4979390269780772\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 3108\n",
      "Loss: 21.10218680431039\n",
      "l2 norm of gradients: 0.2619743253444549\n",
      "l2 norm of weights: 3.497555785041176\n",
      "---------------------\n",
      "Iteration Number: 3109\n",
      "Loss: 21.094912919473273\n",
      "l2 norm of gradients: 0.26183869211163757\n",
      "l2 norm of weights: 3.497172919308751\n",
      "---------------------\n",
      "Iteration Number: 3110\n",
      "Loss: 21.08764483452072\n",
      "l2 norm of gradients: 0.26170314466792166\n",
      "l2 norm of weights: 3.4967904295172287\n",
      "---------------------\n",
      "Iteration Number: 3111\n",
      "Loss: 21.080382544546666\n",
      "l2 norm of gradients: 0.2615676829976895\n",
      "l2 norm of weights: 3.4964083154029444\n",
      "---------------------\n",
      "Iteration Number: 3112\n",
      "Loss: 21.073126044648255\n",
      "l2 norm of gradients: 0.2614323070852209\n",
      "l2 norm of weights: 3.496026576702147\n",
      "---------------------\n",
      "Iteration Number: 3113\n",
      "Loss: 21.065875329925483\n",
      "l2 norm of gradients: 0.2612970169146935\n",
      "l2 norm of weights: 3.4956452131509974\n",
      "---------------------\n",
      "Iteration Number: 3114\n",
      "Loss: 21.05863039548138\n",
      "l2 norm of gradients: 0.2611618124701828\n",
      "l2 norm of weights: 3.495264224485569\n",
      "---------------------\n",
      "Iteration Number: 3115\n",
      "Loss: 21.0513912364223\n",
      "l2 norm of gradients: 0.26102669373566284\n",
      "l2 norm of weights: 3.4948836104418515\n",
      "---------------------\n",
      "Iteration Number: 3116\n",
      "Loss: 21.0441578478574\n",
      "l2 norm of gradients: 0.2608916606950061\n",
      "l2 norm of weights: 3.4945033707557474\n",
      "---------------------\n",
      "Iteration Number: 3117\n",
      "Loss: 21.03693022489899\n",
      "l2 norm of gradients: 0.260756713331984\n",
      "l2 norm of weights: 3.4941235051630746\n",
      "---------------------\n",
      "Iteration Number: 3118\n",
      "Loss: 21.029708362662387\n",
      "l2 norm of gradients: 0.260621851630267\n",
      "l2 norm of weights: 3.4937440133995676\n",
      "---------------------\n",
      "Iteration Number: 3119\n",
      "Loss: 21.022492256265934\n",
      "l2 norm of gradients: 0.2604870755734248\n",
      "l2 norm of weights: 3.493364895200878\n",
      "---------------------\n",
      "Iteration Number: 3120\n",
      "Loss: 21.01528190083125\n",
      "l2 norm of gradients: 0.2603523851449268\n",
      "l2 norm of weights: 3.4929861503025745\n",
      "---------------------\n",
      "Iteration Number: 3121\n",
      "Loss: 21.008077291482618\n",
      "l2 norm of gradients: 0.26021778032814236\n",
      "l2 norm of weights: 3.4926077784401435\n",
      "---------------------\n",
      "Iteration Number: 3122\n",
      "Loss: 21.000878423347682\n",
      "l2 norm of gradients: 0.2600832611063408\n",
      "l2 norm of weights: 3.4922297793489903\n",
      "---------------------\n",
      "Iteration Number: 3123\n",
      "Loss: 20.99368529155705\n",
      "l2 norm of gradients: 0.2599488274626919\n",
      "l2 norm of weights: 3.4918521527644404\n",
      "---------------------\n",
      "Iteration Number: 3124\n",
      "Loss: 20.986497891244372\n",
      "l2 norm of gradients: 0.25981447938026603\n",
      "l2 norm of weights: 3.491474898421738\n",
      "---------------------\n",
      "Iteration Number: 3125\n",
      "Loss: 20.979316217546295\n",
      "l2 norm of gradients: 0.2596802168420346\n",
      "l2 norm of weights: 3.4910980160560494\n",
      "---------------------\n",
      "Iteration Number: 3126\n",
      "Loss: 20.972140265602487\n",
      "l2 norm of gradients: 0.25954603983086993\n",
      "l2 norm of weights: 3.4907215054024614\n",
      "---------------------\n",
      "Iteration Number: 3127\n",
      "Loss: 20.964970030555886\n",
      "l2 norm of gradients: 0.25941194832954595\n",
      "l2 norm of weights: 3.4903453661959847\n",
      "---------------------\n",
      "Iteration Number: 3128\n",
      "Loss: 20.957805507552198\n",
      "l2 norm of gradients: 0.2592779423207381\n",
      "l2 norm of weights: 3.489969598171551\n",
      "---------------------\n",
      "Iteration Number: 3129\n",
      "Loss: 20.9506466917403\n",
      "l2 norm of gradients: 0.2591440217870238\n",
      "l2 norm of weights: 3.489594201064016\n",
      "---------------------\n",
      "Iteration Number: 3130\n",
      "Loss: 20.94349357827211\n",
      "l2 norm of gradients: 0.25901018671088266\n",
      "l2 norm of weights: 3.4892191746081616\n",
      "---------------------\n",
      "Iteration Number: 3131\n",
      "Loss: 20.936346162302545\n",
      "l2 norm of gradients: 0.2588764370746963\n",
      "l2 norm of weights: 3.488844518538692\n",
      "---------------------\n",
      "Iteration Number: 3132\n",
      "Loss: 20.92920443898971\n",
      "l2 norm of gradients: 0.25874277286074954\n",
      "l2 norm of weights: 3.488470232590238\n",
      "---------------------\n",
      "Iteration Number: 3133\n",
      "Loss: 20.922068403494514\n",
      "l2 norm of gradients: 0.2586091940512298\n",
      "l2 norm of weights: 3.4880963164973586\n",
      "---------------------\n",
      "Iteration Number: 3134\n",
      "Loss: 20.914938050981135\n",
      "l2 norm of gradients: 0.2584757006282275\n",
      "l2 norm of weights: 3.4877227699945377\n",
      "---------------------\n",
      "Iteration Number: 3135\n",
      "Loss: 20.90781337661658\n",
      "l2 norm of gradients: 0.2583422925737366\n",
      "l2 norm of weights: 3.4873495928161877\n",
      "---------------------\n",
      "Iteration Number: 3136\n",
      "Loss: 20.900694375571092\n",
      "l2 norm of gradients: 0.25820896986965475\n",
      "l2 norm of weights: 3.4869767846966506\n",
      "---------------------\n",
      "Iteration Number: 3137\n",
      "Loss: 20.8935810430178\n",
      "l2 norm of gradients: 0.25807573249778337\n",
      "l2 norm of weights: 3.4866043453701963\n",
      "---------------------\n",
      "Iteration Number: 3138\n",
      "Loss: 20.886473374132958\n",
      "l2 norm of gradients: 0.25794258043982793\n",
      "l2 norm of weights: 3.4862322745710244\n",
      "---------------------\n",
      "Iteration Number: 3139\n",
      "Loss: 20.879371364095825\n",
      "l2 norm of gradients: 0.2578095136773981\n",
      "l2 norm of weights: 3.485860572033267\n",
      "---------------------\n",
      "Iteration Number: 3140\n",
      "Loss: 20.8722750080887\n",
      "l2 norm of gradients: 0.25767653219200864\n",
      "l2 norm of weights: 3.485489237490987\n",
      "---------------------\n",
      "Iteration Number: 3141\n",
      "Loss: 20.865184301296928\n",
      "l2 norm of gradients: 0.2575436359650785\n",
      "l2 norm of weights: 3.4851182706781785\n",
      "---------------------\n",
      "Iteration Number: 3142\n",
      "Loss: 20.858099238908935\n",
      "l2 norm of gradients: 0.2574108249779321\n",
      "l2 norm of weights: 3.484747671328769\n",
      "---------------------\n",
      "Iteration Number: 3143\n",
      "Loss: 20.85101981611595\n",
      "l2 norm of gradients: 0.257278099211799\n",
      "l2 norm of weights: 3.48437743917662\n",
      "---------------------\n",
      "Iteration Number: 3144\n",
      "Loss: 20.843946028112565\n",
      "l2 norm of gradients: 0.2571454586478143\n",
      "l2 norm of weights: 3.4840075739555263\n",
      "---------------------\n",
      "Iteration Number: 3145\n",
      "Loss: 20.836877870096288\n",
      "l2 norm of gradients: 0.2570129032670187\n",
      "l2 norm of weights: 3.4836380753992198\n",
      "---------------------\n",
      "Iteration Number: 3146\n",
      "Loss: 20.829815337267462\n",
      "l2 norm of gradients: 0.25688043305035924\n",
      "l2 norm of weights: 3.483268943241366\n",
      "---------------------\n",
      "Iteration Number: 3147\n",
      "Loss: 20.822758424829804\n",
      "l2 norm of gradients: 0.2567480479786889\n",
      "l2 norm of weights: 3.4829001772155683\n",
      "---------------------\n",
      "Iteration Number: 3148\n",
      "Loss: 20.81570712798971\n",
      "l2 norm of gradients: 0.2566157480327672\n",
      "l2 norm of weights: 3.4825317770553665\n",
      "---------------------\n",
      "Iteration Number: 3149\n",
      "Loss: 20.80866144195687\n",
      "l2 norm of gradients: 0.2564835331932603\n",
      "l2 norm of weights: 3.4821637424942393\n",
      "---------------------\n",
      "Iteration Number: 3150\n",
      "Loss: 20.80162136194385\n",
      "l2 norm of gradients: 0.25635140344074125\n",
      "l2 norm of weights: 3.481796073265604\n",
      "---------------------\n",
      "Iteration Number: 3151\n",
      "Loss: 20.794586883166406\n",
      "l2 norm of gradients: 0.2562193587556904\n",
      "l2 norm of weights: 3.4814287691028154\n",
      "---------------------\n",
      "Iteration Number: 3152\n",
      "Loss: 20.787558000843138\n",
      "l2 norm of gradients: 0.25608739911849526\n",
      "l2 norm of weights: 3.4810618297391724\n",
      "---------------------\n",
      "Iteration Number: 3153\n",
      "Loss: 20.78053471019581\n",
      "l2 norm of gradients: 0.25595552450945114\n",
      "l2 norm of weights: 3.4806952549079115\n",
      "---------------------\n",
      "Iteration Number: 3154\n",
      "Loss: 20.773517006449197\n",
      "l2 norm of gradients: 0.2558237349087609\n",
      "l2 norm of weights: 3.4803290443422124\n",
      "---------------------\n",
      "Iteration Number: 3155\n",
      "Loss: 20.76650488483098\n",
      "l2 norm of gradients: 0.25569203029653564\n",
      "l2 norm of weights: 3.479963197775197\n",
      "---------------------\n",
      "Iteration Number: 3156\n",
      "Loss: 20.75949834057197\n",
      "l2 norm of gradients: 0.2555604106527946\n",
      "l2 norm of weights: 3.47959771493993\n",
      "---------------------\n",
      "Iteration Number: 3157\n",
      "Loss: 20.752497368906038\n",
      "l2 norm of gradients: 0.2554288759574657\n",
      "l2 norm of weights: 3.4792325955694214\n",
      "---------------------\n",
      "Iteration Number: 3158\n",
      "Loss: 20.745501965070044\n",
      "l2 norm of gradients: 0.2552974261903854\n",
      "l2 norm of weights: 3.478867839396625\n",
      "---------------------\n",
      "Iteration Number: 3159\n",
      "Loss: 20.738512124303796\n",
      "l2 norm of gradients: 0.2551660613312991\n",
      "l2 norm of weights: 3.478503446154439\n",
      "---------------------\n",
      "Iteration Number: 3160\n",
      "Loss: 20.731527841850184\n",
      "l2 norm of gradients: 0.25503478135986135\n",
      "l2 norm of weights: 3.478139415575711\n",
      "---------------------\n",
      "Iteration Number: 3161\n",
      "Loss: 20.724549112955263\n",
      "l2 norm of gradients: 0.2549035862556362\n",
      "l2 norm of weights: 3.4777757473932316\n",
      "---------------------\n",
      "Iteration Number: 3162\n",
      "Loss: 20.71757593286781\n",
      "l2 norm of gradients: 0.254772475998097\n",
      "l2 norm of weights: 3.4774124413397423\n",
      "---------------------\n",
      "Iteration Number: 3163\n",
      "Loss: 20.7106082968399\n",
      "l2 norm of gradients: 0.2546414505666272\n",
      "l2 norm of weights: 3.4770494971479318\n",
      "---------------------\n",
      "Iteration Number: 3164\n",
      "Loss: 20.70364620012656\n",
      "l2 norm of gradients: 0.25451050994052\n",
      "l2 norm of weights: 3.4766869145504384\n",
      "---------------------\n",
      "Iteration Number: 3165\n",
      "Loss: 20.69668963798563\n",
      "l2 norm of gradients: 0.25437965409897906\n",
      "l2 norm of weights: 3.476324693279851\n",
      "---------------------\n",
      "Iteration Number: 3166\n",
      "Loss: 20.68973860567835\n",
      "l2 norm of gradients: 0.2542488830211183\n",
      "l2 norm of weights: 3.4759628330687073\n",
      "---------------------\n",
      "Iteration Number: 3167\n",
      "Loss: 20.682793098468657\n",
      "l2 norm of gradients: 0.2541181966859626\n",
      "l2 norm of weights: 3.4756013336494993\n",
      "---------------------\n",
      "Iteration Number: 3168\n",
      "Loss: 20.675853111623614\n",
      "l2 norm of gradients: 0.2539875950724471\n",
      "l2 norm of weights: 3.4752401947546696\n",
      "---------------------\n",
      "Iteration Number: 3169\n",
      "Loss: 20.668918640413477\n",
      "l2 norm of gradients: 0.25385707815941866\n",
      "l2 norm of weights: 3.474879416116615\n",
      "---------------------\n",
      "Iteration Number: 3170\n",
      "Loss: 20.661989680111198\n",
      "l2 norm of gradients: 0.25372664592563515\n",
      "l2 norm of weights: 3.4745189974676847\n",
      "---------------------\n",
      "Iteration Number: 3171\n",
      "Loss: 20.65506622599301\n",
      "l2 norm of gradients: 0.25359629834976577\n",
      "l2 norm of weights: 3.4741589385401843\n",
      "---------------------\n",
      "Iteration Number: 3172\n",
      "Loss: 20.648148273338027\n",
      "l2 norm of gradients: 0.2534660354103918\n",
      "l2 norm of weights: 3.473799239066374\n",
      "---------------------\n",
      "Iteration Number: 3173\n",
      "Loss: 20.641235817428402\n",
      "l2 norm of gradients: 0.2533358570860062\n",
      "l2 norm of weights: 3.4734398987784707\n",
      "---------------------\n",
      "Iteration Number: 3174\n",
      "Loss: 20.63432885354946\n",
      "l2 norm of gradients: 0.253205763355014\n",
      "l2 norm of weights: 3.473080917408647\n",
      "---------------------\n",
      "Iteration Number: 3175\n",
      "Loss: 20.62742737698929\n",
      "l2 norm of gradients: 0.2530757541957327\n",
      "l2 norm of weights: 3.472722294689036\n",
      "---------------------\n",
      "Iteration Number: 3176\n",
      "Loss: 20.620531383039197\n",
      "l2 norm of gradients: 0.25294582958639217\n",
      "l2 norm of weights: 3.4723640303517262\n",
      "---------------------\n",
      "Iteration Number: 3177\n",
      "Loss: 20.613640866993407\n",
      "l2 norm of gradients: 0.25281598950513523\n",
      "l2 norm of weights: 3.4720061241287676\n",
      "---------------------\n",
      "Iteration Number: 3178\n",
      "Loss: 20.60675582414922\n",
      "l2 norm of gradients: 0.25268623393001743\n",
      "l2 norm of weights: 3.4716485757521696\n",
      "---------------------\n",
      "Iteration Number: 3179\n",
      "Loss: 20.599876249806854\n",
      "l2 norm of gradients: 0.25255656283900757\n",
      "l2 norm of weights: 3.471291384953903\n",
      "---------------------\n",
      "Iteration Number: 3180\n",
      "Loss: 20.593002139269682\n",
      "l2 norm of gradients: 0.2524269762099877\n",
      "l2 norm of weights: 3.4709345514659002\n",
      "---------------------\n",
      "Iteration Number: 3181\n",
      "Loss: 20.586133487843973\n",
      "l2 norm of gradients: 0.2522974740207534\n",
      "l2 norm of weights: 3.4705780750200557\n",
      "---------------------\n",
      "Iteration Number: 3182\n",
      "Loss: 20.57927029083915\n",
      "l2 norm of gradients: 0.2521680562490142\n",
      "l2 norm of weights: 3.470221955348227\n",
      "---------------------\n",
      "Iteration Number: 3183\n",
      "Loss: 20.572412543567488\n",
      "l2 norm of gradients: 0.2520387228723933\n",
      "l2 norm of weights: 3.469866192182238\n",
      "---------------------\n",
      "Iteration Number: 3184\n",
      "Loss: 20.56556024134431\n",
      "l2 norm of gradients: 0.25190947386842816\n",
      "l2 norm of weights: 3.469510785253874\n",
      "---------------------\n",
      "Iteration Number: 3185\n",
      "Loss: 20.55871337948813\n",
      "l2 norm of gradients: 0.25178030921457045\n",
      "l2 norm of weights: 3.469155734294889\n",
      "---------------------\n",
      "Iteration Number: 3186\n",
      "Loss: 20.55187195332027\n",
      "l2 norm of gradients: 0.2516512288881866\n",
      "l2 norm of weights: 3.4688010390370017\n",
      "---------------------\n",
      "Iteration Number: 3187\n",
      "Loss: 20.54503595816514\n",
      "l2 norm of gradients: 0.2515222328665575\n",
      "l2 norm of weights: 3.4684466992118987\n",
      "---------------------\n",
      "Iteration Number: 3188\n",
      "Loss: 20.53820538935022\n",
      "l2 norm of gradients: 0.2513933211268791\n",
      "l2 norm of weights: 3.4680927145512355\n",
      "---------------------\n",
      "Iteration Number: 3189\n",
      "Loss: 20.531380242205937\n",
      "l2 norm of gradients: 0.2512644936462625\n",
      "l2 norm of weights: 3.4677390847866345\n",
      "---------------------\n",
      "Iteration Number: 3190\n",
      "Loss: 20.524560512065662\n",
      "l2 norm of gradients: 0.25113575040173386\n",
      "l2 norm of weights: 3.46738580964969\n",
      "---------------------\n",
      "Iteration Number: 3191\n",
      "Loss: 20.517746194265946\n",
      "l2 norm of gradients: 0.251007091370235\n",
      "l2 norm of weights: 3.4670328888719655\n",
      "---------------------\n",
      "Iteration Number: 3192\n",
      "Loss: 20.510937284146316\n",
      "l2 norm of gradients: 0.25087851652862353\n",
      "l2 norm of weights: 3.4666803221849953\n",
      "---------------------\n",
      "Iteration Number: 3193\n",
      "Loss: 20.50413377704918\n",
      "l2 norm of gradients: 0.2507500258536727\n",
      "l2 norm of weights: 3.4663281093202873\n",
      "---------------------\n",
      "Iteration Number: 3194\n",
      "Loss: 20.497335668320023\n",
      "l2 norm of gradients: 0.2506216193220719\n",
      "l2 norm of weights: 3.465976250009322\n",
      "---------------------\n",
      "Iteration Number: 3195\n",
      "Loss: 20.49054295330757\n",
      "l2 norm of gradients: 0.25049329691042677\n",
      "l2 norm of weights: 3.465624743983552\n",
      "---------------------\n",
      "Iteration Number: 3196\n",
      "Loss: 20.483755627363124\n",
      "l2 norm of gradients: 0.25036505859525937\n",
      "l2 norm of weights: 3.4652735909744075\n",
      "---------------------\n",
      "Iteration Number: 3197\n",
      "Loss: 20.47697368584134\n",
      "l2 norm of gradients: 0.25023690435300855\n",
      "l2 norm of weights: 3.46492279071329\n",
      "---------------------\n",
      "Iteration Number: 3198\n",
      "Loss: 20.470197124099805\n",
      "l2 norm of gradients: 0.2501088341600296\n",
      "l2 norm of weights: 3.4645723429315805\n",
      "---------------------\n",
      "Iteration Number: 3199\n",
      "Loss: 20.46342593749906\n",
      "l2 norm of gradients: 0.24998084799259515\n",
      "l2 norm of weights: 3.4642222473606354\n",
      "---------------------\n",
      "Iteration Number: 3200\n",
      "Loss: 20.45666012140268\n",
      "l2 norm of gradients: 0.24985294582689477\n",
      "l2 norm of weights: 3.4638725037317895\n",
      "---------------------\n",
      "Iteration Number: 3201\n",
      "Loss: 20.44989967117726\n",
      "l2 norm of gradients: 0.24972512763903554\n",
      "l2 norm of weights: 3.463523111776356\n",
      "---------------------\n",
      "Iteration Number: 3202\n",
      "Loss: 20.44314458219243\n",
      "l2 norm of gradients: 0.2495973934050419\n",
      "l2 norm of weights: 3.4631740712256267\n",
      "---------------------\n",
      "Iteration Number: 3203\n",
      "Loss: 20.43639484982091\n",
      "l2 norm of gradients: 0.24946974310085623\n",
      "l2 norm of weights: 3.462825381810875\n",
      "---------------------\n",
      "Iteration Number: 3204\n",
      "Loss: 20.42965046943818\n",
      "l2 norm of gradients: 0.24934217670233863\n",
      "l2 norm of weights: 3.462477043263354\n",
      "---------------------\n",
      "Iteration Number: 3205\n",
      "Loss: 20.422911436422968\n",
      "l2 norm of gradients: 0.24921469418526743\n",
      "l2 norm of weights: 3.4621290553142994\n",
      "---------------------\n",
      "Iteration Number: 3206\n",
      "Loss: 20.416177746156883\n",
      "l2 norm of gradients: 0.24908729552533898\n",
      "l2 norm of weights: 3.4617814176949304\n",
      "---------------------\n",
      "Iteration Number: 3207\n",
      "Loss: 20.409449394024715\n",
      "l2 norm of gradients: 0.24895998069816824\n",
      "l2 norm of weights: 3.461434130136447\n",
      "---------------------\n",
      "Iteration Number: 3208\n",
      "Loss: 20.402726375414023\n",
      "l2 norm of gradients: 0.24883274967928884\n",
      "l2 norm of weights: 3.461087192370037\n",
      "---------------------\n",
      "Iteration Number: 3209\n",
      "Loss: 20.396008685715575\n",
      "l2 norm of gradients: 0.24870560244415307\n",
      "l2 norm of weights: 3.46074060412687\n",
      "---------------------\n",
      "Iteration Number: 3210\n",
      "Loss: 20.389296320322984\n",
      "l2 norm of gradients: 0.24857853896813223\n",
      "l2 norm of weights: 3.4603943651381037\n",
      "---------------------\n",
      "Iteration Number: 3211\n",
      "Loss: 20.38258927463312\n",
      "l2 norm of gradients: 0.24845155922651677\n",
      "l2 norm of weights: 3.460048475134882\n",
      "---------------------\n",
      "Iteration Number: 3212\n",
      "Loss: 20.37588754404563\n",
      "l2 norm of gradients: 0.24832466319451643\n",
      "l2 norm of weights: 3.459702933848336\n",
      "---------------------\n",
      "Iteration Number: 3213\n",
      "Loss: 20.369191123963176\n",
      "l2 norm of gradients: 0.24819785084726045\n",
      "l2 norm of weights: 3.4593577410095855\n",
      "---------------------\n",
      "Iteration Number: 3214\n",
      "Loss: 20.36250000979164\n",
      "l2 norm of gradients: 0.24807112215979776\n",
      "l2 norm of weights: 3.4590128963497397\n",
      "---------------------\n",
      "Iteration Number: 3215\n",
      "Loss: 20.355814196939747\n",
      "l2 norm of gradients: 0.247944477107097\n",
      "l2 norm of weights: 3.4586683995998975\n",
      "---------------------\n",
      "Iteration Number: 3216\n",
      "Loss: 20.349133680819243\n",
      "l2 norm of gradients: 0.24781791566404698\n",
      "l2 norm of weights: 3.458324250491149\n",
      "---------------------\n",
      "Iteration Number: 3217\n",
      "Loss: 20.342458456844952\n",
      "l2 norm of gradients: 0.24769143780545658\n",
      "l2 norm of weights: 3.457980448754576\n",
      "---------------------\n",
      "Iteration Number: 3218\n",
      "Loss: 20.335788520434694\n",
      "l2 norm of gradients: 0.2475650435060551\n",
      "l2 norm of weights: 3.4576369941212532\n",
      "---------------------\n",
      "Iteration Number: 3219\n",
      "Loss: 20.329123867009155\n",
      "l2 norm of gradients: 0.24743873274049227\n",
      "l2 norm of weights: 3.4572938863222467\n",
      "---------------------\n",
      "Iteration Number: 3220\n",
      "Loss: 20.32246449199227\n",
      "l2 norm of gradients: 0.24731250548333852\n",
      "l2 norm of weights: 3.4569511250886196\n",
      "---------------------\n",
      "Iteration Number: 3221\n",
      "Loss: 20.315810390810903\n",
      "l2 norm of gradients: 0.24718636170908512\n",
      "l2 norm of weights: 3.456608710151428\n",
      "---------------------\n",
      "Iteration Number: 3222\n",
      "Loss: 20.309161558894765\n",
      "l2 norm of gradients: 0.24706030139214444\n",
      "l2 norm of weights: 3.4562666412417253\n",
      "---------------------\n",
      "Iteration Number: 3223\n",
      "Loss: 20.302517991676865\n",
      "l2 norm of gradients: 0.24693432450684996\n",
      "l2 norm of weights: 3.45592491809056\n",
      "---------------------\n",
      "Iteration Number: 3224\n",
      "Loss: 20.295879684592965\n",
      "l2 norm of gradients: 0.2468084310274566\n",
      "l2 norm of weights: 3.4555835404289796\n",
      "---------------------\n",
      "Iteration Number: 3225\n",
      "Loss: 20.289246633082012\n",
      "l2 norm of gradients: 0.24668262092814056\n",
      "l2 norm of weights: 3.455242507988029\n",
      "---------------------\n",
      "Iteration Number: 3226\n",
      "Loss: 20.282618832585836\n",
      "l2 norm of gradients: 0.24655689418299986\n",
      "l2 norm of weights: 3.454901820498753\n",
      "---------------------\n",
      "Iteration Number: 3227\n",
      "Loss: 20.27599627854935\n",
      "l2 norm of gradients: 0.24643125076605446\n",
      "l2 norm of weights: 3.454561477692195\n",
      "---------------------\n",
      "Iteration Number: 3228\n",
      "Loss: 20.269378966420543\n",
      "l2 norm of gradients: 0.24630569065124622\n",
      "l2 norm of weights: 3.4542214792994015\n",
      "---------------------\n",
      "Iteration Number: 3229\n",
      "Loss: 20.262766891650283\n",
      "l2 norm of gradients: 0.24618021381243915\n",
      "l2 norm of weights: 3.453881825051419\n",
      "---------------------\n",
      "Iteration Number: 3230\n",
      "Loss: 20.25616004969252\n",
      "l2 norm of gradients: 0.24605482022341954\n",
      "l2 norm of weights: 3.4535425146792966\n",
      "---------------------\n",
      "Iteration Number: 3231\n",
      "Loss: 20.249558436004204\n",
      "l2 norm of gradients: 0.2459295098578963\n",
      "l2 norm of weights: 3.4532035479140877\n",
      "---------------------\n",
      "Iteration Number: 3232\n",
      "Loss: 20.242962046045314\n",
      "l2 norm of gradients: 0.24580428268950086\n",
      "l2 norm of weights: 3.4528649244868492\n",
      "---------------------\n",
      "Iteration Number: 3233\n",
      "Loss: 20.23637087527886\n",
      "l2 norm of gradients: 0.24567913869178754\n",
      "l2 norm of weights: 3.4525266441286426\n",
      "---------------------\n",
      "Iteration Number: 3234\n",
      "Loss: 20.229784919170807\n",
      "l2 norm of gradients: 0.24555407783823363\n",
      "l2 norm of weights: 3.4521887065705363\n",
      "---------------------\n",
      "Iteration Number: 3235\n",
      "Loss: 20.223204173190158\n",
      "l2 norm of gradients: 0.24542910010223937\n",
      "l2 norm of weights: 3.4518511115436046\n",
      "---------------------\n",
      "Iteration Number: 3236\n",
      "Loss: 20.216628632808856\n",
      "l2 norm of gradients: 0.24530420545712853\n",
      "l2 norm of weights: 3.45151385877893\n",
      "---------------------\n",
      "Iteration Number: 3237\n",
      "Loss: 20.21005829350214\n",
      "l2 norm of gradients: 0.24517939387614812\n",
      "l2 norm of weights: 3.451176948007603\n",
      "---------------------\n",
      "Iteration Number: 3238\n",
      "Loss: 20.20349315074789\n",
      "l2 norm of gradients: 0.24505466533246878\n",
      "l2 norm of weights: 3.4508403789607227\n",
      "---------------------\n",
      "Iteration Number: 3239\n",
      "Loss: 20.196933200027214\n",
      "l2 norm of gradients: 0.24493001979918497\n",
      "l2 norm of weights: 3.4505041513693993\n",
      "---------------------\n",
      "Iteration Number: 3240\n",
      "Loss: 20.190378436824172\n",
      "l2 norm of gradients: 0.24480545724931496\n",
      "l2 norm of weights: 3.4501682649647534\n",
      "---------------------\n",
      "Iteration Number: 3241\n",
      "Loss: 20.183828856625876\n",
      "l2 norm of gradients: 0.24468097765580119\n",
      "l2 norm of weights: 3.4498327194779175\n",
      "---------------------\n",
      "Iteration Number: 3242\n",
      "Loss: 20.17728445492241\n",
      "l2 norm of gradients: 0.24455658099151015\n",
      "l2 norm of weights: 3.4494975146400364\n",
      "---------------------\n",
      "Iteration Number: 3243\n",
      "Loss: 20.170745227206957\n",
      "l2 norm of gradients: 0.24443226722923272\n",
      "l2 norm of weights: 3.4491626501822688\n",
      "---------------------\n",
      "Iteration Number: 3244\n",
      "Loss: 20.164211168975566\n",
      "l2 norm of gradients: 0.2443080363416845\n",
      "l2 norm of weights: 3.448828125835787\n",
      "---------------------\n",
      "Iteration Number: 3245\n",
      "Loss: 20.15768227572741\n",
      "l2 norm of gradients: 0.2441838883015055\n",
      "l2 norm of weights: 3.4484939413317783\n",
      "---------------------\n",
      "Iteration Number: 3246\n",
      "Loss: 20.151158542964733\n",
      "l2 norm of gradients: 0.24405982308126054\n",
      "l2 norm of weights: 3.4481600964014465\n",
      "---------------------\n",
      "Iteration Number: 3247\n",
      "Loss: 20.14463996619262\n",
      "l2 norm of gradients: 0.24393584065343965\n",
      "l2 norm of weights: 3.4478265907760126\n",
      "---------------------\n",
      "Iteration Number: 3248\n",
      "Loss: 20.13812654091931\n",
      "l2 norm of gradients: 0.24381194099045778\n",
      "l2 norm of weights: 3.447493424186714\n",
      "---------------------\n",
      "Iteration Number: 3249\n",
      "Loss: 20.131618262656048\n",
      "l2 norm of gradients: 0.24368812406465512\n",
      "l2 norm of weights: 3.4471605963648067\n",
      "---------------------\n",
      "Iteration Number: 3250\n",
      "Loss: 20.125115126916956\n",
      "l2 norm of gradients: 0.24356438984829734\n",
      "l2 norm of weights: 3.446828107041567\n",
      "---------------------\n",
      "Iteration Number: 3251\n",
      "Loss: 20.118617129219466\n",
      "l2 norm of gradients: 0.24344073831357582\n",
      "l2 norm of weights: 3.4464959559482913\n",
      "---------------------\n",
      "Iteration Number: 3252\n",
      "Loss: 20.112124265083693\n",
      "l2 norm of gradients: 0.24331716943260723\n",
      "l2 norm of weights: 3.4461641428162952\n",
      "---------------------\n",
      "Iteration Number: 3253\n",
      "Loss: 20.105636530032907\n",
      "l2 norm of gradients: 0.2431936831774345\n",
      "l2 norm of weights: 3.4458326673769175\n",
      "---------------------\n",
      "Iteration Number: 3254\n",
      "Loss: 20.099153919593572\n",
      "l2 norm of gradients: 0.24307027952002647\n",
      "l2 norm of weights: 3.44550152936152\n",
      "---------------------\n",
      "Iteration Number: 3255\n",
      "Loss: 20.09267642929482\n",
      "l2 norm of gradients: 0.24294695843227787\n",
      "l2 norm of weights: 3.445170728501487\n",
      "---------------------\n",
      "Iteration Number: 3256\n",
      "Loss: 20.086204054669167\n",
      "l2 norm of gradients: 0.24282371988601006\n",
      "l2 norm of weights: 3.444840264528228\n",
      "---------------------\n",
      "Iteration Number: 3257\n",
      "Loss: 20.079736791251815\n",
      "l2 norm of gradients: 0.24270056385297065\n",
      "l2 norm of weights: 3.444510137173177\n",
      "---------------------\n",
      "Iteration Number: 3258\n",
      "Loss: 20.07327463458123\n",
      "l2 norm of gradients: 0.2425774903048338\n",
      "l2 norm of weights: 3.444180346167794\n",
      "---------------------\n",
      "Iteration Number: 3259\n",
      "Loss: 20.066817580198776\n",
      "l2 norm of gradients: 0.24245449921320048\n",
      "l2 norm of weights: 3.4438508912435664\n",
      "---------------------\n",
      "Iteration Number: 3260\n",
      "Loss: 20.060365623648895\n",
      "l2 norm of gradients: 0.2423315905495985\n",
      "l2 norm of weights: 3.443521772132009\n",
      "---------------------\n",
      "Iteration Number: 3261\n",
      "Loss: 20.053918760479064\n",
      "l2 norm of gradients: 0.24220876428548263\n",
      "l2 norm of weights: 3.4431929885646646\n",
      "---------------------\n",
      "Iteration Number: 3262\n",
      "Loss: 20.0474769862396\n",
      "l2 norm of gradients: 0.2420860203922348\n",
      "l2 norm of weights: 3.4428645402731064\n",
      "---------------------\n",
      "Iteration Number: 3263\n",
      "Loss: 20.04104029648413\n",
      "l2 norm of gradients: 0.2419633588411644\n",
      "l2 norm of weights: 3.442536426988937\n",
      "---------------------\n",
      "Iteration Number: 3264\n",
      "Loss: 20.034608686769115\n",
      "l2 norm of gradients: 0.24184077960350792\n",
      "l2 norm of weights: 3.44220864844379\n",
      "---------------------\n",
      "Iteration Number: 3265\n",
      "Loss: 20.028182152654054\n",
      "l2 norm of gradients: 0.24171828265042972\n",
      "l2 norm of weights: 3.4418812043693334\n",
      "---------------------\n",
      "Iteration Number: 3266\n",
      "Loss: 20.021760689701498\n",
      "l2 norm of gradients: 0.24159586795302168\n",
      "l2 norm of weights: 3.441554094497263\n",
      "---------------------\n",
      "Iteration Number: 3267\n",
      "Loss: 20.015344293477\n",
      "l2 norm of gradients: 0.24147353548230363\n",
      "l2 norm of weights: 3.4412273185593123\n",
      "---------------------\n",
      "Iteration Number: 3268\n",
      "Loss: 20.008932959549202\n",
      "l2 norm of gradients: 0.2413512852092233\n",
      "l2 norm of weights: 3.4409008762872477\n",
      "---------------------\n",
      "Iteration Number: 3269\n",
      "Loss: 20.00252668348966\n",
      "l2 norm of gradients: 0.24122911710465664\n",
      "l2 norm of weights: 3.4405747674128713\n",
      "---------------------\n",
      "Iteration Number: 3270\n",
      "Loss: 19.99612546087311\n",
      "l2 norm of gradients: 0.24110703113940776\n",
      "l2 norm of weights: 3.4402489916680206\n",
      "---------------------\n",
      "Iteration Number: 3271\n",
      "Loss: 19.989729287277132\n",
      "l2 norm of gradients: 0.24098502728420926\n",
      "l2 norm of weights: 3.4399235487845705\n",
      "---------------------\n",
      "Iteration Number: 3272\n",
      "Loss: 19.983338158282436\n",
      "l2 norm of gradients: 0.24086310550972226\n",
      "l2 norm of weights: 3.439598438494434\n",
      "---------------------\n",
      "Iteration Number: 3273\n",
      "Loss: 19.976952069472752\n",
      "l2 norm of gradients: 0.2407412657865364\n",
      "l2 norm of weights: 3.4392736605295617\n",
      "---------------------\n",
      "Iteration Number: 3274\n",
      "Loss: 19.970571016434807\n",
      "l2 norm of gradients: 0.24061950808517046\n",
      "l2 norm of weights: 3.4389492146219447\n",
      "---------------------\n",
      "Iteration Number: 3275\n",
      "Loss: 19.96419499475831\n",
      "l2 norm of gradients: 0.24049783237607178\n",
      "l2 norm of weights: 3.438625100503614\n",
      "---------------------\n",
      "Iteration Number: 3276\n",
      "Loss: 19.957824000036176\n",
      "l2 norm of gradients: 0.24037623862961705\n",
      "l2 norm of weights: 3.4383013179066415\n",
      "---------------------\n",
      "Iteration Number: 3277\n",
      "Loss: 19.951458027864206\n",
      "l2 norm of gradients: 0.24025472681611199\n",
      "l2 norm of weights: 3.437977866563141\n",
      "---------------------\n",
      "Iteration Number: 3278\n",
      "Loss: 19.945097073841186\n",
      "l2 norm of gradients: 0.24013329690579183\n",
      "l2 norm of weights: 3.43765474620527\n",
      "---------------------\n",
      "Iteration Number: 3279\n",
      "Loss: 19.938741133568985\n",
      "l2 norm of gradients: 0.24001194886882116\n",
      "l2 norm of weights: 3.437331956565228\n",
      "---------------------\n",
      "Iteration Number: 3280\n",
      "Loss: 19.9323902026526\n",
      "l2 norm of gradients: 0.23989068267529415\n",
      "l2 norm of weights: 3.4370094973752607\n",
      "---------------------\n",
      "Iteration Number: 3281\n",
      "Loss: 19.926044276699937\n",
      "l2 norm of gradients: 0.23976949829523492\n",
      "l2 norm of weights: 3.436687368367658\n",
      "---------------------\n",
      "Iteration Number: 3282\n",
      "Loss: 19.91970335132191\n",
      "l2 norm of gradients: 0.2396483956985972\n",
      "l2 norm of weights: 3.436365569274756\n",
      "---------------------\n",
      "Iteration Number: 3283\n",
      "Loss: 19.913367422132556\n",
      "l2 norm of gradients: 0.23952737485526485\n",
      "l2 norm of weights: 3.436044099828939\n",
      "---------------------\n",
      "Iteration Number: 3284\n",
      "Loss: 19.907036484748872\n",
      "l2 norm of gradients: 0.239406435735052\n",
      "l2 norm of weights: 3.435722959762637\n",
      "---------------------\n",
      "Iteration Number: 3285\n",
      "Loss: 19.900710534791052\n",
      "l2 norm of gradients: 0.23928557830770272\n",
      "l2 norm of weights: 3.4354021488083304\n",
      "---------------------\n",
      "Iteration Number: 3286\n",
      "Loss: 19.89438956788199\n",
      "l2 norm of gradients: 0.23916480254289194\n",
      "l2 norm of weights: 3.4350816666985478\n",
      "---------------------\n",
      "Iteration Number: 3287\n",
      "Loss: 19.888073579647916\n",
      "l2 norm of gradients: 0.2390441084102246\n",
      "l2 norm of weights: 3.434761513165869\n",
      "---------------------\n",
      "Iteration Number: 3288\n",
      "Loss: 19.881762565717967\n",
      "l2 norm of gradients: 0.23892349587923667\n",
      "l2 norm of weights: 3.434441687942925\n",
      "---------------------\n",
      "Iteration Number: 3289\n",
      "Loss: 19.87545652172439\n",
      "l2 norm of gradients: 0.23880296491939476\n",
      "l2 norm of weights: 3.4341221907623978\n",
      "---------------------\n",
      "Iteration Number: 3290\n",
      "Loss: 19.86915544330232\n",
      "l2 norm of gradients: 0.2386825155000965\n",
      "l2 norm of weights: 3.433803021357023\n",
      "---------------------\n",
      "Iteration Number: 3291\n",
      "Loss: 19.862859326090057\n",
      "l2 norm of gradients: 0.23856214759067038\n",
      "l2 norm of weights: 3.43348417945959\n",
      "---------------------\n",
      "Iteration Number: 3292\n",
      "Loss: 19.85656816572893\n",
      "l2 norm of gradients: 0.23844186116037616\n",
      "l2 norm of weights: 3.433165664802942\n",
      "---------------------\n",
      "Iteration Number: 3293\n",
      "Loss: 19.850281957863245\n",
      "l2 norm of gradients: 0.23832165617840503\n",
      "l2 norm of weights: 3.4328474771199775\n",
      "---------------------\n",
      "Iteration Number: 3294\n",
      "Loss: 19.844000698140306\n",
      "l2 norm of gradients: 0.2382015326138795\n",
      "l2 norm of weights: 3.4325296161436514\n",
      "---------------------\n",
      "Iteration Number: 3295\n",
      "Loss: 19.83772438221065\n",
      "l2 norm of gradients: 0.2380814904358535\n",
      "l2 norm of weights: 3.4322120816069757\n",
      "---------------------\n",
      "Iteration Number: 3296\n",
      "Loss: 19.83145300572766\n",
      "l2 norm of gradients: 0.23796152961331285\n",
      "l2 norm of weights: 3.4318948732430203\n",
      "---------------------\n",
      "Iteration Number: 3297\n",
      "Loss: 19.82518656434775\n",
      "l2 norm of gradients: 0.237841650115175\n",
      "l2 norm of weights: 3.431577990784912\n",
      "---------------------\n",
      "Iteration Number: 3298\n",
      "Loss: 19.81892505373056\n",
      "l2 norm of gradients: 0.23772185191028955\n",
      "l2 norm of weights: 3.43126143396584\n",
      "---------------------\n",
      "Iteration Number: 3299\n",
      "Loss: 19.812668469538533\n",
      "l2 norm of gradients: 0.2376021349674379\n",
      "l2 norm of weights: 3.4309452025190508\n",
      "---------------------\n",
      "Iteration Number: 3300\n",
      "Loss: 19.8064168074374\n",
      "l2 norm of gradients: 0.23748249925533388\n",
      "l2 norm of weights: 3.430629296177854\n",
      "---------------------\n",
      "Iteration Number: 3301\n",
      "Loss: 19.800170063095734\n",
      "l2 norm of gradients: 0.23736294474262343\n",
      "l2 norm of weights: 3.4303137146756195\n",
      "---------------------\n",
      "Iteration Number: 3302\n",
      "Loss: 19.793928232185124\n",
      "l2 norm of gradients: 0.23724347139788493\n",
      "l2 norm of weights: 3.4299984577457816\n",
      "---------------------\n",
      "Iteration Number: 3303\n",
      "Loss: 19.787691310380456\n",
      "l2 norm of gradients: 0.23712407918962952\n",
      "l2 norm of weights: 3.429683525121836\n",
      "---------------------\n",
      "Iteration Number: 3304\n",
      "Loss: 19.78145929335942\n",
      "l2 norm of gradients: 0.23700476808630083\n",
      "l2 norm of weights: 3.4293689165373453\n",
      "---------------------\n",
      "Iteration Number: 3305\n",
      "Loss: 19.775232176802877\n",
      "l2 norm of gradients: 0.23688553805627532\n",
      "l2 norm of weights: 3.429054631725936\n",
      "---------------------\n",
      "Iteration Number: 3306\n",
      "Loss: 19.769009956394594\n",
      "l2 norm of gradients: 0.23676638906786235\n",
      "l2 norm of weights: 3.4287406704212984\n",
      "---------------------\n",
      "Iteration Number: 3307\n",
      "Loss: 19.762792627821554\n",
      "l2 norm of gradients: 0.2366473210893045\n",
      "l2 norm of weights: 3.428427032357194\n",
      "---------------------\n",
      "Iteration Number: 3308\n",
      "Loss: 19.75658018677361\n",
      "l2 norm of gradients: 0.23652833408877733\n",
      "l2 norm of weights: 3.428113717267448\n",
      "---------------------\n",
      "Iteration Number: 3309\n",
      "Loss: 19.750372628943847\n",
      "l2 norm of gradients: 0.23640942803438963\n",
      "l2 norm of weights: 3.427800724885957\n",
      "---------------------\n",
      "Iteration Number: 3310\n",
      "Loss: 19.74416995002829\n",
      "l2 norm of gradients: 0.23629060289418394\n",
      "l2 norm of weights: 3.4274880549466844\n",
      "---------------------\n",
      "Iteration Number: 3311\n",
      "Loss: 19.737972145725987\n",
      "l2 norm of gradients: 0.23617185863613596\n",
      "l2 norm of weights: 3.427175707183665\n",
      "---------------------\n",
      "Iteration Number: 3312\n",
      "Loss: 19.731779211739134\n",
      "l2 norm of gradients: 0.2360531952281553\n",
      "l2 norm of weights: 3.426863681331004\n",
      "---------------------\n",
      "Iteration Number: 3313\n",
      "Loss: 19.7255911437728\n",
      "l2 norm of gradients: 0.23593461263808518\n",
      "l2 norm of weights: 3.4265519771228794\n",
      "---------------------\n",
      "Iteration Number: 3314\n",
      "Loss: 19.71940793753531\n",
      "l2 norm of gradients: 0.23581611083370285\n",
      "l2 norm of weights: 3.426240594293539\n",
      "---------------------\n",
      "Iteration Number: 3315\n",
      "Loss: 19.713229588737978\n",
      "l2 norm of gradients: 0.23569768978271952\n",
      "l2 norm of weights: 3.425929532577308\n",
      "---------------------\n",
      "Iteration Number: 3316\n",
      "Loss: 19.70705609309504\n",
      "l2 norm of gradients: 0.23557934945278047\n",
      "l2 norm of weights: 3.425618791708581\n",
      "---------------------\n",
      "Iteration Number: 3317\n",
      "Loss: 19.700887446323904\n",
      "l2 norm of gradients: 0.2354610898114653\n",
      "l2 norm of weights: 3.425308371421831\n",
      "---------------------\n",
      "Iteration Number: 3318\n",
      "Loss: 19.694723644145014\n",
      "l2 norm of gradients: 0.2353429108262881\n",
      "l2 norm of weights: 3.424998271451606\n",
      "---------------------\n",
      "Iteration Number: 3319\n",
      "Loss: 19.688564682281896\n",
      "l2 norm of gradients: 0.2352248124646972\n",
      "l2 norm of weights: 3.4246884915325286\n",
      "---------------------\n",
      "Iteration Number: 3320\n",
      "Loss: 19.682410556461086\n",
      "l2 norm of gradients: 0.23510679469407572\n",
      "l2 norm of weights: 3.424379031399302\n",
      "---------------------\n",
      "Iteration Number: 3321\n",
      "Loss: 19.676261262412183\n",
      "l2 norm of gradients: 0.23498885748174156\n",
      "l2 norm of weights: 3.424069890786704\n",
      "---------------------\n",
      "Iteration Number: 3322\n",
      "Loss: 19.670116795867735\n",
      "l2 norm of gradients: 0.23487100079494735\n",
      "l2 norm of weights: 3.4237610694295952\n",
      "---------------------\n",
      "Iteration Number: 3323\n",
      "Loss: 19.66397715256358\n",
      "l2 norm of gradients: 0.2347532246008807\n",
      "l2 norm of weights: 3.423452567062912\n",
      "---------------------\n",
      "Iteration Number: 3324\n",
      "Loss: 19.657842328238424\n",
      "l2 norm of gradients: 0.23463552886666433\n",
      "l2 norm of weights: 3.4231443834216737\n",
      "---------------------\n",
      "Iteration Number: 3325\n",
      "Loss: 19.651712318634132\n",
      "l2 norm of gradients: 0.23451791355935622\n",
      "l2 norm of weights: 3.4228365182409806\n",
      "---------------------\n",
      "Iteration Number: 3326\n",
      "Loss: 19.645587119495563\n",
      "l2 norm of gradients: 0.23440037864594962\n",
      "l2 norm of weights: 3.4225289712560145\n",
      "---------------------\n",
      "Iteration Number: 3327\n",
      "Loss: 19.63946672657057\n",
      "l2 norm of gradients: 0.23428292409337315\n",
      "l2 norm of weights: 3.4222217422020416\n",
      "---------------------\n",
      "Iteration Number: 3328\n",
      "Loss: 19.63335113561033\n",
      "l2 norm of gradients: 0.23416554986849114\n",
      "l2 norm of weights: 3.421914830814409\n",
      "---------------------\n",
      "Iteration Number: 3329\n",
      "Loss: 19.627240342368705\n",
      "l2 norm of gradients: 0.2340482559381035\n",
      "l2 norm of weights: 3.4216082368285514\n",
      "---------------------\n",
      "Iteration Number: 3330\n",
      "Loss: 19.621134342602968\n",
      "l2 norm of gradients: 0.23393104226894598\n",
      "l2 norm of weights: 3.4213019599799877\n",
      "---------------------\n",
      "Iteration Number: 3331\n",
      "Loss: 19.615033132073233\n",
      "l2 norm of gradients: 0.23381390882769\n",
      "l2 norm of weights: 3.420996000004321\n",
      "---------------------\n",
      "Iteration Number: 3332\n",
      "Loss: 19.60893670654273\n",
      "l2 norm of gradients: 0.2336968555809435\n",
      "l2 norm of weights: 3.420690356637245\n",
      "---------------------\n",
      "Iteration Number: 3333\n",
      "Loss: 19.602845061777792\n",
      "l2 norm of gradients: 0.23357988249525005\n",
      "l2 norm of weights: 3.420385029614537\n",
      "---------------------\n",
      "Iteration Number: 3334\n",
      "Loss: 19.59675819354779\n",
      "l2 norm of gradients: 0.23346298953708972\n",
      "l2 norm of weights: 3.4200800186720666\n",
      "---------------------\n",
      "Iteration Number: 3335\n",
      "Loss: 19.59067609762519\n",
      "l2 norm of gradients: 0.23334617667287894\n",
      "l2 norm of weights: 3.4197753235457897\n",
      "---------------------\n",
      "Iteration Number: 3336\n",
      "Loss: 19.584598769785416\n",
      "l2 norm of gradients: 0.23322944386897063\n",
      "l2 norm of weights: 3.419470943971753\n",
      "---------------------\n",
      "Iteration Number: 3337\n",
      "Loss: 19.578526205807055\n",
      "l2 norm of gradients: 0.23311279109165414\n",
      "l2 norm of weights: 3.4191668796860952\n",
      "---------------------\n",
      "Iteration Number: 3338\n",
      "Loss: 19.572458401471835\n",
      "l2 norm of gradients: 0.23299621830715586\n",
      "l2 norm of weights: 3.4188631304250454\n",
      "---------------------\n",
      "Iteration Number: 3339\n",
      "Loss: 19.566395352564317\n",
      "l2 norm of gradients: 0.23287972548163866\n",
      "l2 norm of weights: 3.4185596959249263\n",
      "---------------------\n",
      "Iteration Number: 3340\n",
      "Loss: 19.56033705487238\n",
      "l2 norm of gradients: 0.2327633125812026\n",
      "l2 norm of weights: 3.418256575922151\n",
      "---------------------\n",
      "Iteration Number: 3341\n",
      "Loss: 19.55428350418679\n",
      "l2 norm of gradients: 0.23264697957188474\n",
      "l2 norm of weights: 3.4179537701532294\n",
      "---------------------\n",
      "Iteration Number: 3342\n",
      "Loss: 19.54823469630151\n",
      "l2 norm of gradients: 0.2325307264196592\n",
      "l2 norm of weights: 3.4176512783547652\n",
      "---------------------\n",
      "Iteration Number: 3343\n",
      "Loss: 19.542190627013543\n",
      "l2 norm of gradients: 0.23241455309043746\n",
      "l2 norm of weights: 3.4173491002634573\n",
      "---------------------\n",
      "Iteration Number: 3344\n",
      "Loss: 19.53615129212287\n",
      "l2 norm of gradients: 0.23229845955006853\n",
      "l2 norm of weights: 3.417047235616101\n",
      "---------------------\n",
      "Iteration Number: 3345\n",
      "Loss: 19.530116687432585\n",
      "l2 norm of gradients: 0.23218244576433866\n",
      "l2 norm of weights: 3.4167456841495882\n",
      "---------------------\n",
      "Iteration Number: 3346\n",
      "Loss: 19.524086808749022\n",
      "l2 norm of gradients: 0.23206651169897205\n",
      "l2 norm of weights: 3.4164444456009098\n",
      "---------------------\n",
      "Iteration Number: 3347\n",
      "Loss: 19.518061651881357\n",
      "l2 norm of gradients: 0.23195065731963033\n",
      "l2 norm of weights: 3.4161435197071537\n",
      "---------------------\n",
      "Iteration Number: 3348\n",
      "Loss: 19.512041212641957\n",
      "l2 norm of gradients: 0.23183488259191323\n",
      "l2 norm of weights: 3.415842906205508\n",
      "---------------------\n",
      "Iteration Number: 3349\n",
      "Loss: 19.506025486846223\n",
      "l2 norm of gradients: 0.23171918748135828\n",
      "l2 norm of weights: 3.415542604833261\n",
      "---------------------\n",
      "Iteration Number: 3350\n",
      "Loss: 19.500014470312717\n",
      "l2 norm of gradients: 0.2316035719534412\n",
      "l2 norm of weights: 3.4152426153278013\n",
      "---------------------\n",
      "Iteration Number: 3351\n",
      "Loss: 19.49400815886299\n",
      "l2 norm of gradients: 0.23148803597357584\n",
      "l2 norm of weights: 3.41494293742662\n",
      "---------------------\n",
      "Iteration Number: 3352\n",
      "Loss: 19.488006548321668\n",
      "l2 norm of gradients: 0.23137257950711437\n",
      "l2 norm of weights: 3.4146435708673084\n",
      "---------------------\n",
      "Iteration Number: 3353\n",
      "Loss: 19.482009634516494\n",
      "l2 norm of gradients: 0.23125720251934737\n",
      "l2 norm of weights: 3.4143445153875644\n",
      "---------------------\n",
      "Iteration Number: 3354\n",
      "Loss: 19.476017413278374\n",
      "l2 norm of gradients: 0.23114190497550402\n",
      "l2 norm of weights: 3.414045770725187\n",
      "---------------------\n",
      "Iteration Number: 3355\n",
      "Loss: 19.470029880441057\n",
      "l2 norm of gradients: 0.2310266868407521\n",
      "l2 norm of weights: 3.413747336618081\n",
      "---------------------\n",
      "Iteration Number: 3356\n",
      "Loss: 19.46404703184156\n",
      "l2 norm of gradients: 0.2309115480801981\n",
      "l2 norm of weights: 3.413449212804256\n",
      "---------------------\n",
      "Iteration Number: 3357\n",
      "Loss: 19.458068863319987\n",
      "l2 norm of gradients: 0.23079648865888733\n",
      "l2 norm of weights: 3.4131513990218276\n",
      "---------------------\n",
      "Iteration Number: 3358\n",
      "Loss: 19.452095370719487\n",
      "l2 norm of gradients: 0.2306815085418043\n",
      "l2 norm of weights: 3.41285389500902\n",
      "---------------------\n",
      "Iteration Number: 3359\n",
      "Loss: 19.446126549886234\n",
      "l2 norm of gradients: 0.23056660769387247\n",
      "l2 norm of weights: 3.412556700504163\n",
      "---------------------\n",
      "Iteration Number: 3360\n",
      "Loss: 19.44016239666954\n",
      "l2 norm of gradients: 0.23045178607995442\n",
      "l2 norm of weights: 3.4122598152456955\n",
      "---------------------\n",
      "Iteration Number: 3361\n",
      "Loss: 19.434202906921822\n",
      "l2 norm of gradients: 0.23033704366485225\n",
      "l2 norm of weights: 3.411963238972166\n",
      "---------------------\n",
      "Iteration Number: 3362\n",
      "Loss: 19.42824807649859\n",
      "l2 norm of gradients: 0.23022238041330723\n",
      "l2 norm of weights: 3.411666971422232\n",
      "---------------------\n",
      "Iteration Number: 3363\n",
      "Loss: 19.4222979012584\n",
      "l2 norm of gradients: 0.23010779629000047\n",
      "l2 norm of weights: 3.411371012334663\n",
      "---------------------\n",
      "Iteration Number: 3364\n",
      "Loss: 19.416352377062847\n",
      "l2 norm of gradients: 0.22999329125955248\n",
      "l2 norm of weights: 3.411075361448338\n",
      "---------------------\n",
      "Iteration Number: 3365\n",
      "Loss: 19.410411499776732\n",
      "l2 norm of gradients: 0.22987886528652363\n",
      "l2 norm of weights: 3.4107800185022494\n",
      "---------------------\n",
      "Iteration Number: 3366\n",
      "Loss: 19.40447526526787\n",
      "l2 norm of gradients: 0.22976451833541425\n",
      "l2 norm of weights: 3.4104849832355018\n",
      "---------------------\n",
      "Iteration Number: 3367\n",
      "Loss: 19.39854366940725\n",
      "l2 norm of gradients: 0.22965025037066447\n",
      "l2 norm of weights: 3.4101902553873145\n",
      "---------------------\n",
      "Iteration Number: 3368\n",
      "Loss: 19.39261670806878\n",
      "l2 norm of gradients: 0.22953606135665458\n",
      "l2 norm of weights: 3.4098958346970196\n",
      "---------------------\n",
      "Iteration Number: 3369\n",
      "Loss: 19.386694377129583\n",
      "l2 norm of gradients: 0.2294219512577052\n",
      "l2 norm of weights: 3.409601720904065\n",
      "---------------------\n",
      "Iteration Number: 3370\n",
      "Loss: 19.380776672469985\n",
      "l2 norm of gradients: 0.22930792003807712\n",
      "l2 norm of weights: 3.4093079137480147\n",
      "---------------------\n",
      "Iteration Number: 3371\n",
      "Loss: 19.374863589973135\n",
      "l2 norm of gradients: 0.2291939676619716\n",
      "l2 norm of weights: 3.4090144129685482\n",
      "---------------------\n",
      "Iteration Number: 3372\n",
      "Loss: 19.368955125525513\n",
      "l2 norm of gradients: 0.2290800940935305\n",
      "l2 norm of weights: 3.408721218305463\n",
      "---------------------\n",
      "Iteration Number: 3373\n",
      "Loss: 19.363051275016602\n",
      "l2 norm of gradients: 0.2289662992968362\n",
      "l2 norm of weights: 3.408428329498674\n",
      "---------------------\n",
      "Iteration Number: 3374\n",
      "Loss: 19.357152034338892\n",
      "l2 norm of gradients: 0.228852583235912\n",
      "l2 norm of weights: 3.4081357462882154\n",
      "---------------------\n",
      "Iteration Number: 3375\n",
      "Loss: 19.351257399388167\n",
      "l2 norm of gradients: 0.228738945874722\n",
      "l2 norm of weights: 3.4078434684142405\n",
      "---------------------\n",
      "Iteration Number: 3376\n",
      "Loss: 19.345367366063183\n",
      "l2 norm of gradients: 0.2286253871771712\n",
      "l2 norm of weights: 3.407551495617022\n",
      "---------------------\n",
      "Iteration Number: 3377\n",
      "Loss: 19.339481930265784\n",
      "l2 norm of gradients: 0.22851190710710578\n",
      "l2 norm of weights: 3.407259827636955\n",
      "---------------------\n",
      "Iteration Number: 3378\n",
      "Loss: 19.333601087900938\n",
      "l2 norm of gradients: 0.22839850562831318\n",
      "l2 norm of weights: 3.4069684642145535\n",
      "---------------------\n",
      "Iteration Number: 3379\n",
      "Loss: 19.327724834876747\n",
      "l2 norm of gradients: 0.22828518270452203\n",
      "l2 norm of weights: 3.406677405090457\n",
      "---------------------\n",
      "Iteration Number: 3380\n",
      "Loss: 19.3218531671044\n",
      "l2 norm of gradients: 0.2281719382994024\n",
      "l2 norm of weights: 3.406386650005426\n",
      "---------------------\n",
      "Iteration Number: 3381\n",
      "Loss: 19.31598608049816\n",
      "l2 norm of gradients: 0.22805877237656605\n",
      "l2 norm of weights: 3.4060961987003444\n",
      "---------------------\n",
      "Iteration Number: 3382\n",
      "Loss: 19.310123570975342\n",
      "l2 norm of gradients: 0.2279456848995661\n",
      "l2 norm of weights: 3.405806050916221\n",
      "---------------------\n",
      "Iteration Number: 3383\n",
      "Loss: 19.304265634456574\n",
      "l2 norm of gradients: 0.22783267583189773\n",
      "l2 norm of weights: 3.40551620639419\n",
      "---------------------\n",
      "Iteration Number: 3384\n",
      "Loss: 19.298412266865313\n",
      "l2 norm of gradients: 0.22771974513699786\n",
      "l2 norm of weights: 3.4052266648755114\n",
      "---------------------\n",
      "Iteration Number: 3385\n",
      "Loss: 19.29256346412831\n",
      "l2 norm of gradients: 0.22760689277824528\n",
      "l2 norm of weights: 3.404937426101571\n",
      "---------------------\n",
      "Iteration Number: 3386\n",
      "Loss: 19.2867192221754\n",
      "l2 norm of gradients: 0.22749411871896097\n",
      "l2 norm of weights: 3.404648489813882\n",
      "---------------------\n",
      "Iteration Number: 3387\n",
      "Loss: 19.280879536939455\n",
      "l2 norm of gradients: 0.22738142292240818\n",
      "l2 norm of weights: 3.4043598557540844\n",
      "---------------------\n",
      "Iteration Number: 3388\n",
      "Loss: 19.275044404356443\n",
      "l2 norm of gradients: 0.22726880535179234\n",
      "l2 norm of weights: 3.40407152366395\n",
      "---------------------\n",
      "Iteration Number: 3389\n",
      "Loss: 19.269213820365625\n",
      "l2 norm of gradients: 0.22715626597026128\n",
      "l2 norm of weights: 3.4037834932853763\n",
      "---------------------\n",
      "Iteration Number: 3390\n",
      "Loss: 19.26338778090911\n",
      "l2 norm of gradients: 0.22704380474090555\n",
      "l2 norm of weights: 3.4034957643603923\n",
      "---------------------\n",
      "Iteration Number: 3391\n",
      "Loss: 19.257566281932245\n",
      "l2 norm of gradients: 0.2269314216267581\n",
      "l2 norm of weights: 3.403208336631158\n",
      "---------------------\n",
      "Iteration Number: 3392\n",
      "Loss: 19.251749319383556\n",
      "l2 norm of gradients: 0.22681911659079484\n",
      "l2 norm of weights: 3.4029212098399633\n",
      "---------------------\n",
      "Iteration Number: 3393\n",
      "Loss: 19.24593688921459\n",
      "l2 norm of gradients: 0.22670688959593427\n",
      "l2 norm of weights: 3.4026343837292314\n",
      "---------------------\n",
      "Iteration Number: 3394\n",
      "Loss: 19.24012898738007\n",
      "l2 norm of gradients: 0.22659474060503815\n",
      "l2 norm of weights: 3.402347858041518\n",
      "---------------------\n",
      "Iteration Number: 3395\n",
      "Loss: 19.234325609837686\n",
      "l2 norm of gradients: 0.22648266958091118\n",
      "l2 norm of weights: 3.402061632519511\n",
      "---------------------\n",
      "Iteration Number: 3396\n",
      "Loss: 19.228526752548458\n",
      "l2 norm of gradients: 0.22637067648630124\n",
      "l2 norm of weights: 3.4017757069060344\n",
      "---------------------\n",
      "Iteration Number: 3397\n",
      "Loss: 19.222732411476297\n",
      "l2 norm of gradients: 0.2262587612838995\n",
      "l2 norm of weights: 3.4014900809440447\n",
      "---------------------\n",
      "Iteration Number: 3398\n",
      "Loss: 19.216942582588455\n",
      "l2 norm of gradients: 0.2261469239363406\n",
      "l2 norm of weights: 3.4012047543766344\n",
      "---------------------\n",
      "Iteration Number: 3399\n",
      "Loss: 19.21115726185513\n",
      "l2 norm of gradients: 0.2260351644062026\n",
      "l2 norm of weights: 3.4009197269470337\n",
      "---------------------\n",
      "Iteration Number: 3400\n",
      "Loss: 19.205376445249705\n",
      "l2 norm of gradients: 0.2259234826560074\n",
      "l2 norm of weights: 3.400634998398607\n",
      "---------------------\n",
      "Iteration Number: 3401\n",
      "Loss: 19.1996001287487\n",
      "l2 norm of gradients: 0.22581187864822044\n",
      "l2 norm of weights: 3.4003505684748574\n",
      "---------------------\n",
      "Iteration Number: 3402\n",
      "Loss: 19.19382830833173\n",
      "l2 norm of gradients: 0.22570035234525102\n",
      "l2 norm of weights: 3.400066436919426\n",
      "---------------------\n",
      "Iteration Number: 3403\n",
      "Loss: 19.188060979981522\n",
      "l2 norm of gradients: 0.22558890370945242\n",
      "l2 norm of weights: 3.399782603476093\n",
      "---------------------\n",
      "Iteration Number: 3404\n",
      "Loss: 19.182298139683887\n",
      "l2 norm of gradients: 0.2254775327031221\n",
      "l2 norm of weights: 3.3994990678887764\n",
      "---------------------\n",
      "Iteration Number: 3405\n",
      "Loss: 19.176539783427884\n",
      "l2 norm of gradients: 0.22536623928850164\n",
      "l2 norm of weights: 3.3992158299015363\n",
      "---------------------\n",
      "Iteration Number: 3406\n",
      "Loss: 19.170785907205588\n",
      "l2 norm of gradients: 0.2252550234277768\n",
      "l2 norm of weights: 3.3989328892585715\n",
      "---------------------\n",
      "Iteration Number: 3407\n",
      "Loss: 19.165036507012186\n",
      "l2 norm of gradients: 0.22514388508307792\n",
      "l2 norm of weights: 3.398650245704224\n",
      "---------------------\n",
      "Iteration Number: 3408\n",
      "Loss: 19.159291578846066\n",
      "l2 norm of gradients: 0.22503282421647966\n",
      "l2 norm of weights: 3.398367898982976\n",
      "---------------------\n",
      "Iteration Number: 3409\n",
      "Loss: 19.15355111870871\n",
      "l2 norm of gradients: 0.22492184079000155\n",
      "l2 norm of weights: 3.3980858488394534\n",
      "---------------------\n",
      "Iteration Number: 3410\n",
      "Loss: 19.14781512260469\n",
      "l2 norm of gradients: 0.22481093476560762\n",
      "l2 norm of weights: 3.397804095018425\n",
      "---------------------\n",
      "Iteration Number: 3411\n",
      "Loss: 19.14208358654173\n",
      "l2 norm of gradients: 0.22470010610520685\n",
      "l2 norm of weights: 3.3975226372648035\n",
      "---------------------\n",
      "Iteration Number: 3412\n",
      "Loss: 19.136356506530742\n",
      "l2 norm of gradients: 0.22458935477065317\n",
      "l2 norm of weights: 3.3972414753236464\n",
      "---------------------\n",
      "Iteration Number: 3413\n",
      "Loss: 19.130633878585645\n",
      "l2 norm of gradients: 0.22447868072374566\n",
      "l2 norm of weights: 3.3969606089401565\n",
      "---------------------\n",
      "Iteration Number: 3414\n",
      "Loss: 19.124915698723576\n",
      "l2 norm of gradients: 0.22436808392622834\n",
      "l2 norm of weights: 3.3966800378596815\n",
      "---------------------\n",
      "Iteration Number: 3415\n",
      "Loss: 19.11920196296486\n",
      "l2 norm of gradients: 0.22425756433979074\n",
      "l2 norm of weights: 3.396399761827717\n",
      "---------------------\n",
      "Iteration Number: 3416\n",
      "Loss: 19.113492667332697\n",
      "l2 norm of gradients: 0.22414712192606775\n",
      "l2 norm of weights: 3.396119780589904\n",
      "---------------------\n",
      "Iteration Number: 3417\n",
      "Loss: 19.107787807853757\n",
      "l2 norm of gradients: 0.22403675664663972\n",
      "l2 norm of weights: 3.395840093892033\n",
      "---------------------\n",
      "Iteration Number: 3418\n",
      "Loss: 19.10208738055758\n",
      "l2 norm of gradients: 0.2239264684630325\n",
      "l2 norm of weights: 3.395560701480041\n",
      "---------------------\n",
      "Iteration Number: 3419\n",
      "Loss: 19.096391381476998\n",
      "l2 norm of gradients: 0.22381625733671798\n",
      "l2 norm of weights: 3.3952816031000155\n",
      "---------------------\n",
      "Iteration Number: 3420\n",
      "Loss: 19.090699806647866\n",
      "l2 norm of gradients: 0.2237061232291137\n",
      "l2 norm of weights: 3.395002798498193\n",
      "---------------------\n",
      "Iteration Number: 3421\n",
      "Loss: 19.085012652109285\n",
      "l2 norm of gradients: 0.22359606610158295\n",
      "l2 norm of weights: 3.394724287420961\n",
      "---------------------\n",
      "Iteration Number: 3422\n",
      "Loss: 19.07932991390342\n",
      "l2 norm of gradients: 0.2234860859154355\n",
      "l2 norm of weights: 3.3944460696148555\n",
      "---------------------\n",
      "Iteration Number: 3423\n",
      "Loss: 19.073651588075506\n",
      "l2 norm of gradients: 0.22337618263192713\n",
      "l2 norm of weights: 3.394168144826567\n",
      "---------------------\n",
      "Iteration Number: 3424\n",
      "Loss: 19.067977670674065\n",
      "l2 norm of gradients: 0.2232663562122597\n",
      "l2 norm of weights: 3.3938905128029355\n",
      "---------------------\n",
      "Iteration Number: 3425\n",
      "Loss: 19.06230815775073\n",
      "l2 norm of gradients: 0.2231566066175818\n",
      "l2 norm of weights: 3.3936131732909547\n",
      "---------------------\n",
      "Iteration Number: 3426\n",
      "Loss: 19.056643045360175\n",
      "l2 norm of gradients: 0.22304693380898827\n",
      "l2 norm of weights: 3.393336126037773\n",
      "---------------------\n",
      "Iteration Number: 3427\n",
      "Loss: 19.050982329560213\n",
      "l2 norm of gradients: 0.22293733774752075\n",
      "l2 norm of weights: 3.3930593707906898\n",
      "---------------------\n",
      "Iteration Number: 3428\n",
      "Loss: 19.045326006411912\n",
      "l2 norm of gradients: 0.2228278183941674\n",
      "l2 norm of weights: 3.392782907297161\n",
      "---------------------\n",
      "Iteration Number: 3429\n",
      "Loss: 19.039674071979483\n",
      "l2 norm of gradients: 0.2227183757098635\n",
      "l2 norm of weights: 3.3925067353047975\n",
      "---------------------\n",
      "Iteration Number: 3430\n",
      "Loss: 19.034026522330105\n",
      "l2 norm of gradients: 0.22260900965549102\n",
      "l2 norm of weights: 3.392230854561365\n",
      "---------------------\n",
      "Iteration Number: 3431\n",
      "Loss: 19.028383353534252\n",
      "l2 norm of gradients: 0.22249972019187922\n",
      "l2 norm of weights: 3.391955264814786\n",
      "---------------------\n",
      "Iteration Number: 3432\n",
      "Loss: 19.022744561665505\n",
      "l2 norm of gradients: 0.22239050727980433\n",
      "l2 norm of weights: 3.3916799658131396\n",
      "---------------------\n",
      "Iteration Number: 3433\n",
      "Loss: 19.017110142800654\n",
      "l2 norm of gradients: 0.22228137087999006\n",
      "l2 norm of weights: 3.3914049573046636\n",
      "---------------------\n",
      "Iteration Number: 3434\n",
      "Loss: 19.011480093019408\n",
      "l2 norm of gradients: 0.2221723109531074\n",
      "l2 norm of weights: 3.391130239037752\n",
      "---------------------\n",
      "Iteration Number: 3435\n",
      "Loss: 19.005854408404954\n",
      "l2 norm of gradients: 0.22206332745977492\n",
      "l2 norm of weights: 3.390855810760958\n",
      "---------------------\n",
      "Iteration Number: 3436\n",
      "Loss: 19.000233085043323\n",
      "l2 norm of gradients: 0.22195442036055876\n",
      "l2 norm of weights: 3.390581672222995\n",
      "---------------------\n",
      "Iteration Number: 3437\n",
      "Loss: 18.994616119023878\n",
      "l2 norm of gradients: 0.2218455896159728\n",
      "l2 norm of weights: 3.3903078231727357\n",
      "---------------------\n",
      "Iteration Number: 3438\n",
      "Loss: 18.98900350643905\n",
      "l2 norm of gradients: 0.2217368351864789\n",
      "l2 norm of weights: 3.390034263359212\n",
      "---------------------\n",
      "Iteration Number: 3439\n",
      "Loss: 18.983395243384486\n",
      "l2 norm of gradients: 0.22162815703248673\n",
      "l2 norm of weights: 3.389760992531619\n",
      "---------------------\n",
      "Iteration Number: 3440\n",
      "Loss: 18.97779132595887\n",
      "l2 norm of gradients: 0.221519555114354\n",
      "l2 norm of weights: 3.3894880104393112\n",
      "---------------------\n",
      "Iteration Number: 3441\n",
      "Loss: 18.972191750264148\n",
      "l2 norm of gradients: 0.2214110293923868\n",
      "l2 norm of weights: 3.389215316831806\n",
      "---------------------\n",
      "Iteration Number: 3442\n",
      "Loss: 18.96659651240537\n",
      "l2 norm of gradients: 0.22130257982683937\n",
      "l2 norm of weights: 3.388942911458784\n",
      "---------------------\n",
      "Iteration Number: 3443\n",
      "Loss: 18.961005608490744\n",
      "l2 norm of gradients: 0.22119420637791448\n",
      "l2 norm of weights: 3.388670794070089\n",
      "---------------------\n",
      "Iteration Number: 3444\n",
      "Loss: 18.9554190346316\n",
      "l2 norm of gradients: 0.22108590900576325\n",
      "l2 norm of weights: 3.3883989644157273\n",
      "---------------------\n",
      "Iteration Number: 3445\n",
      "Loss: 18.949836786942484\n",
      "l2 norm of gradients: 0.22097768767048562\n",
      "l2 norm of weights: 3.388127422245871\n",
      "---------------------\n",
      "Iteration Number: 3446\n",
      "Loss: 18.944258861541048\n",
      "l2 norm of gradients: 0.22086954233213016\n",
      "l2 norm of weights: 3.3878561673108565\n",
      "---------------------\n",
      "Iteration Number: 3447\n",
      "Loss: 18.93868525454807\n",
      "l2 norm of gradients: 0.22076147295069443\n",
      "l2 norm of weights: 3.3875851993611854\n",
      "---------------------\n",
      "Iteration Number: 3448\n",
      "Loss: 18.93311596208757\n",
      "l2 norm of gradients: 0.22065347948612482\n",
      "l2 norm of weights: 3.3873145181475266\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 3449\n",
      "Loss: 18.927550980286703\n",
      "l2 norm of gradients: 0.22054556189831692\n",
      "l2 norm of weights: 3.387044123420714\n",
      "---------------------\n",
      "Iteration Number: 3450\n",
      "Loss: 18.921990305275674\n",
      "l2 norm of gradients: 0.22043772014711546\n",
      "l2 norm of weights: 3.3867740149317496\n",
      "---------------------\n",
      "Iteration Number: 3451\n",
      "Loss: 18.91643393318803\n",
      "l2 norm of gradients: 0.2203299541923145\n",
      "l2 norm of weights: 3.3865041924318025\n",
      "---------------------\n",
      "Iteration Number: 3452\n",
      "Loss: 18.91088186016025\n",
      "l2 norm of gradients: 0.2202222639936576\n",
      "l2 norm of weights: 3.3862346556722107\n",
      "---------------------\n",
      "Iteration Number: 3453\n",
      "Loss: 18.905334082332207\n",
      "l2 norm of gradients: 0.22011464951083776\n",
      "l2 norm of weights: 3.38596540440448\n",
      "---------------------\n",
      "Iteration Number: 3454\n",
      "Loss: 18.8997905958467\n",
      "l2 norm of gradients: 0.22000711070349774\n",
      "l2 norm of weights: 3.3856964383802866\n",
      "---------------------\n",
      "Iteration Number: 3455\n",
      "Loss: 18.89425139684998\n",
      "l2 norm of gradients: 0.21989964753122998\n",
      "l2 norm of weights: 3.385427757351476\n",
      "---------------------\n",
      "Iteration Number: 3456\n",
      "Loss: 18.88871648149113\n",
      "l2 norm of gradients: 0.21979225995357682\n",
      "l2 norm of weights: 3.3851593610700634\n",
      "---------------------\n",
      "Iteration Number: 3457\n",
      "Loss: 18.883185845922615\n",
      "l2 norm of gradients: 0.21968494793003074\n",
      "l2 norm of weights: 3.3848912492882364\n",
      "---------------------\n",
      "Iteration Number: 3458\n",
      "Loss: 18.877659486299976\n",
      "l2 norm of gradients: 0.21957771142003413\n",
      "l2 norm of weights: 3.3846234217583526\n",
      "---------------------\n",
      "Iteration Number: 3459\n",
      "Loss: 18.872137398781998\n",
      "l2 norm of gradients: 0.21947055038297977\n",
      "l2 norm of weights: 3.3843558782329426\n",
      "---------------------\n",
      "Iteration Number: 3460\n",
      "Loss: 18.866619579530493\n",
      "l2 norm of gradients: 0.21936346477821062\n",
      "l2 norm of weights: 3.3840886184647077\n",
      "---------------------\n",
      "Iteration Number: 3461\n",
      "Loss: 18.861106024710562\n",
      "l2 norm of gradients: 0.2192564545650203\n",
      "l2 norm of weights: 3.3838216422065255\n",
      "---------------------\n",
      "Iteration Number: 3462\n",
      "Loss: 18.85559673049044\n",
      "l2 norm of gradients: 0.21914951970265278\n",
      "l2 norm of weights: 3.383554949211443\n",
      "---------------------\n",
      "Iteration Number: 3463\n",
      "Loss: 18.850091693041417\n",
      "l2 norm of gradients: 0.21904266015030296\n",
      "l2 norm of weights: 3.383288539232685\n",
      "---------------------\n",
      "Iteration Number: 3464\n",
      "Loss: 18.844590908538212\n",
      "l2 norm of gradients: 0.21893587586711638\n",
      "l2 norm of weights: 3.383022412023648\n",
      "---------------------\n",
      "Iteration Number: 3465\n",
      "Loss: 18.839094373158357\n",
      "l2 norm of gradients: 0.21882916681218947\n",
      "l2 norm of weights: 3.3827565673379056\n",
      "---------------------\n",
      "Iteration Number: 3466\n",
      "Loss: 18.833602083082823\n",
      "l2 norm of gradients: 0.21872253294456984\n",
      "l2 norm of weights: 3.382491004929205\n",
      "---------------------\n",
      "Iteration Number: 3467\n",
      "Loss: 18.828114034495705\n",
      "l2 norm of gradients: 0.218615974223256\n",
      "l2 norm of weights: 3.3822257245514713\n",
      "---------------------\n",
      "Iteration Number: 3468\n",
      "Loss: 18.822630223584135\n",
      "l2 norm of gradients: 0.21850949060719801\n",
      "l2 norm of weights: 3.381960725958804\n",
      "---------------------\n",
      "Iteration Number: 3469\n",
      "Loss: 18.817150646538593\n",
      "l2 norm of gradients: 0.21840308205529715\n",
      "l2 norm of weights: 3.381696008905483\n",
      "---------------------\n",
      "Iteration Number: 3470\n",
      "Loss: 18.811675299552547\n",
      "l2 norm of gradients: 0.21829674852640624\n",
      "l2 norm of weights: 3.381431573145962\n",
      "---------------------\n",
      "Iteration Number: 3471\n",
      "Loss: 18.80620417882284\n",
      "l2 norm of gradients: 0.21819048997932955\n",
      "l2 norm of weights: 3.381167418434875\n",
      "---------------------\n",
      "Iteration Number: 3472\n",
      "Loss: 18.80073728054923\n",
      "l2 norm of gradients: 0.21808430637282336\n",
      "l2 norm of weights: 3.3809035445270337\n",
      "---------------------\n",
      "Iteration Number: 3473\n",
      "Loss: 18.795274600934885\n",
      "l2 norm of gradients: 0.2179781976655955\n",
      "l2 norm of weights: 3.380639951177429\n",
      "---------------------\n",
      "Iteration Number: 3474\n",
      "Loss: 18.789816136186076\n",
      "l2 norm of gradients: 0.21787216381630603\n",
      "l2 norm of weights: 3.3803766381412323\n",
      "---------------------\n",
      "Iteration Number: 3475\n",
      "Loss: 18.784361882512094\n",
      "l2 norm of gradients: 0.21776620478356679\n",
      "l2 norm of weights: 3.380113605173793\n",
      "---------------------\n",
      "Iteration Number: 3476\n",
      "Loss: 18.77891183612567\n",
      "l2 norm of gradients: 0.21766032052594203\n",
      "l2 norm of weights: 3.379850852030643\n",
      "---------------------\n",
      "Iteration Number: 3477\n",
      "Loss: 18.77346599324249\n",
      "l2 norm of gradients: 0.2175545110019482\n",
      "l2 norm of weights: 3.379588378467493\n",
      "---------------------\n",
      "Iteration Number: 3478\n",
      "Loss: 18.768024350081472\n",
      "l2 norm of gradients: 0.21744877617005418\n",
      "l2 norm of weights: 3.3793261842402367\n",
      "---------------------\n",
      "Iteration Number: 3479\n",
      "Loss: 18.762586902864843\n",
      "l2 norm of gradients: 0.21734311598868133\n",
      "l2 norm of weights: 3.37906426910495\n",
      "---------------------\n",
      "Iteration Number: 3480\n",
      "Loss: 18.75715364781778\n",
      "l2 norm of gradients: 0.21723753041620386\n",
      "l2 norm of weights: 3.3788026328178895\n",
      "---------------------\n",
      "Iteration Number: 3481\n",
      "Loss: 18.751724581168823\n",
      "l2 norm of gradients: 0.21713201941094853\n",
      "l2 norm of weights: 3.3785412751354955\n",
      "---------------------\n",
      "Iteration Number: 3482\n",
      "Loss: 18.74629969914955\n",
      "l2 norm of gradients: 0.2170265829311951\n",
      "l2 norm of weights: 3.378280195814392\n",
      "---------------------\n",
      "Iteration Number: 3483\n",
      "Loss: 18.740878997994816\n",
      "l2 norm of gradients: 0.21692122093517613\n",
      "l2 norm of weights: 3.3780193946113863\n",
      "---------------------\n",
      "Iteration Number: 3484\n",
      "Loss: 18.735462473942658\n",
      "l2 norm of gradients: 0.21681593338107766\n",
      "l2 norm of weights: 3.37775887128347\n",
      "---------------------\n",
      "Iteration Number: 3485\n",
      "Loss: 18.730050123234154\n",
      "l2 norm of gradients: 0.2167107202270387\n",
      "l2 norm of weights: 3.3774986255878185\n",
      "---------------------\n",
      "Iteration Number: 3486\n",
      "Loss: 18.724641942113756\n",
      "l2 norm of gradients: 0.21660558143115158\n",
      "l2 norm of weights: 3.3772386572817945\n",
      "---------------------\n",
      "Iteration Number: 3487\n",
      "Loss: 18.719237926829003\n",
      "l2 norm of gradients: 0.21650051695146225\n",
      "l2 norm of weights: 3.376978966122944\n",
      "---------------------\n",
      "Iteration Number: 3488\n",
      "Loss: 18.713838073630534\n",
      "l2 norm of gradients: 0.21639552674597023\n",
      "l2 norm of weights: 3.376719551869\n",
      "---------------------\n",
      "Iteration Number: 3489\n",
      "Loss: 18.70844237877229\n",
      "l2 norm of gradients: 0.21629061077262862\n",
      "l2 norm of weights: 3.376460414277882\n",
      "---------------------\n",
      "Iteration Number: 3490\n",
      "Loss: 18.703050838511334\n",
      "l2 norm of gradients: 0.2161857689893444\n",
      "l2 norm of weights: 3.3762015531076957\n",
      "---------------------\n",
      "Iteration Number: 3491\n",
      "Loss: 18.697663449107942\n",
      "l2 norm of gradients: 0.2160810013539786\n",
      "l2 norm of weights: 3.3759429681167354\n",
      "---------------------\n",
      "Iteration Number: 3492\n",
      "Loss: 18.69228020682557\n",
      "l2 norm of gradients: 0.21597630782434601\n",
      "l2 norm of weights: 3.375684659063482\n",
      "---------------------\n",
      "Iteration Number: 3493\n",
      "Loss: 18.686901107930762\n",
      "l2 norm of gradients: 0.215871688358216\n",
      "l2 norm of weights: 3.3754266257066052\n",
      "---------------------\n",
      "Iteration Number: 3494\n",
      "Loss: 18.681526148693404\n",
      "l2 norm of gradients: 0.21576714291331187\n",
      "l2 norm of weights: 3.3751688678049634\n",
      "---------------------\n",
      "Iteration Number: 3495\n",
      "Loss: 18.676155325386475\n",
      "l2 norm of gradients: 0.21566267144731152\n",
      "l2 norm of weights: 3.3749113851176027\n",
      "---------------------\n",
      "Iteration Number: 3496\n",
      "Loss: 18.670788634286172\n",
      "l2 norm of gradients: 0.2155582739178473\n",
      "l2 norm of weights: 3.374654177403761\n",
      "---------------------\n",
      "Iteration Number: 3497\n",
      "Loss: 18.665426071671817\n",
      "l2 norm of gradients: 0.21545395028250627\n",
      "l2 norm of weights: 3.3743972444228647\n",
      "---------------------\n",
      "Iteration Number: 3498\n",
      "Loss: 18.660067633825946\n",
      "l2 norm of gradients: 0.21534970049883018\n",
      "l2 norm of weights: 3.374140585934531\n",
      "---------------------\n",
      "Iteration Number: 3499\n",
      "Loss: 18.65471331703426\n",
      "l2 norm of gradients: 0.21524552452431572\n",
      "l2 norm of weights: 3.373884201698566\n",
      "---------------------\n",
      "Iteration Number: 3500\n",
      "Loss: 18.649363117585764\n",
      "l2 norm of gradients: 0.2151414223164145\n",
      "l2 norm of weights: 3.373628091474971\n",
      "---------------------\n",
      "Iteration Number: 3501\n",
      "Loss: 18.644017031772528\n",
      "l2 norm of gradients: 0.21503739383253334\n",
      "l2 norm of weights: 3.3733722550239342\n",
      "---------------------\n",
      "Iteration Number: 3502\n",
      "Loss: 18.638675055889824\n",
      "l2 norm of gradients: 0.21493343903003423\n",
      "l2 norm of weights: 3.3731166921058393\n",
      "---------------------\n",
      "Iteration Number: 3503\n",
      "Loss: 18.633337186236094\n",
      "l2 norm of gradients: 0.2148295578662346\n",
      "l2 norm of weights: 3.372861402481261\n",
      "---------------------\n",
      "Iteration Number: 3504\n",
      "Loss: 18.628003419113043\n",
      "l2 norm of gradients: 0.21472575029840715\n",
      "l2 norm of weights: 3.3726063859109665\n",
      "---------------------\n",
      "Iteration Number: 3505\n",
      "Loss: 18.62267375082557\n",
      "l2 norm of gradients: 0.21462201628378041\n",
      "l2 norm of weights: 3.3723516421559174\n",
      "---------------------\n",
      "Iteration Number: 3506\n",
      "Loss: 18.61734817768167\n",
      "l2 norm of gradients: 0.21451835577953848\n",
      "l2 norm of weights: 3.372097170977267\n",
      "---------------------\n",
      "Iteration Number: 3507\n",
      "Loss: 18.612026695992533\n",
      "l2 norm of gradients: 0.2144147687428214\n",
      "l2 norm of weights: 3.3718429721363643\n",
      "---------------------\n",
      "Iteration Number: 3508\n",
      "Loss: 18.606709302072616\n",
      "l2 norm of gradients: 0.21431125513072505\n",
      "l2 norm of weights: 3.371589045394752\n",
      "---------------------\n",
      "Iteration Number: 3509\n",
      "Loss: 18.601395992239553\n",
      "l2 norm of gradients: 0.21420781490030133\n",
      "l2 norm of weights: 3.3713353905141683\n",
      "---------------------\n",
      "Iteration Number: 3510\n",
      "Loss: 18.59608676281414\n",
      "l2 norm of gradients: 0.21410444800855857\n",
      "l2 norm of weights: 3.371082007256544\n",
      "---------------------\n",
      "Iteration Number: 3511\n",
      "Loss: 18.590781610120292\n",
      "l2 norm of gradients: 0.21400115441246112\n",
      "l2 norm of weights: 3.370828895384009\n",
      "---------------------\n",
      "Iteration Number: 3512\n",
      "Loss: 18.58548053048529\n",
      "l2 norm of gradients: 0.21389793406892998\n",
      "l2 norm of weights: 3.370576054658887\n",
      "---------------------\n",
      "Iteration Number: 3513\n",
      "Loss: 18.580183520239444\n",
      "l2 norm of gradients: 0.21379478693484255\n",
      "l2 norm of weights: 3.3703234848436985\n",
      "---------------------\n",
      "Iteration Number: 3514\n",
      "Loss: 18.57489057571633\n",
      "l2 norm of gradients: 0.21369171296703302\n",
      "l2 norm of weights: 3.3700711857011605\n",
      "---------------------\n",
      "Iteration Number: 3515\n",
      "Loss: 18.56960169325275\n",
      "l2 norm of gradients: 0.21358871212229222\n",
      "l2 norm of weights: 3.369819156994187\n",
      "---------------------\n",
      "Iteration Number: 3516\n",
      "Loss: 18.56431686918858\n",
      "l2 norm of gradients: 0.213485784357368\n",
      "l2 norm of weights: 3.3695673984858905\n",
      "---------------------\n",
      "Iteration Number: 3517\n",
      "Loss: 18.559036099867043\n",
      "l2 norm of gradients: 0.21338292962896518\n",
      "l2 norm of weights: 3.3693159099395795\n",
      "---------------------\n",
      "Iteration Number: 3518\n",
      "Loss: 18.553759381634386\n",
      "l2 norm of gradients: 0.21328014789374578\n",
      "l2 norm of weights: 3.3690646911187616\n",
      "---------------------\n",
      "Iteration Number: 3519\n",
      "Loss: 18.548486710840237\n",
      "l2 norm of gradients: 0.21317743910832904\n",
      "l2 norm of weights: 3.368813741787143\n",
      "---------------------\n",
      "Iteration Number: 3520\n",
      "Loss: 18.543218083837214\n",
      "l2 norm of gradients: 0.2130748032292915\n",
      "l2 norm of weights: 3.3685630617086293\n",
      "---------------------\n",
      "Iteration Number: 3521\n",
      "Loss: 18.537953496981352\n",
      "l2 norm of gradients: 0.21297224021316752\n",
      "l2 norm of weights: 3.368312650647324\n",
      "---------------------\n",
      "Iteration Number: 3522\n",
      "Loss: 18.53269294663162\n",
      "l2 norm of gradients: 0.21286975001644867\n",
      "l2 norm of weights: 3.368062508367531\n",
      "---------------------\n",
      "Iteration Number: 3523\n",
      "Loss: 18.52743642915045\n",
      "l2 norm of gradients: 0.2127673325955846\n",
      "l2 norm of weights: 3.3678126346337542\n",
      "---------------------\n",
      "Iteration Number: 3524\n",
      "Loss: 18.522183940903297\n",
      "l2 norm of gradients: 0.21266498790698257\n",
      "l2 norm of weights: 3.3675630292106975\n",
      "---------------------\n",
      "Iteration Number: 3525\n",
      "Loss: 18.516935478258862\n",
      "l2 norm of gradients: 0.21256271590700804\n",
      "l2 norm of weights: 3.3673136918632656\n",
      "---------------------\n",
      "Iteration Number: 3526\n",
      "Loss: 18.511691037589067\n",
      "l2 norm of gradients: 0.2124605165519845\n",
      "l2 norm of weights: 3.367064622356564\n",
      "---------------------\n",
      "Iteration Number: 3527\n",
      "Loss: 18.50645061526892\n",
      "l2 norm of gradients: 0.21235838979819363\n",
      "l2 norm of weights: 3.3668158204559004\n",
      "---------------------\n",
      "Iteration Number: 3528\n",
      "Loss: 18.501214207676814\n",
      "l2 norm of gradients: 0.21225633560187548\n",
      "l2 norm of weights: 3.3665672859267826\n",
      "---------------------\n",
      "Iteration Number: 3529\n",
      "Loss: 18.49598181119415\n",
      "l2 norm of gradients: 0.21215435391922866\n",
      "l2 norm of weights: 3.3663190185349214\n",
      "---------------------\n",
      "Iteration Number: 3530\n",
      "Loss: 18.49075342220567\n",
      "l2 norm of gradients: 0.2120524447064102\n",
      "l2 norm of weights: 3.36607101804623\n",
      "---------------------\n",
      "Iteration Number: 3531\n",
      "Loss: 18.485529037099226\n",
      "l2 norm of gradients: 0.21195060791953602\n",
      "l2 norm of weights: 3.365823284226824\n",
      "---------------------\n",
      "Iteration Number: 3532\n",
      "Loss: 18.480308652265848\n",
      "l2 norm of gradients: 0.21184884351468072\n",
      "l2 norm of weights: 3.3655758168430214\n",
      "---------------------\n",
      "Iteration Number: 3533\n",
      "Loss: 18.475092264099878\n",
      "l2 norm of gradients: 0.21174715144787806\n",
      "l2 norm of weights: 3.365328615661345\n",
      "---------------------\n",
      "Iteration Number: 3534\n",
      "Loss: 18.469879868998802\n",
      "l2 norm of gradients: 0.21164553167512068\n",
      "l2 norm of weights: 3.3650816804485197\n",
      "---------------------\n",
      "Iteration Number: 3535\n",
      "Loss: 18.464671463363203\n",
      "l2 norm of gradients: 0.21154398415236048\n",
      "l2 norm of weights: 3.364835010971475\n",
      "---------------------\n",
      "Iteration Number: 3536\n",
      "Loss: 18.459467043597037\n",
      "l2 norm of gradients: 0.21144250883550877\n",
      "l2 norm of weights: 3.3645886069973456\n",
      "---------------------\n",
      "Iteration Number: 3537\n",
      "Loss: 18.454266606107304\n",
      "l2 norm of gradients: 0.21134110568043615\n",
      "l2 norm of weights: 3.3643424682934686\n",
      "---------------------\n",
      "Iteration Number: 3538\n",
      "Loss: 18.449070147304276\n",
      "l2 norm of gradients: 0.2112397746429729\n",
      "l2 norm of weights: 3.364096594627388\n",
      "---------------------\n",
      "Iteration Number: 3539\n",
      "Loss: 18.443877663601494\n",
      "l2 norm of gradients: 0.2111385156789089\n",
      "l2 norm of weights: 3.363850985766852\n",
      "---------------------\n",
      "Iteration Number: 3540\n",
      "Loss: 18.438689151415556\n",
      "l2 norm of gradients: 0.211037328743994\n",
      "l2 norm of weights: 3.3636056414798143\n",
      "---------------------\n",
      "Iteration Number: 3541\n",
      "Loss: 18.433504607166327\n",
      "l2 norm of gradients: 0.21093621379393773\n",
      "l2 norm of weights: 3.3633605615344355\n",
      "---------------------\n",
      "Iteration Number: 3542\n",
      "Loss: 18.428324027276886\n",
      "l2 norm of gradients: 0.2108351707844099\n",
      "l2 norm of weights: 3.36311574569908\n",
      "---------------------\n",
      "Iteration Number: 3543\n",
      "Loss: 18.423147408173552\n",
      "l2 norm of gradients: 0.21073419967104037\n",
      "l2 norm of weights: 3.362871193742321\n",
      "---------------------\n",
      "Iteration Number: 3544\n",
      "Loss: 18.41797474628573\n",
      "l2 norm of gradients: 0.21063330040941933\n",
      "l2 norm of weights: 3.362626905432938\n",
      "---------------------\n",
      "Iteration Number: 3545\n",
      "Loss: 18.41280603804606\n",
      "l2 norm of gradients: 0.21053247295509736\n",
      "l2 norm of weights: 3.362382880539916\n",
      "---------------------\n",
      "Iteration Number: 3546\n",
      "Loss: 18.407641279890438\n",
      "l2 norm of gradients: 0.2104317172635856\n",
      "l2 norm of weights: 3.362139118832448\n",
      "---------------------\n",
      "Iteration Number: 3547\n",
      "Loss: 18.402480468258\n",
      "l2 norm of gradients: 0.21033103329035593\n",
      "l2 norm of weights: 3.361895620079936\n",
      "---------------------\n",
      "Iteration Number: 3548\n",
      "Loss: 18.397323599590898\n",
      "l2 norm of gradients: 0.21023042099084083\n",
      "l2 norm of weights: 3.361652384051988\n",
      "---------------------\n",
      "Iteration Number: 3549\n",
      "Loss: 18.392170670334657\n",
      "l2 norm of gradients: 0.21012988032043378\n",
      "l2 norm of weights: 3.36140941051842\n",
      "---------------------\n",
      "Iteration Number: 3550\n",
      "Loss: 18.387021676937987\n",
      "l2 norm of gradients: 0.21002941123448943\n",
      "l2 norm of weights: 3.3611666992492584\n",
      "---------------------\n",
      "Iteration Number: 3551\n",
      "Loss: 18.38187661585271\n",
      "l2 norm of gradients: 0.2099290136883234\n",
      "l2 norm of weights: 3.3609242500147363\n",
      "---------------------\n",
      "Iteration Number: 3552\n",
      "Loss: 18.376735483533906\n",
      "l2 norm of gradients: 0.20982868763721255\n",
      "l2 norm of weights: 3.3606820625852976\n",
      "---------------------\n",
      "Iteration Number: 3553\n",
      "Loss: 18.37159827643983\n",
      "l2 norm of gradients: 0.20972843303639535\n",
      "l2 norm of weights: 3.360440136731593\n",
      "---------------------\n",
      "Iteration Number: 3554\n",
      "Loss: 18.366464991032025\n",
      "l2 norm of gradients: 0.20962824984107153\n",
      "l2 norm of weights: 3.3601984722244844\n",
      "---------------------\n",
      "Iteration Number: 3555\n",
      "Loss: 18.361335623775105\n",
      "l2 norm of gradients: 0.20952813800640266\n",
      "l2 norm of weights: 3.3599570688350435\n",
      "---------------------\n",
      "Iteration Number: 3556\n",
      "Loss: 18.35621017113703\n",
      "l2 norm of gradients: 0.209428097487512\n",
      "l2 norm of weights: 3.3597159263345517\n",
      "---------------------\n",
      "Iteration Number: 3557\n",
      "Loss: 18.351088629588798\n",
      "l2 norm of gradients: 0.20932812823948466\n",
      "l2 norm of weights: 3.3594750444945003\n",
      "---------------------\n",
      "Iteration Number: 3558\n",
      "Loss: 18.34597099560472\n",
      "l2 norm of gradients: 0.20922823021736783\n",
      "l2 norm of weights: 3.359234423086592\n",
      "---------------------\n",
      "Iteration Number: 3559\n",
      "Loss: 18.34085726566231\n",
      "l2 norm of gradients: 0.20912840337617075\n",
      "l2 norm of weights: 3.3589940618827385\n",
      "---------------------\n",
      "Iteration Number: 3560\n",
      "Loss: 18.335747436242247\n",
      "l2 norm of gradients: 0.20902864767086504\n",
      "l2 norm of weights: 3.3587539606550654\n",
      "---------------------\n",
      "Iteration Number: 3561\n",
      "Loss: 18.33064150382841\n",
      "l2 norm of gradients: 0.20892896305638456\n",
      "l2 norm of weights: 3.358514119175907\n",
      "---------------------\n",
      "Iteration Number: 3562\n",
      "Loss: 18.32553946490793\n",
      "l2 norm of gradients: 0.2088293494876257\n",
      "l2 norm of weights: 3.3582745372178104\n",
      "---------------------\n",
      "Iteration Number: 3563\n",
      "Loss: 18.320441315971042\n",
      "l2 norm of gradients: 0.2087298069194476\n",
      "l2 norm of weights: 3.3580352145535346\n",
      "---------------------\n",
      "Iteration Number: 3564\n",
      "Loss: 18.3153470535113\n",
      "l2 norm of gradients: 0.20863033530667185\n",
      "l2 norm of weights: 3.35779615095605\n",
      "---------------------\n",
      "Iteration Number: 3565\n",
      "Loss: 18.31025667402542\n",
      "l2 norm of gradients: 0.20853093460408328\n",
      "l2 norm of weights: 3.3575573461985386\n",
      "---------------------\n",
      "Iteration Number: 3566\n",
      "Loss: 18.305170174013238\n",
      "l2 norm of gradients: 0.20843160476642947\n",
      "l2 norm of weights: 3.3573188000543976\n",
      "---------------------\n",
      "Iteration Number: 3567\n",
      "Loss: 18.300087549977953\n",
      "l2 norm of gradients: 0.20833234574842116\n",
      "l2 norm of weights: 3.357080512297234\n",
      "---------------------\n",
      "Iteration Number: 3568\n",
      "Loss: 18.29500879842584\n",
      "l2 norm of gradients: 0.20823315750473237\n",
      "l2 norm of weights: 3.356842482700869\n",
      "---------------------\n",
      "Iteration Number: 3569\n",
      "Loss: 18.28993391586637\n",
      "l2 norm of gradients: 0.20813403999000035\n",
      "l2 norm of weights: 3.3566047110393376\n",
      "---------------------\n",
      "Iteration Number: 3570\n",
      "Loss: 18.284862898812342\n",
      "l2 norm of gradients: 0.20803499315882606\n",
      "l2 norm of weights: 3.3563671970868865\n",
      "---------------------\n",
      "Iteration Number: 3571\n",
      "Loss: 18.279795743779587\n",
      "l2 norm of gradients: 0.20793601696577385\n",
      "l2 norm of weights: 3.356129940617978\n",
      "---------------------\n",
      "Iteration Number: 3572\n",
      "Loss: 18.274732447287274\n",
      "l2 norm of gradients: 0.207837111365372\n",
      "l2 norm of weights: 3.3558929414072867\n",
      "---------------------\n",
      "Iteration Number: 3573\n",
      "Loss: 18.269673005857744\n",
      "l2 norm of gradients: 0.2077382763121124\n",
      "l2 norm of weights: 3.3556561992297023\n",
      "---------------------\n",
      "Iteration Number: 3574\n",
      "Loss: 18.26461741601654\n",
      "l2 norm of gradients: 0.20763951176045106\n",
      "l2 norm of weights: 3.355419713860328\n",
      "---------------------\n",
      "Iteration Number: 3575\n",
      "Loss: 18.259565674292325\n",
      "l2 norm of gradients: 0.20754081766480814\n",
      "l2 norm of weights: 3.3551834850744826\n",
      "---------------------\n",
      "Iteration Number: 3576\n",
      "Loss: 18.254517777217103\n",
      "l2 norm of gradients: 0.20744219397956792\n",
      "l2 norm of weights: 3.3549475126476986\n",
      "---------------------\n",
      "Iteration Number: 3577\n",
      "Loss: 18.249473721326016\n",
      "l2 norm of gradients: 0.20734364065907895\n",
      "l2 norm of weights: 3.3547117963557236\n",
      "---------------------\n",
      "Iteration Number: 3578\n",
      "Loss: 18.24443350315733\n",
      "l2 norm of gradients: 0.20724515765765456\n",
      "l2 norm of weights: 3.3544763359745207\n",
      "---------------------\n",
      "Iteration Number: 3579\n",
      "Loss: 18.239397119252644\n",
      "l2 norm of gradients: 0.20714674492957225\n",
      "l2 norm of weights: 3.3542411312802685\n",
      "---------------------\n",
      "Iteration Number: 3580\n",
      "Loss: 18.23436456615667\n",
      "l2 norm of gradients: 0.20704840242907463\n",
      "l2 norm of weights: 3.3540061820493596\n",
      "---------------------\n",
      "Iteration Number: 3581\n",
      "Loss: 18.229335840417423\n",
      "l2 norm of gradients: 0.2069501301103688\n",
      "l2 norm of weights: 3.3537714880584044\n",
      "---------------------\n",
      "Iteration Number: 3582\n",
      "Loss: 18.224310938585976\n",
      "l2 norm of gradients: 0.20685192792762702\n",
      "l2 norm of weights: 3.353537049084228\n",
      "---------------------\n",
      "Iteration Number: 3583\n",
      "Loss: 18.219289857216705\n",
      "l2 norm of gradients: 0.2067537958349867\n",
      "l2 norm of weights: 3.353302864903872\n",
      "---------------------\n",
      "Iteration Number: 3584\n",
      "Loss: 18.214272592867204\n",
      "l2 norm of gradients: 0.20665573378655025\n",
      "l2 norm of weights: 3.353068935294594\n",
      "---------------------\n",
      "Iteration Number: 3585\n",
      "Loss: 18.20925914209821\n",
      "l2 norm of gradients: 0.20655774173638547\n",
      "l2 norm of weights: 3.3528352600338684\n",
      "---------------------\n",
      "Iteration Number: 3586\n",
      "Loss: 18.204249501473655\n",
      "l2 norm of gradients: 0.20645981963852575\n",
      "l2 norm of weights: 3.3526018388993863\n",
      "---------------------\n",
      "Iteration Number: 3587\n",
      "Loss: 18.1992436675607\n",
      "l2 norm of gradients: 0.20636196744696986\n",
      "l2 norm of weights: 3.3523686716690557\n",
      "---------------------\n",
      "Iteration Number: 3588\n",
      "Loss: 18.1942416369298\n",
      "l2 norm of gradients: 0.2062641851156824\n",
      "l2 norm of weights: 3.3521357581210007\n",
      "---------------------\n",
      "Iteration Number: 3589\n",
      "Loss: 18.189243406154393\n",
      "l2 norm of gradients: 0.20616647259859378\n",
      "l2 norm of weights: 3.351903098033564\n",
      "---------------------\n",
      "Iteration Number: 3590\n",
      "Loss: 18.18424897181134\n",
      "l2 norm of gradients: 0.20606882984960026\n",
      "l2 norm of weights: 3.351670691185305\n",
      "---------------------\n",
      "Iteration Number: 3591\n",
      "Loss: 18.17925833048056\n",
      "l2 norm of gradients: 0.20597125682256429\n",
      "l2 norm of weights: 3.351438537355\n",
      "---------------------\n",
      "Iteration Number: 3592\n",
      "Loss: 18.174271478745215\n",
      "l2 norm of gradients: 0.2058737534713144\n",
      "l2 norm of weights: 3.3512066363216446\n",
      "---------------------\n",
      "Iteration Number: 3593\n",
      "Loss: 18.16928841319171\n",
      "l2 norm of gradients: 0.20577631974964553\n",
      "l2 norm of weights: 3.3509749878644506\n",
      "---------------------\n",
      "Iteration Number: 3594\n",
      "Loss: 18.16430913040966\n",
      "l2 norm of gradients: 0.205678955611319\n",
      "l2 norm of weights: 3.350743591762849\n",
      "---------------------\n",
      "Iteration Number: 3595\n",
      "Loss: 18.159333626991746\n",
      "l2 norm of gradients: 0.20558166101006267\n",
      "l2 norm of weights: 3.3505124477964885\n",
      "---------------------\n",
      "Iteration Number: 3596\n",
      "Loss: 18.154361899533946\n",
      "l2 norm of gradients: 0.2054844358995711\n",
      "l2 norm of weights: 3.350281555745237\n",
      "---------------------\n",
      "Iteration Number: 3597\n",
      "Loss: 18.149393944635555\n",
      "l2 norm of gradients: 0.20538728023350572\n",
      "l2 norm of weights: 3.350050915389179\n",
      "---------------------\n",
      "Iteration Number: 3598\n",
      "Loss: 18.1444297588988\n",
      "l2 norm of gradients: 0.2052901939654948\n",
      "l2 norm of weights: 3.34982052650862\n",
      "---------------------\n",
      "Iteration Number: 3599\n",
      "Loss: 18.139469338929313\n",
      "l2 norm of gradients: 0.20519317704913367\n",
      "l2 norm of weights: 3.3495903888840823\n",
      "---------------------\n",
      "Iteration Number: 3600\n",
      "Loss: 18.13451268133593\n",
      "l2 norm of gradients: 0.2050962294379849\n",
      "l2 norm of weights: 3.349360502296309\n",
      "---------------------\n",
      "Iteration Number: 3601\n",
      "Loss: 18.129559782730553\n",
      "l2 norm of gradients: 0.2049993510855783\n",
      "l2 norm of weights: 3.349130866526261\n",
      "---------------------\n",
      "Iteration Number: 3602\n",
      "Loss: 18.124610639728363\n",
      "l2 norm of gradients: 0.20490254194541108\n",
      "l2 norm of weights: 3.3489014813551194\n",
      "---------------------\n",
      "Iteration Number: 3603\n",
      "Loss: 18.119665248947783\n",
      "l2 norm of gradients: 0.204805801970948\n",
      "l2 norm of weights: 3.348672346564283\n",
      "---------------------\n",
      "Iteration Number: 3604\n",
      "Loss: 18.114723607010358\n",
      "l2 norm of gradients: 0.20470913111562158\n",
      "l2 norm of weights: 3.348443461935373\n",
      "---------------------\n",
      "Iteration Number: 3605\n",
      "Loss: 18.109785710540812\n",
      "l2 norm of gradients: 0.20461252933283194\n",
      "l2 norm of weights: 3.348214827250228\n",
      "---------------------\n",
      "Iteration Number: 3606\n",
      "Loss: 18.1048515561672\n",
      "l2 norm of gradients: 0.20451599657594735\n",
      "l2 norm of weights: 3.3479864422909063\n",
      "---------------------\n",
      "Iteration Number: 3607\n",
      "Loss: 18.09992114052063\n",
      "l2 norm of gradients: 0.20441953279830385\n",
      "l2 norm of weights: 3.347758306839688\n",
      "---------------------\n",
      "Iteration Number: 3608\n",
      "Loss: 18.0949944602355\n",
      "l2 norm of gradients: 0.20432313795320584\n",
      "l2 norm of weights: 3.347530420679072\n",
      "---------------------\n",
      "Iteration Number: 3609\n",
      "Loss: 18.09007151194938\n",
      "l2 norm of gradients: 0.2042268119939259\n",
      "l2 norm of weights: 3.3473027835917772\n",
      "---------------------\n",
      "Iteration Number: 3610\n",
      "Loss: 18.085152292303015\n",
      "l2 norm of gradients: 0.20413055487370504\n",
      "l2 norm of weights: 3.3470753953607435\n",
      "---------------------\n",
      "Iteration Number: 3611\n",
      "Loss: 18.080236797940398\n",
      "l2 norm of gradients: 0.20403436654575274\n",
      "l2 norm of weights: 3.346848255769131\n",
      "---------------------\n",
      "Iteration Number: 3612\n",
      "Loss: 18.075325025508647\n",
      "l2 norm of gradients: 0.20393824696324717\n",
      "l2 norm of weights: 3.3466213646003204\n",
      "---------------------\n",
      "Iteration Number: 3613\n",
      "Loss: 18.070416971658158\n",
      "l2 norm of gradients: 0.2038421960793353\n",
      "l2 norm of weights: 3.3463947216379135\n",
      "---------------------\n",
      "Iteration Number: 3614\n",
      "Loss: 18.06551263304243\n",
      "l2 norm of gradients: 0.2037462138471329\n",
      "l2 norm of weights: 3.346168326665732\n",
      "---------------------\n",
      "Iteration Number: 3615\n",
      "Loss: 18.060612006318244\n",
      "l2 norm of gradients: 0.20365030021972477\n",
      "l2 norm of weights: 3.3459421794678184\n",
      "---------------------\n",
      "Iteration Number: 3616\n",
      "Loss: 18.055715088145565\n",
      "l2 norm of gradients: 0.20355445515016496\n",
      "l2 norm of weights: 3.345716279828438\n",
      "---------------------\n",
      "Iteration Number: 3617\n",
      "Loss: 18.05082187518749\n",
      "l2 norm of gradients: 0.2034586785914766\n",
      "l2 norm of weights: 3.345490627532075\n",
      "---------------------\n",
      "Iteration Number: 3618\n",
      "Loss: 18.045932364110392\n",
      "l2 norm of gradients: 0.2033629704966522\n",
      "l2 norm of weights: 3.3452652223634365\n",
      "---------------------\n",
      "Iteration Number: 3619\n",
      "Loss: 18.041046551583765\n",
      "l2 norm of gradients: 0.203267330818654\n",
      "l2 norm of weights: 3.3450400641074505\n",
      "---------------------\n",
      "Iteration Number: 3620\n",
      "Loss: 18.036164434280366\n",
      "l2 norm of gradients: 0.2031717595104136\n",
      "l2 norm of weights: 3.3448151525492653\n",
      "---------------------\n",
      "Iteration Number: 3621\n",
      "Loss: 18.031286008876158\n",
      "l2 norm of gradients: 0.2030762565248325\n",
      "l2 norm of weights: 3.3445904874742527\n",
      "---------------------\n",
      "Iteration Number: 3622\n",
      "Loss: 18.02641127205013\n",
      "l2 norm of gradients: 0.20298082181478191\n",
      "l2 norm of weights: 3.344366068668005\n",
      "---------------------\n",
      "Iteration Number: 3623\n",
      "Loss: 18.02154022048471\n",
      "l2 norm of gradients: 0.2028854553331031\n",
      "l2 norm of weights: 3.344141895916336\n",
      "---------------------\n",
      "Iteration Number: 3624\n",
      "Loss: 18.016672850865284\n",
      "l2 norm of gradients: 0.20279015703260758\n",
      "l2 norm of weights: 3.3439179690052816\n",
      "---------------------\n",
      "Iteration Number: 3625\n",
      "Loss: 18.011809159880645\n",
      "l2 norm of gradients: 0.2026949268660769\n",
      "l2 norm of weights: 3.3436942877210996\n",
      "---------------------\n",
      "Iteration Number: 3626\n",
      "Loss: 18.006949144222652\n",
      "l2 norm of gradients: 0.20259976478626301\n",
      "l2 norm of weights: 3.3434708518502707\n",
      "---------------------\n",
      "Iteration Number: 3627\n",
      "Loss: 18.0020928005864\n",
      "l2 norm of gradients: 0.20250467074588832\n",
      "l2 norm of weights: 3.343247661179496\n",
      "---------------------\n",
      "Iteration Number: 3628\n",
      "Loss: 17.9972401256701\n",
      "l2 norm of gradients: 0.2024096446976459\n",
      "l2 norm of weights: 3.3430247154957\n",
      "---------------------\n",
      "Iteration Number: 3629\n",
      "Loss: 17.99239111617525\n",
      "l2 norm of gradients: 0.20231468659419952\n",
      "l2 norm of weights: 3.3428020145860287\n",
      "---------------------\n",
      "Iteration Number: 3630\n",
      "Loss: 17.987545768806513\n",
      "l2 norm of gradients: 0.2022197963881836\n",
      "l2 norm of weights: 3.3425795582378512\n",
      "---------------------\n",
      "Iteration Number: 3631\n",
      "Loss: 17.98270408027173\n",
      "l2 norm of gradients: 0.20212497403220375\n",
      "l2 norm of weights: 3.3423573462387584\n",
      "---------------------\n",
      "Iteration Number: 3632\n",
      "Loss: 17.977866047281903\n",
      "l2 norm of gradients: 0.2020302194788365\n",
      "l2 norm of weights: 3.342135378376564\n",
      "---------------------\n",
      "Iteration Number: 3633\n",
      "Loss: 17.97303166655128\n",
      "l2 norm of gradients: 0.20193553268062972\n",
      "l2 norm of weights: 3.3419136544393035\n",
      "---------------------\n",
      "Iteration Number: 3634\n",
      "Loss: 17.968200934797313\n",
      "l2 norm of gradients: 0.20184091359010248\n",
      "l2 norm of weights: 3.3416921742152357\n",
      "---------------------\n",
      "Iteration Number: 3635\n",
      "Loss: 17.963373848740527\n",
      "l2 norm of gradients: 0.20174636215974528\n",
      "l2 norm of weights: 3.341470937492842\n",
      "---------------------\n",
      "Iteration Number: 3636\n",
      "Loss: 17.958550405104784\n",
      "l2 norm of gradients: 0.2016518783420202\n",
      "l2 norm of weights: 3.3412499440608268\n",
      "---------------------\n",
      "Iteration Number: 3637\n",
      "Loss: 17.953730600617003\n",
      "l2 norm of gradients: 0.201557462089361\n",
      "l2 norm of weights: 3.3410291937081165\n",
      "---------------------\n",
      "Iteration Number: 3638\n",
      "Loss: 17.948914432007378\n",
      "l2 norm of gradients: 0.2014631133541733\n",
      "l2 norm of weights: 3.3408086862238604\n",
      "---------------------\n",
      "Iteration Number: 3639\n",
      "Loss: 17.944101896009286\n",
      "l2 norm of gradients: 0.20136883208883447\n",
      "l2 norm of weights: 3.340588421397432\n",
      "---------------------\n",
      "Iteration Number: 3640\n",
      "Loss: 17.93929298935921\n",
      "l2 norm of gradients: 0.2012746182456941\n",
      "l2 norm of weights: 3.340368399018426\n",
      "---------------------\n",
      "Iteration Number: 3641\n",
      "Loss: 17.934487708796944\n",
      "l2 norm of gradients: 0.20118047177707385\n",
      "l2 norm of weights: 3.340148618876661\n",
      "---------------------\n",
      "Iteration Number: 3642\n",
      "Loss: 17.929686051065335\n",
      "l2 norm of gradients: 0.2010863926352676\n",
      "l2 norm of weights: 3.3399290807621793\n",
      "---------------------\n",
      "Iteration Number: 3643\n",
      "Loss: 17.9248880129105\n",
      "l2 norm of gradients: 0.20099238077254178\n",
      "l2 norm of weights: 3.3397097844652452\n",
      "---------------------\n",
      "Iteration Number: 3644\n",
      "Loss: 17.920093591081752\n",
      "l2 norm of gradients: 0.20089843614113514\n",
      "l2 norm of weights: 3.3394907297763456\n",
      "---------------------\n",
      "Iteration Number: 3645\n",
      "Loss: 17.915302782331548\n",
      "l2 norm of gradients: 0.20080455869325925\n",
      "l2 norm of weights: 3.3392719164861933\n",
      "---------------------\n",
      "Iteration Number: 3646\n",
      "Loss: 17.910515583415524\n",
      "l2 norm of gradients: 0.2007107483810983\n",
      "l2 norm of weights: 3.339053344385721\n",
      "---------------------\n",
      "Iteration Number: 3647\n",
      "Loss: 17.9057319910925\n",
      "l2 norm of gradients: 0.20061700515680933\n",
      "l2 norm of weights: 3.3388350132660873\n",
      "---------------------\n",
      "Iteration Number: 3648\n",
      "Loss: 17.90095200212449\n",
      "l2 norm of gradients: 0.20052332897252256\n",
      "l2 norm of weights: 3.338616922918673\n",
      "---------------------\n",
      "Iteration Number: 3649\n",
      "Loss: 17.896175613276743\n",
      "l2 norm of gradients: 0.20042971978034113\n",
      "l2 norm of weights: 3.3383990731350814\n",
      "---------------------\n",
      "Iteration Number: 3650\n",
      "Loss: 17.891402821317598\n",
      "l2 norm of gradients: 0.2003361775323415\n",
      "l2 norm of weights: 3.338181463707141\n",
      "---------------------\n",
      "Iteration Number: 3651\n",
      "Loss: 17.886633623018636\n",
      "l2 norm of gradients: 0.20024270218057347\n",
      "l2 norm of weights: 3.3379640944269027\n",
      "---------------------\n",
      "Iteration Number: 3652\n",
      "Loss: 17.88186801515456\n",
      "l2 norm of gradients: 0.20014929367706027\n",
      "l2 norm of weights: 3.3377469650866405\n",
      "---------------------\n",
      "Iteration Number: 3653\n",
      "Loss: 17.877105994503324\n",
      "l2 norm of gradients: 0.20005595197379872\n",
      "l2 norm of weights: 3.3375300754788526\n",
      "---------------------\n",
      "Iteration Number: 3654\n",
      "Loss: 17.872347557846062\n",
      "l2 norm of gradients: 0.19996267702275938\n",
      "l2 norm of weights: 3.3373134253962604\n",
      "---------------------\n",
      "Iteration Number: 3655\n",
      "Loss: 17.867592701966984\n",
      "l2 norm of gradients: 0.19986946877588668\n",
      "l2 norm of weights: 3.3370970146318086\n",
      "---------------------\n",
      "Iteration Number: 3656\n",
      "Loss: 17.86284142365359\n",
      "l2 norm of gradients: 0.19977632718509875\n",
      "l2 norm of weights: 3.336880842978666\n",
      "---------------------\n",
      "Iteration Number: 3657\n",
      "Loss: 17.858093719696516\n",
      "l2 norm of gradients: 0.19968325220228808\n",
      "l2 norm of weights: 3.336664910230224\n",
      "---------------------\n",
      "Iteration Number: 3658\n",
      "Loss: 17.853349586889596\n",
      "l2 norm of gradients: 0.1995902437793212\n",
      "l2 norm of weights: 3.3364492161800983\n",
      "---------------------\n",
      "Iteration Number: 3659\n",
      "Loss: 17.848609022029773\n",
      "l2 norm of gradients: 0.19949730186803885\n",
      "l2 norm of weights: 3.3362337606221284\n",
      "---------------------\n",
      "Iteration Number: 3660\n",
      "Loss: 17.84387202191722\n",
      "l2 norm of gradients: 0.19940442642025635\n",
      "l2 norm of weights: 3.3360185433503764\n",
      "---------------------\n",
      "Iteration Number: 3661\n",
      "Loss: 17.839138583355336\n",
      "l2 norm of gradients: 0.19931161738776335\n",
      "l2 norm of weights: 3.335803564159129\n",
      "---------------------\n",
      "Iteration Number: 3662\n",
      "Loss: 17.834408703150572\n",
      "l2 norm of gradients: 0.19921887472232436\n",
      "l2 norm of weights: 3.3355888228428965\n",
      "---------------------\n",
      "Iteration Number: 3663\n",
      "Loss: 17.829682378112604\n",
      "l2 norm of gradients: 0.1991261983756785\n",
      "l2 norm of weights: 3.3353743191964114\n",
      "---------------------\n",
      "Iteration Number: 3664\n",
      "Loss: 17.824959605054396\n",
      "l2 norm of gradients: 0.19903358829953982\n",
      "l2 norm of weights: 3.335160053014631\n",
      "---------------------\n",
      "Iteration Number: 3665\n",
      "Loss: 17.82024038079192\n",
      "l2 norm of gradients: 0.1989410444455973\n",
      "l2 norm of weights: 3.334946024092736\n",
      "---------------------\n",
      "Iteration Number: 3666\n",
      "Loss: 17.815524702144405\n",
      "l2 norm of gradients: 0.19884856676551518\n",
      "l2 norm of weights: 3.3347322322261315\n",
      "---------------------\n",
      "Iteration Number: 3667\n",
      "Loss: 17.810812565934192\n",
      "l2 norm of gradients: 0.19875615521093276\n",
      "l2 norm of weights: 3.3345186772104447\n",
      "---------------------\n",
      "Iteration Number: 3668\n",
      "Loss: 17.80610396898688\n",
      "l2 norm of gradients: 0.19866380973346479\n",
      "l2 norm of weights: 3.3343053588415272\n",
      "---------------------\n",
      "Iteration Number: 3669\n",
      "Loss: 17.801398908131162\n",
      "l2 norm of gradients: 0.19857153028470143\n",
      "l2 norm of weights: 3.334092276915454\n",
      "---------------------\n",
      "Iteration Number: 3670\n",
      "Loss: 17.796697380198957\n",
      "l2 norm of gradients: 0.19847931681620845\n",
      "l2 norm of weights: 3.333879431228524\n",
      "---------------------\n",
      "Iteration Number: 3671\n",
      "Loss: 17.791999382025303\n",
      "l2 norm of gradients: 0.19838716927952726\n",
      "l2 norm of weights: 3.333666821577259\n",
      "---------------------\n",
      "Iteration Number: 3672\n",
      "Loss: 17.787304910448476\n",
      "l2 norm of gradients: 0.19829508762617512\n",
      "l2 norm of weights: 3.333454447758406\n",
      "---------------------\n",
      "Iteration Number: 3673\n",
      "Loss: 17.782613962309796\n",
      "l2 norm of gradients: 0.19820307180764526\n",
      "l2 norm of weights: 3.333242309568933\n",
      "---------------------\n",
      "Iteration Number: 3674\n",
      "Loss: 17.777926534453915\n",
      "l2 norm of gradients: 0.1981111217754068\n",
      "l2 norm of weights: 3.333030406806033\n",
      "---------------------\n",
      "Iteration Number: 3675\n",
      "Loss: 17.773242623728567\n",
      "l2 norm of gradients: 0.19801923748090522\n",
      "l2 norm of weights: 3.3328187392671227\n",
      "---------------------\n",
      "Iteration Number: 3676\n",
      "Loss: 17.768562226984553\n",
      "l2 norm of gradients: 0.197927418875562\n",
      "l2 norm of weights: 3.3326073067498423\n",
      "---------------------\n",
      "Iteration Number: 3677\n",
      "Loss: 17.76388534107602\n",
      "l2 norm of gradients: 0.19783566591077534\n",
      "l2 norm of weights: 3.332396109052054\n",
      "---------------------\n",
      "Iteration Number: 3678\n",
      "Loss: 17.75921196286013\n",
      "l2 norm of gradients: 0.19774397853791953\n",
      "l2 norm of weights: 3.332185145971846\n",
      "---------------------\n",
      "Iteration Number: 3679\n",
      "Loss: 17.754542089197336\n",
      "l2 norm of gradients: 0.19765235670834588\n",
      "l2 norm of weights: 3.331974417307528\n",
      "---------------------\n",
      "Iteration Number: 3680\n",
      "Loss: 17.7498757169512\n",
      "l2 norm of gradients: 0.19756080037338214\n",
      "l2 norm of weights: 3.331763922857633\n",
      "---------------------\n",
      "Iteration Number: 3681\n",
      "Loss: 17.745212842988376\n",
      "l2 norm of gradients: 0.19746930948433306\n",
      "l2 norm of weights: 3.331553662420918\n",
      "---------------------\n",
      "Iteration Number: 3682\n",
      "Loss: 17.74055346417879\n",
      "l2 norm of gradients: 0.19737788399248024\n",
      "l2 norm of weights: 3.3313436357963635\n",
      "---------------------\n",
      "Iteration Number: 3683\n",
      "Loss: 17.735897577395466\n",
      "l2 norm of gradients: 0.19728652384908238\n",
      "l2 norm of weights: 3.331133842783173\n",
      "---------------------\n",
      "Iteration Number: 3684\n",
      "Loss: 17.731245179514605\n",
      "l2 norm of gradients: 0.19719522900537542\n",
      "l2 norm of weights: 3.3309242831807726\n",
      "---------------------\n",
      "Iteration Number: 3685\n",
      "Loss: 17.726596267415523\n",
      "l2 norm of gradients: 0.1971039994125725\n",
      "l2 norm of weights: 3.330714956788812\n",
      "---------------------\n",
      "Iteration Number: 3686\n",
      "Loss: 17.72195083798081\n",
      "l2 norm of gradients: 0.19701283502186423\n",
      "l2 norm of weights: 3.3305058634071663\n",
      "---------------------\n",
      "Iteration Number: 3687\n",
      "Loss: 17.717308888096085\n",
      "l2 norm of gradients: 0.1969217357844187\n",
      "l2 norm of weights: 3.3302970028359296\n",
      "---------------------\n",
      "Iteration Number: 3688\n",
      "Loss: 17.712670414650237\n",
      "l2 norm of gradients: 0.19683070165138172\n",
      "l2 norm of weights: 3.3300883748754218\n",
      "---------------------\n",
      "Iteration Number: 3689\n",
      "Loss: 17.70803541453518\n",
      "l2 norm of gradients: 0.19673973257387678\n",
      "l2 norm of weights: 3.3298799793261846\n",
      "---------------------\n",
      "Iteration Number: 3690\n",
      "Loss: 17.70340388464609\n",
      "l2 norm of gradients: 0.19664882850300516\n",
      "l2 norm of weights: 3.329671815988985\n",
      "---------------------\n",
      "Iteration Number: 3691\n",
      "Loss: 17.698775821881295\n",
      "l2 norm of gradients: 0.19655798938984628\n",
      "l2 norm of weights: 3.3294638846648095\n",
      "---------------------\n",
      "Iteration Number: 3692\n",
      "Loss: 17.694151223142182\n",
      "l2 norm of gradients: 0.19646721518545746\n",
      "l2 norm of weights: 3.32925618515487\n",
      "---------------------\n",
      "Iteration Number: 3693\n",
      "Loss: 17.689530085333388\n",
      "l2 norm of gradients: 0.1963765058408743\n",
      "l2 norm of weights: 3.3290487172606\n",
      "---------------------\n",
      "Iteration Number: 3694\n",
      "Loss: 17.68491240536271\n",
      "l2 norm of gradients: 0.19628586130711076\n",
      "l2 norm of weights: 3.328841480783656\n",
      "---------------------\n",
      "Iteration Number: 3695\n",
      "Loss: 17.68029818014099\n",
      "l2 norm of gradients: 0.19619528153515914\n",
      "l2 norm of weights: 3.3286344755259187\n",
      "---------------------\n",
      "Iteration Number: 3696\n",
      "Loss: 17.675687406582306\n",
      "l2 norm of gradients: 0.19610476647599034\n",
      "l2 norm of weights: 3.3284277012894887\n",
      "---------------------\n",
      "Iteration Number: 3697\n",
      "Loss: 17.671080081603915\n",
      "l2 norm of gradients: 0.19601431608055372\n",
      "l2 norm of weights: 3.328221157876691\n",
      "---------------------\n",
      "Iteration Number: 3698\n",
      "Loss: 17.666476202126113\n",
      "l2 norm of gradients: 0.1959239302997777\n",
      "l2 norm of weights: 3.328014845090073\n",
      "---------------------\n",
      "Iteration Number: 3699\n",
      "Loss: 17.661875765072445\n",
      "l2 norm of gradients: 0.19583360908456934\n",
      "l2 norm of weights: 3.327808762732404\n",
      "---------------------\n",
      "Iteration Number: 3700\n",
      "Loss: 17.657278767369505\n",
      "l2 norm of gradients: 0.1957433523858146\n",
      "l2 norm of weights: 3.3276029106066765\n",
      "---------------------\n",
      "Iteration Number: 3701\n",
      "Loss: 17.65268520594717\n",
      "l2 norm of gradients: 0.19565316015437878\n",
      "l2 norm of weights: 3.3273972885161047\n",
      "---------------------\n",
      "Iteration Number: 3702\n",
      "Loss: 17.648095077738333\n",
      "l2 norm of gradients: 0.19556303234110617\n",
      "l2 norm of weights: 3.3271918962641247\n",
      "---------------------\n",
      "Iteration Number: 3703\n",
      "Loss: 17.643508379679112\n",
      "l2 norm of gradients: 0.19547296889682045\n",
      "l2 norm of weights: 3.326986733654396\n",
      "---------------------\n",
      "Iteration Number: 3704\n",
      "Loss: 17.638925108708754\n",
      "l2 norm of gradients: 0.1953829697723247\n",
      "l2 norm of weights: 3.3267818004907994\n",
      "---------------------\n",
      "Iteration Number: 3705\n",
      "Loss: 17.63434526176959\n",
      "l2 norm of gradients: 0.19529303491840147\n",
      "l2 norm of weights: 3.326577096577437\n",
      "---------------------\n",
      "Iteration Number: 3706\n",
      "Loss: 17.629768835807162\n",
      "l2 norm of gradients: 0.19520316428581302\n",
      "l2 norm of weights: 3.326372621718635\n",
      "---------------------\n",
      "Iteration Number: 3707\n",
      "Loss: 17.62519582777015\n",
      "l2 norm of gradients: 0.19511335782530131\n",
      "l2 norm of weights: 3.3261683757189395\n",
      "---------------------\n",
      "Iteration Number: 3708\n",
      "Loss: 17.620626234610345\n",
      "l2 norm of gradients: 0.1950236154875881\n",
      "l2 norm of weights: 3.325964358383119\n",
      "---------------------\n",
      "Iteration Number: 3709\n",
      "Loss: 17.6160600532827\n",
      "l2 norm of gradients: 0.19493393722337524\n",
      "l2 norm of weights: 3.3257605695161643\n",
      "---------------------\n",
      "Iteration Number: 3710\n",
      "Loss: 17.611497280745223\n",
      "l2 norm of gradients: 0.19484432298334453\n",
      "l2 norm of weights: 3.3255570089232864\n",
      "---------------------\n",
      "Iteration Number: 3711\n",
      "Loss: 17.606937913959214\n",
      "l2 norm of gradients: 0.19475477271815794\n",
      "l2 norm of weights: 3.3253536764099207\n",
      "---------------------\n",
      "Iteration Number: 3712\n",
      "Loss: 17.60238194988904\n",
      "l2 norm of gradients: 0.1946652863784578\n",
      "l2 norm of weights: 3.32515057178172\n",
      "---------------------\n",
      "Iteration Number: 3713\n",
      "Loss: 17.597829385502095\n",
      "l2 norm of gradients: 0.19457586391486684\n",
      "l2 norm of weights: 3.3249476948445627\n",
      "---------------------\n",
      "Iteration Number: 3714\n",
      "Loss: 17.593280217769124\n",
      "l2 norm of gradients: 0.19448650527798814\n",
      "l2 norm of weights: 3.3247450454045446\n",
      "---------------------\n",
      "Iteration Number: 3715\n",
      "Loss: 17.588734443663814\n",
      "l2 norm of gradients: 0.1943972104184055\n",
      "l2 norm of weights: 3.3245426232679858\n",
      "---------------------\n",
      "Iteration Number: 3716\n",
      "Loss: 17.58419206016305\n",
      "l2 norm of gradients: 0.19430797928668342\n",
      "l2 norm of weights: 3.3243404282414257\n",
      "---------------------\n",
      "Iteration Number: 3717\n",
      "Loss: 17.579653064246937\n",
      "l2 norm of gradients: 0.19421881183336728\n",
      "l2 norm of weights: 3.3241384601316253\n",
      "---------------------\n",
      "Iteration Number: 3718\n",
      "Loss: 17.575117452898535\n",
      "l2 norm of gradients: 0.19412970800898324\n",
      "l2 norm of weights: 3.323936718745567\n",
      "---------------------\n",
      "Iteration Number: 3719\n",
      "Loss: 17.5705852231042\n",
      "l2 norm of gradients: 0.1940406677640386\n",
      "l2 norm of weights: 3.3237352038904526\n",
      "---------------------\n",
      "Iteration Number: 3720\n",
      "Loss: 17.56605637185337\n",
      "l2 norm of gradients: 0.1939516910490217\n",
      "l2 norm of weights: 3.323533915373706\n",
      "---------------------\n",
      "Iteration Number: 3721\n",
      "Loss: 17.56153089613852\n",
      "l2 norm of gradients: 0.19386277781440228\n",
      "l2 norm of weights: 3.3233328530029715\n",
      "---------------------\n",
      "Iteration Number: 3722\n",
      "Loss: 17.557008792955347\n",
      "l2 norm of gradients: 0.19377392801063129\n",
      "l2 norm of weights: 3.323132016586113\n",
      "---------------------\n",
      "Iteration Number: 3723\n",
      "Loss: 17.552490059302677\n",
      "l2 norm of gradients: 0.1936851415881412\n",
      "l2 norm of weights: 3.322931405931216\n",
      "---------------------\n",
      "Iteration Number: 3724\n",
      "Loss: 17.547974692182425\n",
      "l2 norm of gradients: 0.1935964184973461\n",
      "l2 norm of weights: 3.322731020846585\n",
      "---------------------\n",
      "Iteration Number: 3725\n",
      "Loss: 17.543462688599636\n",
      "l2 norm of gradients: 0.1935077586886415\n",
      "l2 norm of weights: 3.3225308611407454\n",
      "---------------------\n",
      "Iteration Number: 3726\n",
      "Loss: 17.5389540455625\n",
      "l2 norm of gradients: 0.19341916211240498\n",
      "l2 norm of weights: 3.3223309266224432\n",
      "---------------------\n",
      "Iteration Number: 3727\n",
      "Loss: 17.534448760082352\n",
      "l2 norm of gradients: 0.1933306287189958\n",
      "l2 norm of weights: 3.322131217100644\n",
      "---------------------\n",
      "Iteration Number: 3728\n",
      "Loss: 17.52994682917354\n",
      "l2 norm of gradients: 0.19324215845875534\n",
      "l2 norm of weights: 3.321931732384532\n",
      "---------------------\n",
      "Iteration Number: 3729\n",
      "Loss: 17.525448249853632\n",
      "l2 norm of gradients: 0.19315375128200685\n",
      "l2 norm of weights: 3.321732472283513\n",
      "---------------------\n",
      "Iteration Number: 3730\n",
      "Loss: 17.52095301914335\n",
      "l2 norm of gradients: 0.193065407139056\n",
      "l2 norm of weights: 3.321533436607212\n",
      "---------------------\n",
      "Iteration Number: 3731\n",
      "Loss: 17.51646113406639\n",
      "l2 norm of gradients: 0.19297712598019073\n",
      "l2 norm of weights: 3.321334625165472\n",
      "---------------------\n",
      "Iteration Number: 3732\n",
      "Loss: 17.51197259164967\n",
      "l2 norm of gradients: 0.19288890775568107\n",
      "l2 norm of weights: 3.3211360377683574\n",
      "---------------------\n",
      "Iteration Number: 3733\n",
      "Loss: 17.507487388923273\n",
      "l2 norm of gradients: 0.19280075241578007\n",
      "l2 norm of weights: 3.3209376742261507\n",
      "---------------------\n",
      "Iteration Number: 3734\n",
      "Loss: 17.503005522920247\n",
      "l2 norm of gradients: 0.19271265991072278\n",
      "l2 norm of weights: 3.3207395343493533\n",
      "---------------------\n",
      "Iteration Number: 3735\n",
      "Loss: 17.498526990676908\n",
      "l2 norm of gradients: 0.19262463019072748\n",
      "l2 norm of weights: 3.3205416179486873\n",
      "---------------------\n",
      "Iteration Number: 3736\n",
      "Loss: 17.49405178923257\n",
      "l2 norm of gradients: 0.1925366632059949\n",
      "l2 norm of weights: 3.320343924835092\n",
      "---------------------\n",
      "Iteration Number: 3737\n",
      "Loss: 17.489579915629747\n",
      "l2 norm of gradients: 0.19244875890670896\n",
      "l2 norm of weights: 3.320146454819725\n",
      "---------------------\n",
      "Iteration Number: 3738\n",
      "Loss: 17.48511136691397\n",
      "l2 norm of gradients: 0.1923609172430363\n",
      "l2 norm of weights: 3.3199492077139645\n",
      "---------------------\n",
      "Iteration Number: 3739\n",
      "Loss: 17.48064614013399\n",
      "l2 norm of gradients: 0.19227313816512687\n",
      "l2 norm of weights: 3.319752183329406\n",
      "---------------------\n",
      "Iteration Number: 3740\n",
      "Loss: 17.4761842323416\n",
      "l2 norm of gradients: 0.1921854216231138\n",
      "l2 norm of weights: 3.3195553814778647\n",
      "---------------------\n",
      "Iteration Number: 3741\n",
      "Loss: 17.471725640591718\n",
      "l2 norm of gradients: 0.1920977675671135\n",
      "l2 norm of weights: 3.319358801971371\n",
      "---------------------\n",
      "Iteration Number: 3742\n",
      "Loss: 17.467270361942372\n",
      "l2 norm of gradients: 0.19201017594722575\n",
      "l2 norm of weights: 3.3191624446221764\n",
      "---------------------\n",
      "Iteration Number: 3743\n",
      "Loss: 17.462818393454626\n",
      "l2 norm of gradients: 0.19192264671353396\n",
      "l2 norm of weights: 3.3189663092427493\n",
      "---------------------\n",
      "Iteration Number: 3744\n",
      "Loss: 17.458369732192853\n",
      "l2 norm of gradients: 0.1918351798161051\n",
      "l2 norm of weights: 3.318770395645776\n",
      "---------------------\n",
      "Iteration Number: 3745\n",
      "Loss: 17.4539243752243\n",
      "l2 norm of gradients: 0.19174777520498987\n",
      "l2 norm of weights: 3.3185747036441606\n",
      "---------------------\n",
      "Iteration Number: 3746\n",
      "Loss: 17.4494823196194\n",
      "l2 norm of gradients: 0.19166043283022266\n",
      "l2 norm of weights: 3.3183792330510244\n",
      "---------------------\n",
      "Iteration Number: 3747\n",
      "Loss: 17.445043562451747\n",
      "l2 norm of gradients: 0.19157315264182204\n",
      "l2 norm of weights: 3.3181839836797065\n",
      "---------------------\n",
      "Iteration Number: 3748\n",
      "Loss: 17.44060810079794\n",
      "l2 norm of gradients: 0.19148593458979032\n",
      "l2 norm of weights: 3.3179889553437625\n",
      "---------------------\n",
      "Iteration Number: 3749\n",
      "Loss: 17.436175931737836\n",
      "l2 norm of gradients: 0.19139877862411414\n",
      "l2 norm of weights: 3.317794147856967\n",
      "---------------------\n",
      "Iteration Number: 3750\n",
      "Loss: 17.431747052354144\n",
      "l2 norm of gradients: 0.19131168469476428\n",
      "l2 norm of weights: 3.3175995610333096\n",
      "---------------------\n",
      "Iteration Number: 3751\n",
      "Loss: 17.427321459732905\n",
      "l2 norm of gradients: 0.1912246527516957\n",
      "l2 norm of weights: 3.317405194686997\n",
      "---------------------\n",
      "Iteration Number: 3752\n",
      "Loss: 17.422899150963115\n",
      "l2 norm of gradients: 0.19113768274484805\n",
      "l2 norm of weights: 3.3172110486324544\n",
      "---------------------\n",
      "Iteration Number: 3753\n",
      "Loss: 17.41848012313697\n",
      "l2 norm of gradients: 0.19105077462414521\n",
      "l2 norm of weights: 3.3170171226843213\n",
      "---------------------\n",
      "Iteration Number: 3754\n",
      "Loss: 17.41406437334965\n",
      "l2 norm of gradients: 0.19096392833949583\n",
      "l2 norm of weights: 3.3168234166574546\n",
      "---------------------\n",
      "Iteration Number: 3755\n",
      "Loss: 17.409651898699465\n",
      "l2 norm of gradients: 0.19087714384079327\n",
      "l2 norm of weights: 3.3166299303669278\n",
      "---------------------\n",
      "Iteration Number: 3756\n",
      "Loss: 17.405242696287925\n",
      "l2 norm of gradients: 0.19079042107791563\n",
      "l2 norm of weights: 3.31643666362803\n",
      "---------------------\n",
      "Iteration Number: 3757\n",
      "Loss: 17.400836763219488\n",
      "l2 norm of gradients: 0.1907037600007259\n",
      "l2 norm of weights: 3.316243616256266\n",
      "---------------------\n",
      "Iteration Number: 3758\n",
      "Loss: 17.396434096601745\n",
      "l2 norm of gradients: 0.19061716055907216\n",
      "l2 norm of weights: 3.316050788067357\n",
      "---------------------\n",
      "Iteration Number: 3759\n",
      "Loss: 17.39203469354541\n",
      "l2 norm of gradients: 0.19053062270278748\n",
      "l2 norm of weights: 3.315858178877239\n",
      "---------------------\n",
      "Iteration Number: 3760\n",
      "Loss: 17.387638551164233\n",
      "l2 norm of gradients: 0.19044414638169016\n",
      "l2 norm of weights: 3.315665788502065\n",
      "---------------------\n",
      "Iteration Number: 3761\n",
      "Loss: 17.38324566657513\n",
      "l2 norm of gradients: 0.19035773154558383\n",
      "l2 norm of weights: 3.3154736167582013\n",
      "---------------------\n",
      "Iteration Number: 3762\n",
      "Loss: 17.378856036898053\n",
      "l2 norm of gradients: 0.19027137814425749\n",
      "l2 norm of weights: 3.3152816634622306\n",
      "---------------------\n",
      "Iteration Number: 3763\n",
      "Loss: 17.374469659255997\n",
      "l2 norm of gradients: 0.19018508612748553\n",
      "l2 norm of weights: 3.3150899284309507\n",
      "---------------------\n",
      "Iteration Number: 3764\n",
      "Loss: 17.370086530775126\n",
      "l2 norm of gradients: 0.19009885544502808\n",
      "l2 norm of weights: 3.3148984114813733\n",
      "---------------------\n",
      "Iteration Number: 3765\n",
      "Loss: 17.365706648584613\n",
      "l2 norm of gradients: 0.1900126860466308\n",
      "l2 norm of weights: 3.314707112430726\n",
      "---------------------\n",
      "Iteration Number: 3766\n",
      "Loss: 17.36133000981675\n",
      "l2 norm of gradients: 0.18992657788202516\n",
      "l2 norm of weights: 3.3145160310964488\n",
      "---------------------\n",
      "Iteration Number: 3767\n",
      "Loss: 17.356956611606925\n",
      "l2 norm of gradients: 0.1898405309009285\n",
      "l2 norm of weights: 3.3143251672961993\n",
      "---------------------\n",
      "Iteration Number: 3768\n",
      "Loss: 17.352586451093575\n",
      "l2 norm of gradients: 0.18975454505304412\n",
      "l2 norm of weights: 3.314134520847846\n",
      "---------------------\n",
      "Iteration Number: 3769\n",
      "Loss: 17.348219525418237\n",
      "l2 norm of gradients: 0.1896686202880614\n",
      "l2 norm of weights: 3.3139440915694722\n",
      "---------------------\n",
      "Iteration Number: 3770\n",
      "Loss: 17.343855831725463\n",
      "l2 norm of gradients: 0.18958275655565568\n",
      "l2 norm of weights: 3.313753879279377\n",
      "---------------------\n",
      "Iteration Number: 3771\n",
      "Loss: 17.339495367162975\n",
      "l2 norm of gradients: 0.18949695380548884\n",
      "l2 norm of weights: 3.3135638837960713\n",
      "---------------------\n",
      "Iteration Number: 3772\n",
      "Loss: 17.335138128881525\n",
      "l2 norm of gradients: 0.18941121198720884\n",
      "l2 norm of weights: 3.3133741049382794\n",
      "---------------------\n",
      "Iteration Number: 3773\n",
      "Loss: 17.330784114034927\n",
      "l2 norm of gradients: 0.18932553105045025\n",
      "l2 norm of weights: 3.3131845425249398\n",
      "---------------------\n",
      "Iteration Number: 3774\n",
      "Loss: 17.326433319780044\n",
      "l2 norm of gradients: 0.18923991094483394\n",
      "l2 norm of weights: 3.312995196375203\n",
      "---------------------\n",
      "Iteration Number: 3775\n",
      "Loss: 17.322085743276855\n",
      "l2 norm of gradients: 0.1891543516199676\n",
      "l2 norm of weights: 3.312806066308434\n",
      "---------------------\n",
      "Iteration Number: 3776\n",
      "Loss: 17.317741381688442\n",
      "l2 norm of gradients: 0.18906885302544552\n",
      "l2 norm of weights: 3.3126171521442083\n",
      "---------------------\n",
      "Iteration Number: 3777\n",
      "Loss: 17.313400232180896\n",
      "l2 norm of gradients: 0.18898341511084876\n",
      "l2 norm of weights: 3.312428453702317\n",
      "---------------------\n",
      "Iteration Number: 3778\n",
      "Loss: 17.309062291923333\n",
      "l2 norm of gradients: 0.18889803782574544\n",
      "l2 norm of weights: 3.312239970802761\n",
      "---------------------\n",
      "Iteration Number: 3779\n",
      "Loss: 17.304727558088064\n",
      "l2 norm of gradients: 0.1888127211196903\n",
      "l2 norm of weights: 3.3120517032657535\n",
      "---------------------\n",
      "Iteration Number: 3780\n",
      "Loss: 17.300396027850343\n",
      "l2 norm of gradients: 0.18872746494222561\n",
      "l2 norm of weights: 3.311863650911723\n",
      "---------------------\n",
      "Iteration Number: 3781\n",
      "Loss: 17.29606769838854\n",
      "l2 norm of gradients: 0.18864226924288044\n",
      "l2 norm of weights: 3.3116758135613047\n",
      "---------------------\n",
      "Iteration Number: 3782\n",
      "Loss: 17.291742566884086\n",
      "l2 norm of gradients: 0.1885571339711713\n",
      "l2 norm of weights: 3.3114881910353495\n",
      "---------------------\n",
      "Iteration Number: 3783\n",
      "Loss: 17.287420630521513\n",
      "l2 norm of gradients: 0.18847205907660192\n",
      "l2 norm of weights: 3.3113007831549193\n",
      "---------------------\n",
      "Iteration Number: 3784\n",
      "Loss: 17.28310188648832\n",
      "l2 norm of gradients: 0.1883870445086636\n",
      "l2 norm of weights: 3.311113589741285\n",
      "---------------------\n",
      "Iteration Number: 3785\n",
      "Loss: 17.278786331975166\n",
      "l2 norm of gradients: 0.188302090216835\n",
      "l2 norm of weights: 3.3109266106159305\n",
      "---------------------\n",
      "Iteration Number: 3786\n",
      "Loss: 17.274473964175662\n",
      "l2 norm of gradients: 0.18821719615058258\n",
      "l2 norm of weights: 3.3107398456005512\n",
      "---------------------\n",
      "Iteration Number: 3787\n",
      "Loss: 17.270164780286557\n",
      "l2 norm of gradients: 0.1881323622593604\n",
      "l2 norm of weights: 3.3105532945170513\n",
      "---------------------\n",
      "Iteration Number: 3788\n",
      "Loss: 17.26585877750762\n",
      "l2 norm of gradients: 0.18804758849261027\n",
      "l2 norm of weights: 3.310366957187546\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 3789\n",
      "Loss: 17.261555953041675\n",
      "l2 norm of gradients: 0.18796287479976198\n",
      "l2 norm of weights: 3.3101808334343628\n",
      "---------------------\n",
      "Iteration Number: 3790\n",
      "Loss: 17.257256304094636\n",
      "l2 norm of gradients: 0.18787822113023314\n",
      "l2 norm of weights: 3.3099949230800365\n",
      "---------------------\n",
      "Iteration Number: 3791\n",
      "Loss: 17.252959827875408\n",
      "l2 norm of gradients: 0.18779362743342967\n",
      "l2 norm of weights: 3.309809225947314\n",
      "---------------------\n",
      "Iteration Number: 3792\n",
      "Loss: 17.24866652159603\n",
      "l2 norm of gradients: 0.1877090936587454\n",
      "l2 norm of weights: 3.3096237418591503\n",
      "---------------------\n",
      "Iteration Number: 3793\n",
      "Loss: 17.24437638247147\n",
      "l2 norm of gradients: 0.1876246197555626\n",
      "l2 norm of weights: 3.309438470638712\n",
      "---------------------\n",
      "Iteration Number: 3794\n",
      "Loss: 17.24008940771983\n",
      "l2 norm of gradients: 0.18754020567325164\n",
      "l2 norm of weights: 3.3092534121093733\n",
      "---------------------\n",
      "Iteration Number: 3795\n",
      "Loss: 17.235805594562244\n",
      "l2 norm of gradients: 0.18745585136117146\n",
      "l2 norm of weights: 3.309068566094718\n",
      "---------------------\n",
      "Iteration Number: 3796\n",
      "Loss: 17.23152494022289\n",
      "l2 norm of gradients: 0.1873715567686694\n",
      "l2 norm of weights: 3.3088839324185395\n",
      "---------------------\n",
      "Iteration Number: 3797\n",
      "Loss: 17.227247441928974\n",
      "l2 norm of gradients: 0.1872873218450815\n",
      "l2 norm of weights: 3.3086995109048387\n",
      "---------------------\n",
      "Iteration Number: 3798\n",
      "Loss: 17.2229730969108\n",
      "l2 norm of gradients: 0.18720314653973247\n",
      "l2 norm of weights: 3.308515301377827\n",
      "---------------------\n",
      "Iteration Number: 3799\n",
      "Loss: 17.218701902401584\n",
      "l2 norm of gradients: 0.18711903080193562\n",
      "l2 norm of weights: 3.308331303661922\n",
      "---------------------\n",
      "Iteration Number: 3800\n",
      "Loss: 17.214433855637747\n",
      "l2 norm of gradients: 0.18703497458099327\n",
      "l2 norm of weights: 3.308147517581751\n",
      "---------------------\n",
      "Iteration Number: 3801\n",
      "Loss: 17.21016895385865\n",
      "l2 norm of gradients: 0.18695097782619657\n",
      "l2 norm of weights: 3.3079639429621492\n",
      "---------------------\n",
      "Iteration Number: 3802\n",
      "Loss: 17.20590719430667\n",
      "l2 norm of gradients: 0.18686704048682584\n",
      "l2 norm of weights: 3.3077805796281576\n",
      "---------------------\n",
      "Iteration Number: 3803\n",
      "Loss: 17.20164857422728\n",
      "l2 norm of gradients: 0.18678316251215027\n",
      "l2 norm of weights: 3.3075974274050277\n",
      "---------------------\n",
      "Iteration Number: 3804\n",
      "Loss: 17.19739309086896\n",
      "l2 norm of gradients: 0.1866993438514285\n",
      "l2 norm of weights: 3.307414486118216\n",
      "---------------------\n",
      "Iteration Number: 3805\n",
      "Loss: 17.193140741483276\n",
      "l2 norm of gradients: 0.1866155844539082\n",
      "l2 norm of weights: 3.3072317555933872\n",
      "---------------------\n",
      "Iteration Number: 3806\n",
      "Loss: 17.18889152332473\n",
      "l2 norm of gradients: 0.1865318842688266\n",
      "l2 norm of weights: 3.307049235656413\n",
      "---------------------\n",
      "Iteration Number: 3807\n",
      "Loss: 17.184645433650914\n",
      "l2 norm of gradients: 0.1864482432454102\n",
      "l2 norm of weights: 3.306866926133371\n",
      "---------------------\n",
      "Iteration Number: 3808\n",
      "Loss: 17.180402469722438\n",
      "l2 norm of gradients: 0.18636466133287521\n",
      "l2 norm of weights: 3.306684826850546\n",
      "---------------------\n",
      "Iteration Number: 3809\n",
      "Loss: 17.176162628802984\n",
      "l2 norm of gradients: 0.18628113848042735\n",
      "l2 norm of weights: 3.3065029376344293\n",
      "---------------------\n",
      "Iteration Number: 3810\n",
      "Loss: 17.171925908159178\n",
      "l2 norm of gradients: 0.18619767463726197\n",
      "l2 norm of weights: 3.3063212583117165\n",
      "---------------------\n",
      "Iteration Number: 3811\n",
      "Loss: 17.167692305060754\n",
      "l2 norm of gradients: 0.1861142697525644\n",
      "l2 norm of weights: 3.306139788709312\n",
      "---------------------\n",
      "Iteration Number: 3812\n",
      "Loss: 17.163461816780373\n",
      "l2 norm of gradients: 0.1860309237755096\n",
      "l2 norm of weights: 3.305958528654323\n",
      "---------------------\n",
      "Iteration Number: 3813\n",
      "Loss: 17.15923444059383\n",
      "l2 norm of gradients: 0.18594763665526273\n",
      "l2 norm of weights: 3.3057774779740643\n",
      "---------------------\n",
      "Iteration Number: 3814\n",
      "Loss: 17.15501017377986\n",
      "l2 norm of gradients: 0.18586440834097873\n",
      "l2 norm of weights: 3.3055966364960536\n",
      "---------------------\n",
      "Iteration Number: 3815\n",
      "Loss: 17.15078901362027\n",
      "l2 norm of gradients: 0.18578123878180278\n",
      "l2 norm of weights: 3.3054160040480163\n",
      "---------------------\n",
      "Iteration Number: 3816\n",
      "Loss: 17.146570957399817\n",
      "l2 norm of gradients: 0.18569812792687027\n",
      "l2 norm of weights: 3.30523558045788\n",
      "---------------------\n",
      "Iteration Number: 3817\n",
      "Loss: 17.142356002406355\n",
      "l2 norm of gradients: 0.1856150757253069\n",
      "l2 norm of weights: 3.3050553655537795\n",
      "---------------------\n",
      "Iteration Number: 3818\n",
      "Loss: 17.138144145930728\n",
      "l2 norm of gradients: 0.1855320821262286\n",
      "l2 norm of weights: 3.3048753591640514\n",
      "---------------------\n",
      "Iteration Number: 3819\n",
      "Loss: 17.133935385266785\n",
      "l2 norm of gradients: 0.1854491470787418\n",
      "l2 norm of weights: 3.3046955611172377\n",
      "---------------------\n",
      "Iteration Number: 3820\n",
      "Loss: 17.129729717711363\n",
      "l2 norm of gradients: 0.18536627053194357\n",
      "l2 norm of weights: 3.304515971242084\n",
      "---------------------\n",
      "Iteration Number: 3821\n",
      "Loss: 17.125527140564333\n",
      "l2 norm of gradients: 0.18528345243492136\n",
      "l2 norm of weights: 3.3043365893675394\n",
      "---------------------\n",
      "Iteration Number: 3822\n",
      "Loss: 17.12132765112864\n",
      "l2 norm of gradients: 0.18520069273675352\n",
      "l2 norm of weights: 3.304157415322757\n",
      "---------------------\n",
      "Iteration Number: 3823\n",
      "Loss: 17.11713124671015\n",
      "l2 norm of gradients: 0.185117991386509\n",
      "l2 norm of weights: 3.303978448937093\n",
      "---------------------\n",
      "Iteration Number: 3824\n",
      "Loss: 17.112937924617754\n",
      "l2 norm of gradients: 0.18503534833324783\n",
      "l2 norm of weights: 3.303799690040106\n",
      "---------------------\n",
      "Iteration Number: 3825\n",
      "Loss: 17.108747682163354\n",
      "l2 norm of gradients: 0.18495276352602064\n",
      "l2 norm of weights: 3.303621138461558\n",
      "---------------------\n",
      "Iteration Number: 3826\n",
      "Loss: 17.10456051666192\n",
      "l2 norm of gradients: 0.1848702369138693\n",
      "l2 norm of weights: 3.303442794031413\n",
      "---------------------\n",
      "Iteration Number: 3827\n",
      "Loss: 17.100376425431335\n",
      "l2 norm of gradients: 0.18478776844582667\n",
      "l2 norm of weights: 3.3032646565798376\n",
      "---------------------\n",
      "Iteration Number: 3828\n",
      "Loss: 17.096195405792532\n",
      "l2 norm of gradients: 0.18470535807091687\n",
      "l2 norm of weights: 3.303086725937201\n",
      "---------------------\n",
      "Iteration Number: 3829\n",
      "Loss: 17.092017455069445\n",
      "l2 norm of gradients: 0.18462300573815524\n",
      "l2 norm of weights: 3.3029090019340726\n",
      "---------------------\n",
      "Iteration Number: 3830\n",
      "Loss: 17.087842570588986\n",
      "l2 norm of gradients: 0.18454071139654835\n",
      "l2 norm of weights: 3.302731484401225\n",
      "---------------------\n",
      "Iteration Number: 3831\n",
      "Loss: 17.083670749681104\n",
      "l2 norm of gradients: 0.18445847499509427\n",
      "l2 norm of weights: 3.302554173169632\n",
      "---------------------\n",
      "Iteration Number: 3832\n",
      "Loss: 17.079501989678658\n",
      "l2 norm of gradients: 0.18437629648278248\n",
      "l2 norm of weights: 3.3023770680704687\n",
      "---------------------\n",
      "Iteration Number: 3833\n",
      "Loss: 17.07533628791767\n",
      "l2 norm of gradients: 0.1842941758085942\n",
      "l2 norm of weights: 3.30220016893511\n",
      "---------------------\n",
      "Iteration Number: 3834\n",
      "Loss: 17.071173641736948\n",
      "l2 norm of gradients: 0.18421211292150197\n",
      "l2 norm of weights: 3.3020234755951314\n",
      "---------------------\n",
      "Iteration Number: 3835\n",
      "Loss: 17.067014048478466\n",
      "l2 norm of gradients: 0.18413010777047042\n",
      "l2 norm of weights: 3.3018469878823113\n",
      "---------------------\n",
      "Iteration Number: 3836\n",
      "Loss: 17.062857505487095\n",
      "l2 norm of gradients: 0.18404816030445562\n",
      "l2 norm of weights: 3.3016707056286263\n",
      "---------------------\n",
      "Iteration Number: 3837\n",
      "Loss: 17.058704010110702\n",
      "l2 norm of gradients: 0.18396627047240585\n",
      "l2 norm of weights: 3.3014946286662523\n",
      "---------------------\n",
      "Iteration Number: 3838\n",
      "Loss: 17.054553559700196\n",
      "l2 norm of gradients: 0.18388443822326103\n",
      "l2 norm of weights: 3.3013187568275675\n",
      "---------------------\n",
      "Iteration Number: 3839\n",
      "Loss: 17.050406151609426\n",
      "l2 norm of gradients: 0.18380266350595337\n",
      "l2 norm of weights: 3.3011430899451466\n",
      "---------------------\n",
      "Iteration Number: 3840\n",
      "Loss: 17.046261783195266\n",
      "l2 norm of gradients: 0.18372094626940702\n",
      "l2 norm of weights: 3.3009676278517666\n",
      "---------------------\n",
      "Iteration Number: 3841\n",
      "Loss: 17.0421204518175\n",
      "l2 norm of gradients: 0.18363928646253835\n",
      "l2 norm of weights: 3.3007923703804014\n",
      "---------------------\n",
      "Iteration Number: 3842\n",
      "Loss: 17.037982154838964\n",
      "l2 norm of gradients: 0.18355768403425607\n",
      "l2 norm of weights: 3.300617317364224\n",
      "---------------------\n",
      "Iteration Number: 3843\n",
      "Loss: 17.033846889625508\n",
      "l2 norm of gradients: 0.18347613893346107\n",
      "l2 norm of weights: 3.300442468636607\n",
      "---------------------\n",
      "Iteration Number: 3844\n",
      "Loss: 17.02971465354583\n",
      "l2 norm of gradients: 0.18339465110904674\n",
      "l2 norm of weights: 3.3002678240311205\n",
      "---------------------\n",
      "Iteration Number: 3845\n",
      "Loss: 17.025585443971735\n",
      "l2 norm of gradients: 0.183313220509899\n",
      "l2 norm of weights: 3.3000933833815327\n",
      "---------------------\n",
      "Iteration Number: 3846\n",
      "Loss: 17.021459258277957\n",
      "l2 norm of gradients: 0.18323184708489632\n",
      "l2 norm of weights: 3.29991914652181\n",
      "---------------------\n",
      "Iteration Number: 3847\n",
      "Loss: 17.017336093842175\n",
      "l2 norm of gradients: 0.1831505307829097\n",
      "l2 norm of weights: 3.2997451132861157\n",
      "---------------------\n",
      "Iteration Number: 3848\n",
      "Loss: 17.0132159480451\n",
      "l2 norm of gradients: 0.1830692715528029\n",
      "l2 norm of weights: 3.2995712835088113\n",
      "---------------------\n",
      "Iteration Number: 3849\n",
      "Loss: 17.009098818270417\n",
      "l2 norm of gradients: 0.18298806934343254\n",
      "l2 norm of weights: 3.299397657024455\n",
      "---------------------\n",
      "Iteration Number: 3850\n",
      "Loss: 17.0049847019047\n",
      "l2 norm of gradients: 0.1829069241036481\n",
      "l2 norm of weights: 3.2992242336678026\n",
      "---------------------\n",
      "Iteration Number: 3851\n",
      "Loss: 17.000873596337627\n",
      "l2 norm of gradients: 0.1828258357822919\n",
      "l2 norm of weights: 3.299051013273805\n",
      "---------------------\n",
      "Iteration Number: 3852\n",
      "Loss: 16.99676549896168\n",
      "l2 norm of gradients: 0.1827448043281993\n",
      "l2 norm of weights: 3.298877995677611\n",
      "---------------------\n",
      "Iteration Number: 3853\n",
      "Loss: 16.992660407172455\n",
      "l2 norm of gradients: 0.18266382969019884\n",
      "l2 norm of weights: 3.298705180714564\n",
      "---------------------\n",
      "Iteration Number: 3854\n",
      "Loss: 16.988558318368433\n",
      "l2 norm of gradients: 0.18258291181711211\n",
      "l2 norm of weights: 3.2985325682202054\n",
      "---------------------\n",
      "Iteration Number: 3855\n",
      "Loss: 16.984459229951103\n",
      "l2 norm of gradients: 0.182502050657754\n",
      "l2 norm of weights: 3.29836015803027\n",
      "---------------------\n",
      "Iteration Number: 3856\n",
      "Loss: 16.980363139324865\n",
      "l2 norm of gradients: 0.1824212461609326\n",
      "l2 norm of weights: 3.298187949980689\n",
      "---------------------\n",
      "Iteration Number: 3857\n",
      "Loss: 16.976270043897124\n",
      "l2 norm of gradients: 0.18234049827544957\n",
      "l2 norm of weights: 3.29801594390759\n",
      "---------------------\n",
      "Iteration Number: 3858\n",
      "Loss: 16.972179941078295\n",
      "l2 norm of gradients: 0.18225980695009977\n",
      "l2 norm of weights: 3.2978441396472933\n",
      "---------------------\n",
      "Iteration Number: 3859\n",
      "Loss: 16.968092828281577\n",
      "l2 norm of gradients: 0.18217917213367188\n",
      "l2 norm of weights: 3.297672537036315\n",
      "---------------------\n",
      "Iteration Number: 3860\n",
      "Loss: 16.96400870292333\n",
      "l2 norm of gradients: 0.18209859377494794\n",
      "l2 norm of weights: 3.2975011359113653\n",
      "---------------------\n",
      "Iteration Number: 3861\n",
      "Loss: 16.959927562422724\n",
      "l2 norm of gradients: 0.18201807182270374\n",
      "l2 norm of weights: 3.297329936109349\n",
      "---------------------\n",
      "Iteration Number: 3862\n",
      "Loss: 16.95584940420197\n",
      "l2 norm of gradients: 0.1819376062257089\n",
      "l2 norm of weights: 3.2971589374673638\n",
      "---------------------\n",
      "Iteration Number: 3863\n",
      "Loss: 16.951774225686183\n",
      "l2 norm of gradients: 0.18185719693272673\n",
      "l2 norm of weights: 3.2969881398227026\n",
      "---------------------\n",
      "Iteration Number: 3864\n",
      "Loss: 16.947702024303474\n",
      "l2 norm of gradients: 0.18177684389251447\n",
      "l2 norm of weights: 3.2968175430128506\n",
      "---------------------\n",
      "Iteration Number: 3865\n",
      "Loss: 16.943632797484852\n",
      "l2 norm of gradients: 0.18169654705382332\n",
      "l2 norm of weights: 3.2966471468754857\n",
      "---------------------\n",
      "Iteration Number: 3866\n",
      "Loss: 16.939566542664284\n",
      "l2 norm of gradients: 0.1816163063653986\n",
      "l2 norm of weights: 3.29647695124848\n",
      "---------------------\n",
      "Iteration Number: 3867\n",
      "Loss: 16.935503257278757\n",
      "l2 norm of gradients: 0.18153612177597958\n",
      "l2 norm of weights: 3.296306955969898\n",
      "---------------------\n",
      "Iteration Number: 3868\n",
      "Loss: 16.931442938768097\n",
      "l2 norm of gradients: 0.18145599323429976\n",
      "l2 norm of weights: 3.296137160877995\n",
      "---------------------\n",
      "Iteration Number: 3869\n",
      "Loss: 16.927385584575124\n",
      "l2 norm of gradients: 0.18137592068908695\n",
      "l2 norm of weights: 3.29596756581122\n",
      "---------------------\n",
      "Iteration Number: 3870\n",
      "Loss: 16.923331192145596\n",
      "l2 norm of gradients: 0.18129590408906318\n",
      "l2 norm of weights: 3.2957981706082147\n",
      "---------------------\n",
      "Iteration Number: 3871\n",
      "Loss: 16.919279758928255\n",
      "l2 norm of gradients: 0.18121594338294494\n",
      "l2 norm of weights: 3.2956289751078094\n",
      "---------------------\n",
      "Iteration Number: 3872\n",
      "Loss: 16.915231282374723\n",
      "l2 norm of gradients: 0.1811360385194432\n",
      "l2 norm of weights: 3.295459979149029\n",
      "---------------------\n",
      "Iteration Number: 3873\n",
      "Loss: 16.911185759939563\n",
      "l2 norm of gradients: 0.18105618944726332\n",
      "l2 norm of weights: 3.295291182571087\n",
      "---------------------\n",
      "Iteration Number: 3874\n",
      "Loss: 16.907143189080305\n",
      "l2 norm of gradients: 0.18097639611510538\n",
      "l2 norm of weights: 3.2951225852133894\n",
      "---------------------\n",
      "Iteration Number: 3875\n",
      "Loss: 16.903103567257382\n",
      "l2 norm of gradients: 0.18089665847166417\n",
      "l2 norm of weights: 3.294954186915532\n",
      "---------------------\n",
      "Iteration Number: 3876\n",
      "Loss: 16.899066891934222\n",
      "l2 norm of gradients: 0.18081697646562905\n",
      "l2 norm of weights: 3.294785987517302\n",
      "---------------------\n",
      "Iteration Number: 3877\n",
      "Loss: 16.895033160577135\n",
      "l2 norm of gradients: 0.1807373500456843\n",
      "l2 norm of weights: 3.2946179868586736\n",
      "---------------------\n",
      "Iteration Number: 3878\n",
      "Loss: 16.89100237065532\n",
      "l2 norm of gradients: 0.18065777916050904\n",
      "l2 norm of weights: 3.2944501847798153\n",
      "---------------------\n",
      "Iteration Number: 3879\n",
      "Loss: 16.88697451964097\n",
      "l2 norm of gradients: 0.18057826375877747\n",
      "l2 norm of weights: 3.294282581121082\n",
      "---------------------\n",
      "Iteration Number: 3880\n",
      "Loss: 16.88294960500921\n",
      "l2 norm of gradients: 0.18049880378915853\n",
      "l2 norm of weights: 3.294115175723019\n",
      "---------------------\n",
      "Iteration Number: 3881\n",
      "Loss: 16.87892762423805\n",
      "l2 norm of gradients: 0.18041939920031652\n",
      "l2 norm of weights: 3.2939479684263597\n",
      "---------------------\n",
      "Iteration Number: 3882\n",
      "Loss: 16.874908574808472\n",
      "l2 norm of gradients: 0.18034004994091066\n",
      "l2 norm of weights: 3.293780959072028\n",
      "---------------------\n",
      "Iteration Number: 3883\n",
      "Loss: 16.870892454204352\n",
      "l2 norm of gradients: 0.18026075595959568\n",
      "l2 norm of weights: 3.2936141475011342\n",
      "---------------------\n",
      "Iteration Number: 3884\n",
      "Loss: 16.86687925991243\n",
      "l2 norm of gradients: 0.1801815172050213\n",
      "l2 norm of weights: 3.2934475335549793\n",
      "---------------------\n",
      "Iteration Number: 3885\n",
      "Loss: 16.86286898942251\n",
      "l2 norm of gradients: 0.18010233362583267\n",
      "l2 norm of weights: 3.2932811170750504\n",
      "---------------------\n",
      "Iteration Number: 3886\n",
      "Loss: 16.858861640227186\n",
      "l2 norm of gradients: 0.18002320517067064\n",
      "l2 norm of weights: 3.2931148979030223\n",
      "---------------------\n",
      "Iteration Number: 3887\n",
      "Loss: 16.854857209822026\n",
      "l2 norm of gradients: 0.17994413178817115\n",
      "l2 norm of weights: 3.292948875880758\n",
      "---------------------\n",
      "Iteration Number: 3888\n",
      "Loss: 16.850855695705462\n",
      "l2 norm of gradients: 0.17986511342696598\n",
      "l2 norm of weights: 3.2927830508503084\n",
      "---------------------\n",
      "Iteration Number: 3889\n",
      "Loss: 16.846857095378894\n",
      "l2 norm of gradients: 0.1797861500356824\n",
      "l2 norm of weights: 3.29261742265391\n",
      "---------------------\n",
      "Iteration Number: 3890\n",
      "Loss: 16.84286140634665\n",
      "l2 norm of gradients: 0.1797072415629435\n",
      "l2 norm of weights: 3.2924519911339862\n",
      "---------------------\n",
      "Iteration Number: 3891\n",
      "Loss: 16.83886862611589\n",
      "l2 norm of gradients: 0.179628387957368\n",
      "l2 norm of weights: 3.2922867561331475\n",
      "---------------------\n",
      "Iteration Number: 3892\n",
      "Loss: 16.834878752196772\n",
      "l2 norm of gradients: 0.17954958916757058\n",
      "l2 norm of weights: 3.2921217174941897\n",
      "---------------------\n",
      "Iteration Number: 3893\n",
      "Loss: 16.8308917821023\n",
      "l2 norm of gradients: 0.17947084514216166\n",
      "l2 norm of weights: 3.291956875060095\n",
      "---------------------\n",
      "Iteration Number: 3894\n",
      "Loss: 16.8269077133484\n",
      "l2 norm of gradients: 0.1793921558297478\n",
      "l2 norm of weights: 3.2917922286740318\n",
      "---------------------\n",
      "Iteration Number: 3895\n",
      "Loss: 16.822926543453907\n",
      "l2 norm of gradients: 0.1793135211789316\n",
      "l2 norm of weights: 3.291627778179352\n",
      "---------------------\n",
      "Iteration Number: 3896\n",
      "Loss: 16.818948269940552\n",
      "l2 norm of gradients: 0.17923494113831157\n",
      "l2 norm of weights: 3.2914635234195946\n",
      "---------------------\n",
      "Iteration Number: 3897\n",
      "Loss: 16.81497289033301\n",
      "l2 norm of gradients: 0.17915641565648252\n",
      "l2 norm of weights: 3.2912994642384827\n",
      "---------------------\n",
      "Iteration Number: 3898\n",
      "Loss: 16.811000402158765\n",
      "l2 norm of gradients: 0.1790779446820355\n",
      "l2 norm of weights: 3.2911356004799233\n",
      "---------------------\n",
      "Iteration Number: 3899\n",
      "Loss: 16.80703080294827\n",
      "l2 norm of gradients: 0.1789995281635578\n",
      "l2 norm of weights: 3.290971931988008\n",
      "---------------------\n",
      "Iteration Number: 3900\n",
      "Loss: 16.80306409023486\n",
      "l2 norm of gradients: 0.17892116604963318\n",
      "l2 norm of weights: 3.2908084586070134\n",
      "---------------------\n",
      "Iteration Number: 3901\n",
      "Loss: 16.799100261554774\n",
      "l2 norm of gradients: 0.17884285828884172\n",
      "l2 norm of weights: 3.2906451801813987\n",
      "---------------------\n",
      "Iteration Number: 3902\n",
      "Loss: 16.795139314447145\n",
      "l2 norm of gradients: 0.1787646048297601\n",
      "l2 norm of weights: 3.290482096555807\n",
      "---------------------\n",
      "Iteration Number: 3903\n",
      "Loss: 16.791181246453927\n",
      "l2 norm of gradients: 0.17868640562096139\n",
      "l2 norm of weights: 3.2903192075750645\n",
      "---------------------\n",
      "Iteration Number: 3904\n",
      "Loss: 16.78722605512012\n",
      "l2 norm of gradients: 0.17860826061101548\n",
      "l2 norm of weights: 3.2901565130841806\n",
      "---------------------\n",
      "Iteration Number: 3905\n",
      "Loss: 16.783273737993436\n",
      "l2 norm of gradients: 0.17853016974848898\n",
      "l2 norm of weights: 3.289994012928347\n",
      "---------------------\n",
      "Iteration Number: 3906\n",
      "Loss: 16.77932429262459\n",
      "l2 norm of gradients: 0.178452132981945\n",
      "l2 norm of weights: 3.2898317069529384\n",
      "---------------------\n",
      "Iteration Number: 3907\n",
      "Loss: 16.775377716567146\n",
      "l2 norm of gradients: 0.17837415025994377\n",
      "l2 norm of weights: 3.2896695950035104\n",
      "---------------------\n",
      "Iteration Number: 3908\n",
      "Loss: 16.77143400737755\n",
      "l2 norm of gradients: 0.17829622153104227\n",
      "l2 norm of weights: 3.289507676925802\n",
      "---------------------\n",
      "Iteration Number: 3909\n",
      "Loss: 16.767493162615104\n",
      "l2 norm of gradients: 0.17821834674379436\n",
      "l2 norm of weights: 3.2893459525657334\n",
      "---------------------\n",
      "Iteration Number: 3910\n",
      "Loss: 16.763555179842093\n",
      "l2 norm of gradients: 0.1781405258467512\n",
      "l2 norm of weights: 3.289184421769405\n",
      "---------------------\n",
      "Iteration Number: 3911\n",
      "Loss: 16.75962005662355\n",
      "l2 norm of gradients: 0.17806275878846067\n",
      "l2 norm of weights: 3.289023084383099\n",
      "---------------------\n",
      "Iteration Number: 3912\n",
      "Loss: 16.7556877905275\n",
      "l2 norm of gradients: 0.17798504551746808\n",
      "l2 norm of weights: 3.2888619402532795\n",
      "---------------------\n",
      "Iteration Number: 3913\n",
      "Loss: 16.75175837912471\n",
      "l2 norm of gradients: 0.17790738598231592\n",
      "l2 norm of weights: 3.2887009892265895\n",
      "---------------------\n",
      "Iteration Number: 3914\n",
      "Loss: 16.747831819988978\n",
      "l2 norm of gradients: 0.1778297801315438\n",
      "l2 norm of weights: 3.288540231149853\n",
      "---------------------\n",
      "Iteration Number: 3915\n",
      "Loss: 16.74390811069689\n",
      "l2 norm of gradients: 0.17775222791368883\n",
      "l2 norm of weights: 3.288379665870073\n",
      "---------------------\n",
      "Iteration Number: 3916\n",
      "Loss: 16.7399872488279\n",
      "l2 norm of gradients: 0.1776747292772856\n",
      "l2 norm of weights: 3.288219293234434\n",
      "---------------------\n",
      "Iteration Number: 3917\n",
      "Loss: 16.736069231964308\n",
      "l2 norm of gradients: 0.17759728417086587\n",
      "l2 norm of weights: 3.2880591130902985\n",
      "---------------------\n",
      "Iteration Number: 3918\n",
      "Loss: 16.732154057691382\n",
      "l2 norm of gradients: 0.17751989254295927\n",
      "l2 norm of weights: 3.287899125285209\n",
      "---------------------\n",
      "Iteration Number: 3919\n",
      "Loss: 16.728241723597144\n",
      "l2 norm of gradients: 0.17744255434209286\n",
      "l2 norm of weights: 3.287739329666885\n",
      "---------------------\n",
      "Iteration Number: 3920\n",
      "Loss: 16.72433222727257\n",
      "l2 norm of gradients: 0.17736526951679146\n",
      "l2 norm of weights: 3.2875797260832282\n",
      "---------------------\n",
      "Iteration Number: 3921\n",
      "Loss: 16.720425566311437\n",
      "l2 norm of gradients: 0.17728803801557746\n",
      "l2 norm of weights: 3.287420314382315\n",
      "---------------------\n",
      "Iteration Number: 3922\n",
      "Loss: 16.71652173831042\n",
      "l2 norm of gradients: 0.17721085978697124\n",
      "l2 norm of weights: 3.287261094412402\n",
      "---------------------\n",
      "Iteration Number: 3923\n",
      "Loss: 16.712620740869053\n",
      "l2 norm of gradients: 0.17713373477949096\n",
      "l2 norm of weights: 3.2871020660219226\n",
      "---------------------\n",
      "Iteration Number: 3924\n",
      "Loss: 16.708722571589682\n",
      "l2 norm of gradients: 0.17705666294165262\n",
      "l2 norm of weights: 3.286943229059488\n",
      "---------------------\n",
      "Iteration Number: 3925\n",
      "Loss: 16.70482722807756\n",
      "l2 norm of gradients: 0.17697964422197046\n",
      "l2 norm of weights: 3.2867845833738873\n",
      "---------------------\n",
      "Iteration Number: 3926\n",
      "Loss: 16.700934707940803\n",
      "l2 norm of gradients: 0.17690267856895645\n",
      "l2 norm of weights: 3.2866261288140852\n",
      "---------------------\n",
      "Iteration Number: 3927\n",
      "Loss: 16.697045008790347\n",
      "l2 norm of gradients: 0.1768257659311209\n",
      "l2 norm of weights: 3.2864678652292247\n",
      "---------------------\n",
      "Iteration Number: 3928\n",
      "Loss: 16.693158128239975\n",
      "l2 norm of gradients: 0.17674890625697218\n",
      "l2 norm of weights: 3.2863097924686233\n",
      "---------------------\n",
      "Iteration Number: 3929\n",
      "Loss: 16.68927406390632\n",
      "l2 norm of gradients: 0.17667209949501697\n",
      "l2 norm of weights: 3.2861519103817765\n",
      "---------------------\n",
      "Iteration Number: 3930\n",
      "Loss: 16.685392813408953\n",
      "l2 norm of gradients: 0.17659534559376025\n",
      "l2 norm of weights: 3.285994218818355\n",
      "---------------------\n",
      "Iteration Number: 3931\n",
      "Loss: 16.68151437437018\n",
      "l2 norm of gradients: 0.1765186445017054\n",
      "l2 norm of weights: 3.285836717628205\n",
      "---------------------\n",
      "Iteration Number: 3932\n",
      "Loss: 16.67763874441516\n",
      "l2 norm of gradients: 0.17644199616735412\n",
      "l2 norm of weights: 3.2856794066613473\n",
      "---------------------\n",
      "Iteration Number: 3933\n",
      "Loss: 16.673765921172002\n",
      "l2 norm of gradients: 0.17636540053920666\n",
      "l2 norm of weights: 3.285522285767979\n",
      "---------------------\n",
      "Iteration Number: 3934\n",
      "Loss: 16.669895902271534\n",
      "l2 norm of gradients: 0.17628885756576193\n",
      "l2 norm of weights: 3.285365354798471\n",
      "---------------------\n",
      "Iteration Number: 3935\n",
      "Loss: 16.666028685347456\n",
      "l2 norm of gradients: 0.17621236719551736\n",
      "l2 norm of weights: 3.2852086136033694\n",
      "---------------------\n",
      "Iteration Number: 3936\n",
      "Loss: 16.662164268036395\n",
      "l2 norm of gradients: 0.17613592937696898\n",
      "l2 norm of weights: 3.2850520620333943\n",
      "---------------------\n",
      "Iteration Number: 3937\n",
      "Loss: 16.65830264797769\n",
      "l2 norm of gradients: 0.17605954405861177\n",
      "l2 norm of weights: 3.28489569993944\n",
      "---------------------\n",
      "Iteration Number: 3938\n",
      "Loss: 16.654443822813608\n",
      "l2 norm of gradients: 0.1759832111889394\n",
      "l2 norm of weights: 3.2847395271725732\n",
      "---------------------\n",
      "Iteration Number: 3939\n",
      "Loss: 16.650587790189167\n",
      "l2 norm of gradients: 0.17590693071644442\n",
      "l2 norm of weights: 3.2845835435840356\n",
      "---------------------\n",
      "Iteration Number: 3940\n",
      "Loss: 16.646734547752338\n",
      "l2 norm of gradients: 0.1758307025896184\n",
      "l2 norm of weights: 3.284427749025241\n",
      "---------------------\n",
      "Iteration Number: 3941\n",
      "Loss: 16.64288409315381\n",
      "l2 norm of gradients: 0.17575452675695175\n",
      "l2 norm of weights: 3.284272143347777\n",
      "---------------------\n",
      "Iteration Number: 3942\n",
      "Loss: 16.639036424047138\n",
      "l2 norm of gradients: 0.17567840316693412\n",
      "l2 norm of weights: 3.284116726403402\n",
      "---------------------\n",
      "Iteration Number: 3943\n",
      "Loss: 16.635191538088705\n",
      "l2 norm of gradients: 0.17560233176805415\n",
      "l2 norm of weights: 3.283961498044049\n",
      "---------------------\n",
      "Iteration Number: 3944\n",
      "Loss: 16.631349432937736\n",
      "l2 norm of gradients: 0.17552631250879974\n",
      "l2 norm of weights: 3.2838064581218203\n",
      "---------------------\n",
      "Iteration Number: 3945\n",
      "Loss: 16.62751010625627\n",
      "l2 norm of gradients: 0.17545034533765808\n",
      "l2 norm of weights: 3.283651606488993\n",
      "---------------------\n",
      "Iteration Number: 3946\n",
      "Loss: 16.623673555709182\n",
      "l2 norm of gradients: 0.17537443020311552\n",
      "l2 norm of weights: 3.2834969429980134\n",
      "---------------------\n",
      "Iteration Number: 3947\n",
      "Loss: 16.619839778964128\n",
      "l2 norm of gradients: 0.17529856705365798\n",
      "l2 norm of weights: 3.2833424675014995\n",
      "---------------------\n",
      "Iteration Number: 3948\n",
      "Loss: 16.616008773691597\n",
      "l2 norm of gradients: 0.17522275583777067\n",
      "l2 norm of weights: 3.2831881798522407\n",
      "---------------------\n",
      "Iteration Number: 3949\n",
      "Loss: 16.612180537564928\n",
      "l2 norm of gradients: 0.17514699650393842\n",
      "l2 norm of weights: 3.2830340799031967\n",
      "---------------------\n",
      "Iteration Number: 3950\n",
      "Loss: 16.608355068260245\n",
      "l2 norm of gradients: 0.17507128900064553\n",
      "l2 norm of weights: 3.2828801675074972\n",
      "---------------------\n",
      "Iteration Number: 3951\n",
      "Loss: 16.60453236345653\n",
      "l2 norm of gradients: 0.17499563327637593\n",
      "l2 norm of weights: 3.2827264425184426\n",
      "---------------------\n",
      "Iteration Number: 3952\n",
      "Loss: 16.60071242083549\n",
      "l2 norm of gradients: 0.17492002927961334\n",
      "l2 norm of weights: 3.282572904789502\n",
      "---------------------\n",
      "Iteration Number: 3953\n",
      "Loss: 16.596895238081707\n",
      "l2 norm of gradients: 0.17484447695884112\n",
      "l2 norm of weights: 3.282419554174316\n",
      "---------------------\n",
      "Iteration Number: 3954\n",
      "Loss: 16.59308081288257\n",
      "l2 norm of gradients: 0.1747689762625425\n",
      "l2 norm of weights: 3.2822663905266922\n",
      "---------------------\n",
      "Iteration Number: 3955\n",
      "Loss: 16.58926914292831\n",
      "l2 norm of gradients: 0.17469352713920067\n",
      "l2 norm of weights: 3.2821134137006087\n",
      "---------------------\n",
      "Iteration Number: 3956\n",
      "Loss: 16.585460225911834\n",
      "l2 norm of gradients: 0.17461812953729858\n",
      "l2 norm of weights: 3.2819606235502117\n",
      "---------------------\n",
      "Iteration Number: 3957\n",
      "Loss: 16.58165405952901\n",
      "l2 norm of gradients: 0.17454278340531934\n",
      "l2 norm of weights: 3.2818080199298154\n",
      "---------------------\n",
      "Iteration Number: 3958\n",
      "Loss: 16.57785064147842\n",
      "l2 norm of gradients: 0.174467488691746\n",
      "l2 norm of weights: 3.2816556026939034\n",
      "---------------------\n",
      "Iteration Number: 3959\n",
      "Loss: 16.574049969461463\n",
      "l2 norm of gradients: 0.1743922453450619\n",
      "l2 norm of weights: 3.2815033716971254\n",
      "---------------------\n",
      "Iteration Number: 3960\n",
      "Loss: 16.570252041182297\n",
      "l2 norm of gradients: 0.1743170533137504\n",
      "l2 norm of weights: 3.2813513267943\n",
      "---------------------\n",
      "Iteration Number: 3961\n",
      "Loss: 16.566456854347997\n",
      "l2 norm of gradients: 0.17424191254629526\n",
      "l2 norm of weights: 3.2811994678404126\n",
      "---------------------\n",
      "Iteration Number: 3962\n",
      "Loss: 16.56266440666829\n",
      "l2 norm of gradients: 0.17416682299118044\n",
      "l2 norm of weights: 3.281047794690616\n",
      "---------------------\n",
      "Iteration Number: 3963\n",
      "Loss: 16.55887469585584\n",
      "l2 norm of gradients: 0.17409178459689037\n",
      "l2 norm of weights: 3.2808963072002286\n",
      "---------------------\n",
      "Iteration Number: 3964\n",
      "Loss: 16.55508771962595\n",
      "l2 norm of gradients: 0.17401679731190986\n",
      "l2 norm of weights: 3.280745005224737\n",
      "---------------------\n",
      "Iteration Number: 3965\n",
      "Loss: 16.5513034756968\n",
      "l2 norm of gradients: 0.17394186108472426\n",
      "l2 norm of weights: 3.280593888619792\n",
      "---------------------\n",
      "Iteration Number: 3966\n",
      "Loss: 16.547521961789403\n",
      "l2 norm of gradients: 0.17386697586381952\n",
      "l2 norm of weights: 3.280442957241212\n",
      "---------------------\n",
      "Iteration Number: 3967\n",
      "Loss: 16.543743175627462\n",
      "l2 norm of gradients: 0.17379214159768216\n",
      "l2 norm of weights: 3.2802922109449804\n",
      "---------------------\n",
      "Iteration Number: 3968\n",
      "Loss: 16.53996711493749\n",
      "l2 norm of gradients: 0.17371735823479945\n",
      "l2 norm of weights: 3.2801416495872453\n",
      "---------------------\n",
      "Iteration Number: 3969\n",
      "Loss: 16.53619377744887\n",
      "l2 norm of gradients: 0.17364262572365932\n",
      "l2 norm of weights: 3.279991273024321\n",
      "---------------------\n",
      "Iteration Number: 3970\n",
      "Loss: 16.532423160893636\n",
      "l2 norm of gradients: 0.1735679440127506\n",
      "l2 norm of weights: 3.2798410811126857\n",
      "---------------------\n",
      "Iteration Number: 3971\n",
      "Loss: 16.5286552630067\n",
      "l2 norm of gradients: 0.17349331305056312\n",
      "l2 norm of weights: 3.2796910737089826\n",
      "---------------------\n",
      "Iteration Number: 3972\n",
      "Loss: 16.524890081525694\n",
      "l2 norm of gradients: 0.17341873278558737\n",
      "l2 norm of weights: 3.279541250670019\n",
      "---------------------\n",
      "Iteration Number: 3973\n",
      "Loss: 16.521127614191066\n",
      "l2 norm of gradients: 0.1733442031663151\n",
      "l2 norm of weights: 3.279391611852766\n",
      "---------------------\n",
      "Iteration Number: 3974\n",
      "Loss: 16.517367858746034\n",
      "l2 norm of gradients: 0.173269724141239\n",
      "l2 norm of weights: 3.279242157114358\n",
      "---------------------\n",
      "Iteration Number: 3975\n",
      "Loss: 16.513610812936598\n",
      "l2 norm of gradients: 0.17319529565885297\n",
      "l2 norm of weights: 3.2790928863120943\n",
      "---------------------\n",
      "Iteration Number: 3976\n",
      "Loss: 16.509856474511455\n",
      "l2 norm of gradients: 0.17312091766765203\n",
      "l2 norm of weights: 3.2789437993034354\n",
      "---------------------\n",
      "Iteration Number: 3977\n",
      "Loss: 16.50610484122216\n",
      "l2 norm of gradients: 0.17304659011613258\n",
      "l2 norm of weights: 3.278794895946006\n",
      "---------------------\n",
      "Iteration Number: 3978\n",
      "Loss: 16.502355910823013\n",
      "l2 norm of gradients: 0.17297231295279222\n",
      "l2 norm of weights: 3.2786461760975922\n",
      "---------------------\n",
      "Iteration Number: 3979\n",
      "Loss: 16.49860968107103\n",
      "l2 norm of gradients: 0.17289808612613006\n",
      "l2 norm of weights: 3.278497639616144\n",
      "---------------------\n",
      "Iteration Number: 3980\n",
      "Loss: 16.494866149726104\n",
      "l2 norm of gradients: 0.17282390958464655\n",
      "l2 norm of weights: 3.2783492863597714\n",
      "---------------------\n",
      "Iteration Number: 3981\n",
      "Loss: 16.491125314550768\n",
      "l2 norm of gradients: 0.17274978327684373\n",
      "l2 norm of weights: 3.2782011161867484\n",
      "---------------------\n",
      "Iteration Number: 3982\n",
      "Loss: 16.4873871733104\n",
      "l2 norm of gradients: 0.17267570715122516\n",
      "l2 norm of weights: 3.2780531289555084\n",
      "---------------------\n",
      "Iteration Number: 3983\n",
      "Loss: 16.48365172377308\n",
      "l2 norm of gradients: 0.17260168115629612\n",
      "l2 norm of weights: 3.277905324524647\n",
      "---------------------\n",
      "Iteration Number: 3984\n",
      "Loss: 16.47991896370972\n",
      "l2 norm of gradients: 0.17252770524056357\n",
      "l2 norm of weights: 3.27775770275292\n",
      "---------------------\n",
      "Iteration Number: 3985\n",
      "Loss: 16.476188890893862\n",
      "l2 norm of gradients: 0.1724537793525362\n",
      "l2 norm of weights: 3.2776102634992452\n",
      "---------------------\n",
      "Iteration Number: 3986\n",
      "Loss: 16.472461503101936\n",
      "l2 norm of gradients: 0.1723799034407246\n",
      "l2 norm of weights: 3.2774630066226993\n",
      "---------------------\n",
      "Iteration Number: 3987\n",
      "Loss: 16.468736798113078\n",
      "l2 norm of gradients: 0.17230607745364124\n",
      "l2 norm of weights: 3.27731593198252\n",
      "---------------------\n",
      "Iteration Number: 3988\n",
      "Loss: 16.46501477370912\n",
      "l2 norm of gradients: 0.17223230133980041\n",
      "l2 norm of weights: 3.2771690394381032\n",
      "---------------------\n",
      "Iteration Number: 3989\n",
      "Loss: 16.461295427674745\n",
      "l2 norm of gradients: 0.17215857504771873\n",
      "l2 norm of weights: 3.277022328849007\n",
      "---------------------\n",
      "Iteration Number: 3990\n",
      "Loss: 16.457578757797304\n",
      "l2 norm of gradients: 0.17208489852591466\n",
      "l2 norm of weights: 3.2768758000749463\n",
      "---------------------\n",
      "Iteration Number: 3991\n",
      "Loss: 16.453864761866853\n",
      "l2 norm of gradients: 0.17201127172290892\n",
      "l2 norm of weights: 3.276729452975796\n",
      "---------------------\n",
      "Iteration Number: 3992\n",
      "Loss: 16.450153437676367\n",
      "l2 norm of gradients: 0.17193769458722444\n",
      "l2 norm of weights: 3.276583287411589\n",
      "---------------------\n",
      "Iteration Number: 3993\n",
      "Loss: 16.44644478302141\n",
      "l2 norm of gradients: 0.1718641670673864\n",
      "l2 norm of weights: 3.2764373032425187\n",
      "---------------------\n",
      "Iteration Number: 3994\n",
      "Loss: 16.442738795700276\n",
      "l2 norm of gradients: 0.1717906891119223\n",
      "l2 norm of weights: 3.2762915003289335\n",
      "---------------------\n",
      "Iteration Number: 3995\n",
      "Loss: 16.43903547351414\n",
      "l2 norm of gradients: 0.1717172606693622\n",
      "l2 norm of weights: 3.2761458785313415\n",
      "---------------------\n",
      "Iteration Number: 3996\n",
      "Loss: 16.43533481426673\n",
      "l2 norm of gradients: 0.1716438816882385\n",
      "l2 norm of weights: 3.2760004377104086\n",
      "---------------------\n",
      "Iteration Number: 3997\n",
      "Loss: 16.43163681576465\n",
      "l2 norm of gradients: 0.17157055211708627\n",
      "l2 norm of weights: 3.2758551777269567\n",
      "---------------------\n",
      "Iteration Number: 3998\n",
      "Loss: 16.42794147581721\n",
      "l2 norm of gradients: 0.17149727190444294\n",
      "l2 norm of weights: 3.2757100984419654\n",
      "---------------------\n",
      "Iteration Number: 3999\n",
      "Loss: 16.424248792236376\n",
      "l2 norm of gradients: 0.171424040998849\n",
      "l2 norm of weights: 3.2755651997165716\n",
      "---------------------\n",
      "Iteration Number: 4000\n",
      "Loss: 16.420558762836947\n",
      "l2 norm of gradients: 0.17135085934884736\n",
      "l2 norm of weights: 3.275420481412068\n",
      "---------------------\n",
      "Iteration Number: 4001\n",
      "Loss: 16.416871385436373\n",
      "l2 norm of gradients: 0.17127772690298387\n",
      "l2 norm of weights: 3.2752759433899032\n",
      "---------------------\n",
      "Iteration Number: 4002\n",
      "Loss: 16.413186657854848\n",
      "l2 norm of gradients: 0.17120464360980722\n",
      "l2 norm of weights: 3.2751315855116823\n",
      "---------------------\n",
      "Iteration Number: 4003\n",
      "Loss: 16.4095045779153\n",
      "l2 norm of gradients: 0.171131609417869\n",
      "l2 norm of weights: 3.2749874076391654\n",
      "---------------------\n",
      "Iteration Number: 4004\n",
      "Loss: 16.405825143443398\n",
      "l2 norm of gradients: 0.17105862427572394\n",
      "l2 norm of weights: 3.274843409634269\n",
      "---------------------\n",
      "Iteration Number: 4005\n",
      "Loss: 16.402148352267506\n",
      "l2 norm of gradients: 0.17098568813192971\n",
      "l2 norm of weights: 3.274699591359064\n",
      "---------------------\n",
      "Iteration Number: 4006\n",
      "Loss: 16.39847420221868\n",
      "l2 norm of gradients: 0.17091280093504718\n",
      "l2 norm of weights: 3.2745559526757755\n",
      "---------------------\n",
      "Iteration Number: 4007\n",
      "Loss: 16.394802691130813\n",
      "l2 norm of gradients: 0.17083996263364032\n",
      "l2 norm of weights: 3.274412493446784\n",
      "---------------------\n",
      "Iteration Number: 4008\n",
      "Loss: 16.39113381684033\n",
      "l2 norm of gradients: 0.17076717317627652\n",
      "l2 norm of weights: 3.274269213534625\n",
      "---------------------\n",
      "Iteration Number: 4009\n",
      "Loss: 16.387467577186477\n",
      "l2 norm of gradients: 0.17069443251152633\n",
      "l2 norm of weights: 3.274126112801985\n",
      "---------------------\n",
      "Iteration Number: 4010\n",
      "Loss: 16.383803970011225\n",
      "l2 norm of gradients: 0.1706217405879639\n",
      "l2 norm of weights: 3.2739831911117077\n",
      "---------------------\n",
      "Iteration Number: 4011\n",
      "Loss: 16.380142993159247\n",
      "l2 norm of gradients: 0.1705490973541667\n",
      "l2 norm of weights: 3.273840448326788\n",
      "---------------------\n",
      "Iteration Number: 4012\n",
      "Loss: 16.376484644477866\n",
      "l2 norm of gradients: 0.1704765027587158\n",
      "l2 norm of weights: 3.273697884310375\n",
      "---------------------\n",
      "Iteration Number: 4013\n",
      "Loss: 16.37282892181718\n",
      "l2 norm of gradients: 0.17040395675019585\n",
      "l2 norm of weights: 3.27355549892577\n",
      "---------------------\n",
      "Iteration Number: 4014\n",
      "Loss: 16.36917582302994\n",
      "l2 norm of gradients: 0.1703314592771951\n",
      "l2 norm of weights: 3.273413292036427\n",
      "---------------------\n",
      "Iteration Number: 4015\n",
      "Loss: 16.365525345971594\n",
      "l2 norm of gradients: 0.1702590102883057\n",
      "l2 norm of weights: 3.2732712635059533\n",
      "---------------------\n",
      "Iteration Number: 4016\n",
      "Loss: 16.36187748850037\n",
      "l2 norm of gradients: 0.1701866097321235\n",
      "l2 norm of weights: 3.273129413198107\n",
      "---------------------\n",
      "Iteration Number: 4017\n",
      "Loss: 16.35823224847713\n",
      "l2 norm of gradients: 0.17011425755724818\n",
      "l2 norm of weights: 3.2729877409767973\n",
      "---------------------\n",
      "Iteration Number: 4018\n",
      "Loss: 16.354589623765413\n",
      "l2 norm of gradients: 0.17004195371228348\n",
      "l2 norm of weights: 3.272846246706088\n",
      "---------------------\n",
      "Iteration Number: 4019\n",
      "Loss: 16.35094961223151\n",
      "l2 norm of gradients: 0.16996969814583704\n",
      "l2 norm of weights: 3.272704930250191\n",
      "---------------------\n",
      "Iteration Number: 4020\n",
      "Loss: 16.347312211744388\n",
      "l2 norm of gradients: 0.1698974908065206\n",
      "l2 norm of weights: 3.27256379147347\n",
      "---------------------\n",
      "Iteration Number: 4021\n",
      "Loss: 16.343677420175652\n",
      "l2 norm of gradients: 0.16982533164295013\n",
      "l2 norm of weights: 3.2724228302404406\n",
      "---------------------\n",
      "Iteration Number: 4022\n",
      "Loss: 16.340045235399685\n",
      "l2 norm of gradients: 0.1697532206037457\n",
      "l2 norm of weights: 3.2722820464157674\n",
      "---------------------\n",
      "Iteration Number: 4023\n",
      "Loss: 16.33641565529349\n",
      "l2 norm of gradients: 0.16968115763753172\n",
      "l2 norm of weights: 3.272141439864266\n",
      "---------------------\n",
      "Iteration Number: 4024\n",
      "Loss: 16.332788677736815\n",
      "l2 norm of gradients: 0.16960914269293692\n",
      "l2 norm of weights: 3.2720010104509\n",
      "---------------------\n",
      "Iteration Number: 4025\n",
      "Loss: 16.329164300611993\n",
      "l2 norm of gradients: 0.16953717571859447\n",
      "l2 norm of weights: 3.2718607580407855\n",
      "---------------------\n",
      "Iteration Number: 4026\n",
      "Loss: 16.32554252180416\n",
      "l2 norm of gradients: 0.16946525666314205\n",
      "l2 norm of weights: 3.271720682499186\n",
      "---------------------\n",
      "Iteration Number: 4027\n",
      "Loss: 16.321923339201035\n",
      "l2 norm of gradients: 0.16939338547522184\n",
      "l2 norm of weights: 3.2715807836915145\n",
      "---------------------\n",
      "Iteration Number: 4028\n",
      "Loss: 16.31830675069305\n",
      "l2 norm of gradients: 0.16932156210348073\n",
      "l2 norm of weights: 3.2714410614833325\n",
      "---------------------\n",
      "Iteration Number: 4029\n",
      "Loss: 16.314692754173368\n",
      "l2 norm of gradients: 0.16924978649657024\n",
      "l2 norm of weights: 3.2713015157403507\n",
      "---------------------\n",
      "Iteration Number: 4030\n",
      "Loss: 16.311081347537737\n",
      "l2 norm of gradients: 0.16917805860314666\n",
      "l2 norm of weights: 3.2711621463284266\n",
      "---------------------\n",
      "Iteration Number: 4031\n",
      "Loss: 16.30747252868459\n",
      "l2 norm of gradients: 0.16910637837187117\n",
      "l2 norm of weights: 3.2710229531135675\n",
      "---------------------\n",
      "Iteration Number: 4032\n",
      "Loss: 16.303866295515117\n",
      "l2 norm of gradients: 0.1690347457514098\n",
      "l2 norm of weights: 3.2708839359619275\n",
      "---------------------\n",
      "Iteration Number: 4033\n",
      "Loss: 16.300262645933095\n",
      "l2 norm of gradients: 0.16896316069043363\n",
      "l2 norm of weights: 3.270745094739808\n",
      "---------------------\n",
      "Iteration Number: 4034\n",
      "Loss: 16.296661577844993\n",
      "l2 norm of gradients: 0.16889162313761877\n",
      "l2 norm of weights: 3.2706064293136565\n",
      "---------------------\n",
      "Iteration Number: 4035\n",
      "Loss: 16.293063089159887\n",
      "l2 norm of gradients: 0.1688201330416464\n",
      "l2 norm of weights: 3.2704679395500698\n",
      "---------------------\n",
      "Iteration Number: 4036\n",
      "Loss: 16.289467177789653\n",
      "l2 norm of gradients: 0.16874869035120285\n",
      "l2 norm of weights: 3.270329625315789\n",
      "---------------------\n",
      "Iteration Number: 4037\n",
      "Loss: 16.28587384164871\n",
      "l2 norm of gradients: 0.16867729501497994\n",
      "l2 norm of weights: 3.2701914864777035\n",
      "---------------------\n",
      "Iteration Number: 4038\n",
      "Loss: 16.282283078654185\n",
      "l2 norm of gradients: 0.1686059469816745\n",
      "l2 norm of weights: 3.2700535229028467\n",
      "---------------------\n",
      "Iteration Number: 4039\n",
      "Loss: 16.27869488672582\n",
      "l2 norm of gradients: 0.1685346461999891\n",
      "l2 norm of weights: 3.2699157344583996\n",
      "---------------------\n",
      "Iteration Number: 4040\n",
      "Loss: 16.27510926378609\n",
      "l2 norm of gradients: 0.16846339261863155\n",
      "l2 norm of weights: 3.269778121011688\n",
      "---------------------\n",
      "Iteration Number: 4041\n",
      "Loss: 16.271526207760026\n",
      "l2 norm of gradients: 0.16839218618631538\n",
      "l2 norm of weights: 3.2696406824301816\n",
      "---------------------\n",
      "Iteration Number: 4042\n",
      "Loss: 16.267945716575422\n",
      "l2 norm of gradients: 0.1683210268517596\n",
      "l2 norm of weights: 3.2695034185814973\n",
      "---------------------\n",
      "Iteration Number: 4043\n",
      "Loss: 16.264367788162616\n",
      "l2 norm of gradients: 0.16824991456368898\n",
      "l2 norm of weights: 3.269366329333396\n",
      "---------------------\n",
      "Iteration Number: 4044\n",
      "Loss: 16.260792420454603\n",
      "l2 norm of gradients: 0.16817884927083412\n",
      "l2 norm of weights: 3.2692294145537817\n",
      "---------------------\n",
      "Iteration Number: 4045\n",
      "Loss: 16.25721961138717\n",
      "l2 norm of gradients: 0.1681078309219314\n",
      "l2 norm of weights: 3.2690926741107043\n",
      "---------------------\n",
      "Iteration Number: 4046\n",
      "Loss: 16.253649358898553\n",
      "l2 norm of gradients: 0.1680368594657231\n",
      "l2 norm of weights: 3.268956107872357\n",
      "---------------------\n",
      "Iteration Number: 4047\n",
      "Loss: 16.250081660929773\n",
      "l2 norm of gradients: 0.1679659348509576\n",
      "l2 norm of weights: 3.2688197157070764\n",
      "---------------------\n",
      "Iteration Number: 4048\n",
      "Loss: 16.24651651542436\n",
      "l2 norm of gradients: 0.16789505702638918\n",
      "l2 norm of weights: 3.268683497483342\n",
      "---------------------\n",
      "Iteration Number: 4049\n",
      "Loss: 16.24295392032862\n",
      "l2 norm of gradients: 0.16782422594077834\n",
      "l2 norm of weights: 3.2685474530697776\n",
      "---------------------\n",
      "Iteration Number: 4050\n",
      "Loss: 16.23939387359141\n",
      "l2 norm of gradients: 0.16775344154289193\n",
      "l2 norm of weights: 3.268411582335149\n",
      "---------------------\n",
      "Iteration Number: 4051\n",
      "Loss: 16.23583637316424\n",
      "l2 norm of gradients: 0.16768270378150288\n",
      "l2 norm of weights: 3.268275885148365\n",
      "---------------------\n",
      "Iteration Number: 4052\n",
      "Loss: 16.232281417001264\n",
      "l2 norm of gradients: 0.1676120126053906\n",
      "l2 norm of weights: 3.268140361378476\n",
      "---------------------\n",
      "Iteration Number: 4053\n",
      "Loss: 16.228729003059293\n",
      "l2 norm of gradients: 0.16754136796334096\n",
      "l2 norm of weights: 3.268005010894675\n",
      "---------------------\n",
      "Iteration Number: 4054\n",
      "Loss: 16.225179129297665\n",
      "l2 norm of gradients: 0.1674707698041462\n",
      "l2 norm of weights: 3.2678698335662966\n",
      "---------------------\n",
      "Iteration Number: 4055\n",
      "Loss: 16.22163179367844\n",
      "l2 norm of gradients: 0.16740021807660538\n",
      "l2 norm of weights: 3.2677348292628166\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 4056\n",
      "Loss: 16.21808699416631\n",
      "l2 norm of gradients: 0.16732971272952404\n",
      "l2 norm of weights: 3.2675999978538526\n",
      "---------------------\n",
      "Iteration Number: 4057\n",
      "Loss: 16.214544728728495\n",
      "l2 norm of gradients: 0.16725925371171452\n",
      "l2 norm of weights: 3.267465339209163\n",
      "---------------------\n",
      "Iteration Number: 4058\n",
      "Loss: 16.21100499533496\n",
      "l2 norm of gradients: 0.16718884097199602\n",
      "l2 norm of weights: 3.2673308531986467\n",
      "---------------------\n",
      "Iteration Number: 4059\n",
      "Loss: 16.207467791958177\n",
      "l2 norm of gradients: 0.16711847445919462\n",
      "l2 norm of weights: 3.267196539692342\n",
      "---------------------\n",
      "Iteration Number: 4060\n",
      "Loss: 16.20393311657329\n",
      "l2 norm of gradients: 0.16704815412214336\n",
      "l2 norm of weights: 3.2670623985604306\n",
      "---------------------\n",
      "Iteration Number: 4061\n",
      "Loss: 16.20040096715806\n",
      "l2 norm of gradients: 0.1669778799096823\n",
      "l2 norm of weights: 3.2669284296732295\n",
      "---------------------\n",
      "Iteration Number: 4062\n",
      "Loss: 16.196871341692876\n",
      "l2 norm of gradients: 0.16690765177065875\n",
      "l2 norm of weights: 3.266794632901199\n",
      "---------------------\n",
      "Iteration Number: 4063\n",
      "Loss: 16.19334423816065\n",
      "l2 norm of gradients: 0.16683746965392715\n",
      "l2 norm of weights: 3.2666610081149376\n",
      "---------------------\n",
      "Iteration Number: 4064\n",
      "Loss: 16.18981965454705\n",
      "l2 norm of gradients: 0.1667673335083492\n",
      "l2 norm of weights: 3.2665275551851822\n",
      "---------------------\n",
      "Iteration Number: 4065\n",
      "Loss: 16.186297588840194\n",
      "l2 norm of gradients: 0.16669724328279398\n",
      "l2 norm of weights: 3.266394273982809\n",
      "---------------------\n",
      "Iteration Number: 4066\n",
      "Loss: 16.18277803903093\n",
      "l2 norm of gradients: 0.16662719892613806\n",
      "l2 norm of weights: 3.2662611643788333\n",
      "---------------------\n",
      "Iteration Number: 4067\n",
      "Loss: 16.179261003112632\n",
      "l2 norm of gradients: 0.16655720038726554\n",
      "l2 norm of weights: 3.2661282262444082\n",
      "---------------------\n",
      "Iteration Number: 4068\n",
      "Loss: 16.175746479081326\n",
      "l2 norm of gradients: 0.16648724761506806\n",
      "l2 norm of weights: 3.2659954594508247\n",
      "---------------------\n",
      "Iteration Number: 4069\n",
      "Loss: 16.172234464935574\n",
      "l2 norm of gradients: 0.16641734055844495\n",
      "l2 norm of weights: 3.265862863869512\n",
      "---------------------\n",
      "Iteration Number: 4070\n",
      "Loss: 16.1687249586766\n",
      "l2 norm of gradients: 0.16634747916630335\n",
      "l2 norm of weights: 3.265730439372037\n",
      "---------------------\n",
      "Iteration Number: 4071\n",
      "Loss: 16.1652179583082\n",
      "l2 norm of gradients: 0.16627766338755828\n",
      "l2 norm of weights: 3.2655981858301026\n",
      "---------------------\n",
      "Iteration Number: 4072\n",
      "Loss: 16.16171346183674\n",
      "l2 norm of gradients: 0.16620789317113255\n",
      "l2 norm of weights: 3.2654661031155503\n",
      "---------------------\n",
      "Iteration Number: 4073\n",
      "Loss: 16.158211467271183\n",
      "l2 norm of gradients: 0.1661381684659571\n",
      "l2 norm of weights: 3.2653341911003566\n",
      "---------------------\n",
      "Iteration Number: 4074\n",
      "Loss: 16.154711972623154\n",
      "l2 norm of gradients: 0.16606848922097092\n",
      "l2 norm of weights: 3.2652024496566368\n",
      "---------------------\n",
      "Iteration Number: 4075\n",
      "Loss: 16.15121497590676\n",
      "l2 norm of gradients: 0.16599885538512113\n",
      "l2 norm of weights: 3.26507087865664\n",
      "---------------------\n",
      "Iteration Number: 4076\n",
      "Loss: 16.147720475138776\n",
      "l2 norm of gradients: 0.16592926690736318\n",
      "l2 norm of weights: 3.2649394779727525\n",
      "---------------------\n",
      "Iteration Number: 4077\n",
      "Loss: 16.144228468338483\n",
      "l2 norm of gradients: 0.16585972373666077\n",
      "l2 norm of weights: 3.2648082474774966\n",
      "---------------------\n",
      "Iteration Number: 4078\n",
      "Loss: 16.140738953527787\n",
      "l2 norm of gradients: 0.16579022582198605\n",
      "l2 norm of weights: 3.264677187043529\n",
      "---------------------\n",
      "Iteration Number: 4079\n",
      "Loss: 16.137251928731203\n",
      "l2 norm of gradients: 0.16572077311231972\n",
      "l2 norm of weights: 3.2645462965436423\n",
      "---------------------\n",
      "Iteration Number: 4080\n",
      "Loss: 16.133767391975766\n",
      "l2 norm of gradients: 0.16565136555665094\n",
      "l2 norm of weights: 3.2644155758507636\n",
      "---------------------\n",
      "Iteration Number: 4081\n",
      "Loss: 16.13028534129108\n",
      "l2 norm of gradients: 0.16558200310397767\n",
      "l2 norm of weights: 3.264285024837955\n",
      "---------------------\n",
      "Iteration Number: 4082\n",
      "Loss: 16.126805774709396\n",
      "l2 norm of gradients: 0.16551268570330652\n",
      "l2 norm of weights: 3.2641546433784128\n",
      "---------------------\n",
      "Iteration Number: 4083\n",
      "Loss: 16.123328690265488\n",
      "l2 norm of gradients: 0.165443413303653\n",
      "l2 norm of weights: 3.2640244313454674\n",
      "---------------------\n",
      "Iteration Number: 4084\n",
      "Loss: 16.11985408599668\n",
      "l2 norm of gradients: 0.1653741858540415\n",
      "l2 norm of weights: 3.263894388612583\n",
      "---------------------\n",
      "Iteration Number: 4085\n",
      "Loss: 16.11638195994291\n",
      "l2 norm of gradients: 0.16530500330350545\n",
      "l2 norm of weights: 3.263764515053358\n",
      "---------------------\n",
      "Iteration Number: 4086\n",
      "Loss: 16.11291231014664\n",
      "l2 norm of gradients: 0.1652358656010873\n",
      "l2 norm of weights: 3.2636348105415225\n",
      "---------------------\n",
      "Iteration Number: 4087\n",
      "Loss: 16.109445134652873\n",
      "l2 norm of gradients: 0.16516677269583868\n",
      "l2 norm of weights: 3.2635052749509423\n",
      "---------------------\n",
      "Iteration Number: 4088\n",
      "Loss: 16.10598043150929\n",
      "l2 norm of gradients: 0.16509772453682062\n",
      "l2 norm of weights: 3.2633759081556137\n",
      "---------------------\n",
      "Iteration Number: 4089\n",
      "Loss: 16.102518198765964\n",
      "l2 norm of gradients: 0.1650287210731033\n",
      "l2 norm of weights: 3.263246710029667\n",
      "---------------------\n",
      "Iteration Number: 4090\n",
      "Loss: 16.09905843447568\n",
      "l2 norm of gradients: 0.16495976225376646\n",
      "l2 norm of weights: 3.2631176804473636\n",
      "---------------------\n",
      "Iteration Number: 4091\n",
      "Loss: 16.09560113669366\n",
      "l2 norm of gradients: 0.16489084802789933\n",
      "l2 norm of weights: 3.2629888192830987\n",
      "---------------------\n",
      "Iteration Number: 4092\n",
      "Loss: 16.092146303477758\n",
      "l2 norm of gradients: 0.1648219783446007\n",
      "l2 norm of weights: 3.2628601264113977\n",
      "---------------------\n",
      "Iteration Number: 4093\n",
      "Loss: 16.08869393288831\n",
      "l2 norm of gradients: 0.16475315315297911\n",
      "l2 norm of weights: 3.262731601706918\n",
      "---------------------\n",
      "Iteration Number: 4094\n",
      "Loss: 16.085244022988267\n",
      "l2 norm of gradients: 0.16468437240215286\n",
      "l2 norm of weights: 3.2626032450444487\n",
      "---------------------\n",
      "Iteration Number: 4095\n",
      "Loss: 16.081796571843068\n",
      "l2 norm of gradients: 0.16461563604125012\n",
      "l2 norm of weights: 3.2624750562989093\n",
      "---------------------\n",
      "Iteration Number: 4096\n",
      "Loss: 16.07835157752074\n",
      "l2 norm of gradients: 0.16454694401940906\n",
      "l2 norm of weights: 3.2623470353453508\n",
      "---------------------\n",
      "Iteration Number: 4097\n",
      "Loss: 16.07490903809184\n",
      "l2 norm of gradients: 0.16447829628577776\n",
      "l2 norm of weights: 3.262219182058955\n",
      "---------------------\n",
      "Iteration Number: 4098\n",
      "Loss: 16.071468951629427\n",
      "l2 norm of gradients: 0.1644096927895146\n",
      "l2 norm of weights: 3.262091496315032\n",
      "---------------------\n",
      "Iteration Number: 4099\n",
      "Loss: 16.068031316209172\n",
      "l2 norm of gradients: 0.1643411334797881\n",
      "l2 norm of weights: 3.2619639779890246\n",
      "---------------------\n",
      "Iteration Number: 4100\n",
      "Loss: 16.06459612990921\n",
      "l2 norm of gradients: 0.16427261830577716\n",
      "l2 norm of weights: 3.261836626956504\n",
      "---------------------\n",
      "Iteration Number: 4101\n",
      "Loss: 16.061163390810258\n",
      "l2 norm of gradients: 0.16420414721667104\n",
      "l2 norm of weights: 3.26170944309317\n",
      "---------------------\n",
      "Iteration Number: 4102\n",
      "Loss: 16.057733096995516\n",
      "l2 norm of gradients: 0.16413572016166944\n",
      "l2 norm of weights: 3.2615824262748534\n",
      "---------------------\n",
      "Iteration Number: 4103\n",
      "Loss: 16.05430524655076\n",
      "l2 norm of gradients: 0.16406733708998278\n",
      "l2 norm of weights: 3.2614555763775135\n",
      "---------------------\n",
      "Iteration Number: 4104\n",
      "Loss: 16.050879837564267\n",
      "l2 norm of gradients: 0.16399899795083203\n",
      "l2 norm of weights: 3.2613288932772377\n",
      "---------------------\n",
      "Iteration Number: 4105\n",
      "Loss: 16.047456868126858\n",
      "l2 norm of gradients: 0.1639307026934491\n",
      "l2 norm of weights: 3.261202376850243\n",
      "---------------------\n",
      "Iteration Number: 4106\n",
      "Loss: 16.044036336331864\n",
      "l2 norm of gradients: 0.1638624512670765\n",
      "l2 norm of weights: 3.2610760269728734\n",
      "---------------------\n",
      "Iteration Number: 4107\n",
      "Loss: 16.040618240275098\n",
      "l2 norm of gradients: 0.163794243620968\n",
      "l2 norm of weights: 3.2609498435216016\n",
      "---------------------\n",
      "Iteration Number: 4108\n",
      "Loss: 16.037202578054995\n",
      "l2 norm of gradients: 0.16372607970438818\n",
      "l2 norm of weights: 3.260823826373028\n",
      "---------------------\n",
      "Iteration Number: 4109\n",
      "Loss: 16.033789347772398\n",
      "l2 norm of gradients: 0.16365795946661293\n",
      "l2 norm of weights: 3.2606979754038816\n",
      "---------------------\n",
      "Iteration Number: 4110\n",
      "Loss: 16.030378547530724\n",
      "l2 norm of gradients: 0.16358988285692933\n",
      "l2 norm of weights: 3.260572290491016\n",
      "---------------------\n",
      "Iteration Number: 4111\n",
      "Loss: 16.026970175435885\n",
      "l2 norm of gradients: 0.1635218498246357\n",
      "l2 norm of weights: 3.2604467715114143\n",
      "---------------------\n",
      "Iteration Number: 4112\n",
      "Loss: 16.023564229596314\n",
      "l2 norm of gradients: 0.16345386031904188\n",
      "l2 norm of weights: 3.260321418342185\n",
      "---------------------\n",
      "Iteration Number: 4113\n",
      "Loss: 16.02016070812292\n",
      "l2 norm of gradients: 0.16338591428946925\n",
      "l2 norm of weights: 3.260196230860564\n",
      "---------------------\n",
      "Iteration Number: 4114\n",
      "Loss: 16.016759609129117\n",
      "l2 norm of gradients: 0.16331801168525067\n",
      "l2 norm of weights: 3.260071208943913\n",
      "---------------------\n",
      "Iteration Number: 4115\n",
      "Loss: 16.013360930730904\n",
      "l2 norm of gradients: 0.16325015245573096\n",
      "l2 norm of weights: 3.259946352469719\n",
      "---------------------\n",
      "Iteration Number: 4116\n",
      "Loss: 16.00996467104669\n",
      "l2 norm of gradients: 0.16318233655026648\n",
      "l2 norm of weights: 3.2598216613155966\n",
      "---------------------\n",
      "Iteration Number: 4117\n",
      "Loss: 16.00657082819742\n",
      "l2 norm of gradients: 0.16311456391822562\n",
      "l2 norm of weights: 3.259697135359284\n",
      "---------------------\n",
      "Iteration Number: 4118\n",
      "Loss: 16.003179400306514\n",
      "l2 norm of gradients: 0.16304683450898885\n",
      "l2 norm of weights: 3.259572774478645\n",
      "---------------------\n",
      "Iteration Number: 4119\n",
      "Loss: 15.999790385499894\n",
      "l2 norm of gradients: 0.1629791482719486\n",
      "l2 norm of weights: 3.2594485785516705\n",
      "---------------------\n",
      "Iteration Number: 4120\n",
      "Loss: 15.99640378190604\n",
      "l2 norm of gradients: 0.1629115051565096\n",
      "l2 norm of weights: 3.259324547456474\n",
      "---------------------\n",
      "Iteration Number: 4121\n",
      "Loss: 15.99301958765581\n",
      "l2 norm of gradients: 0.1628439051120887\n",
      "l2 norm of weights: 3.259200681071293\n",
      "---------------------\n",
      "Iteration Number: 4122\n",
      "Loss: 15.989637800882639\n",
      "l2 norm of gradients: 0.16277634808811545\n",
      "l2 norm of weights: 3.2590769792744916\n",
      "---------------------\n",
      "Iteration Number: 4123\n",
      "Loss: 15.986258419722374\n",
      "l2 norm of gradients: 0.1627088340340316\n",
      "l2 norm of weights: 3.258953441944556\n",
      "---------------------\n",
      "Iteration Number: 4124\n",
      "Loss: 15.982881442313431\n",
      "l2 norm of gradients: 0.1626413628992916\n",
      "l2 norm of weights: 3.2588300689600973\n",
      "---------------------\n",
      "Iteration Number: 4125\n",
      "Loss: 15.979506866796621\n",
      "l2 norm of gradients: 0.16257393463336267\n",
      "l2 norm of weights: 3.25870686019985\n",
      "---------------------\n",
      "Iteration Number: 4126\n",
      "Loss: 15.976134691315282\n",
      "l2 norm of gradients: 0.16250654918572474\n",
      "l2 norm of weights: 3.258583815542671\n",
      "---------------------\n",
      "Iteration Number: 4127\n",
      "Loss: 15.972764914015276\n",
      "l2 norm of gradients: 0.1624392065058707\n",
      "l2 norm of weights: 3.258460934867542\n",
      "---------------------\n",
      "Iteration Number: 4128\n",
      "Loss: 15.969397533044802\n",
      "l2 norm of gradients: 0.16237190654330624\n",
      "l2 norm of weights: 3.2583382180535656\n",
      "---------------------\n",
      "Iteration Number: 4129\n",
      "Loss: 15.966032546554649\n",
      "l2 norm of gradients: 0.16230464924755042\n",
      "l2 norm of weights: 3.258215664979968\n",
      "---------------------\n",
      "Iteration Number: 4130\n",
      "Loss: 15.962669952698056\n",
      "l2 norm of gradients: 0.1622374345681353\n",
      "l2 norm of weights: 3.2580932755260976\n",
      "---------------------\n",
      "Iteration Number: 4131\n",
      "Loss: 15.959309749630716\n",
      "l2 norm of gradients: 0.1621702624546063\n",
      "l2 norm of weights: 3.2579710495714247\n",
      "---------------------\n",
      "Iteration Number: 4132\n",
      "Loss: 15.955951935510761\n",
      "l2 norm of gradients: 0.16210313285652234\n",
      "l2 norm of weights: 3.257848986995542\n",
      "---------------------\n",
      "Iteration Number: 4133\n",
      "Loss: 15.95259650849883\n",
      "l2 norm of gradients: 0.1620360457234556\n",
      "l2 norm of weights: 3.257727087678163\n",
      "---------------------\n",
      "Iteration Number: 4134\n",
      "Loss: 15.94924346675799\n",
      "l2 norm of gradients: 0.16196900100499212\n",
      "l2 norm of weights: 3.257605351499123\n",
      "---------------------\n",
      "Iteration Number: 4135\n",
      "Loss: 15.945892808453818\n",
      "l2 norm of gradients: 0.16190199865073154\n",
      "l2 norm of weights: 3.2574837783383788\n",
      "---------------------\n",
      "Iteration Number: 4136\n",
      "Loss: 15.942544531754283\n",
      "l2 norm of gradients: 0.1618350386102873\n",
      "l2 norm of weights: 3.257362368076007\n",
      "---------------------\n",
      "Iteration Number: 4137\n",
      "Loss: 15.939198634829825\n",
      "l2 norm of gradients: 0.16176812083328684\n",
      "l2 norm of weights: 3.257241120592206\n",
      "---------------------\n",
      "Iteration Number: 4138\n",
      "Loss: 15.935855115853386\n",
      "l2 norm of gradients: 0.16170124526937157\n",
      "l2 norm of weights: 3.257120035767294\n",
      "---------------------\n",
      "Iteration Number: 4139\n",
      "Loss: 15.932513973000303\n",
      "l2 norm of gradients: 0.16163441186819702\n",
      "l2 norm of weights: 3.256999113481709\n",
      "---------------------\n",
      "Iteration Number: 4140\n",
      "Loss: 15.929175204448386\n",
      "l2 norm of gradients: 0.16156762057943305\n",
      "l2 norm of weights: 3.25687835361601\n",
      "---------------------\n",
      "Iteration Number: 4141\n",
      "Loss: 15.925838808377842\n",
      "l2 norm of gradients: 0.1615008713527637\n",
      "l2 norm of weights: 3.256757756050875\n",
      "---------------------\n",
      "Iteration Number: 4142\n",
      "Loss: 15.92250478297139\n",
      "l2 norm of gradients: 0.16143416413788758\n",
      "l2 norm of weights: 3.2566373206671013\n",
      "---------------------\n",
      "Iteration Number: 4143\n",
      "Loss: 15.91917312641419\n",
      "l2 norm of gradients: 0.1613674988845179\n",
      "l2 norm of weights: 3.2565170473456058\n",
      "---------------------\n",
      "Iteration Number: 4144\n",
      "Loss: 15.91584383689377\n",
      "l2 norm of gradients: 0.1613008755423824\n",
      "l2 norm of weights: 3.2563969359674236\n",
      "---------------------\n",
      "Iteration Number: 4145\n",
      "Loss: 15.912516912600141\n",
      "l2 norm of gradients: 0.16123429406122375\n",
      "l2 norm of weights: 3.2562769864137096\n",
      "---------------------\n",
      "Iteration Number: 4146\n",
      "Loss: 15.90919235172574\n",
      "l2 norm of gradients: 0.16116775439079942\n",
      "l2 norm of weights: 3.2561571985657367\n",
      "---------------------\n",
      "Iteration Number: 4147\n",
      "Loss: 15.905870152465443\n",
      "l2 norm of gradients: 0.16110125648088186\n",
      "l2 norm of weights: 3.256037572304896\n",
      "---------------------\n",
      "Iteration Number: 4148\n",
      "Loss: 15.902550313016565\n",
      "l2 norm of gradients: 0.1610348002812586\n",
      "l2 norm of weights: 3.255918107512697\n",
      "---------------------\n",
      "Iteration Number: 4149\n",
      "Loss: 15.899232831578804\n",
      "l2 norm of gradients: 0.1609683857417325\n",
      "l2 norm of weights: 3.255798804070766\n",
      "---------------------\n",
      "Iteration Number: 4150\n",
      "Loss: 15.895917706354307\n",
      "l2 norm of gradients: 0.1609020128121216\n",
      "l2 norm of weights: 3.2556796618608477\n",
      "---------------------\n",
      "Iteration Number: 4151\n",
      "Loss: 15.892604935547688\n",
      "l2 norm of gradients: 0.16083568144225951\n",
      "l2 norm of weights: 3.2555606807648045\n",
      "---------------------\n",
      "Iteration Number: 4152\n",
      "Loss: 15.889294517365895\n",
      "l2 norm of gradients: 0.16076939158199538\n",
      "l2 norm of weights: 3.2554418606646145\n",
      "---------------------\n",
      "Iteration Number: 4153\n",
      "Loss: 15.88598645001835\n",
      "l2 norm of gradients: 0.16070314318119377\n",
      "l2 norm of weights: 3.2553232014423736\n",
      "---------------------\n",
      "Iteration Number: 4154\n",
      "Loss: 15.882680731716896\n",
      "l2 norm of gradients: 0.1606369361897354\n",
      "l2 norm of weights: 3.2552047029802953\n",
      "---------------------\n",
      "Iteration Number: 4155\n",
      "Loss: 15.879377360675734\n",
      "l2 norm of gradients: 0.1605707705575166\n",
      "l2 norm of weights: 3.255086365160707\n",
      "---------------------\n",
      "Iteration Number: 4156\n",
      "Loss: 15.87607633511156\n",
      "l2 norm of gradients: 0.1605046462344497\n",
      "l2 norm of weights: 3.2549681878660537\n",
      "---------------------\n",
      "Iteration Number: 4157\n",
      "Loss: 15.872777653243356\n",
      "l2 norm of gradients: 0.16043856317046332\n",
      "l2 norm of weights: 3.2548501709788966\n",
      "---------------------\n",
      "Iteration Number: 4158\n",
      "Loss: 15.86948131329265\n",
      "l2 norm of gradients: 0.16037252131550214\n",
      "l2 norm of weights: 3.2547323143819127\n",
      "---------------------\n",
      "Iteration Number: 4159\n",
      "Loss: 15.866187313483284\n",
      "l2 norm of gradients: 0.16030652061952724\n",
      "l2 norm of weights: 3.2546146179578925\n",
      "---------------------\n",
      "Iteration Number: 4160\n",
      "Loss: 15.862895652041534\n",
      "l2 norm of gradients: 0.16024056103251613\n",
      "l2 norm of weights: 3.2544970815897454\n",
      "---------------------\n",
      "Iteration Number: 4161\n",
      "Loss: 15.859606327196023\n",
      "l2 norm of gradients: 0.16017464250446292\n",
      "l2 norm of weights: 3.254379705160491\n",
      "---------------------\n",
      "Iteration Number: 4162\n",
      "Loss: 15.856319337177823\n",
      "l2 norm of gradients: 0.16010876498537838\n",
      "l2 norm of weights: 3.254262488553268\n",
      "---------------------\n",
      "Iteration Number: 4163\n",
      "Loss: 15.853034680220425\n",
      "l2 norm of gradients: 0.16004292842529008\n",
      "l2 norm of weights: 3.2541454316513265\n",
      "---------------------\n",
      "Iteration Number: 4164\n",
      "Loss: 15.849752354559632\n",
      "l2 norm of gradients: 0.15997713277424253\n",
      "l2 norm of weights: 3.2540285343380333\n",
      "---------------------\n",
      "Iteration Number: 4165\n",
      "Loss: 15.846472358433694\n",
      "l2 norm of gradients: 0.1599113779822973\n",
      "l2 norm of weights: 3.2539117964968676\n",
      "---------------------\n",
      "Iteration Number: 4166\n",
      "Loss: 15.843194690083191\n",
      "l2 norm of gradients: 0.15984566399953296\n",
      "l2 norm of weights: 3.253795218011423\n",
      "---------------------\n",
      "Iteration Number: 4167\n",
      "Loss: 15.8399193477512\n",
      "l2 norm of gradients: 0.15977999077604554\n",
      "l2 norm of weights: 3.2536787987654074\n",
      "---------------------\n",
      "Iteration Number: 4168\n",
      "Loss: 15.836646329683024\n",
      "l2 norm of gradients: 0.15971435826194835\n",
      "l2 norm of weights: 3.2535625386426403\n",
      "---------------------\n",
      "Iteration Number: 4169\n",
      "Loss: 15.833375634126508\n",
      "l2 norm of gradients: 0.1596487664073723\n",
      "l2 norm of weights: 3.253446437527056\n",
      "---------------------\n",
      "Iteration Number: 4170\n",
      "Loss: 15.830107259331717\n",
      "l2 norm of gradients: 0.1595832151624658\n",
      "l2 norm of weights: 3.2533304953027016\n",
      "---------------------\n",
      "Iteration Number: 4171\n",
      "Loss: 15.8268412035512\n",
      "l2 norm of gradients: 0.15951770447739505\n",
      "l2 norm of weights: 3.253214711853736\n",
      "---------------------\n",
      "Iteration Number: 4172\n",
      "Loss: 15.823577465039852\n",
      "l2 norm of gradients: 0.15945223430234426\n",
      "l2 norm of weights: 3.2530990870644305\n",
      "---------------------\n",
      "Iteration Number: 4173\n",
      "Loss: 15.820316042054873\n",
      "l2 norm of gradients: 0.15938680458751536\n",
      "l2 norm of weights: 3.2529836208191707\n",
      "---------------------\n",
      "Iteration Number: 4174\n",
      "Loss: 15.81705693285597\n",
      "l2 norm of gradients: 0.1593214152831287\n",
      "l2 norm of weights: 3.2528683130024514\n",
      "---------------------\n",
      "Iteration Number: 4175\n",
      "Loss: 15.813800135705051\n",
      "l2 norm of gradients: 0.15925606633942258\n",
      "l2 norm of weights: 3.252753163498881\n",
      "---------------------\n",
      "Iteration Number: 4176\n",
      "Loss: 15.810545648866523\n",
      "l2 norm of gradients: 0.1591907577066538\n",
      "l2 norm of weights: 3.2526381721931785\n",
      "---------------------\n",
      "Iteration Number: 4177\n",
      "Loss: 15.807293470607052\n",
      "l2 norm of gradients: 0.15912548933509763\n",
      "l2 norm of weights: 3.2525233389701755\n",
      "---------------------\n",
      "Iteration Number: 4178\n",
      "Loss: 15.804043599195722\n",
      "l2 norm of gradients: 0.15906026117504798\n",
      "l2 norm of weights: 3.2524086637148133\n",
      "---------------------\n",
      "Iteration Number: 4179\n",
      "Loss: 15.800796032903929\n",
      "l2 norm of gradients: 0.15899507317681738\n",
      "l2 norm of weights: 3.252294146312145\n",
      "---------------------\n",
      "Iteration Number: 4180\n",
      "Loss: 15.797550770005486\n",
      "l2 norm of gradients: 0.15892992529073724\n",
      "l2 norm of weights: 3.2521797866473343\n",
      "---------------------\n",
      "Iteration Number: 4181\n",
      "Loss: 15.794307808776487\n",
      "l2 norm of gradients: 0.15886481746715794\n",
      "l2 norm of weights: 3.2520655846056545\n",
      "---------------------\n",
      "Iteration Number: 4182\n",
      "Loss: 15.791067147495397\n",
      "l2 norm of gradients: 0.15879974965644908\n",
      "l2 norm of weights: 3.2519515400724903\n",
      "---------------------\n",
      "Iteration Number: 4183\n",
      "Loss: 15.787828784443029\n",
      "l2 norm of gradients: 0.15873472180899925\n",
      "l2 norm of weights: 3.251837652933336\n",
      "---------------------\n",
      "Iteration Number: 4184\n",
      "Loss: 15.784592717902534\n",
      "l2 norm of gradients: 0.15866973387521666\n",
      "l2 norm of weights: 3.251723923073795\n",
      "---------------------\n",
      "Iteration Number: 4185\n",
      "Loss: 15.781358946159408\n",
      "l2 norm of gradients: 0.15860478580552875\n",
      "l2 norm of weights: 3.251610350379581\n",
      "---------------------\n",
      "Iteration Number: 4186\n",
      "Loss: 15.778127467501502\n",
      "l2 norm of gradients: 0.15853987755038274\n",
      "l2 norm of weights: 3.251496934736517\n",
      "---------------------\n",
      "Iteration Number: 4187\n",
      "Loss: 15.77489828021895\n",
      "l2 norm of gradients: 0.15847500906024545\n",
      "l2 norm of weights: 3.251383676030535\n",
      "---------------------\n",
      "Iteration Number: 4188\n",
      "Loss: 15.771671382604262\n",
      "l2 norm of gradients: 0.15841018028560364\n",
      "l2 norm of weights: 3.251270574147676\n",
      "---------------------\n",
      "Iteration Number: 4189\n",
      "Loss: 15.768446772952286\n",
      "l2 norm of gradients: 0.1583453911769641\n",
      "l2 norm of weights: 3.251157628974089\n",
      "---------------------\n",
      "Iteration Number: 4190\n",
      "Loss: 15.765224449560124\n",
      "l2 norm of gradients: 0.15828064168485362\n",
      "l2 norm of weights: 3.2510448403960326\n",
      "---------------------\n",
      "Iteration Number: 4191\n",
      "Loss: 15.762004410727279\n",
      "l2 norm of gradients: 0.15821593175981927\n",
      "l2 norm of weights: 3.2509322082998726\n",
      "---------------------\n",
      "Iteration Number: 4192\n",
      "Loss: 15.75878665475556\n",
      "l2 norm of gradients: 0.1581512613524285\n",
      "l2 norm of weights: 3.2508197325720842\n",
      "---------------------\n",
      "Iteration Number: 4193\n",
      "Loss: 15.755571179949106\n",
      "l2 norm of gradients: 0.15808663041326934\n",
      "l2 norm of weights: 3.2507074130992484\n",
      "---------------------\n",
      "Iteration Number: 4194\n",
      "Loss: 15.752357984614282\n",
      "l2 norm of gradients: 0.1580220388929504\n",
      "l2 norm of weights: 3.2505952497680553\n",
      "---------------------\n",
      "Iteration Number: 4195\n",
      "Loss: 15.74914706705988\n",
      "l2 norm of gradients: 0.15795748674210097\n",
      "l2 norm of weights: 3.250483242465302\n",
      "---------------------\n",
      "Iteration Number: 4196\n",
      "Loss: 15.745938425596972\n",
      "l2 norm of gradients: 0.15789297391137141\n",
      "l2 norm of weights: 3.250371391077893\n",
      "---------------------\n",
      "Iteration Number: 4197\n",
      "Loss: 15.742732058538913\n",
      "l2 norm of gradients: 0.15782850035143295\n",
      "l2 norm of weights: 3.2502596954928387\n",
      "---------------------\n",
      "Iteration Number: 4198\n",
      "Loss: 15.739527964201358\n",
      "l2 norm of gradients: 0.15776406601297804\n",
      "l2 norm of weights: 3.2501481555972576\n",
      "---------------------\n",
      "Iteration Number: 4199\n",
      "Loss: 15.7363261409023\n",
      "l2 norm of gradients: 0.1576996708467205\n",
      "l2 norm of weights: 3.2500367712783738\n",
      "---------------------\n",
      "Iteration Number: 4200\n",
      "Loss: 15.73312658696204\n",
      "l2 norm of gradients: 0.15763531480339557\n",
      "l2 norm of weights: 3.249925542423518\n",
      "---------------------\n",
      "Iteration Number: 4201\n",
      "Loss: 15.72992930070312\n",
      "l2 norm of gradients: 0.15757099783375986\n",
      "l2 norm of weights: 3.2498144689201265\n",
      "---------------------\n",
      "Iteration Number: 4202\n",
      "Loss: 15.72673428045046\n",
      "l2 norm of gradients: 0.15750671988859194\n",
      "l2 norm of weights: 3.2497035506557426\n",
      "---------------------\n",
      "Iteration Number: 4203\n",
      "Loss: 15.723541524531182\n",
      "l2 norm of gradients: 0.157442480918692\n",
      "l2 norm of weights: 3.249592787518014\n",
      "---------------------\n",
      "Iteration Number: 4204\n",
      "Loss: 15.720351031274765\n",
      "l2 norm of gradients: 0.1573782808748824\n",
      "l2 norm of weights: 3.2494821793946946\n",
      "---------------------\n",
      "Iteration Number: 4205\n",
      "Loss: 15.717162799012957\n",
      "l2 norm of gradients: 0.15731411970800746\n",
      "l2 norm of weights: 3.249371726173644\n",
      "---------------------\n",
      "Iteration Number: 4206\n",
      "Loss: 15.713976826079803\n",
      "l2 norm of gradients: 0.15724999736893372\n",
      "l2 norm of weights: 3.2492614277428244\n",
      "---------------------\n",
      "Iteration Number: 4207\n",
      "Loss: 15.710793110811554\n",
      "l2 norm of gradients: 0.1571859138085503\n",
      "l2 norm of weights: 3.2491512839903067\n",
      "---------------------\n",
      "Iteration Number: 4208\n",
      "Loss: 15.707611651546895\n",
      "l2 norm of gradients: 0.1571218689777686\n",
      "l2 norm of weights: 3.249041294804262\n",
      "---------------------\n",
      "Iteration Number: 4209\n",
      "Loss: 15.704432446626612\n",
      "l2 norm of gradients: 0.15705786282752274\n",
      "l2 norm of weights: 3.2489314600729697\n",
      "---------------------\n",
      "Iteration Number: 4210\n",
      "Loss: 15.701255494393903\n",
      "l2 norm of gradients: 0.15699389530876975\n",
      "l2 norm of weights: 3.248821779684811\n",
      "---------------------\n",
      "Iteration Number: 4211\n",
      "Loss: 15.698080793194196\n",
      "l2 norm of gradients: 0.15692996637248946\n",
      "l2 norm of weights: 3.248712253528271\n",
      "---------------------\n",
      "Iteration Number: 4212\n",
      "Loss: 15.694908341375134\n",
      "l2 norm of gradients: 0.1568660759696848\n",
      "l2 norm of weights: 3.24860288149194\n",
      "---------------------\n",
      "Iteration Number: 4213\n",
      "Loss: 15.691738137286707\n",
      "l2 norm of gradients: 0.15680222405138194\n",
      "l2 norm of weights: 3.248493663464511\n",
      "---------------------\n",
      "Iteration Number: 4214\n",
      "Loss: 15.688570179281113\n",
      "l2 norm of gradients: 0.15673841056863036\n",
      "l2 norm of weights: 3.2483845993347806\n",
      "---------------------\n",
      "Iteration Number: 4215\n",
      "Loss: 15.685404465712848\n",
      "l2 norm of gradients: 0.15667463547250307\n",
      "l2 norm of weights: 3.2482756889916473\n",
      "---------------------\n",
      "Iteration Number: 4216\n",
      "Loss: 15.682240994938656\n",
      "l2 norm of gradients: 0.15661089871409667\n",
      "l2 norm of weights: 3.2481669323241147\n",
      "---------------------\n",
      "Iteration Number: 4217\n",
      "Loss: 15.679079765317526\n",
      "l2 norm of gradients: 0.15654720024453156\n",
      "l2 norm of weights: 3.2480583292212875\n",
      "---------------------\n",
      "Iteration Number: 4218\n",
      "Loss: 15.675920775210683\n",
      "l2 norm of gradients: 0.15648354001495204\n",
      "l2 norm of weights: 3.247949879572373\n",
      "---------------------\n",
      "Iteration Number: 4219\n",
      "Loss: 15.67276402298164\n",
      "l2 norm of gradients: 0.15641991797652657\n",
      "l2 norm of weights: 3.2478415832666814\n",
      "---------------------\n",
      "Iteration Number: 4220\n",
      "Loss: 15.669609506996151\n",
      "l2 norm of gradients: 0.15635633408044758\n",
      "l2 norm of weights: 3.247733440193624\n",
      "---------------------\n",
      "Iteration Number: 4221\n",
      "Loss: 15.66645722562224\n",
      "l2 norm of gradients: 0.15629278827793203\n",
      "l2 norm of weights: 3.2476254502427153\n",
      "---------------------\n",
      "Iteration Number: 4222\n",
      "Loss: 15.663307177230068\n",
      "l2 norm of gradients: 0.15622928052022134\n",
      "l2 norm of weights: 3.2475176133035712\n",
      "---------------------\n",
      "Iteration Number: 4223\n",
      "Loss: 15.66015936019216\n",
      "l2 norm of gradients: 0.15616581075858152\n",
      "l2 norm of weights: 3.247409929265908\n",
      "---------------------\n",
      "Iteration Number: 4224\n",
      "Loss: 15.65701377288324\n",
      "l2 norm of gradients: 0.15610237894430334\n",
      "l2 norm of weights: 3.247302398019544\n",
      "---------------------\n",
      "Iteration Number: 4225\n",
      "Loss: 15.65387041368019\n",
      "l2 norm of gradients: 0.15603898502870256\n",
      "l2 norm of weights: 3.2471950194543986\n",
      "---------------------\n",
      "Iteration Number: 4226\n",
      "Loss: 15.65072928096226\n",
      "l2 norm of gradients: 0.15597562896312\n",
      "l2 norm of weights: 3.247087793460491\n",
      "---------------------\n",
      "Iteration Number: 4227\n",
      "Loss: 15.647590373110841\n",
      "l2 norm of gradients: 0.15591231069892164\n",
      "l2 norm of weights: 3.2469807199279437\n",
      "---------------------\n",
      "Iteration Number: 4228\n",
      "Loss: 15.64445368850951\n",
      "l2 norm of gradients: 0.15584903018749885\n",
      "l2 norm of weights: 3.2468737987469765\n",
      "---------------------\n",
      "Iteration Number: 4229\n",
      "Loss: 15.641319225544159\n",
      "l2 norm of gradients: 0.15578578738026844\n",
      "l2 norm of weights: 3.246767029807912\n",
      "---------------------\n",
      "Iteration Number: 4230\n",
      "Loss: 15.638186982602887\n",
      "l2 norm of gradients: 0.15572258222867305\n",
      "l2 norm of weights: 3.2466604130011705\n",
      "---------------------\n",
      "Iteration Number: 4231\n",
      "Loss: 15.635056958075946\n",
      "l2 norm of gradients: 0.1556594146841809\n",
      "l2 norm of weights: 3.246553948217274\n",
      "---------------------\n",
      "Iteration Number: 4232\n",
      "Loss: 15.63192915035588\n",
      "l2 norm of gradients: 0.15559628469828635\n",
      "l2 norm of weights: 3.2464476353468434\n",
      "---------------------\n",
      "Iteration Number: 4233\n",
      "Loss: 15.62880355783739\n",
      "l2 norm of gradients: 0.1555331922225098\n",
      "l2 norm of weights: 3.246341474280599\n",
      "---------------------\n",
      "Iteration Number: 4234\n",
      "Loss: 15.625680178917394\n",
      "l2 norm of gradients: 0.15547013720839778\n",
      "l2 norm of weights: 3.2462354649093603\n",
      "---------------------\n",
      "Iteration Number: 4235\n",
      "Loss: 15.622559011995069\n",
      "l2 norm of gradients: 0.1554071196075234\n",
      "l2 norm of weights: 3.246129607124046\n",
      "---------------------\n",
      "Iteration Number: 4236\n",
      "Loss: 15.619440055471692\n",
      "l2 norm of gradients: 0.15534413937148622\n",
      "l2 norm of weights: 3.2460239008156737\n",
      "---------------------\n",
      "Iteration Number: 4237\n",
      "Loss: 15.616323307750871\n",
      "l2 norm of gradients: 0.15528119645191255\n",
      "l2 norm of weights: 3.2459183458753595\n",
      "---------------------\n",
      "Iteration Number: 4238\n",
      "Loss: 15.613208767238284\n",
      "l2 norm of gradients: 0.1552182908004554\n",
      "l2 norm of weights: 3.2458129421943176\n",
      "---------------------\n",
      "Iteration Number: 4239\n",
      "Loss: 15.61009643234192\n",
      "l2 norm of gradients: 0.15515542236879504\n",
      "l2 norm of weights: 3.2457076896638615\n",
      "---------------------\n",
      "Iteration Number: 4240\n",
      "Loss: 15.60698630147185\n",
      "l2 norm of gradients: 0.15509259110863874\n",
      "l2 norm of weights: 3.2456025881754016\n",
      "---------------------\n",
      "Iteration Number: 4241\n",
      "Loss: 15.603878373040459\n",
      "l2 norm of gradients: 0.155029796971721\n",
      "l2 norm of weights: 3.245497637620446\n",
      "---------------------\n",
      "Iteration Number: 4242\n",
      "Loss: 15.600772645462165\n",
      "l2 norm of gradients: 0.15496703990980398\n",
      "l2 norm of weights: 3.2453928378906025\n",
      "---------------------\n",
      "Iteration Number: 4243\n",
      "Loss: 15.59766911715373\n",
      "l2 norm of gradients: 0.15490431987467732\n",
      "l2 norm of weights: 3.245288188877573\n",
      "---------------------\n",
      "Iteration Number: 4244\n",
      "Loss: 15.59456778653398\n",
      "l2 norm of gradients: 0.1548416368181584\n",
      "l2 norm of weights: 3.24518369047316\n",
      "---------------------\n",
      "Iteration Number: 4245\n",
      "Loss: 15.591468652023956\n",
      "l2 norm of gradients: 0.1547789906920927\n",
      "l2 norm of weights: 3.2450793425692606\n",
      "---------------------\n",
      "Iteration Number: 4246\n",
      "Loss: 15.588371712046914\n",
      "l2 norm of gradients: 0.15471638144835353\n",
      "l2 norm of weights: 3.2449751450578703\n",
      "---------------------\n",
      "Iteration Number: 4247\n",
      "Loss: 15.58527696502823\n",
      "l2 norm of gradients: 0.15465380903884263\n",
      "l2 norm of weights: 3.2448710978310804\n",
      "---------------------\n",
      "Iteration Number: 4248\n",
      "Loss: 15.58218440939543\n",
      "l2 norm of gradients: 0.15459127341549003\n",
      "l2 norm of weights: 3.2447672007810793\n",
      "---------------------\n",
      "Iteration Number: 4249\n",
      "Loss: 15.579094043578289\n",
      "l2 norm of gradients: 0.15452877453025435\n",
      "l2 norm of weights: 3.2446634538001504\n",
      "---------------------\n",
      "Iteration Number: 4250\n",
      "Loss: 15.576005866008696\n",
      "l2 norm of gradients: 0.15446631233512284\n",
      "l2 norm of weights: 3.2445598567806755\n",
      "---------------------\n",
      "Iteration Number: 4251\n",
      "Loss: 15.572919875120698\n",
      "l2 norm of gradients: 0.15440388678211173\n",
      "l2 norm of weights: 3.2444564096151303\n",
      "---------------------\n",
      "Iteration Number: 4252\n",
      "Loss: 15.569836069350506\n",
      "l2 norm of gradients: 0.15434149782326612\n",
      "l2 norm of weights: 3.244353112196086\n",
      "---------------------\n",
      "Iteration Number: 4253\n",
      "Loss: 15.566754447136459\n",
      "l2 norm of gradients: 0.15427914541066043\n",
      "l2 norm of weights: 3.2442499644162113\n",
      "---------------------\n",
      "Iteration Number: 4254\n",
      "Loss: 15.563675006919112\n",
      "l2 norm of gradients: 0.1542168294963982\n",
      "l2 norm of weights: 3.244146966168268\n",
      "---------------------\n",
      "Iteration Number: 4255\n",
      "Loss: 15.560597747141134\n",
      "l2 norm of gradients: 0.15415455003261275\n",
      "l2 norm of weights: 3.2440441173451147\n",
      "---------------------\n",
      "Iteration Number: 4256\n",
      "Loss: 15.557522666247301\n",
      "l2 norm of gradients: 0.15409230697146675\n",
      "l2 norm of weights: 3.243941417839704\n",
      "---------------------\n",
      "Iteration Number: 4257\n",
      "Loss: 15.554449762684621\n",
      "l2 norm of gradients: 0.1540301002651529\n",
      "l2 norm of weights: 3.2438388675450844\n",
      "---------------------\n",
      "Iteration Number: 4258\n",
      "Loss: 15.551379034902151\n",
      "l2 norm of gradients: 0.15396792986589375\n",
      "l2 norm of weights: 3.2437364663543966\n",
      "---------------------\n",
      "Iteration Number: 4259\n",
      "Loss: 15.54831048135115\n",
      "l2 norm of gradients: 0.153905795725942\n",
      "l2 norm of weights: 3.243634214160878\n",
      "---------------------\n",
      "Iteration Number: 4260\n",
      "Loss: 15.545244100484975\n",
      "l2 norm of gradients: 0.15384369779758067\n",
      "l2 norm of weights: 3.2435321108578594\n",
      "---------------------\n",
      "Iteration Number: 4261\n",
      "Loss: 15.542179890759149\n",
      "l2 norm of gradients: 0.1537816360331232\n",
      "l2 norm of weights: 3.2434301563387646\n",
      "---------------------\n",
      "Iteration Number: 4262\n",
      "Loss: 15.539117850631259\n",
      "l2 norm of gradients: 0.1537196103849136\n",
      "l2 norm of weights: 3.2433283504971135\n",
      "---------------------\n",
      "Iteration Number: 4263\n",
      "Loss: 15.536057978561077\n",
      "l2 norm of gradients: 0.15365762080532674\n",
      "l2 norm of weights: 3.243226693226517\n",
      "---------------------\n",
      "Iteration Number: 4264\n",
      "Loss: 15.533000273010511\n",
      "l2 norm of gradients: 0.15359566724676837\n",
      "l2 norm of weights: 3.243125184420681\n",
      "---------------------\n",
      "Iteration Number: 4265\n",
      "Loss: 15.529944732443534\n",
      "l2 norm of gradients: 0.15353374966167538\n",
      "l2 norm of weights: 3.2430238239734046\n",
      "---------------------\n",
      "Iteration Number: 4266\n",
      "Loss: 15.526891355326235\n",
      "l2 norm of gradients: 0.15347186800251586\n",
      "l2 norm of weights: 3.242922611778578\n",
      "---------------------\n",
      "Iteration Number: 4267\n",
      "Loss: 15.523840140126888\n",
      "l2 norm of gradients: 0.15341002222178937\n",
      "l2 norm of weights: 3.242821547730188\n",
      "---------------------\n",
      "Iteration Number: 4268\n",
      "Loss: 15.520791085315784\n",
      "l2 norm of gradients: 0.15334821227202702\n",
      "l2 norm of weights: 3.2427206317223107\n",
      "---------------------\n",
      "Iteration Number: 4269\n",
      "Loss: 15.517744189365416\n",
      "l2 norm of gradients: 0.15328643810579176\n",
      "l2 norm of weights: 3.242619863649116\n",
      "---------------------\n",
      "Iteration Number: 4270\n",
      "Loss: 15.514699450750303\n",
      "l2 norm of gradients: 0.1532246996756783\n",
      "l2 norm of weights: 3.242519243404866\n",
      "---------------------\n",
      "Iteration Number: 4271\n",
      "Loss: 15.511656867947082\n",
      "l2 norm of gradients: 0.15316299693431368\n",
      "l2 norm of weights: 3.2424187708839156\n",
      "---------------------\n",
      "Iteration Number: 4272\n",
      "Loss: 15.508616439434562\n",
      "l2 norm of gradients: 0.15310132983435684\n",
      "l2 norm of weights: 3.2423184459807106\n",
      "---------------------\n",
      "Iteration Number: 4273\n",
      "Loss: 15.505578163693528\n",
      "l2 norm of gradients: 0.15303969832849948\n",
      "l2 norm of weights: 3.242218268589788\n",
      "---------------------\n",
      "Iteration Number: 4274\n",
      "Loss: 15.502542039206954\n",
      "l2 norm of gradients: 0.15297810236946563\n",
      "l2 norm of weights: 3.2421182386057787\n",
      "---------------------\n",
      "Iteration Number: 4275\n",
      "Loss: 15.499508064459826\n",
      "l2 norm of gradients: 0.1529165419100123\n",
      "l2 norm of weights: 3.2420183559234026\n",
      "---------------------\n",
      "Iteration Number: 4276\n",
      "Loss: 15.496476237939312\n",
      "l2 norm of gradients: 0.15285501690292916\n",
      "l2 norm of weights: 3.241918620437472\n",
      "---------------------\n",
      "Iteration Number: 4277\n",
      "Loss: 15.493446558134544\n",
      "l2 norm of gradients: 0.15279352730103915\n",
      "l2 norm of weights: 3.24181903204289\n",
      "---------------------\n",
      "Iteration Number: 4278\n",
      "Loss: 15.490419023536875\n",
      "l2 norm of gradients: 0.15273207305719835\n",
      "l2 norm of weights: 3.241719590634651\n",
      "---------------------\n",
      "Iteration Number: 4279\n",
      "Loss: 15.48739363263958\n",
      "l2 norm of gradients: 0.15267065412429637\n",
      "l2 norm of weights: 3.2416202961078393\n",
      "---------------------\n",
      "Iteration Number: 4280\n",
      "Loss: 15.484370383938144\n",
      "l2 norm of gradients: 0.15260927045525627\n",
      "l2 norm of weights: 3.24152114835763\n",
      "---------------------\n",
      "Iteration Number: 4281\n",
      "Loss: 15.481349275930041\n",
      "l2 norm of gradients: 0.15254792200303496\n",
      "l2 norm of weights: 3.2414221472792883\n",
      "---------------------\n",
      "Iteration Number: 4282\n",
      "Loss: 15.478330307114849\n",
      "l2 norm of gradients: 0.1524866087206233\n",
      "l2 norm of weights: 3.2413232927681697\n",
      "---------------------\n",
      "Iteration Number: 4283\n",
      "Loss: 15.475313475994177\n",
      "l2 norm of gradients: 0.15242533056104618\n",
      "l2 norm of weights: 3.2412245847197205\n",
      "---------------------\n",
      "Iteration Number: 4284\n",
      "Loss: 15.472298781071732\n",
      "l2 norm of gradients: 0.15236408747736274\n",
      "l2 norm of weights: 3.2411260230294743\n",
      "---------------------\n",
      "Iteration Number: 4285\n",
      "Loss: 15.469286220853292\n",
      "l2 norm of gradients: 0.15230287942266668\n",
      "l2 norm of weights: 3.241027607593058\n",
      "---------------------\n",
      "Iteration Number: 4286\n",
      "Loss: 15.466275793846606\n",
      "l2 norm of gradients: 0.15224170635008608\n",
      "l2 norm of weights: 3.2409293383061835\n",
      "---------------------\n",
      "Iteration Number: 4287\n",
      "Loss: 15.463267498561596\n",
      "l2 norm of gradients: 0.15218056821278406\n",
      "l2 norm of weights: 3.240831215064656\n",
      "---------------------\n",
      "Iteration Number: 4288\n",
      "Loss: 15.460261333510092\n",
      "l2 norm of gradients: 0.1521194649639585\n",
      "l2 norm of weights: 3.2407332377643674\n",
      "---------------------\n",
      "Iteration Number: 4289\n",
      "Loss: 15.457257297206128\n",
      "l2 norm of gradients: 0.15205839655684253\n",
      "l2 norm of weights: 3.240635406301299\n",
      "---------------------\n",
      "Iteration Number: 4290\n",
      "Loss: 15.454255388165642\n",
      "l2 norm of gradients: 0.15199736294470442\n",
      "l2 norm of weights: 3.240537720571521\n",
      "---------------------\n",
      "Iteration Number: 4291\n",
      "Loss: 15.4512556049067\n",
      "l2 norm of gradients: 0.15193636408084812\n",
      "l2 norm of weights: 3.240440180471192\n",
      "---------------------\n",
      "Iteration Number: 4292\n",
      "Loss: 15.448257945949376\n",
      "l2 norm of gradients: 0.151875399918613\n",
      "l2 norm of weights: 3.240342785896559\n",
      "---------------------\n",
      "Iteration Number: 4293\n",
      "Loss: 15.445262409815767\n",
      "l2 norm of gradients: 0.1518144704113744\n",
      "l2 norm of weights: 3.240245536743957\n",
      "---------------------\n",
      "Iteration Number: 4294\n",
      "Loss: 15.442268995030007\n",
      "l2 norm of gradients: 0.15175357551254356\n",
      "l2 norm of weights: 3.2401484329098094\n",
      "---------------------\n",
      "Iteration Number: 4295\n",
      "Loss: 15.439277700118264\n",
      "l2 norm of gradients: 0.15169271517556798\n",
      "l2 norm of weights: 3.2400514742906266\n",
      "---------------------\n",
      "Iteration Number: 4296\n",
      "Loss: 15.436288523608717\n",
      "l2 norm of gradients: 0.15163188935393132\n",
      "l2 norm of weights: 3.239954660783008\n",
      "---------------------\n",
      "Iteration Number: 4297\n",
      "Loss: 15.433301464031594\n",
      "l2 norm of gradients: 0.15157109800115393\n",
      "l2 norm of weights: 3.2398579922836386\n",
      "---------------------\n",
      "Iteration Number: 4298\n",
      "Loss: 15.430316519919101\n",
      "l2 norm of gradients: 0.15151034107079275\n",
      "l2 norm of weights: 3.2397614686892937\n",
      "---------------------\n",
      "Iteration Number: 4299\n",
      "Loss: 15.427333689805474\n",
      "l2 norm of gradients: 0.15144961851644165\n",
      "l2 norm of weights: 3.2396650898968318\n",
      "---------------------\n",
      "Iteration Number: 4300\n",
      "Loss: 15.42435297222698\n",
      "l2 norm of gradients: 0.15138893029173142\n",
      "l2 norm of weights: 3.239568855803202\n",
      "---------------------\n",
      "Iteration Number: 4301\n",
      "Loss: 15.421374365721887\n",
      "l2 norm of gradients: 0.15132827635033017\n",
      "l2 norm of weights: 3.2394727663054375\n",
      "---------------------\n",
      "Iteration Number: 4302\n",
      "Loss: 15.41839786883042\n",
      "l2 norm of gradients: 0.15126765664594338\n",
      "l2 norm of weights: 3.2393768213006595\n",
      "---------------------\n",
      "Iteration Number: 4303\n",
      "Loss: 15.415423480094882\n",
      "l2 norm of gradients: 0.15120707113231405\n",
      "l2 norm of weights: 3.239281020686076\n",
      "---------------------\n",
      "Iteration Number: 4304\n",
      "Loss: 15.41245119805952\n",
      "l2 norm of gradients: 0.15114651976322294\n",
      "l2 norm of weights: 3.23918536435898\n",
      "---------------------\n",
      "Iteration Number: 4305\n",
      "Loss: 15.409481021270576\n",
      "l2 norm of gradients: 0.15108600249248885\n",
      "l2 norm of weights: 3.2390898522167517\n",
      "---------------------\n",
      "Iteration Number: 4306\n",
      "Loss: 15.406512948276326\n",
      "l2 norm of gradients: 0.1510255192739684\n",
      "l2 norm of weights: 3.2389944841568568\n",
      "---------------------\n",
      "Iteration Number: 4307\n",
      "Loss: 15.403546977627014\n",
      "l2 norm of gradients: 0.15096507006155677\n",
      "l2 norm of weights: 3.2388992600768463\n",
      "---------------------\n",
      "Iteration Number: 4308\n",
      "Loss: 15.400583107874823\n",
      "l2 norm of gradients: 0.15090465480918744\n",
      "l2 norm of weights: 3.238804179874358\n",
      "---------------------\n",
      "Iteration Number: 4309\n",
      "Loss: 15.397621337574012\n",
      "l2 norm of gradients: 0.15084427347083254\n",
      "l2 norm of weights: 3.238709243447114\n",
      "---------------------\n",
      "Iteration Number: 4310\n",
      "Loss: 15.394661665280697\n",
      "l2 norm of gradients: 0.15078392600050308\n",
      "l2 norm of weights: 3.238614450692923\n",
      "---------------------\n",
      "Iteration Number: 4311\n",
      "Loss: 15.391704089553096\n",
      "l2 norm of gradients: 0.15072361235224904\n",
      "l2 norm of weights: 3.2385198015096774\n",
      "---------------------\n",
      "Iteration Number: 4312\n",
      "Loss: 15.388748608951316\n",
      "l2 norm of gradients: 0.15066333248015942\n",
      "l2 norm of weights: 3.2384252957953543\n",
      "---------------------\n",
      "Iteration Number: 4313\n",
      "Loss: 15.385795222037455\n",
      "l2 norm of gradients: 0.15060308633836278\n",
      "l2 norm of weights: 3.2383309334480175\n",
      "---------------------\n",
      "Iteration Number: 4314\n",
      "Loss: 15.3828439273756\n",
      "l2 norm of gradients: 0.15054287388102716\n",
      "l2 norm of weights: 3.2382367143658146\n",
      "---------------------\n",
      "Iteration Number: 4315\n",
      "Loss: 15.379894723531752\n",
      "l2 norm of gradients: 0.15048269506236017\n",
      "l2 norm of weights: 3.238142638446976\n",
      "---------------------\n",
      "Iteration Number: 4316\n",
      "Loss: 15.37694760907393\n",
      "l2 norm of gradients: 0.15042254983660946\n",
      "l2 norm of weights: 3.2380487055898186\n",
      "---------------------\n",
      "Iteration Number: 4317\n",
      "Loss: 15.374002582572034\n",
      "l2 norm of gradients: 0.1503624381580627\n",
      "l2 norm of weights: 3.2379549156927423\n",
      "---------------------\n",
      "Iteration Number: 4318\n",
      "Loss: 15.37105964259798\n",
      "l2 norm of gradients: 0.15030235998104777\n",
      "l2 norm of weights: 3.2378612686542314\n",
      "---------------------\n",
      "Iteration Number: 4319\n",
      "Loss: 15.368118787725619\n",
      "l2 norm of gradients: 0.1502423152599331\n",
      "l2 norm of weights: 3.2377677643728537\n",
      "---------------------\n",
      "Iteration Number: 4320\n",
      "Loss: 15.36518001653073\n",
      "l2 norm of gradients: 0.1501823039491276\n",
      "l2 norm of weights: 3.237674402747261\n",
      "---------------------\n",
      "Iteration Number: 4321\n",
      "Loss: 15.362243327591065\n",
      "l2 norm of gradients: 0.1501223260030811\n",
      "l2 norm of weights: 3.237581183676187\n",
      "---------------------\n",
      "Iteration Number: 4322\n",
      "Loss: 15.359308719486258\n",
      "l2 norm of gradients: 0.15006238137628428\n",
      "l2 norm of weights: 3.2374881070584522\n",
      "---------------------\n",
      "Iteration Number: 4323\n",
      "Loss: 15.356376190797953\n",
      "l2 norm of gradients: 0.15000247002326908\n",
      "l2 norm of weights: 3.237395172792956\n",
      "---------------------\n",
      "Iteration Number: 4324\n",
      "Loss: 15.353445740109676\n",
      "l2 norm of gradients: 0.14994259189860876\n",
      "l2 norm of weights: 3.2373023807786843\n",
      "---------------------\n",
      "Iteration Number: 4325\n",
      "Loss: 15.350517366006878\n",
      "l2 norm of gradients: 0.14988274695691806\n",
      "l2 norm of weights: 3.237209730914703\n",
      "---------------------\n",
      "Iteration Number: 4326\n",
      "Loss: 15.34759106707697\n",
      "l2 norm of gradients: 0.14982293515285353\n",
      "l2 norm of weights: 3.237117223100163\n",
      "---------------------\n",
      "Iteration Number: 4327\n",
      "Loss: 15.34466684190925\n",
      "l2 norm of gradients: 0.1497631564411135\n",
      "l2 norm of weights: 3.2370248572342963\n",
      "---------------------\n",
      "Iteration Number: 4328\n",
      "Loss: 15.341744689094975\n",
      "l2 norm of gradients: 0.14970341077643842\n",
      "l2 norm of weights: 3.236932633216418\n",
      "---------------------\n",
      "Iteration Number: 4329\n",
      "Loss: 15.33882460722729\n",
      "l2 norm of gradients: 0.14964369811361106\n",
      "l2 norm of weights: 3.2368405509459244\n",
      "---------------------\n",
      "Iteration Number: 4330\n",
      "Loss: 15.335906594901246\n",
      "l2 norm of gradients: 0.1495840184074566\n",
      "l2 norm of weights: 3.2367486103222953\n",
      "---------------------\n",
      "Iteration Number: 4331\n",
      "Loss: 15.332990650713814\n",
      "l2 norm of gradients: 0.14952437161284277\n",
      "l2 norm of weights: 3.236656811245091\n",
      "---------------------\n",
      "Iteration Number: 4332\n",
      "Loss: 15.330076773263894\n",
      "l2 norm of gradients: 0.14946475768468023\n",
      "l2 norm of weights: 3.236565153613954\n",
      "---------------------\n",
      "Iteration Number: 4333\n",
      "Loss: 15.327164961152246\n",
      "l2 norm of gradients: 0.14940517657792254\n",
      "l2 norm of weights: 3.2364736373286087\n",
      "---------------------\n",
      "Iteration Number: 4334\n",
      "Loss: 15.324255212981537\n",
      "l2 norm of gradients: 0.1493456282475665\n",
      "l2 norm of weights: 3.23638226228886\n",
      "---------------------\n",
      "Iteration Number: 4335\n",
      "Loss: 15.321347527356325\n",
      "l2 norm of gradients: 0.14928611264865232\n",
      "l2 norm of weights: 3.236291028394595\n",
      "---------------------\n",
      "Iteration Number: 4336\n",
      "Loss: 15.31844190288313\n",
      "l2 norm of gradients: 0.1492266297362637\n",
      "l2 norm of weights: 3.2361999355457822\n",
      "---------------------\n",
      "Iteration Number: 4337\n",
      "Loss: 15.315538338170258\n",
      "l2 norm of gradients: 0.14916717946552804\n",
      "l2 norm of weights: 3.2361089836424695\n",
      "---------------------\n",
      "Iteration Number: 4338\n",
      "Loss: 15.312636831827955\n",
      "l2 norm of gradients: 0.14910776179161672\n",
      "l2 norm of weights: 3.2360181725847865\n",
      "---------------------\n",
      "Iteration Number: 4339\n",
      "Loss: 15.309737382468306\n",
      "l2 norm of gradients: 0.1490483766697453\n",
      "l2 norm of weights: 3.2359275022729435\n",
      "---------------------\n",
      "Iteration Number: 4340\n",
      "Loss: 15.306839988705379\n",
      "l2 norm of gradients: 0.14898902405517347\n",
      "l2 norm of weights: 3.235836972607231\n",
      "---------------------\n",
      "Iteration Number: 4341\n",
      "Loss: 15.30394464915498\n",
      "l2 norm of gradients: 0.1489297039032056\n",
      "l2 norm of weights: 3.23574658348802\n",
      "---------------------\n",
      "Iteration Number: 4342\n",
      "Loss: 15.301051362434901\n",
      "l2 norm of gradients: 0.14887041616919053\n",
      "l2 norm of weights: 3.235656334815762\n",
      "---------------------\n",
      "Iteration Number: 4343\n",
      "Loss: 15.298160127164685\n",
      "l2 norm of gradients: 0.1488111608085221\n",
      "l2 norm of weights: 3.2355662264909872\n",
      "---------------------\n",
      "Iteration Number: 4344\n",
      "Loss: 15.295270941965846\n",
      "l2 norm of gradients: 0.14875193777663911\n",
      "l2 norm of weights: 3.235476258414307\n",
      "---------------------\n",
      "Iteration Number: 4345\n",
      "Loss: 15.292383805461718\n",
      "l2 norm of gradients: 0.14869274702902577\n",
      "l2 norm of weights: 3.235386430486413\n",
      "---------------------\n",
      "Iteration Number: 4346\n",
      "Loss: 15.289498716277478\n",
      "l2 norm of gradients: 0.1486335885212114\n",
      "l2 norm of weights: 3.2352967426080737\n",
      "---------------------\n",
      "Iteration Number: 4347\n",
      "Loss: 15.286615673040147\n",
      "l2 norm of gradients: 0.14857446220877113\n",
      "l2 norm of weights: 3.23520719468014\n",
      "---------------------\n",
      "Iteration Number: 4348\n",
      "Loss: 15.283734674378673\n",
      "l2 norm of gradients: 0.14851536804732593\n",
      "l2 norm of weights: 3.2351177866035394\n",
      "---------------------\n",
      "Iteration Number: 4349\n",
      "Loss: 15.280855718923744\n",
      "l2 norm of gradients: 0.1484563059925426\n",
      "l2 norm of weights: 3.235028518279282\n",
      "---------------------\n",
      "Iteration Number: 4350\n",
      "Loss: 15.277978805307985\n",
      "l2 norm of gradients: 0.14839727600013417\n",
      "l2 norm of weights: 3.234939389608453\n",
      "---------------------\n",
      "Iteration Number: 4351\n",
      "Loss: 15.275103932165768\n",
      "l2 norm of gradients: 0.14833827802586008\n",
      "l2 norm of weights: 3.234850400492218\n",
      "---------------------\n",
      "Iteration Number: 4352\n",
      "Loss: 15.272231098133409\n",
      "l2 norm of gradients: 0.14827931202552616\n",
      "l2 norm of weights: 3.234761550831822\n",
      "---------------------\n",
      "Iteration Number: 4353\n",
      "Loss: 15.269360301848963\n",
      "l2 norm of gradients: 0.1482203779549852\n",
      "l2 norm of weights: 3.2346728405285887\n",
      "---------------------\n",
      "Iteration Number: 4354\n",
      "Loss: 15.266491541952341\n",
      "l2 norm of gradients: 0.14816147577013666\n",
      "l2 norm of weights: 3.234584269483918\n",
      "---------------------\n",
      "Iteration Number: 4355\n",
      "Loss: 15.263624817085313\n",
      "l2 norm of gradients: 0.14810260542692735\n",
      "l2 norm of weights: 3.2344958375992903\n",
      "---------------------\n",
      "Iteration Number: 4356\n",
      "Loss: 15.26076012589145\n",
      "l2 norm of gradients: 0.14804376688135124\n",
      "l2 norm of weights: 3.234407544776262\n",
      "---------------------\n",
      "Iteration Number: 4357\n",
      "Loss: 15.2578974670161\n",
      "l2 norm of gradients: 0.14798496008944978\n",
      "l2 norm of weights: 3.234319390916469\n",
      "---------------------\n",
      "Iteration Number: 4358\n",
      "Loss: 15.255036839106479\n",
      "l2 norm of gradients: 0.1479261850073121\n",
      "l2 norm of weights: 3.234231375921625\n",
      "---------------------\n",
      "Iteration Number: 4359\n",
      "Loss: 15.252178240811649\n",
      "l2 norm of gradients: 0.14786744159107534\n",
      "l2 norm of weights: 3.2341434996935203\n",
      "---------------------\n",
      "Iteration Number: 4360\n",
      "Loss: 15.249321670782345\n",
      "l2 norm of gradients: 0.14780872979692447\n",
      "l2 norm of weights: 3.2340557621340236\n",
      "---------------------\n",
      "Iteration Number: 4361\n",
      "Loss: 15.246467127671245\n",
      "l2 norm of gradients: 0.1477500495810928\n",
      "l2 norm of weights: 3.23396816314508\n",
      "---------------------\n",
      "Iteration Number: 4362\n",
      "Loss: 15.243614610132754\n",
      "l2 norm of gradients: 0.14769140089986213\n",
      "l2 norm of weights: 3.2338807026287126\n",
      "---------------------\n",
      "Iteration Number: 4363\n",
      "Loss: 15.240764116823117\n",
      "l2 norm of gradients: 0.14763278370956284\n",
      "l2 norm of weights: 3.2337933804870214\n",
      "---------------------\n",
      "Iteration Number: 4364\n",
      "Loss: 15.237915646400317\n",
      "l2 norm of gradients: 0.14757419796657406\n",
      "l2 norm of weights: 3.233706196622183\n",
      "---------------------\n",
      "Iteration Number: 4365\n",
      "Loss: 15.235069197524178\n",
      "l2 norm of gradients: 0.14751564362732408\n",
      "l2 norm of weights: 3.2336191509364514\n",
      "---------------------\n",
      "Iteration Number: 4366\n",
      "Loss: 15.232224768856257\n",
      "l2 norm of gradients: 0.14745712064829014\n",
      "l2 norm of weights: 3.2335322433321556\n",
      "---------------------\n",
      "Iteration Number: 4367\n",
      "Loss: 15.229382359059978\n",
      "l2 norm of gradients: 0.14739862898599904\n",
      "l2 norm of weights: 3.2334454737117033\n",
      "---------------------\n",
      "Iteration Number: 4368\n",
      "Loss: 15.22654196680047\n",
      "l2 norm of gradients: 0.14734016859702717\n",
      "l2 norm of weights: 3.2333588419775774\n",
      "---------------------\n",
      "Iteration Number: 4369\n",
      "Loss: 15.223703590744648\n",
      "l2 norm of gradients: 0.14728173943800055\n",
      "l2 norm of weights: 3.2332723480323367\n",
      "---------------------\n",
      "Iteration Number: 4370\n",
      "Loss: 15.220867229561215\n",
      "l2 norm of gradients: 0.1472233414655952\n",
      "l2 norm of weights: 3.2331859917786163\n",
      "---------------------\n",
      "Iteration Number: 4371\n",
      "Loss: 15.218032881920655\n",
      "l2 norm of gradients: 0.14716497463653733\n",
      "l2 norm of weights: 3.233099773119128\n",
      "---------------------\n",
      "Iteration Number: 4372\n",
      "Loss: 15.215200546495181\n",
      "l2 norm of gradients: 0.14710663890760345\n",
      "l2 norm of weights: 3.233013691956658\n",
      "---------------------\n",
      "Iteration Number: 4373\n",
      "Loss: 15.212370221958803\n",
      "l2 norm of gradients: 0.14704833423562055\n",
      "l2 norm of weights: 3.2329277481940686\n",
      "---------------------\n",
      "Iteration Number: 4374\n",
      "Loss: 15.209541906987255\n",
      "l2 norm of gradients: 0.14699006057746633\n",
      "l2 norm of weights: 3.232841941734298\n",
      "---------------------\n",
      "Iteration Number: 4375\n",
      "Loss: 15.20671560025806\n",
      "l2 norm of gradients: 0.1469318178900695\n",
      "l2 norm of weights: 3.2327562724803607\n",
      "---------------------\n",
      "Iteration Number: 4376\n",
      "Loss: 15.203891300450444\n",
      "l2 norm of gradients: 0.14687360613040978\n",
      "l2 norm of weights: 3.2326707403353434\n",
      "---------------------\n",
      "Iteration Number: 4377\n",
      "Loss: 15.20106900624544\n",
      "l2 norm of gradients: 0.14681542525551808\n",
      "l2 norm of weights: 3.232585345202411\n",
      "---------------------\n",
      "Iteration Number: 4378\n",
      "Loss: 15.198248716325761\n",
      "l2 norm of gradients: 0.14675727522247695\n",
      "l2 norm of weights: 3.232500086984802\n",
      "---------------------\n",
      "Iteration Number: 4379\n",
      "Loss: 15.195430429375913\n",
      "l2 norm of gradients: 0.14669915598842048\n",
      "l2 norm of weights: 3.2324149655858294\n",
      "---------------------\n",
      "Iteration Number: 4380\n",
      "Loss: 15.192614144082086\n",
      "l2 norm of gradients: 0.14664106751053468\n",
      "l2 norm of weights: 3.2323299809088812\n",
      "---------------------\n",
      "Iteration Number: 4381\n",
      "Loss: 15.189799859132242\n",
      "l2 norm of gradients: 0.1465830097460576\n",
      "l2 norm of weights: 3.2322451328574204\n",
      "---------------------\n",
      "Iteration Number: 4382\n",
      "Loss: 15.186987573216088\n",
      "l2 norm of gradients: 0.1465249826522794\n",
      "l2 norm of weights: 3.2321604213349837\n",
      "---------------------\n",
      "Iteration Number: 4383\n",
      "Loss: 15.184177285024973\n",
      "l2 norm of gradients: 0.14646698618654289\n",
      "l2 norm of weights: 3.2320758462451824\n",
      "---------------------\n",
      "Iteration Number: 4384\n",
      "Loss: 15.18136899325204\n",
      "l2 norm of gradients: 0.14640902030624325\n",
      "l2 norm of weights: 3.231991407491702\n",
      "---------------------\n",
      "Iteration Number: 4385\n",
      "Loss: 15.178562696592126\n",
      "l2 norm of gradients: 0.14635108496882865\n",
      "l2 norm of weights: 3.2319071049783012\n",
      "---------------------\n",
      "Iteration Number: 4386\n",
      "Loss: 15.175758393741807\n",
      "l2 norm of gradients: 0.1462931801318001\n",
      "l2 norm of weights: 3.231822938608814\n",
      "---------------------\n",
      "Iteration Number: 4387\n",
      "Loss: 15.17295608339931\n",
      "l2 norm of gradients: 0.14623530575271196\n",
      "l2 norm of weights: 3.2317389082871473\n",
      "---------------------\n",
      "Iteration Number: 4388\n",
      "Loss: 15.170155764264623\n",
      "l2 norm of gradients: 0.1461774617891718\n",
      "l2 norm of weights: 3.2316550139172806\n",
      "---------------------\n",
      "Iteration Number: 4389\n",
      "Loss: 15.167357435039419\n",
      "l2 norm of gradients: 0.14611964819884088\n",
      "l2 norm of weights: 3.2315712554032694\n",
      "---------------------\n",
      "Iteration Number: 4390\n",
      "Loss: 15.164561094427036\n",
      "l2 norm of gradients: 0.1460618649394341\n",
      "l2 norm of weights: 3.2314876326492397\n",
      "---------------------\n",
      "Iteration Number: 4391\n",
      "Loss: 15.161766741132558\n",
      "l2 norm of gradients: 0.14600411196872043\n",
      "l2 norm of weights: 3.2314041455593925\n",
      "---------------------\n",
      "Iteration Number: 4392\n",
      "Loss: 15.158974373862753\n",
      "l2 norm of gradients: 0.14594638924452288\n",
      "l2 norm of weights: 3.2313207940380004\n",
      "---------------------\n",
      "Iteration Number: 4393\n",
      "Loss: 15.15618399132602\n",
      "l2 norm of gradients: 0.14588869672471877\n",
      "l2 norm of weights: 3.2312375779894116\n",
      "---------------------\n",
      "Iteration Number: 4394\n",
      "Loss: 15.153395592232524\n",
      "l2 norm of gradients: 0.14583103436724004\n",
      "l2 norm of weights: 3.231154497318044\n",
      "---------------------\n",
      "Iteration Number: 4395\n",
      "Loss: 15.150609175294045\n",
      "l2 norm of gradients: 0.14577340213007323\n",
      "l2 norm of weights: 3.2310715519283906\n",
      "---------------------\n",
      "Iteration Number: 4396\n",
      "Loss: 15.147824739224106\n",
      "l2 norm of gradients: 0.1457157999712598\n",
      "l2 norm of weights: 3.230988741725015\n",
      "---------------------\n",
      "Iteration Number: 4397\n",
      "Loss: 15.145042282737784\n",
      "l2 norm of gradients: 0.14565822784889643\n",
      "l2 norm of weights: 3.2309060666125546\n",
      "---------------------\n",
      "Iteration Number: 4398\n",
      "Loss: 15.14226180455195\n",
      "l2 norm of gradients: 0.14560068572113477\n",
      "l2 norm of weights: 3.2308235264957172\n",
      "---------------------\n",
      "Iteration Number: 4399\n",
      "Loss: 15.139483303385084\n",
      "l2 norm of gradients: 0.1455431735461822\n",
      "l2 norm of weights: 3.230741121279286\n",
      "---------------------\n",
      "Iteration Number: 4400\n",
      "Loss: 15.136706777957322\n",
      "l2 norm of gradients: 0.14548569128230168\n",
      "l2 norm of weights: 3.230658850868113\n",
      "---------------------\n",
      "Iteration Number: 4401\n",
      "Loss: 15.133932226990487\n",
      "l2 norm of gradients: 0.14542823888781203\n",
      "l2 norm of weights: 3.2305767151671234\n",
      "---------------------\n",
      "Iteration Number: 4402\n",
      "Loss: 15.131159649208016\n",
      "l2 norm of gradients: 0.14537081632108798\n",
      "l2 norm of weights: 3.230494714081315\n",
      "---------------------\n",
      "Iteration Number: 4403\n",
      "Loss: 15.128389043334996\n",
      "l2 norm of gradients: 0.14531342354056062\n",
      "l2 norm of weights: 3.2304128475157556\n",
      "---------------------\n",
      "Iteration Number: 4404\n",
      "Loss: 15.125620408098246\n",
      "l2 norm of gradients: 0.1452560605047174\n",
      "l2 norm of weights: 3.2303311153755865\n",
      "---------------------\n",
      "Iteration Number: 4405\n",
      "Loss: 15.12285374222609\n",
      "l2 norm of gradients: 0.14519872717210236\n",
      "l2 norm of weights: 3.2302495175660177\n",
      "---------------------\n",
      "Iteration Number: 4406\n",
      "Loss: 15.120089044448603\n",
      "l2 norm of gradients: 0.14514142350131634\n",
      "l2 norm of weights: 3.2301680539923328\n",
      "---------------------\n",
      "Iteration Number: 4407\n",
      "Loss: 15.117326313497424\n",
      "l2 norm of gradients: 0.14508414945101714\n",
      "l2 norm of weights: 3.2300867245598854\n",
      "---------------------\n",
      "Iteration Number: 4408\n",
      "Loss: 15.114565548105878\n",
      "l2 norm of gradients: 0.1450269049799198\n",
      "l2 norm of weights: 3.2300055291741008\n",
      "---------------------\n",
      "Iteration Number: 4409\n",
      "Loss: 15.111806747008863\n",
      "l2 norm of gradients: 0.14496969004679658\n",
      "l2 norm of weights: 3.2299244677404744\n",
      "---------------------\n",
      "Iteration Number: 4410\n",
      "Loss: 15.109049908942962\n",
      "l2 norm of gradients: 0.14491250461047736\n",
      "l2 norm of weights: 3.2298435401645724\n",
      "---------------------\n",
      "Iteration Number: 4411\n",
      "Loss: 15.10629503264632\n",
      "l2 norm of gradients: 0.14485534862984986\n",
      "l2 norm of weights: 3.2297627463520326\n",
      "---------------------\n",
      "Iteration Number: 4412\n",
      "Loss: 15.1035421168587\n",
      "l2 norm of gradients: 0.14479822206385945\n",
      "l2 norm of weights: 3.2296820862085625\n",
      "---------------------\n",
      "Iteration Number: 4413\n",
      "Loss: 15.100791160321533\n",
      "l2 norm of gradients: 0.1447411248715099\n",
      "l2 norm of weights: 3.2296015596399408\n",
      "---------------------\n",
      "Iteration Number: 4414\n",
      "Loss: 15.098042161777808\n",
      "l2 norm of gradients: 0.14468405701186304\n",
      "l2 norm of weights: 3.229521166552015\n",
      "---------------------\n",
      "Iteration Number: 4415\n",
      "Loss: 15.095295119972135\n",
      "l2 norm of gradients: 0.14462701844403936\n",
      "l2 norm of weights: 3.2294409068507033\n",
      "---------------------\n",
      "Iteration Number: 4416\n",
      "Loss: 15.092550033650681\n",
      "l2 norm of gradients: 0.14457000912721787\n",
      "l2 norm of weights: 3.2293607804419953\n",
      "---------------------\n",
      "Iteration Number: 4417\n",
      "Loss: 15.089806901561332\n",
      "l2 norm of gradients: 0.14451302902063654\n",
      "l2 norm of weights: 3.2292807872319487\n",
      "---------------------\n",
      "Iteration Number: 4418\n",
      "Loss: 15.087065722453392\n",
      "l2 norm of gradients: 0.14445607808359243\n",
      "l2 norm of weights: 3.2292009271266924\n",
      "---------------------\n",
      "Iteration Number: 4419\n",
      "Loss: 15.084326495077883\n",
      "l2 norm of gradients: 0.1443991562754416\n",
      "l2 norm of weights: 3.229121200032423\n",
      "---------------------\n",
      "Iteration Number: 4420\n",
      "Loss: 15.0815892181874\n",
      "l2 norm of gradients: 0.14434226355559981\n",
      "l2 norm of weights: 3.22904160585541\n",
      "---------------------\n",
      "Iteration Number: 4421\n",
      "Loss: 15.078853890536026\n",
      "l2 norm of gradients: 0.14428539988354228\n",
      "l2 norm of weights: 3.2289621445019883\n",
      "---------------------\n",
      "Iteration Number: 4422\n",
      "Loss: 15.07612051087956\n",
      "l2 norm of gradients: 0.14422856521880403\n",
      "l2 norm of weights: 3.2288828158785647\n",
      "---------------------\n",
      "Iteration Number: 4423\n",
      "Loss: 15.073389077975255\n",
      "l2 norm of gradients: 0.14417175952098008\n",
      "l2 norm of weights: 3.228803619891615\n",
      "---------------------\n",
      "Iteration Number: 4424\n",
      "Loss: 15.07065959058201\n",
      "l2 norm of gradients: 0.1441149827497256\n",
      "l2 norm of weights: 3.2287245564476827\n",
      "---------------------\n",
      "Iteration Number: 4425\n",
      "Loss: 15.06793204746024\n",
      "l2 norm of gradients: 0.14405823486475614\n",
      "l2 norm of weights: 3.228645625453382\n",
      "---------------------\n",
      "Iteration Number: 4426\n",
      "Loss: 15.065206447371933\n",
      "l2 norm of gradients: 0.14400151582584775\n",
      "l2 norm of weights: 3.2285668268153946\n",
      "---------------------\n",
      "Iteration Number: 4427\n",
      "Loss: 15.062482789080642\n",
      "l2 norm of gradients: 0.14394482559283717\n",
      "l2 norm of weights: 3.2284881604404716\n",
      "---------------------\n",
      "Iteration Number: 4428\n",
      "Loss: 15.059761071351481\n",
      "l2 norm of gradients: 0.14388816412562216\n",
      "l2 norm of weights: 3.2284096262354325\n",
      "---------------------\n",
      "Iteration Number: 4429\n",
      "Loss: 15.057041292951121\n",
      "l2 norm of gradients: 0.14383153138416147\n",
      "l2 norm of weights: 3.228331224107165\n",
      "---------------------\n",
      "Iteration Number: 4430\n",
      "Loss: 15.054323452647761\n",
      "l2 norm of gradients: 0.14377492732847513\n",
      "l2 norm of weights: 3.2282529539626257\n",
      "---------------------\n",
      "Iteration Number: 4431\n",
      "Loss: 15.05160754921114\n",
      "l2 norm of gradients: 0.14371835191864465\n",
      "l2 norm of weights: 3.2281748157088392\n",
      "---------------------\n",
      "Iteration Number: 4432\n",
      "Loss: 15.048893581412534\n",
      "l2 norm of gradients: 0.14366180511481325\n",
      "l2 norm of weights: 3.2280968092528988\n",
      "---------------------\n",
      "Iteration Number: 4433\n",
      "Loss: 15.046181548024764\n",
      "l2 norm of gradients: 0.14360528687718588\n",
      "l2 norm of weights: 3.2280189345019634\n",
      "---------------------\n",
      "Iteration Number: 4434\n",
      "Loss: 15.04347144782219\n",
      "l2 norm of gradients: 0.14354879716602953\n",
      "l2 norm of weights: 3.227941191363264\n",
      "---------------------\n",
      "Iteration Number: 4435\n",
      "Loss: 15.040763279580657\n",
      "l2 norm of gradients: 0.14349233594167346\n",
      "l2 norm of weights: 3.227863579744095\n",
      "---------------------\n",
      "Iteration Number: 4436\n",
      "Loss: 15.038057042077602\n",
      "l2 norm of gradients: 0.14343590316450922\n",
      "l2 norm of weights: 3.227786099551822\n",
      "---------------------\n",
      "Iteration Number: 4437\n",
      "Loss: 15.035352734091909\n",
      "l2 norm of gradients: 0.1433794987949909\n",
      "l2 norm of weights: 3.227708750693876\n",
      "---------------------\n",
      "Iteration Number: 4438\n",
      "Loss: 15.032650354404039\n",
      "l2 norm of gradients: 0.14332312279363538\n",
      "l2 norm of weights: 3.2276315330777563\n",
      "---------------------\n",
      "Iteration Number: 4439\n",
      "Loss: 15.029949901795899\n",
      "l2 norm of gradients: 0.14326677512102257\n",
      "l2 norm of weights: 3.2275544466110295\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 4440\n",
      "Loss: 15.027251375050977\n",
      "l2 norm of gradients: 0.14321045573779528\n",
      "l2 norm of weights: 3.2274774912013293\n",
      "---------------------\n",
      "Iteration Number: 4441\n",
      "Loss: 15.024554772954168\n",
      "l2 norm of gradients: 0.14315416460465974\n",
      "l2 norm of weights: 3.227400666756356\n",
      "---------------------\n",
      "Iteration Number: 4442\n",
      "Loss: 15.02186009429198\n",
      "l2 norm of gradients: 0.14309790168238568\n",
      "l2 norm of weights: 3.227323973183878\n",
      "---------------------\n",
      "Iteration Number: 4443\n",
      "Loss: 15.019167337852329\n",
      "l2 norm of gradients: 0.14304166693180634\n",
      "l2 norm of weights: 3.2272474103917297\n",
      "---------------------\n",
      "Iteration Number: 4444\n",
      "Loss: 15.016476502424686\n",
      "l2 norm of gradients: 0.14298546031381898\n",
      "l2 norm of weights: 3.2271709782878135\n",
      "---------------------\n",
      "Iteration Number: 4445\n",
      "Loss: 15.013787586799921\n",
      "l2 norm of gradients: 0.14292928178938472\n",
      "l2 norm of weights: 3.2270946767800965\n",
      "---------------------\n",
      "Iteration Number: 4446\n",
      "Loss: 15.011100589770482\n",
      "l2 norm of gradients: 0.14287313131952892\n",
      "l2 norm of weights: 3.2270185057766136\n",
      "---------------------\n",
      "Iteration Number: 4447\n",
      "Loss: 15.008415510130254\n",
      "l2 norm of gradients: 0.14281700886534135\n",
      "l2 norm of weights: 3.2269424651854663\n",
      "---------------------\n",
      "Iteration Number: 4448\n",
      "Loss: 15.005732346674577\n",
      "l2 norm of gradients: 0.14276091438797628\n",
      "l2 norm of weights: 3.2268665549148223\n",
      "---------------------\n",
      "Iteration Number: 4449\n",
      "Loss: 15.003051098200288\n",
      "l2 norm of gradients: 0.14270484784865275\n",
      "l2 norm of weights: 3.226790774872915\n",
      "---------------------\n",
      "Iteration Number: 4450\n",
      "Loss: 15.00037176350572\n",
      "l2 norm of gradients: 0.14264880920865475\n",
      "l2 norm of weights: 3.2267151249680444\n",
      "---------------------\n",
      "Iteration Number: 4451\n",
      "Loss: 14.997694341390597\n",
      "l2 norm of gradients: 0.1425927984293312\n",
      "l2 norm of weights: 3.226639605108577\n",
      "---------------------\n",
      "Iteration Number: 4452\n",
      "Loss: 14.99501883065618\n",
      "l2 norm of gradients: 0.1425368154720965\n",
      "l2 norm of weights: 3.226564215202944\n",
      "---------------------\n",
      "Iteration Number: 4453\n",
      "Loss: 14.992345230105123\n",
      "l2 norm of gradients: 0.14248086029843035\n",
      "l2 norm of weights: 3.226488955159643\n",
      "---------------------\n",
      "Iteration Number: 4454\n",
      "Loss: 14.989673538541584\n",
      "l2 norm of gradients: 0.14242493286987817\n",
      "l2 norm of weights: 3.2264138248872385\n",
      "---------------------\n",
      "Iteration Number: 4455\n",
      "Loss: 14.987003754771138\n",
      "l2 norm of gradients: 0.14236903314805102\n",
      "l2 norm of weights: 3.226338824294358\n",
      "---------------------\n",
      "Iteration Number: 4456\n",
      "Loss: 14.984335877600774\n",
      "l2 norm of gradients: 0.14231316109462605\n",
      "l2 norm of weights: 3.226263953289697\n",
      "---------------------\n",
      "Iteration Number: 4457\n",
      "Loss: 14.981669905839\n",
      "l2 norm of gradients: 0.14225731667134664\n",
      "l2 norm of weights: 3.2261892117820143\n",
      "---------------------\n",
      "Iteration Number: 4458\n",
      "Loss: 14.97900583829568\n",
      "l2 norm of gradients: 0.14220149984002234\n",
      "l2 norm of weights: 3.226114599680136\n",
      "---------------------\n",
      "Iteration Number: 4459\n",
      "Loss: 14.976343673782164\n",
      "l2 norm of gradients: 0.14214571056252928\n",
      "l2 norm of weights: 3.2260401168929524\n",
      "---------------------\n",
      "Iteration Number: 4460\n",
      "Loss: 14.973683411111194\n",
      "l2 norm of gradients: 0.14208994880081027\n",
      "l2 norm of weights: 3.2259657633294183\n",
      "---------------------\n",
      "Iteration Number: 4461\n",
      "Loss: 14.971025049096992\n",
      "l2 norm of gradients: 0.142034214516875\n",
      "l2 norm of weights: 3.225891538898555\n",
      "---------------------\n",
      "Iteration Number: 4462\n",
      "Loss: 14.968368586555115\n",
      "l2 norm of gradients: 0.1419785076728001\n",
      "l2 norm of weights: 3.2258174435094458\n",
      "---------------------\n",
      "Iteration Number: 4463\n",
      "Loss: 14.965714022302635\n",
      "l2 norm of gradients: 0.14192282823072946\n",
      "l2 norm of weights: 3.2257434770712425\n",
      "---------------------\n",
      "Iteration Number: 4464\n",
      "Loss: 14.963061355157915\n",
      "l2 norm of gradients: 0.1418671761528743\n",
      "l2 norm of weights: 3.2256696394931597\n",
      "---------------------\n",
      "Iteration Number: 4465\n",
      "Loss: 14.960410583940863\n",
      "l2 norm of gradients: 0.1418115514015135\n",
      "l2 norm of weights: 3.2255959306844755\n",
      "---------------------\n",
      "Iteration Number: 4466\n",
      "Loss: 14.957761707472663\n",
      "l2 norm of gradients: 0.14175595393899354\n",
      "l2 norm of weights: 3.2255223505545345\n",
      "---------------------\n",
      "Iteration Number: 4467\n",
      "Loss: 14.955114724575985\n",
      "l2 norm of gradients: 0.14170038372772883\n",
      "l2 norm of weights: 3.225448899012744\n",
      "---------------------\n",
      "Iteration Number: 4468\n",
      "Loss: 14.952469634074868\n",
      "l2 norm of gradients: 0.14164484073020184\n",
      "l2 norm of weights: 3.2253755759685765\n",
      "---------------------\n",
      "Iteration Number: 4469\n",
      "Loss: 14.949826434794756\n",
      "l2 norm of gradients: 0.1415893249089633\n",
      "l2 norm of weights: 3.2253023813315687\n",
      "---------------------\n",
      "Iteration Number: 4470\n",
      "Loss: 14.947185125562406\n",
      "l2 norm of gradients: 0.1415338362266323\n",
      "l2 norm of weights: 3.2252293150113207\n",
      "---------------------\n",
      "Iteration Number: 4471\n",
      "Loss: 14.944545705206101\n",
      "l2 norm of gradients: 0.14147837464589652\n",
      "l2 norm of weights: 3.2251563769174965\n",
      "---------------------\n",
      "Iteration Number: 4472\n",
      "Loss: 14.941908172555367\n",
      "l2 norm of gradients: 0.14142294012951243\n",
      "l2 norm of weights: 3.2250835669598255\n",
      "---------------------\n",
      "Iteration Number: 4473\n",
      "Loss: 14.939272526441206\n",
      "l2 norm of gradients: 0.14136753264030538\n",
      "l2 norm of weights: 3.225010885048099\n",
      "---------------------\n",
      "Iteration Number: 4474\n",
      "Loss: 14.936638765695891\n",
      "l2 norm of gradients: 0.14131215214116977\n",
      "l2 norm of weights: 3.224938331092173\n",
      "---------------------\n",
      "Iteration Number: 4475\n",
      "Loss: 14.934006889153151\n",
      "l2 norm of gradients: 0.14125679859506932\n",
      "l2 norm of weights: 3.2248659050019666\n",
      "---------------------\n",
      "Iteration Number: 4476\n",
      "Loss: 14.931376895648057\n",
      "l2 norm of gradients: 0.14120147196503707\n",
      "l2 norm of weights: 3.2247936066874616\n",
      "---------------------\n",
      "Iteration Number: 4477\n",
      "Loss: 14.928748784017035\n",
      "l2 norm of gradients: 0.14114617221417575\n",
      "l2 norm of weights: 3.2247214360587053\n",
      "---------------------\n",
      "Iteration Number: 4478\n",
      "Loss: 14.92612255309784\n",
      "l2 norm of gradients: 0.14109089930565782\n",
      "l2 norm of weights: 3.224649393025807\n",
      "---------------------\n",
      "Iteration Number: 4479\n",
      "Loss: 14.923498201729604\n",
      "l2 norm of gradients: 0.14103565320272562\n",
      "l2 norm of weights: 3.224577477498938\n",
      "---------------------\n",
      "Iteration Number: 4480\n",
      "Loss: 14.920875728752803\n",
      "l2 norm of gradients: 0.14098043386869158\n",
      "l2 norm of weights: 3.224505689388335\n",
      "---------------------\n",
      "Iteration Number: 4481\n",
      "Loss: 14.918255133009266\n",
      "l2 norm of gradients: 0.14092524126693842\n",
      "l2 norm of weights: 3.224434028604296\n",
      "---------------------\n",
      "Iteration Number: 4482\n",
      "Loss: 14.915636413342142\n",
      "l2 norm of gradients: 0.14087007536091925\n",
      "l2 norm of weights: 3.2243624950571816\n",
      "---------------------\n",
      "Iteration Number: 4483\n",
      "Loss: 14.91301956859593\n",
      "l2 norm of gradients: 0.14081493611415774\n",
      "l2 norm of weights: 3.224291088657418\n",
      "---------------------\n",
      "Iteration Number: 4484\n",
      "Loss: 14.910404597616491\n",
      "l2 norm of gradients: 0.14075982349024835\n",
      "l2 norm of weights: 3.2242198093154903\n",
      "---------------------\n",
      "Iteration Number: 4485\n",
      "Loss: 14.907791499250926\n",
      "l2 norm of gradients: 0.14070473745285644\n",
      "l2 norm of weights: 3.2241486569419484\n",
      "---------------------\n",
      "Iteration Number: 4486\n",
      "Loss: 14.90518027234774\n",
      "l2 norm of gradients: 0.1406496779657184\n",
      "l2 norm of weights: 3.224077631447405\n",
      "---------------------\n",
      "Iteration Number: 4487\n",
      "Loss: 14.902570915756751\n",
      "l2 norm of gradients: 0.14059464499264185\n",
      "l2 norm of weights: 3.224006732742533\n",
      "---------------------\n",
      "Iteration Number: 4488\n",
      "Loss: 14.899963428329018\n",
      "l2 norm of gradients: 0.14053963849750586\n",
      "l2 norm of weights: 3.2239359607380704\n",
      "---------------------\n",
      "Iteration Number: 4489\n",
      "Loss: 14.897357808917\n",
      "l2 norm of gradients: 0.14048465844426106\n",
      "l2 norm of weights: 3.223865315344815\n",
      "---------------------\n",
      "Iteration Number: 4490\n",
      "Loss: 14.894754056374465\n",
      "l2 norm of gradients: 0.14042970479692968\n",
      "l2 norm of weights: 3.2237947964736287\n",
      "---------------------\n",
      "Iteration Number: 4491\n",
      "Loss: 14.89215216955637\n",
      "l2 norm of gradients: 0.14037477751960595\n",
      "l2 norm of weights: 3.223724404035434\n",
      "---------------------\n",
      "Iteration Number: 4492\n",
      "Loss: 14.889552147319115\n",
      "l2 norm of gradients: 0.14031987657645606\n",
      "l2 norm of weights: 3.223654137941216\n",
      "---------------------\n",
      "Iteration Number: 4493\n",
      "Loss: 14.886953988520307\n",
      "l2 norm of gradients: 0.14026500193171845\n",
      "l2 norm of weights: 3.2235839981020216\n",
      "---------------------\n",
      "Iteration Number: 4494\n",
      "Loss: 14.884357692018868\n",
      "l2 norm of gradients: 0.14021015354970384\n",
      "l2 norm of weights: 3.2235139844289593\n",
      "---------------------\n",
      "Iteration Number: 4495\n",
      "Loss: 14.881763256675022\n",
      "l2 norm of gradients: 0.14015533139479552\n",
      "l2 norm of weights: 3.2234440968331994\n",
      "---------------------\n",
      "Iteration Number: 4496\n",
      "Loss: 14.879170681350233\n",
      "l2 norm of gradients: 0.1401005354314494\n",
      "l2 norm of weights: 3.223374335225973\n",
      "---------------------\n",
      "Iteration Number: 4497\n",
      "Loss: 14.876579964907295\n",
      "l2 norm of gradients: 0.14004576562419435\n",
      "l2 norm of weights: 3.2233046995185743\n",
      "---------------------\n",
      "Iteration Number: 4498\n",
      "Loss: 14.873991106210255\n",
      "l2 norm of gradients: 0.13999102193763197\n",
      "l2 norm of weights: 3.2232351896223577\n",
      "---------------------\n",
      "Iteration Number: 4499\n",
      "Loss: 14.871404104124414\n",
      "l2 norm of gradients: 0.13993630433643717\n",
      "l2 norm of weights: 3.223165805448739\n",
      "---------------------\n",
      "Iteration Number: 4500\n",
      "Loss: 14.86881895751636\n",
      "l2 norm of gradients: 0.1398816127853581\n",
      "l2 norm of weights: 3.2230965469091952\n",
      "---------------------\n",
      "Iteration Number: 4501\n",
      "Loss: 14.866235665253981\n",
      "l2 norm of gradients: 0.13982694724921638\n",
      "l2 norm of weights: 3.2230274139152653\n",
      "---------------------\n",
      "Iteration Number: 4502\n",
      "Loss: 14.863654226206323\n",
      "l2 norm of gradients: 0.13977230769290716\n",
      "l2 norm of weights: 3.222958406378548\n",
      "---------------------\n",
      "Iteration Number: 4503\n",
      "Loss: 14.861074639243785\n",
      "l2 norm of gradients: 0.13971769408139945\n",
      "l2 norm of weights: 3.2228895242107036\n",
      "---------------------\n",
      "Iteration Number: 4504\n",
      "Loss: 14.858496903237976\n",
      "l2 norm of gradients: 0.139663106379736\n",
      "l2 norm of weights: 3.2228207673234537\n",
      "---------------------\n",
      "Iteration Number: 4505\n",
      "Loss: 14.855921017061751\n",
      "l2 norm of gradients: 0.13960854455303376\n",
      "l2 norm of weights: 3.2227521356285798\n",
      "---------------------\n",
      "Iteration Number: 4506\n",
      "Loss: 14.853346979589206\n",
      "l2 norm of gradients: 0.13955400856648384\n",
      "l2 norm of weights: 3.2226836290379244\n",
      "---------------------\n",
      "Iteration Number: 4507\n",
      "Loss: 14.850774789695677\n",
      "l2 norm of gradients: 0.13949949838535164\n",
      "l2 norm of weights: 3.222615247463391\n",
      "---------------------\n",
      "Iteration Number: 4508\n",
      "Loss: 14.848204446257757\n",
      "l2 norm of gradients: 0.13944501397497713\n",
      "l2 norm of weights: 3.222546990816943\n",
      "---------------------\n",
      "Iteration Number: 4509\n",
      "Loss: 14.845635948153243\n",
      "l2 norm of gradients: 0.1393905553007749\n",
      "l2 norm of weights: 3.222478859010605\n",
      "---------------------\n",
      "Iteration Number: 4510\n",
      "Loss: 14.843069294261168\n",
      "l2 norm of gradients: 0.13933612232823436\n",
      "l2 norm of weights: 3.2224108519564614\n",
      "---------------------\n",
      "Iteration Number: 4511\n",
      "Loss: 14.840504483461814\n",
      "l2 norm of gradients: 0.13928171502291986\n",
      "l2 norm of weights: 3.2223429695666557\n",
      "---------------------\n",
      "Iteration Number: 4512\n",
      "Loss: 14.837941514636604\n",
      "l2 norm of gradients: 0.1392273333504708\n",
      "l2 norm of weights: 3.2222752117533946\n",
      "---------------------\n",
      "Iteration Number: 4513\n",
      "Loss: 14.835380386668259\n",
      "l2 norm of gradients: 0.1391729772766019\n",
      "l2 norm of weights: 3.2222075784289417\n",
      "---------------------\n",
      "Iteration Number: 4514\n",
      "Loss: 14.832821098440657\n",
      "l2 norm of gradients: 0.13911864676710314\n",
      "l2 norm of weights: 3.222140069505622\n",
      "---------------------\n",
      "Iteration Number: 4515\n",
      "Loss: 14.830263648838914\n",
      "l2 norm of gradients: 0.13906434178784022\n",
      "l2 norm of weights: 3.222072684895821\n",
      "---------------------\n",
      "Iteration Number: 4516\n",
      "Loss: 14.827708036749351\n",
      "l2 norm of gradients: 0.13901006230475438\n",
      "l2 norm of weights: 3.222005424511984\n",
      "---------------------\n",
      "Iteration Number: 4517\n",
      "Loss: 14.825154261059446\n",
      "l2 norm of gradients: 0.1389558082838626\n",
      "l2 norm of weights: 3.221938288266614\n",
      "---------------------\n",
      "Iteration Number: 4518\n",
      "Loss: 14.822602320657925\n",
      "l2 norm of gradients: 0.1389015796912581\n",
      "l2 norm of weights: 3.221871276072275\n",
      "---------------------\n",
      "Iteration Number: 4519\n",
      "Loss: 14.82005221443467\n",
      "l2 norm of gradients: 0.13884737649310988\n",
      "l2 norm of weights: 3.221804387841592\n",
      "---------------------\n",
      "Iteration Number: 4520\n",
      "Loss: 14.81750394128072\n",
      "l2 norm of gradients: 0.1387931986556634\n",
      "l2 norm of weights: 3.2217376234872472\n",
      "---------------------\n",
      "Iteration Number: 4521\n",
      "Loss: 14.814957500088386\n",
      "l2 norm of gradients: 0.13873904614524044\n",
      "l2 norm of weights: 3.2216709829219834\n",
      "---------------------\n",
      "Iteration Number: 4522\n",
      "Loss: 14.812412889751055\n",
      "l2 norm of gradients: 0.13868491892823925\n",
      "l2 norm of weights: 3.2216044660586025\n",
      "---------------------\n",
      "Iteration Number: 4523\n",
      "Loss: 14.809870109163375\n",
      "l2 norm of gradients: 0.1386308169711348\n",
      "l2 norm of weights: 3.2215380728099654\n",
      "---------------------\n",
      "Iteration Number: 4524\n",
      "Loss: 14.807329157221101\n",
      "l2 norm of gradients: 0.13857674024047892\n",
      "l2 norm of weights: 3.221471803088993\n",
      "---------------------\n",
      "Iteration Number: 4525\n",
      "Loss: 14.804790032821202\n",
      "l2 norm of gradients: 0.13852268870290027\n",
      "l2 norm of weights: 3.2214056568086638\n",
      "---------------------\n",
      "Iteration Number: 4526\n",
      "Loss: 14.802252734861746\n",
      "l2 norm of gradients: 0.13846866232510466\n",
      "l2 norm of weights: 3.221339633882017\n",
      "---------------------\n",
      "Iteration Number: 4527\n",
      "Loss: 14.799717262242037\n",
      "l2 norm of gradients: 0.13841466107387507\n",
      "l2 norm of weights: 3.2212737342221494\n",
      "---------------------\n",
      "Iteration Number: 4528\n",
      "Loss: 14.797183613862465\n",
      "l2 norm of gradients: 0.1383606849160718\n",
      "l2 norm of weights: 3.2212079577422177\n",
      "---------------------\n",
      "Iteration Number: 4529\n",
      "Loss: 14.794651788624591\n",
      "l2 norm of gradients: 0.13830673381863273\n",
      "l2 norm of weights: 3.221142304355436\n",
      "---------------------\n",
      "Iteration Number: 4530\n",
      "Loss: 14.792121785431162\n",
      "l2 norm of gradients: 0.1382528077485733\n",
      "l2 norm of weights: 3.2210767739750787\n",
      "---------------------\n",
      "Iteration Number: 4531\n",
      "Loss: 14.789593603185999\n",
      "l2 norm of gradients: 0.1381989066729867\n",
      "l2 norm of weights: 3.2210113665144777\n",
      "---------------------\n",
      "Iteration Number: 4532\n",
      "Loss: 14.787067240794116\n",
      "l2 norm of gradients: 0.13814503055904398\n",
      "l2 norm of weights: 3.220946081887024\n",
      "---------------------\n",
      "Iteration Number: 4533\n",
      "Loss: 14.7845426971616\n",
      "l2 norm of gradients: 0.1380911793739943\n",
      "l2 norm of weights: 3.220880920006166\n",
      "---------------------\n",
      "Iteration Number: 4534\n",
      "Loss: 14.782019971195767\n",
      "l2 norm of gradients: 0.13803735308516485\n",
      "l2 norm of weights: 3.220815880785412\n",
      "---------------------\n",
      "Iteration Number: 4535\n",
      "Loss: 14.779499061804938\n",
      "l2 norm of gradients: 0.13798355165996115\n",
      "l2 norm of weights: 3.220750964138328\n",
      "---------------------\n",
      "Iteration Number: 4536\n",
      "Loss: 14.776979967898656\n",
      "l2 norm of gradients: 0.13792977506586715\n",
      "l2 norm of weights: 3.220686169978537\n",
      "---------------------\n",
      "Iteration Number: 4537\n",
      "Loss: 14.774462688387494\n",
      "l2 norm of gradients: 0.1378760232704453\n",
      "l2 norm of weights: 3.2206214982197228\n",
      "---------------------\n",
      "Iteration Number: 4538\n",
      "Loss: 14.771947222183233\n",
      "l2 norm of gradients: 0.13782229624133668\n",
      "l2 norm of weights: 3.2205569487756245\n",
      "---------------------\n",
      "Iteration Number: 4539\n",
      "Loss: 14.769433568198659\n",
      "l2 norm of gradients: 0.13776859394626126\n",
      "l2 norm of weights: 3.2204925215600406\n",
      "---------------------\n",
      "Iteration Number: 4540\n",
      "Loss: 14.766921725347787\n",
      "l2 norm of gradients: 0.13771491635301786\n",
      "l2 norm of weights: 3.2204282164868268\n",
      "---------------------\n",
      "Iteration Number: 4541\n",
      "Loss: 14.764411692545588\n",
      "l2 norm of gradients: 0.13766126342948426\n",
      "l2 norm of weights: 3.220364033469898\n",
      "---------------------\n",
      "Iteration Number: 4542\n",
      "Loss: 14.761903468708265\n",
      "l2 norm of gradients: 0.1376076351436176\n",
      "l2 norm of weights: 3.2202999724232253\n",
      "---------------------\n",
      "Iteration Number: 4543\n",
      "Loss: 14.75939705275303\n",
      "l2 norm of gradients: 0.13755403146345413\n",
      "l2 norm of weights: 3.2202360332608384\n",
      "---------------------\n",
      "Iteration Number: 4544\n",
      "Loss: 14.756892443598188\n",
      "l2 norm of gradients: 0.13750045235710961\n",
      "l2 norm of weights: 3.2201722158968247\n",
      "---------------------\n",
      "Iteration Number: 4545\n",
      "Loss: 14.75438964016319\n",
      "l2 norm of gradients: 0.13744689779277933\n",
      "l2 norm of weights: 3.2201085202453275\n",
      "---------------------\n",
      "Iteration Number: 4546\n",
      "Loss: 14.751888641368504\n",
      "l2 norm of gradients: 0.13739336773873817\n",
      "l2 norm of weights: 3.22004494622055\n",
      "---------------------\n",
      "Iteration Number: 4547\n",
      "Loss: 14.749389446135694\n",
      "l2 norm of gradients: 0.1373398621633409\n",
      "l2 norm of weights: 3.219981493736751\n",
      "---------------------\n",
      "Iteration Number: 4548\n",
      "Loss: 14.746892053387407\n",
      "l2 norm of gradients: 0.13728638103502203\n",
      "l2 norm of weights: 3.219918162708247\n",
      "---------------------\n",
      "Iteration Number: 4549\n",
      "Loss: 14.74439646204736\n",
      "l2 norm of gradients: 0.13723292432229622\n",
      "l2 norm of weights: 3.2198549530494125\n",
      "---------------------\n",
      "Iteration Number: 4550\n",
      "Loss: 14.741902671040325\n",
      "l2 norm of gradients: 0.1371794919937582\n",
      "l2 norm of weights: 3.219791864674679\n",
      "---------------------\n",
      "Iteration Number: 4551\n",
      "Loss: 14.73941067929211\n",
      "l2 norm of gradients: 0.13712608401808302\n",
      "l2 norm of weights: 3.2197288974985327\n",
      "---------------------\n",
      "Iteration Number: 4552\n",
      "Loss: 14.736920485729668\n",
      "l2 norm of gradients: 0.137072700364026\n",
      "l2 norm of weights: 3.2196660514355213\n",
      "---------------------\n",
      "Iteration Number: 4553\n",
      "Loss: 14.734432089280865\n",
      "l2 norm of gradients: 0.137019341000423\n",
      "l2 norm of weights: 3.2196033264002453\n",
      "---------------------\n",
      "Iteration Number: 4554\n",
      "Loss: 14.731945488874759\n",
      "l2 norm of gradients: 0.13696600589619043\n",
      "l2 norm of weights: 3.2195407223073644\n",
      "---------------------\n",
      "Iteration Number: 4555\n",
      "Loss: 14.729460683441351\n",
      "l2 norm of gradients: 0.13691269502032552\n",
      "l2 norm of weights: 3.2194782390715937\n",
      "---------------------\n",
      "Iteration Number: 4556\n",
      "Loss: 14.726977671911747\n",
      "l2 norm of gradients: 0.13685940834190619\n",
      "l2 norm of weights: 3.219415876607706\n",
      "---------------------\n",
      "Iteration Number: 4557\n",
      "Loss: 14.724496453218038\n",
      "l2 norm of gradients: 0.1368061458300914\n",
      "l2 norm of weights: 3.219353634830531\n",
      "---------------------\n",
      "Iteration Number: 4558\n",
      "Loss: 14.722017026293388\n",
      "l2 norm of gradients: 0.1367529074541211\n",
      "l2 norm of weights: 3.219291513654954\n",
      "---------------------\n",
      "Iteration Number: 4559\n",
      "Loss: 14.719539390071965\n",
      "l2 norm of gradients: 0.13669969318331643\n",
      "l2 norm of weights: 3.2192295129959168\n",
      "---------------------\n",
      "Iteration Number: 4560\n",
      "Loss: 14.717063543488988\n",
      "l2 norm of gradients: 0.13664650298707975\n",
      "l2 norm of weights: 3.2191676327684187\n",
      "---------------------\n",
      "Iteration Number: 4561\n",
      "Loss: 14.71458948548064\n",
      "l2 norm of gradients: 0.1365933368348949\n",
      "l2 norm of weights: 3.219105872887515\n",
      "---------------------\n",
      "Iteration Number: 4562\n",
      "Loss: 14.712117214984215\n",
      "l2 norm of gradients: 0.13654019469632706\n",
      "l2 norm of weights: 3.219044233268317\n",
      "---------------------\n",
      "Iteration Number: 4563\n",
      "Loss: 14.709646730937912\n",
      "l2 norm of gradients: 0.1364870765410231\n",
      "l2 norm of weights: 3.218982713825991\n",
      "---------------------\n",
      "Iteration Number: 4564\n",
      "Loss: 14.707178032281035\n",
      "l2 norm of gradients: 0.13643398233871157\n",
      "l2 norm of weights: 3.218921314475763\n",
      "---------------------\n",
      "Iteration Number: 4565\n",
      "Loss: 14.704711117953806\n",
      "l2 norm of gradients: 0.1363809120592028\n",
      "l2 norm of weights: 3.2188600351329115\n",
      "---------------------\n",
      "Iteration Number: 4566\n",
      "Loss: 14.702245986897514\n",
      "l2 norm of gradients: 0.13632786567238905\n",
      "l2 norm of weights: 3.218798875712772\n",
      "---------------------\n",
      "Iteration Number: 4567\n",
      "Loss: 14.69978263805444\n",
      "l2 norm of gradients: 0.1362748431482446\n",
      "l2 norm of weights: 3.2187378361307375\n",
      "---------------------\n",
      "Iteration Number: 4568\n",
      "Loss: 14.697321070367813\n",
      "l2 norm of gradients: 0.13622184445682575\n",
      "l2 norm of weights: 3.218676916302255\n",
      "---------------------\n",
      "Iteration Number: 4569\n",
      "Loss: 14.694861282781895\n",
      "l2 norm of gradients: 0.1361688695682712\n",
      "l2 norm of weights: 3.218616116142828\n",
      "---------------------\n",
      "Iteration Number: 4570\n",
      "Loss: 14.692403274241887\n",
      "l2 norm of gradients: 0.13611591845280177\n",
      "l2 norm of weights: 3.218555435568016\n",
      "---------------------\n",
      "Iteration Number: 4571\n",
      "Loss: 14.689947043694024\n",
      "l2 norm of gradients: 0.1360629910807208\n",
      "l2 norm of weights: 3.2184948744934343\n",
      "---------------------\n",
      "Iteration Number: 4572\n",
      "Loss: 14.68749259008549\n",
      "l2 norm of gradients: 0.13601008742241416\n",
      "l2 norm of weights: 3.218434432834753\n",
      "---------------------\n",
      "Iteration Number: 4573\n",
      "Loss: 14.685039912364422\n",
      "l2 norm of gradients: 0.13595720744835024\n",
      "l2 norm of weights: 3.218374110507699\n",
      "---------------------\n",
      "Iteration Number: 4574\n",
      "Loss: 14.682589009479956\n",
      "l2 norm of gradients: 0.13590435112908014\n",
      "l2 norm of weights: 3.2183139074280525\n",
      "---------------------\n",
      "Iteration Number: 4575\n",
      "Loss: 14.680139880382226\n",
      "l2 norm of gradients: 0.1358515184352379\n",
      "l2 norm of weights: 3.218253823511652\n",
      "---------------------\n",
      "Iteration Number: 4576\n",
      "Loss: 14.677692524022245\n",
      "l2 norm of gradients: 0.13579870933754032\n",
      "l2 norm of weights: 3.2181938586743892\n",
      "---------------------\n",
      "Iteration Number: 4577\n",
      "Loss: 14.675246939352014\n",
      "l2 norm of gradients: 0.1357459238067873\n",
      "l2 norm of weights: 3.218134012832212\n",
      "---------------------\n",
      "Iteration Number: 4578\n",
      "Loss: 14.672803125324545\n",
      "l2 norm of gradients: 0.1356931618138617\n",
      "l2 norm of weights: 3.218074285901123\n",
      "---------------------\n",
      "Iteration Number: 4579\n",
      "Loss: 14.670361080893702\n",
      "l2 norm of gradients: 0.13564042332972967\n",
      "l2 norm of weights: 3.2180146777971803\n",
      "---------------------\n",
      "Iteration Number: 4580\n",
      "Loss: 14.667920805014354\n",
      "l2 norm of gradients: 0.1355877083254406\n",
      "l2 norm of weights: 3.217955188436498\n",
      "---------------------\n",
      "Iteration Number: 4581\n",
      "Loss: 14.665482296642326\n",
      "l2 norm of gradients: 0.13553501677212715\n",
      "l2 norm of weights: 3.2178958177352426\n",
      "---------------------\n",
      "Iteration Number: 4582\n",
      "Loss: 14.663045554734342\n",
      "l2 norm of gradients: 0.13548234864100558\n",
      "l2 norm of weights: 3.217836565609639\n",
      "---------------------\n",
      "Iteration Number: 4583\n",
      "Loss: 14.660610578248052\n",
      "l2 norm of gradients: 0.1354297039033756\n",
      "l2 norm of weights: 3.2177774319759638\n",
      "---------------------\n",
      "Iteration Number: 4584\n",
      "Loss: 14.658177366142086\n",
      "l2 norm of gradients: 0.13537708253062056\n",
      "l2 norm of weights: 3.21771841675055\n",
      "---------------------\n",
      "Iteration Number: 4585\n",
      "Loss: 14.655745917375945\n",
      "l2 norm of gradients: 0.1353244844942075\n",
      "l2 norm of weights: 3.2176595198497866\n",
      "---------------------\n",
      "Iteration Number: 4586\n",
      "Loss: 14.653316230910104\n",
      "l2 norm of gradients: 0.13527190976568737\n",
      "l2 norm of weights: 3.2176007411901146\n",
      "---------------------\n",
      "Iteration Number: 4587\n",
      "Loss: 14.6508883057059\n",
      "l2 norm of gradients: 0.13521935831669482\n",
      "l2 norm of weights: 3.2175420806880313\n",
      "---------------------\n",
      "Iteration Number: 4588\n",
      "Loss: 14.648462140725625\n",
      "l2 norm of gradients: 0.13516683011894862\n",
      "l2 norm of weights: 3.217483538260088\n",
      "---------------------\n",
      "Iteration Number: 4589\n",
      "Loss: 14.646037734932438\n",
      "l2 norm of gradients: 0.13511432514425145\n",
      "l2 norm of weights: 3.2174251138228915\n",
      "---------------------\n",
      "Iteration Number: 4590\n",
      "Loss: 14.643615087290476\n",
      "l2 norm of gradients: 0.1350618433644903\n",
      "l2 norm of weights: 3.2173668072931014\n",
      "---------------------\n",
      "Iteration Number: 4591\n",
      "Loss: 14.64119419676474\n",
      "l2 norm of gradients: 0.13500938475163624\n",
      "l2 norm of weights: 3.217308618587433\n",
      "---------------------\n",
      "Iteration Number: 4592\n",
      "Loss: 14.638775062321075\n",
      "l2 norm of gradients: 0.13495694927774468\n",
      "l2 norm of weights: 3.2172505476226556\n",
      "---------------------\n",
      "Iteration Number: 4593\n",
      "Loss: 14.636357682926281\n",
      "l2 norm of gradients: 0.13490453691495538\n",
      "l2 norm of weights: 3.217192594315592\n",
      "---------------------\n",
      "Iteration Number: 4594\n",
      "Loss: 14.633942057548056\n",
      "l2 norm of gradients: 0.13485214763549253\n",
      "l2 norm of weights: 3.217134758583121\n",
      "---------------------\n",
      "Iteration Number: 4595\n",
      "Loss: 14.63152818515496\n",
      "l2 norm of gradients: 0.1347997814116649\n",
      "l2 norm of weights: 3.2170770403421733\n",
      "---------------------\n",
      "Iteration Number: 4596\n",
      "Loss: 14.629116064716445\n",
      "l2 norm of gradients: 0.13474743821586585\n",
      "l2 norm of weights: 3.2170194395097362\n",
      "---------------------\n",
      "Iteration Number: 4597\n",
      "Loss: 14.626705695202821\n",
      "l2 norm of gradients: 0.1346951180205734\n",
      "l2 norm of weights: 3.2169619560028484\n",
      "---------------------\n",
      "Iteration Number: 4598\n",
      "Loss: 14.624297075585282\n",
      "l2 norm of gradients: 0.13464282079835033\n",
      "l2 norm of weights: 3.216904589738604\n",
      "---------------------\n",
      "Iteration Number: 4599\n",
      "Loss: 14.621890204835928\n",
      "l2 norm of gradients: 0.13459054652184418\n",
      "l2 norm of weights: 3.2168473406341516\n",
      "---------------------\n",
      "Iteration Number: 4600\n",
      "Loss: 14.619485081927655\n",
      "l2 norm of gradients: 0.1345382951637875\n",
      "l2 norm of weights: 3.2167902086066915\n",
      "---------------------\n",
      "Iteration Number: 4601\n",
      "Loss: 14.617081705834297\n",
      "l2 norm of gradients: 0.13448606669699778\n",
      "l2 norm of weights: 3.21673319357348\n",
      "---------------------\n",
      "Iteration Number: 4602\n",
      "Loss: 14.614680075530531\n",
      "l2 norm of gradients: 0.13443386109437747\n",
      "l2 norm of weights: 3.2166762954518258\n",
      "---------------------\n",
      "Iteration Number: 4603\n",
      "Loss: 14.612280189991822\n",
      "l2 norm of gradients: 0.13438167832891412\n",
      "l2 norm of weights: 3.2166195141590914\n",
      "---------------------\n",
      "Iteration Number: 4604\n",
      "Loss: 14.609882048194567\n",
      "l2 norm of gradients: 0.13432951837368065\n",
      "l2 norm of weights: 3.216562849612694\n",
      "---------------------\n",
      "Iteration Number: 4605\n",
      "Loss: 14.607485649115967\n",
      "l2 norm of gradients: 0.134277381201835\n",
      "l2 norm of weights: 3.2165063017301025\n",
      "---------------------\n",
      "Iteration Number: 4606\n",
      "Loss: 14.605090991734082\n",
      "l2 norm of gradients: 0.1342252667866205\n",
      "l2 norm of weights: 3.2164498704288413\n",
      "---------------------\n",
      "Iteration Number: 4607\n",
      "Loss: 14.60269807502783\n",
      "l2 norm of gradients: 0.13417317510136584\n",
      "l2 norm of weights: 3.216393555626486\n",
      "---------------------\n",
      "Iteration Number: 4608\n",
      "Loss: 14.600306897976935\n",
      "l2 norm of gradients: 0.13412110611948522\n",
      "l2 norm of weights: 3.2163373572406684\n",
      "---------------------\n",
      "Iteration Number: 4609\n",
      "Loss: 14.597917459561945\n",
      "l2 norm of gradients: 0.1340690598144782\n",
      "l2 norm of weights: 3.21628127518907\n",
      "---------------------\n",
      "Iteration Number: 4610\n",
      "Loss: 14.595529758764277\n",
      "l2 norm of gradients: 0.13401703615993\n",
      "l2 norm of weights: 3.2162253093894293\n",
      "---------------------\n",
      "Iteration Number: 4611\n",
      "Loss: 14.593143794566137\n",
      "l2 norm of gradients: 0.1339650351295115\n",
      "l2 norm of weights: 3.2161694597595347\n",
      "---------------------\n",
      "Iteration Number: 4612\n",
      "Loss: 14.590759565950592\n",
      "l2 norm of gradients: 0.13391305669697912\n",
      "l2 norm of weights: 3.21611372621723\n",
      "---------------------\n",
      "Iteration Number: 4613\n",
      "Loss: 14.588377071901446\n",
      "l2 norm of gradients: 0.13386110083617514\n",
      "l2 norm of weights: 3.216058108680411\n",
      "---------------------\n",
      "Iteration Number: 4614\n",
      "Loss: 14.585996311403424\n",
      "l2 norm of gradients: 0.13380916752102767\n",
      "l2 norm of weights: 3.216002607067028\n",
      "---------------------\n",
      "Iteration Number: 4615\n",
      "Loss: 14.583617283441994\n",
      "l2 norm of gradients: 0.1337572567255505\n",
      "l2 norm of weights: 3.215947221295081\n",
      "---------------------\n",
      "Iteration Number: 4616\n",
      "Loss: 14.581239987003414\n",
      "l2 norm of gradients: 0.13370536842384356\n",
      "l2 norm of weights: 3.2158919512826256\n",
      "---------------------\n",
      "Iteration Number: 4617\n",
      "Loss: 14.578864421074769\n",
      "l2 norm of gradients: 0.13365350259009254\n",
      "l2 norm of weights: 3.2158367969477712\n",
      "---------------------\n",
      "Iteration Number: 4618\n",
      "Loss: 14.576490584644002\n",
      "l2 norm of gradients: 0.1336016591985693\n",
      "l2 norm of weights: 3.215781758208676\n",
      "---------------------\n",
      "Iteration Number: 4619\n",
      "Loss: 14.574118476699727\n",
      "l2 norm of gradients: 0.13354983822363165\n",
      "l2 norm of weights: 3.2157268349835553\n",
      "---------------------\n",
      "Iteration Number: 4620\n",
      "Loss: 14.571748096231465\n",
      "l2 norm of gradients: 0.13349803963972373\n",
      "l2 norm of weights: 3.2156720271906734\n",
      "---------------------\n",
      "Iteration Number: 4621\n",
      "Loss: 14.569379442229419\n",
      "l2 norm of gradients: 0.1334462634213756\n",
      "l2 norm of weights: 3.2156173347483508\n",
      "---------------------\n",
      "Iteration Number: 4622\n",
      "Loss: 14.567012513684679\n",
      "l2 norm of gradients: 0.13339450954320373\n",
      "l2 norm of weights: 3.2155627575749564\n",
      "---------------------\n",
      "Iteration Number: 4623\n",
      "Loss: 14.564647309589033\n",
      "l2 norm of gradients: 0.1333427779799108\n",
      "l2 norm of weights: 3.215508295588916\n",
      "---------------------\n",
      "Iteration Number: 4624\n",
      "Loss: 14.562283828935069\n",
      "l2 norm of gradients: 0.13329106870628576\n",
      "l2 norm of weights: 3.215453948708704\n",
      "---------------------\n",
      "Iteration Number: 4625\n",
      "Loss: 14.559922070716187\n",
      "l2 norm of gradients: 0.13323938169720412\n",
      "l2 norm of weights: 3.2153997168528505\n",
      "---------------------\n",
      "Iteration Number: 4626\n",
      "Loss: 14.557562033926487\n",
      "l2 norm of gradients: 0.13318771692762754\n",
      "l2 norm of weights: 3.2153455999399356\n",
      "---------------------\n",
      "Iteration Number: 4627\n",
      "Loss: 14.555203717560856\n",
      "l2 norm of gradients: 0.13313607437260433\n",
      "l2 norm of weights: 3.215291597888592\n",
      "---------------------\n",
      "Iteration Number: 4628\n",
      "Loss: 14.552847120615013\n",
      "l2 norm of gradients: 0.1330844540072692\n",
      "l2 norm of weights: 3.2152377106175063\n",
      "---------------------\n",
      "Iteration Number: 4629\n",
      "Loss: 14.550492242085312\n",
      "l2 norm of gradients: 0.13303285580684357\n",
      "l2 norm of weights: 3.215183938045415\n",
      "---------------------\n",
      "Iteration Number: 4630\n",
      "Loss: 14.548139080968934\n",
      "l2 norm of gradients: 0.13298127974663526\n",
      "l2 norm of weights: 3.215130280091108\n",
      "---------------------\n",
      "Iteration Number: 4631\n",
      "Loss: 14.545787636263796\n",
      "l2 norm of gradients: 0.1329297258020388\n",
      "l2 norm of weights: 3.2150767366734274\n",
      "---------------------\n",
      "Iteration Number: 4632\n",
      "Loss: 14.543437906968599\n",
      "l2 norm of gradients: 0.13287819394853534\n",
      "l2 norm of weights: 3.2150233077112667\n",
      "---------------------\n",
      "Iteration Number: 4633\n",
      "Loss: 14.541089892082667\n",
      "l2 norm of gradients: 0.13282668416169285\n",
      "l2 norm of weights: 3.214969993123572\n",
      "---------------------\n",
      "Iteration Number: 4634\n",
      "Loss: 14.538743590606213\n",
      "l2 norm of gradients: 0.13277519641716595\n",
      "l2 norm of weights: 3.2149167928293405\n",
      "---------------------\n",
      "Iteration Number: 4635\n",
      "Loss: 14.536399001540087\n",
      "l2 norm of gradients: 0.13272373069069607\n",
      "l2 norm of weights: 3.214863706747622\n",
      "---------------------\n",
      "Iteration Number: 4636\n",
      "Loss: 14.534056123885872\n",
      "l2 norm of gradients: 0.1326722869581114\n",
      "l2 norm of weights: 3.214810734797517\n",
      "---------------------\n",
      "Iteration Number: 4637\n",
      "Loss: 14.531714956645931\n",
      "l2 norm of gradients: 0.1326208651953271\n",
      "l2 norm of weights: 3.21475787689818\n",
      "---------------------\n",
      "Iteration Number: 4638\n",
      "Loss: 14.529375498823306\n",
      "l2 norm of gradients: 0.1325694653783451\n",
      "l2 norm of weights: 3.214705132968814\n",
      "---------------------\n",
      "Iteration Number: 4639\n",
      "Loss: 14.527037749421801\n",
      "l2 norm of gradients: 0.13251808748325428\n",
      "l2 norm of weights: 3.2146525029286774\n",
      "---------------------\n",
      "Iteration Number: 4640\n",
      "Loss: 14.524701707445862\n",
      "l2 norm of gradients: 0.13246673148623048\n",
      "l2 norm of weights: 3.2145999866970763\n",
      "---------------------\n",
      "Iteration Number: 4641\n",
      "Loss: 14.522367371900726\n",
      "l2 norm of gradients: 0.1324153973635365\n",
      "l2 norm of weights: 3.214547584193372\n",
      "---------------------\n",
      "Iteration Number: 4642\n",
      "Loss: 14.52003474179229\n",
      "l2 norm of gradients: 0.1323640850915222\n",
      "l2 norm of weights: 3.214495295336974\n",
      "---------------------\n",
      "Iteration Number: 4643\n",
      "Loss: 14.51770381612718\n",
      "l2 norm of gradients: 0.1323127946466244\n",
      "l2 norm of weights: 3.214443120047345\n",
      "---------------------\n",
      "Iteration Number: 4644\n",
      "Loss: 14.515374593912732\n",
      "l2 norm of gradients: 0.13226152600536698\n",
      "l2 norm of weights: 3.214391058243999\n",
      "---------------------\n",
      "Iteration Number: 4645\n",
      "Loss: 14.513047074156924\n",
      "l2 norm of gradients: 0.13221027914436095\n",
      "l2 norm of weights: 3.2143391098465015\n",
      "---------------------\n",
      "Iteration Number: 4646\n",
      "Loss: 14.510721255868482\n",
      "l2 norm of gradients: 0.13215905404030445\n",
      "l2 norm of weights: 3.214287274774469\n",
      "---------------------\n",
      "Iteration Number: 4647\n",
      "Loss: 14.508397138056829\n",
      "l2 norm of gradients: 0.1321078506699827\n",
      "l2 norm of weights: 3.214235552947569\n",
      "---------------------\n",
      "Iteration Number: 4648\n",
      "Loss: 14.506074719732018\n",
      "l2 norm of gradients: 0.13205666901026805\n",
      "l2 norm of weights: 3.21418394428552\n",
      "---------------------\n",
      "Iteration Number: 4649\n",
      "Loss: 14.50375399990486\n",
      "l2 norm of gradients: 0.13200550903812014\n",
      "l2 norm of weights: 3.214132448708092\n",
      "---------------------\n",
      "Iteration Number: 4650\n",
      "Loss: 14.501434977586767\n",
      "l2 norm of gradients: 0.13195437073058572\n",
      "l2 norm of weights: 3.214081066135106\n",
      "---------------------\n",
      "Iteration Number: 4651\n",
      "Loss: 14.49911765178989\n",
      "l2 norm of gradients: 0.13190325406479883\n",
      "l2 norm of weights: 3.214029796486435\n",
      "---------------------\n",
      "Iteration Number: 4652\n",
      "Loss: 14.49680202152701\n",
      "l2 norm of gradients: 0.13185215901798064\n",
      "l2 norm of weights: 3.2139786396820007\n",
      "---------------------\n",
      "Iteration Number: 4653\n",
      "Loss: 14.494488085811629\n",
      "l2 norm of gradients: 0.1318010855674397\n",
      "l2 norm of weights: 3.2139275956417777\n",
      "---------------------\n",
      "Iteration Number: 4654\n",
      "Loss: 14.492175843657835\n",
      "l2 norm of gradients: 0.1317500336905717\n",
      "l2 norm of weights: 3.2138766642857908\n",
      "---------------------\n",
      "Iteration Number: 4655\n",
      "Loss: 14.489865294080452\n",
      "l2 norm of gradients: 0.13169900336485973\n",
      "l2 norm of weights: 3.2138258455341155\n",
      "---------------------\n",
      "Iteration Number: 4656\n",
      "Loss: 14.48755643609493\n",
      "l2 norm of gradients: 0.13164799456787415\n",
      "l2 norm of weights: 3.2137751393068785\n",
      "---------------------\n",
      "Iteration Number: 4657\n",
      "Loss: 14.48524926871737\n",
      "l2 norm of gradients: 0.13159700727727267\n",
      "l2 norm of weights: 3.213724545524257\n",
      "---------------------\n",
      "Iteration Number: 4658\n",
      "Loss: 14.482943790964518\n",
      "l2 norm of gradients: 0.1315460414708002\n",
      "l2 norm of weights: 3.2136740641064785\n",
      "---------------------\n",
      "Iteration Number: 4659\n",
      "Loss: 14.480640001853832\n",
      "l2 norm of gradients: 0.13149509712628912\n",
      "l2 norm of weights: 3.2136236949738213\n",
      "---------------------\n",
      "Iteration Number: 4660\n",
      "Loss: 14.47833790040329\n",
      "l2 norm of gradients: 0.13144417422165905\n",
      "l2 norm of weights: 3.213573438046615\n",
      "---------------------\n",
      "Iteration Number: 4661\n",
      "Loss: 14.476037485631613\n",
      "l2 norm of gradients: 0.13139327273491713\n",
      "l2 norm of weights: 3.213523293245238\n",
      "---------------------\n",
      "Iteration Number: 4662\n",
      "Loss: 14.473738756558154\n",
      "l2 norm of gradients: 0.1313423926441577\n",
      "l2 norm of weights: 3.213473260490122\n",
      "---------------------\n",
      "Iteration Number: 4663\n",
      "Loss: 14.471441712202832\n",
      "l2 norm of gradients: 0.13129153392756254\n",
      "l2 norm of weights: 3.2134233397017464\n",
      "---------------------\n",
      "Iteration Number: 4664\n",
      "Loss: 14.469146351586259\n",
      "l2 norm of gradients: 0.13124069656340082\n",
      "l2 norm of weights: 3.213373530800642\n",
      "---------------------\n",
      "Iteration Number: 4665\n",
      "Loss: 14.466852673729655\n",
      "l2 norm of gradients: 0.13118988053002906\n",
      "l2 norm of weights: 3.21332383370739\n",
      "---------------------\n",
      "Iteration Number: 4666\n",
      "Loss: 14.464560677654866\n",
      "l2 norm of gradients: 0.13113908580589123\n",
      "l2 norm of weights: 3.2132742483426218\n",
      "---------------------\n",
      "Iteration Number: 4667\n",
      "Loss: 14.462270362384306\n",
      "l2 norm of gradients: 0.1310883123695186\n",
      "l2 norm of weights: 3.2132247746270197\n",
      "---------------------\n",
      "Iteration Number: 4668\n",
      "Loss: 14.459981726941084\n",
      "l2 norm of gradients: 0.13103756019952983\n",
      "l2 norm of weights: 3.213175412481315\n",
      "---------------------\n",
      "Iteration Number: 4669\n",
      "Loss: 14.457694770348889\n",
      "l2 norm of gradients: 0.13098682927463115\n",
      "l2 norm of weights: 3.213126161826289\n",
      "---------------------\n",
      "Iteration Number: 4670\n",
      "Loss: 14.455409491632015\n",
      "l2 norm of gradients: 0.130936119573616\n",
      "l2 norm of weights: 3.213077022582775\n",
      "---------------------\n",
      "Iteration Number: 4671\n",
      "Loss: 14.45312588981535\n",
      "l2 norm of gradients: 0.1308854310753652\n",
      "l2 norm of weights: 3.2130279946716542\n",
      "---------------------\n",
      "Iteration Number: 4672\n",
      "Loss: 14.450843963924385\n",
      "l2 norm of gradients: 0.1308347637588471\n",
      "l2 norm of weights: 3.212979078013859\n",
      "---------------------\n",
      "Iteration Number: 4673\n",
      "Loss: 14.448563712985257\n",
      "l2 norm of gradients: 0.13078411760311737\n",
      "l2 norm of weights: 3.212930272530372\n",
      "---------------------\n",
      "Iteration Number: 4674\n",
      "Loss: 14.44628513602464\n",
      "l2 norm of gradients: 0.1307334925873189\n",
      "l2 norm of weights: 3.2128815781422237\n",
      "---------------------\n",
      "Iteration Number: 4675\n",
      "Loss: 14.444008232069839\n",
      "l2 norm of gradients: 0.13068288869068231\n",
      "l2 norm of weights: 3.212832994770497\n",
      "---------------------\n",
      "Iteration Number: 4676\n",
      "Loss: 14.441733000148705\n",
      "l2 norm of gradients: 0.13063230589252525\n",
      "l2 norm of weights: 3.2127845223363227\n",
      "---------------------\n",
      "Iteration Number: 4677\n",
      "Loss: 14.439459439289715\n",
      "l2 norm of gradients: 0.13058174417225288\n",
      "l2 norm of weights: 3.2127361607608833\n",
      "---------------------\n",
      "Iteration Number: 4678\n",
      "Loss: 14.43718754852192\n",
      "l2 norm of gradients: 0.1305312035093577\n",
      "l2 norm of weights: 3.2126879099654087\n",
      "---------------------\n",
      "Iteration Number: 4679\n",
      "Loss: 14.43491732687491\n",
      "l2 norm of gradients: 0.13048068388341957\n",
      "l2 norm of weights: 3.2126397698711795\n",
      "---------------------\n",
      "Iteration Number: 4680\n",
      "Loss: 14.432648773378903\n",
      "l2 norm of gradients: 0.13043018527410566\n",
      "l2 norm of weights: 3.212591740399527\n",
      "---------------------\n",
      "Iteration Number: 4681\n",
      "Loss: 14.430381887064657\n",
      "l2 norm of gradients: 0.1303797076611704\n",
      "l2 norm of weights: 3.2125438214718303\n",
      "---------------------\n",
      "Iteration Number: 4682\n",
      "Loss: 14.428116666963508\n",
      "l2 norm of gradients: 0.13032925102445564\n",
      "l2 norm of weights: 3.212496013009519\n",
      "---------------------\n",
      "Iteration Number: 4683\n",
      "Loss: 14.425853112107365\n",
      "l2 norm of gradients: 0.13027881534389044\n",
      "l2 norm of weights: 3.212448314934072\n",
      "---------------------\n",
      "Iteration Number: 4684\n",
      "Loss: 14.423591221528678\n",
      "l2 norm of gradients: 0.13022840059949117\n",
      "l2 norm of weights: 3.212400727167017\n",
      "---------------------\n",
      "Iteration Number: 4685\n",
      "Loss: 14.421330994260455\n",
      "l2 norm of gradients: 0.13017800677136143\n",
      "l2 norm of weights: 3.2123532496299316\n",
      "---------------------\n",
      "Iteration Number: 4686\n",
      "Loss: 14.419072429336289\n",
      "l2 norm of gradients: 0.13012763383969217\n",
      "l2 norm of weights: 3.2123058822444435\n",
      "---------------------\n",
      "Iteration Number: 4687\n",
      "Loss: 14.416815525790286\n",
      "l2 norm of gradients: 0.13007728178476138\n",
      "l2 norm of weights: 3.212258624932229\n",
      "---------------------\n",
      "Iteration Number: 4688\n",
      "Loss: 14.414560282657106\n",
      "l2 norm of gradients: 0.13002695058693448\n",
      "l2 norm of weights: 3.212211477615013\n",
      "---------------------\n",
      "Iteration Number: 4689\n",
      "Loss: 14.412306698971989\n",
      "l2 norm of gradients: 0.12997664022666383\n",
      "l2 norm of weights: 3.2121644402145697\n",
      "---------------------\n",
      "Iteration Number: 4690\n",
      "Loss: 14.410054773770671\n",
      "l2 norm of gradients: 0.12992635068448924\n",
      "l2 norm of weights: 3.212117512652724\n",
      "---------------------\n",
      "Iteration Number: 4691\n",
      "Loss: 14.407804506089475\n",
      "l2 norm of gradients: 0.1298760819410374\n",
      "l2 norm of weights: 3.212070694851348\n",
      "---------------------\n",
      "Iteration Number: 4692\n",
      "Loss: 14.405555894965186\n",
      "l2 norm of gradients: 0.1298258339770223\n",
      "l2 norm of weights: 3.212023986732364\n",
      "---------------------\n",
      "Iteration Number: 4693\n",
      "Loss: 14.403308939435208\n",
      "l2 norm of gradients: 0.12977560677324496\n",
      "l2 norm of weights: 3.2119773882177434\n",
      "---------------------\n",
      "Iteration Number: 4694\n",
      "Loss: 14.401063638537389\n",
      "l2 norm of gradients: 0.12972540031059346\n",
      "l2 norm of weights: 3.2119308992295053\n",
      "---------------------\n",
      "Iteration Number: 4695\n",
      "Loss: 14.398819991310168\n",
      "l2 norm of gradients: 0.1296752145700429\n",
      "l2 norm of weights: 3.211884519689719\n",
      "---------------------\n",
      "Iteration Number: 4696\n",
      "Loss: 14.396577996792447\n",
      "l2 norm of gradients: 0.12962504953265555\n",
      "l2 norm of weights: 3.211838249520502\n",
      "---------------------\n",
      "Iteration Number: 4697\n",
      "Loss: 14.394337654023708\n",
      "l2 norm of gradients: 0.12957490517958048\n",
      "l2 norm of weights: 3.211792088644021\n",
      "---------------------\n",
      "Iteration Number: 4698\n",
      "Loss: 14.392098962043915\n",
      "l2 norm of gradients: 0.1295247814920538\n",
      "l2 norm of weights: 3.211746036982491\n",
      "---------------------\n",
      "Iteration Number: 4699\n",
      "Loss: 14.389861919893525\n",
      "l2 norm of gradients: 0.1294746784513986\n",
      "l2 norm of weights: 3.2117000944581764\n",
      "---------------------\n",
      "Iteration Number: 4700\n",
      "Loss: 14.387626526613532\n",
      "l2 norm of gradients: 0.1294245960390248\n",
      "l2 norm of weights: 3.2116542609933894\n",
      "---------------------\n",
      "Iteration Number: 4701\n",
      "Loss: 14.385392781245436\n",
      "l2 norm of gradients: 0.12937453423642917\n",
      "l2 norm of weights: 3.211608536510492\n",
      "---------------------\n",
      "Iteration Number: 4702\n",
      "Loss: 14.383160682831212\n",
      "l2 norm of gradients: 0.12932449302519544\n",
      "l2 norm of weights: 3.211562920931894\n",
      "---------------------\n",
      "Iteration Number: 4703\n",
      "Loss: 14.380930230413364\n",
      "l2 norm of gradients: 0.12927447238699405\n",
      "l2 norm of weights: 3.211517414180054\n",
      "---------------------\n",
      "Iteration Number: 4704\n",
      "Loss: 14.378701423034906\n",
      "l2 norm of gradients: 0.12922447230358214\n",
      "l2 norm of weights: 3.2114720161774786\n",
      "---------------------\n",
      "Iteration Number: 4705\n",
      "Loss: 14.376474259739249\n",
      "l2 norm of gradients: 0.12917449275680365\n",
      "l2 norm of weights: 3.211426726846723\n",
      "---------------------\n",
      "Iteration Number: 4706\n",
      "Loss: 14.374248739570413\n",
      "l2 norm of gradients: 0.12912453372858929\n",
      "l2 norm of weights: 3.2113815461103923\n",
      "---------------------\n",
      "Iteration Number: 4707\n",
      "Loss: 14.372024861572854\n",
      "l2 norm of gradients: 0.12907459520095624\n",
      "l2 norm of weights: 3.2113364738911376\n",
      "---------------------\n",
      "Iteration Number: 4708\n",
      "Loss: 14.369802624791488\n",
      "l2 norm of gradients: 0.12902467715600854\n",
      "l2 norm of weights: 3.2112915101116606\n",
      "---------------------\n",
      "Iteration Number: 4709\n",
      "Loss: 14.36758202827174\n",
      "l2 norm of gradients: 0.12897477957593648\n",
      "l2 norm of weights: 3.2112466546947087\n",
      "---------------------\n",
      "Iteration Number: 4710\n",
      "Loss: 14.365363071059486\n",
      "l2 norm of gradients: 0.12892490244301716\n",
      "l2 norm of weights: 3.2112019075630807\n",
      "---------------------\n",
      "Iteration Number: 4711\n",
      "Loss: 14.36314575220113\n",
      "l2 norm of gradients: 0.12887504573961406\n",
      "l2 norm of weights: 3.2111572686396204\n",
      "---------------------\n",
      "Iteration Number: 4712\n",
      "Loss: 14.360930070743468\n",
      "l2 norm of gradients: 0.1288252094481771\n",
      "l2 norm of weights: 3.2111127378472224\n",
      "---------------------\n",
      "Iteration Number: 4713\n",
      "Loss: 14.358716025733827\n",
      "l2 norm of gradients: 0.12877539355124265\n",
      "l2 norm of weights: 3.211068315108827\n",
      "---------------------\n",
      "Iteration Number: 4714\n",
      "Loss: 14.356503616219968\n",
      "l2 norm of gradients: 0.12872559803143335\n",
      "l2 norm of weights: 3.211024000347425\n",
      "---------------------\n",
      "Iteration Number: 4715\n",
      "Loss: 14.354292841250139\n",
      "l2 norm of gradients: 0.1286758228714582\n",
      "l2 norm of weights: 3.210979793486054\n",
      "---------------------\n",
      "Iteration Number: 4716\n",
      "Loss: 14.35208369987297\n",
      "l2 norm of gradients: 0.12862606805411247\n",
      "l2 norm of weights: 3.2109356944477985\n",
      "---------------------\n",
      "Iteration Number: 4717\n",
      "Loss: 14.349876191137664\n",
      "l2 norm of gradients: 0.12857633356227774\n",
      "l2 norm of weights: 3.2108917031557933\n",
      "---------------------\n",
      "Iteration Number: 4718\n",
      "Loss: 14.347670314093802\n",
      "l2 norm of gradients: 0.12852661937892151\n",
      "l2 norm of weights: 3.210847819533219\n",
      "---------------------\n",
      "Iteration Number: 4719\n",
      "Loss: 14.345466067791385\n",
      "l2 norm of gradients: 0.12847692548709758\n",
      "l2 norm of weights: 3.2108040435033054\n",
      "---------------------\n",
      "Iteration Number: 4720\n",
      "Loss: 14.343263451280922\n",
      "l2 norm of gradients: 0.1284272518699458\n",
      "l2 norm of weights: 3.210760374989329\n",
      "---------------------\n",
      "Iteration Number: 4721\n",
      "Loss: 14.341062463613358\n",
      "l2 norm of gradients: 0.128377598510692\n",
      "l2 norm of weights: 3.210716813914615\n",
      "---------------------\n",
      "Iteration Number: 4722\n",
      "Loss: 14.33886310384004\n",
      "l2 norm of gradients: 0.12832796539264796\n",
      "l2 norm of weights: 3.2106733602025357\n",
      "---------------------\n",
      "Iteration Number: 4723\n",
      "Loss: 14.336665371012783\n",
      "l2 norm of gradients: 0.12827835249921138\n",
      "l2 norm of weights: 3.210630013776511\n",
      "---------------------\n",
      "Iteration Number: 4724\n",
      "Loss: 14.334469264183818\n",
      "l2 norm of gradients: 0.1282287598138658\n",
      "l2 norm of weights: 3.2105867745600096\n",
      "---------------------\n",
      "Iteration Number: 4725\n",
      "Loss: 14.332274782405804\n",
      "l2 norm of gradients: 0.12817918732018058\n",
      "l2 norm of weights: 3.210543642476546\n",
      "---------------------\n",
      "Iteration Number: 4726\n",
      "Loss: 14.330081924731857\n",
      "l2 norm of gradients: 0.12812963500181082\n",
      "l2 norm of weights: 3.2105006174496835\n",
      "---------------------\n",
      "Iteration Number: 4727\n",
      "Loss: 14.327890690215487\n",
      "l2 norm of gradients: 0.12808010284249718\n",
      "l2 norm of weights: 3.210457699403032\n",
      "---------------------\n",
      "Iteration Number: 4728\n",
      "Loss: 14.325701077910642\n",
      "l2 norm of gradients: 0.12803059082606616\n",
      "l2 norm of weights: 3.21041488826025\n",
      "---------------------\n",
      "Iteration Number: 4729\n",
      "Loss: 14.323513086871653\n",
      "l2 norm of gradients: 0.12798109893642964\n",
      "l2 norm of weights: 3.210372183945042\n",
      "---------------------\n",
      "Iteration Number: 4730\n",
      "Loss: 14.321326716153312\n",
      "l2 norm of gradients: 0.12793162715758502\n",
      "l2 norm of weights: 3.210329586381161\n",
      "---------------------\n",
      "Iteration Number: 4731\n",
      "Loss: 14.319141964810807\n",
      "l2 norm of gradients: 0.12788217547361522\n",
      "l2 norm of weights: 3.210287095492407\n",
      "---------------------\n",
      "Iteration Number: 4732\n",
      "Loss: 14.316958831899719\n",
      "l2 norm of gradients: 0.12783274386868845\n",
      "l2 norm of weights: 3.2102447112026264\n",
      "---------------------\n",
      "Iteration Number: 4733\n",
      "Loss: 14.314777316476073\n",
      "l2 norm of gradients: 0.1277833323270583\n",
      "l2 norm of weights: 3.210202433435714\n",
      "---------------------\n",
      "Iteration Number: 4734\n",
      "Loss: 14.312597417596258\n",
      "l2 norm of gradients: 0.1277339408330636\n",
      "l2 norm of weights: 3.210160262115612\n",
      "---------------------\n",
      "Iteration Number: 4735\n",
      "Loss: 14.310419134317092\n",
      "l2 norm of gradients: 0.1276845693711283\n",
      "l2 norm of weights: 3.2101181971663086\n",
      "---------------------\n",
      "Iteration Number: 4736\n",
      "Loss: 14.308242465695754\n",
      "l2 norm of gradients: 0.1276352179257615\n",
      "l2 norm of weights: 3.210076238511839\n",
      "---------------------\n",
      "Iteration Number: 4737\n",
      "Loss: 14.306067410789867\n",
      "l2 norm of gradients: 0.12758588648155733\n",
      "l2 norm of weights: 3.2100343860762868\n",
      "---------------------\n",
      "Iteration Number: 4738\n",
      "Loss: 14.303893968657388\n",
      "l2 norm of gradients: 0.12753657502319496\n",
      "l2 norm of weights: 3.2099926397837812\n",
      "---------------------\n",
      "Iteration Number: 4739\n",
      "Loss: 14.301722138356737\n",
      "l2 norm of gradients: 0.12748728353543842\n",
      "l2 norm of weights: 3.2099509995584996\n",
      "---------------------\n",
      "Iteration Number: 4740\n",
      "Loss: 14.299551918946658\n",
      "l2 norm of gradients: 0.1274380120031366\n",
      "l2 norm of weights: 3.2099094653246656\n",
      "---------------------\n",
      "Iteration Number: 4741\n",
      "Loss: 14.2973833094863\n",
      "l2 norm of gradients: 0.12738876041122313\n",
      "l2 norm of weights: 3.209868037006549\n",
      "---------------------\n",
      "Iteration Number: 4742\n",
      "Loss: 14.295216309035196\n",
      "l2 norm of gradients: 0.12733952874471635\n",
      "l2 norm of weights: 3.2098267145284685\n",
      "---------------------\n",
      "Iteration Number: 4743\n",
      "Loss: 14.293050916653243\n",
      "l2 norm of gradients: 0.12729031698871926\n",
      "l2 norm of weights: 3.2097854978147877\n",
      "---------------------\n",
      "Iteration Number: 4744\n",
      "Loss: 14.290887131400725\n",
      "l2 norm of gradients: 0.12724112512841934\n",
      "l2 norm of weights: 3.209744386789917\n",
      "---------------------\n",
      "Iteration Number: 4745\n",
      "Loss: 14.28872495233828\n",
      "l2 norm of gradients: 0.1271919531490886\n",
      "l2 norm of weights: 3.2097033813783145\n",
      "---------------------\n",
      "Iteration Number: 4746\n",
      "Loss: 14.286564378526954\n",
      "l2 norm of gradients: 0.12714280103608347\n",
      "l2 norm of weights: 3.2096624815044845\n",
      "---------------------\n",
      "Iteration Number: 4747\n",
      "Loss: 14.28440540902811\n",
      "l2 norm of gradients: 0.12709366877484463\n",
      "l2 norm of weights: 3.2096216870929783\n",
      "---------------------\n",
      "Iteration Number: 4748\n",
      "Loss: 14.282248042903516\n",
      "l2 norm of gradients: 0.12704455635089704\n",
      "l2 norm of weights: 3.2095809980683927\n",
      "---------------------\n",
      "Iteration Number: 4749\n",
      "Loss: 14.280092279215278\n",
      "l2 norm of gradients: 0.12699546374984988\n",
      "l2 norm of weights: 3.2095404143553723\n",
      "---------------------\n",
      "Iteration Number: 4750\n",
      "Loss: 14.27793811702586\n",
      "l2 norm of gradients: 0.12694639095739635\n",
      "l2 norm of weights: 3.2094999358786076\n",
      "---------------------\n",
      "Iteration Number: 4751\n",
      "Loss: 14.2757855553981\n",
      "l2 norm of gradients: 0.12689733795931368\n",
      "l2 norm of weights: 3.209459562562835\n",
      "---------------------\n",
      "Iteration Number: 4752\n",
      "Loss: 14.273634593395162\n",
      "l2 norm of gradients: 0.12684830474146297\n",
      "l2 norm of weights: 3.2094192943328377\n",
      "---------------------\n",
      "Iteration Number: 4753\n",
      "Loss: 14.271485230080533\n",
      "l2 norm of gradients: 0.12679929128978937\n",
      "l2 norm of weights: 3.209379131113446\n",
      "---------------------\n",
      "Iteration Number: 4754\n",
      "Loss: 14.269337464518161\n",
      "l2 norm of gradients: 0.1267502975903215\n",
      "l2 norm of weights: 3.2093390728295366\n",
      "---------------------\n",
      "Iteration Number: 4755\n",
      "Loss: 14.267191295772191\n",
      "l2 norm of gradients: 0.12670132362917186\n",
      "l2 norm of weights: 3.20929911940603\n",
      "---------------------\n",
      "Iteration Number: 4756\n",
      "Loss: 14.265046722907243\n",
      "l2 norm of gradients: 0.1266523693925365\n",
      "l2 norm of weights: 3.209259270767896\n",
      "---------------------\n",
      "Iteration Number: 4757\n",
      "Loss: 14.262903744988142\n",
      "l2 norm of gradients: 0.12660343486669498\n",
      "l2 norm of weights: 3.209219526840149\n",
      "---------------------\n",
      "Iteration Number: 4758\n",
      "Loss: 14.260762361080165\n",
      "l2 norm of gradients: 0.1265545200380103\n",
      "l2 norm of weights: 3.209179887547849\n",
      "---------------------\n",
      "Iteration Number: 4759\n",
      "Loss: 14.258622570248878\n",
      "l2 norm of gradients: 0.1265056248929287\n",
      "l2 norm of weights: 3.2091403528161044\n",
      "---------------------\n",
      "Iteration Number: 4760\n",
      "Loss: 14.256484371560163\n",
      "l2 norm of gradients: 0.12645674941797977\n",
      "l2 norm of weights: 3.209100922570067\n",
      "---------------------\n",
      "Iteration Number: 4761\n",
      "Loss: 14.254347764080222\n",
      "l2 norm of gradients: 0.12640789359977622\n",
      "l2 norm of weights: 3.2090615967349363\n",
      "---------------------\n",
      "Iteration Number: 4762\n",
      "Loss: 14.252212746875626\n",
      "l2 norm of gradients: 0.12635905742501388\n",
      "l2 norm of weights: 3.2090223752359566\n",
      "---------------------\n",
      "Iteration Number: 4763\n",
      "Loss: 14.250079319013246\n",
      "l2 norm of gradients: 0.12631024088047152\n",
      "l2 norm of weights: 3.2089832579984194\n",
      "---------------------\n",
      "Iteration Number: 4764\n",
      "Loss: 14.247947479560278\n",
      "l2 norm of gradients: 0.12626144395301078\n",
      "l2 norm of weights: 3.2089442449476606\n",
      "---------------------\n",
      "Iteration Number: 4765\n",
      "Loss: 14.245817227584203\n",
      "l2 norm of gradients: 0.12621266662957614\n",
      "l2 norm of weights: 3.208905336009064\n",
      "---------------------\n",
      "Iteration Number: 4766\n",
      "Loss: 14.243688562152842\n",
      "l2 norm of gradients: 0.1261639088971947\n",
      "l2 norm of weights: 3.2088665311080566\n",
      "---------------------\n",
      "Iteration Number: 4767\n",
      "Loss: 14.241561482334323\n",
      "l2 norm of gradients: 0.12611517074297623\n",
      "l2 norm of weights: 3.2088278301701134\n",
      "---------------------\n",
      "Iteration Number: 4768\n",
      "Loss: 14.239435987197131\n",
      "l2 norm of gradients: 0.12606645215411308\n",
      "l2 norm of weights: 3.2087892331207537\n",
      "---------------------\n",
      "Iteration Number: 4769\n",
      "Loss: 14.237312075809964\n",
      "l2 norm of gradients: 0.1260177531178799\n",
      "l2 norm of weights: 3.2087507398855433\n",
      "---------------------\n",
      "Iteration Number: 4770\n",
      "Loss: 14.235189747241911\n",
      "l2 norm of gradients: 0.1259690736216337\n",
      "l2 norm of weights: 3.208712350390093\n",
      "---------------------\n",
      "Iteration Number: 4771\n",
      "Loss: 14.233069000562283\n",
      "l2 norm of gradients: 0.12592041365281378\n",
      "l2 norm of weights: 3.2086740645600598\n",
      "---------------------\n",
      "Iteration Number: 4772\n",
      "Loss: 14.230949834840775\n",
      "l2 norm of gradients: 0.1258717731989414\n",
      "l2 norm of weights: 3.208635882321145\n",
      "---------------------\n",
      "Iteration Number: 4773\n",
      "Loss: 14.228832249147306\n",
      "l2 norm of gradients: 0.12582315224762014\n",
      "l2 norm of weights: 3.2085978035990976\n",
      "---------------------\n",
      "Iteration Number: 4774\n",
      "Loss: 14.22671624255214\n",
      "l2 norm of gradients: 0.12577455078653518\n",
      "l2 norm of weights: 3.2085598283197094\n",
      "---------------------\n",
      "Iteration Number: 4775\n",
      "Loss: 14.224601814125787\n",
      "l2 norm of gradients: 0.12572596880345374\n",
      "l2 norm of weights: 3.2085219564088194\n",
      "---------------------\n",
      "Iteration Number: 4776\n",
      "Loss: 14.222488962939062\n",
      "l2 norm of gradients: 0.1256774062862247\n",
      "l2 norm of weights: 3.2084841877923114\n",
      "---------------------\n",
      "Iteration Number: 4777\n",
      "Loss: 14.22037768806311\n",
      "l2 norm of gradients: 0.12562886322277858\n",
      "l2 norm of weights: 3.2084465223961143\n",
      "---------------------\n",
      "Iteration Number: 4778\n",
      "Loss: 14.218267988569293\n",
      "l2 norm of gradients: 0.12558033960112736\n",
      "l2 norm of weights: 3.2084089601462025\n",
      "---------------------\n",
      "Iteration Number: 4779\n",
      "Loss: 14.216159863529292\n",
      "l2 norm of gradients: 0.12553183540936452\n",
      "l2 norm of weights: 3.208371500968596\n",
      "---------------------\n",
      "Iteration Number: 4780\n",
      "Loss: 14.214053312015063\n",
      "l2 norm of gradients: 0.12548335063566488\n",
      "l2 norm of weights: 3.2083341447893594\n",
      "---------------------\n",
      "Iteration Number: 4781\n",
      "Loss: 14.211948333098812\n",
      "l2 norm of gradients: 0.12543488526828434\n",
      "l2 norm of weights: 3.208296891534603\n",
      "---------------------\n",
      "Iteration Number: 4782\n",
      "Loss: 14.209844925853051\n",
      "l2 norm of gradients: 0.12538643929556\n",
      "l2 norm of weights: 3.2082597411304805\n",
      "---------------------\n",
      "Iteration Number: 4783\n",
      "Loss: 14.207743089350572\n",
      "l2 norm of gradients: 0.12533801270590994\n",
      "l2 norm of weights: 3.2082226935031932\n",
      "---------------------\n",
      "Iteration Number: 4784\n",
      "Loss: 14.205642822664386\n",
      "l2 norm of gradients: 0.1252896054878331\n",
      "l2 norm of weights: 3.2081857485789866\n",
      "---------------------\n",
      "Iteration Number: 4785\n",
      "Loss: 14.203544124867797\n",
      "l2 norm of gradients: 0.12524121762990925\n",
      "l2 norm of weights: 3.208148906284149\n",
      "---------------------\n",
      "Iteration Number: 4786\n",
      "Loss: 14.20144699503441\n",
      "l2 norm of gradients: 0.12519284912079878\n",
      "l2 norm of weights: 3.2081121665450167\n",
      "---------------------\n",
      "Iteration Number: 4787\n",
      "Loss: 14.199351432238027\n",
      "l2 norm of gradients: 0.1251444999492426\n",
      "l2 norm of weights: 3.2080755292879695\n",
      "---------------------\n",
      "Iteration Number: 4788\n",
      "Loss: 14.197257435552743\n",
      "l2 norm of gradients: 0.1250961701040622\n",
      "l2 norm of weights: 3.2080389944394314\n",
      "---------------------\n",
      "Iteration Number: 4789\n",
      "Loss: 14.19516500405291\n",
      "l2 norm of gradients: 0.12504785957415934\n",
      "l2 norm of weights: 3.2080025619258716\n",
      "---------------------\n",
      "Iteration Number: 4790\n",
      "Loss: 14.193074136813111\n",
      "l2 norm of gradients: 0.12499956834851594\n",
      "l2 norm of weights: 3.2079662316738045\n",
      "---------------------\n",
      "Iteration Number: 4791\n",
      "Loss: 14.190984832908239\n",
      "l2 norm of gradients: 0.12495129641619418\n",
      "l2 norm of weights: 3.20793000360979\n",
      "---------------------\n",
      "Iteration Number: 4792\n",
      "Loss: 14.18889709141336\n",
      "l2 norm of gradients: 0.12490304376633604\n",
      "l2 norm of weights: 3.2078938776604304\n",
      "---------------------\n",
      "Iteration Number: 4793\n",
      "Loss: 14.186810911403818\n",
      "l2 norm of gradients: 0.12485481038816364\n",
      "l2 norm of weights: 3.2078578537523743\n",
      "---------------------\n",
      "Iteration Number: 4794\n",
      "Loss: 14.184726291955245\n",
      "l2 norm of gradients: 0.12480659627097862\n",
      "l2 norm of weights: 3.207821931812314\n",
      "---------------------\n",
      "Iteration Number: 4795\n",
      "Loss: 14.182643232143446\n",
      "l2 norm of gradients: 0.12475840140416244\n",
      "l2 norm of weights: 3.207786111766987\n",
      "---------------------\n",
      "Iteration Number: 4796\n",
      "Loss: 14.180561731044504\n",
      "l2 norm of gradients: 0.12471022577717598\n",
      "l2 norm of weights: 3.2077503935431744\n",
      "---------------------\n",
      "Iteration Number: 4797\n",
      "Loss: 14.178481787734741\n",
      "l2 norm of gradients: 0.12466206937955968\n",
      "l2 norm of weights: 3.207714777067703\n",
      "---------------------\n",
      "Iteration Number: 4798\n",
      "Loss: 14.176403401290703\n",
      "l2 norm of gradients: 0.12461393220093311\n",
      "l2 norm of weights: 3.2076792622674426\n",
      "---------------------\n",
      "Iteration Number: 4799\n",
      "Loss: 14.174326570789162\n",
      "l2 norm of gradients: 0.12456581423099514\n",
      "l2 norm of weights: 3.2076438490693087\n",
      "---------------------\n",
      "Iteration Number: 4800\n",
      "Loss: 14.172251295307143\n",
      "l2 norm of gradients: 0.12451771545952375\n",
      "l2 norm of weights: 3.2076085374002594\n",
      "---------------------\n",
      "Iteration Number: 4801\n",
      "Loss: 14.170177573921858\n",
      "l2 norm of gradients: 0.12446963587637569\n",
      "l2 norm of weights: 3.2075733271872986\n",
      "---------------------\n",
      "Iteration Number: 4802\n",
      "Loss: 14.16810540571083\n",
      "l2 norm of gradients: 0.12442157547148668\n",
      "l2 norm of weights: 3.207538218357474\n",
      "---------------------\n",
      "Iteration Number: 4803\n",
      "Loss: 14.166034789751693\n",
      "l2 norm of gradients: 0.12437353423487106\n",
      "l2 norm of weights: 3.207503210837877\n",
      "---------------------\n",
      "Iteration Number: 4804\n",
      "Loss: 14.163965725122402\n",
      "l2 norm of gradients: 0.12432551215662183\n",
      "l2 norm of weights: 3.207468304555644\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 4805\n",
      "Loss: 14.161898210901077\n",
      "l2 norm of gradients: 0.12427750922691032\n",
      "l2 norm of weights: 3.2074334994379536\n",
      "---------------------\n",
      "Iteration Number: 4806\n",
      "Loss: 14.159832246166044\n",
      "l2 norm of gradients: 0.12422952543598632\n",
      "l2 norm of weights: 3.207398795412031\n",
      "---------------------\n",
      "Iteration Number: 4807\n",
      "Loss: 14.15776782999589\n",
      "l2 norm of gradients: 0.12418156077417772\n",
      "l2 norm of weights: 3.2073641924051426\n",
      "---------------------\n",
      "Iteration Number: 4808\n",
      "Loss: 14.155704961469402\n",
      "l2 norm of gradients: 0.12413361523189052\n",
      "l2 norm of weights: 3.207329690344602\n",
      "---------------------\n",
      "Iteration Number: 4809\n",
      "Loss: 14.153643639665557\n",
      "l2 norm of gradients: 0.12408568879960878\n",
      "l2 norm of weights: 3.2072952891577637\n",
      "---------------------\n",
      "Iteration Number: 4810\n",
      "Loss: 14.15158386366353\n",
      "l2 norm of gradients: 0.12403778146789422\n",
      "l2 norm of weights: 3.2072609887720276\n",
      "---------------------\n",
      "Iteration Number: 4811\n",
      "Loss: 14.149525632542755\n",
      "l2 norm of gradients: 0.12398989322738639\n",
      "l2 norm of weights: 3.207226789114837\n",
      "---------------------\n",
      "Iteration Number: 4812\n",
      "Loss: 14.147468945382819\n",
      "l2 norm of gradients: 0.12394202406880232\n",
      "l2 norm of weights: 3.2071926901136787\n",
      "---------------------\n",
      "Iteration Number: 4813\n",
      "Loss: 14.14541380126354\n",
      "l2 norm of gradients: 0.12389417398293653\n",
      "l2 norm of weights: 3.207158691696084\n",
      "---------------------\n",
      "Iteration Number: 4814\n",
      "Loss: 14.14336019926491\n",
      "l2 norm of gradients: 0.1238463429606609\n",
      "l2 norm of weights: 3.207124793789627\n",
      "---------------------\n",
      "Iteration Number: 4815\n",
      "Loss: 14.141308138467146\n",
      "l2 norm of gradients: 0.12379853099292434\n",
      "l2 norm of weights: 3.207090996321926\n",
      "---------------------\n",
      "Iteration Number: 4816\n",
      "Loss: 14.139257617950648\n",
      "l2 norm of gradients: 0.12375073807075304\n",
      "l2 norm of weights: 3.2070572992206423\n",
      "---------------------\n",
      "Iteration Number: 4817\n",
      "Loss: 14.137208636796014\n",
      "l2 norm of gradients: 0.12370296418524994\n",
      "l2 norm of weights: 3.2070237024134816\n",
      "---------------------\n",
      "Iteration Number: 4818\n",
      "Loss: 14.135161194084011\n",
      "l2 norm of gradients: 0.12365520932759486\n",
      "l2 norm of weights: 3.206990205828192\n",
      "---------------------\n",
      "Iteration Number: 4819\n",
      "Loss: 14.133115288895636\n",
      "l2 norm of gradients: 0.12360747348904413\n",
      "l2 norm of weights: 3.2069568093925658\n",
      "---------------------\n",
      "Iteration Number: 4820\n",
      "Loss: 14.13107092031204\n",
      "l2 norm of gradients: 0.12355975666093084\n",
      "l2 norm of weights: 3.2069235130344382\n",
      "---------------------\n",
      "Iteration Number: 4821\n",
      "Loss: 14.12902808741456\n",
      "l2 norm of gradients: 0.1235120588346643\n",
      "l2 norm of weights: 3.2068903166816884\n",
      "---------------------\n",
      "Iteration Number: 4822\n",
      "Loss: 14.12698678928473\n",
      "l2 norm of gradients: 0.12346438000173013\n",
      "l2 norm of weights: 3.206857220262239\n",
      "---------------------\n",
      "Iteration Number: 4823\n",
      "Loss: 14.12494702500426\n",
      "l2 norm of gradients: 0.12341672015369005\n",
      "l2 norm of weights: 3.206824223704054\n",
      "---------------------\n",
      "Iteration Number: 4824\n",
      "Loss: 14.122908793655041\n",
      "l2 norm of gradients: 0.12336907928218184\n",
      "l2 norm of weights: 3.2067913269351433\n",
      "---------------------\n",
      "Iteration Number: 4825\n",
      "Loss: 14.120872094319143\n",
      "l2 norm of gradients: 0.12332145737891902\n",
      "l2 norm of weights: 3.2067585298835577\n",
      "---------------------\n",
      "Iteration Number: 4826\n",
      "Loss: 14.118836926078798\n",
      "l2 norm of gradients: 0.12327385443569083\n",
      "l2 norm of weights: 3.2067258324773924\n",
      "---------------------\n",
      "Iteration Number: 4827\n",
      "Loss: 14.116803288016406\n",
      "l2 norm of gradients: 0.12322627044436224\n",
      "l2 norm of weights: 3.206693234644785\n",
      "---------------------\n",
      "Iteration Number: 4828\n",
      "Loss: 14.114771179214555\n",
      "l2 norm of gradients: 0.12317870539687346\n",
      "l2 norm of weights: 3.206660736313917\n",
      "---------------------\n",
      "Iteration Number: 4829\n",
      "Loss: 14.112740598756005\n",
      "l2 norm of gradients: 0.12313115928524011\n",
      "l2 norm of weights: 3.2066283374130116\n",
      "---------------------\n",
      "Iteration Number: 4830\n",
      "Loss: 14.110711545723674\n",
      "l2 norm of gradients: 0.12308363210155296\n",
      "l2 norm of weights: 3.206596037870336\n",
      "---------------------\n",
      "Iteration Number: 4831\n",
      "Loss: 14.108684019200627\n",
      "l2 norm of gradients: 0.12303612383797778\n",
      "l2 norm of weights: 3.2065638376142\n",
      "---------------------\n",
      "Iteration Number: 4832\n",
      "Loss: 14.106658018270133\n",
      "l2 norm of gradients: 0.12298863448675519\n",
      "l2 norm of weights: 3.206531736572956\n",
      "---------------------\n",
      "Iteration Number: 4833\n",
      "Loss: 14.104633542015572\n",
      "l2 norm of gradients: 0.12294116404020058\n",
      "l2 norm of weights: 3.206499734674999\n",
      "---------------------\n",
      "Iteration Number: 4834\n",
      "Loss: 14.102610589520523\n",
      "l2 norm of gradients: 0.12289371249070394\n",
      "l2 norm of weights: 3.2064678318487676\n",
      "---------------------\n",
      "Iteration Number: 4835\n",
      "Loss: 14.100589159868688\n",
      "l2 norm of gradients: 0.12284627983072972\n",
      "l2 norm of weights: 3.206436028022742\n",
      "---------------------\n",
      "Iteration Number: 4836\n",
      "Loss: 14.098569252143987\n",
      "l2 norm of gradients: 0.12279886605281658\n",
      "l2 norm of weights: 3.2064043231254464\n",
      "---------------------\n",
      "Iteration Number: 4837\n",
      "Loss: 14.096550865430398\n",
      "l2 norm of gradients: 0.12275147114957748\n",
      "l2 norm of weights: 3.2063727170854457\n",
      "---------------------\n",
      "Iteration Number: 4838\n",
      "Loss: 14.094533998812114\n",
      "l2 norm of gradients: 0.12270409511369938\n",
      "l2 norm of weights: 3.206341209831349\n",
      "---------------------\n",
      "Iteration Number: 4839\n",
      "Loss: 14.092518651373485\n",
      "l2 norm of gradients: 0.12265673793794304\n",
      "l2 norm of weights: 3.206309801291807\n",
      "---------------------\n",
      "Iteration Number: 4840\n",
      "Loss: 14.090504822198971\n",
      "l2 norm of gradients: 0.12260939961514296\n",
      "l2 norm of weights: 3.2062784913955142\n",
      "---------------------\n",
      "Iteration Number: 4841\n",
      "Loss: 14.088492510373175\n",
      "l2 norm of gradients: 0.12256208013820727\n",
      "l2 norm of weights: 3.2062472800712056\n",
      "---------------------\n",
      "Iteration Number: 4842\n",
      "Loss: 14.08648171498089\n",
      "l2 norm of gradients: 0.12251477950011759\n",
      "l2 norm of weights: 3.2062161672476597\n",
      "---------------------\n",
      "Iteration Number: 4843\n",
      "Loss: 14.084472435107012\n",
      "l2 norm of gradients: 0.1224674976939287\n",
      "l2 norm of weights: 3.2061851528536964\n",
      "---------------------\n",
      "Iteration Number: 4844\n",
      "Loss: 14.0824646698366\n",
      "l2 norm of gradients: 0.12242023471276861\n",
      "l2 norm of weights: 3.20615423681818\n",
      "---------------------\n",
      "Iteration Number: 4845\n",
      "Loss: 14.080458418254807\n",
      "l2 norm of gradients: 0.12237299054983834\n",
      "l2 norm of weights: 3.206123419070014\n",
      "---------------------\n",
      "Iteration Number: 4846\n",
      "Loss: 14.078453679446978\n",
      "l2 norm of gradients: 0.12232576519841168\n",
      "l2 norm of weights: 3.206092699538147\n",
      "---------------------\n",
      "Iteration Number: 4847\n",
      "Loss: 14.076450452498564\n",
      "l2 norm of gradients: 0.12227855865183518\n",
      "l2 norm of weights: 3.2060620781515676\n",
      "---------------------\n",
      "Iteration Number: 4848\n",
      "Loss: 14.074448736495127\n",
      "l2 norm of gradients: 0.1222313709035279\n",
      "l2 norm of weights: 3.2060315548393077\n",
      "---------------------\n",
      "Iteration Number: 4849\n",
      "Loss: 14.072448530522411\n",
      "l2 norm of gradients: 0.1221842019469813\n",
      "l2 norm of weights: 3.2060011295304407\n",
      "---------------------\n",
      "Iteration Number: 4850\n",
      "Loss: 14.070449833666258\n",
      "l2 norm of gradients: 0.12213705177575915\n",
      "l2 norm of weights: 3.2059708021540816\n",
      "---------------------\n",
      "Iteration Number: 4851\n",
      "Loss: 14.068452645012629\n",
      "l2 norm of gradients: 0.12208992038349731\n",
      "l2 norm of weights: 3.205940572639388\n",
      "---------------------\n",
      "Iteration Number: 4852\n",
      "Loss: 14.066456963647633\n",
      "l2 norm of gradients: 0.12204280776390346\n",
      "l2 norm of weights: 3.2059104409155585\n",
      "---------------------\n",
      "Iteration Number: 4853\n",
      "Loss: 14.064462788657503\n",
      "l2 norm of gradients: 0.12199571391075716\n",
      "l2 norm of weights: 3.2058804069118354\n",
      "---------------------\n",
      "Iteration Number: 4854\n",
      "Loss: 14.062470119128562\n",
      "l2 norm of gradients: 0.12194863881790965\n",
      "l2 norm of weights: 3.2058504705575017\n",
      "---------------------\n",
      "Iteration Number: 4855\n",
      "Loss: 14.060478954147275\n",
      "l2 norm of gradients: 0.12190158247928358\n",
      "l2 norm of weights: 3.2058206317818807\n",
      "---------------------\n",
      "Iteration Number: 4856\n",
      "Loss: 14.058489292800234\n",
      "l2 norm of gradients: 0.12185454488887298\n",
      "l2 norm of weights: 3.205790890514339\n",
      "---------------------\n",
      "Iteration Number: 4857\n",
      "Loss: 14.05650113417414\n",
      "l2 norm of gradients: 0.12180752604074306\n",
      "l2 norm of weights: 3.205761246684286\n",
      "---------------------\n",
      "Iteration Number: 4858\n",
      "Loss: 14.054514477355795\n",
      "l2 norm of gradients: 0.12176052592902997\n",
      "l2 norm of weights: 3.20573170022117\n",
      "---------------------\n",
      "Iteration Number: 4859\n",
      "Loss: 14.052529321432134\n",
      "l2 norm of gradients: 0.12171354454794085\n",
      "l2 norm of weights: 3.205702251054482\n",
      "---------------------\n",
      "Iteration Number: 4860\n",
      "Loss: 14.05054566549021\n",
      "l2 norm of gradients: 0.12166658189175343\n",
      "l2 norm of weights: 3.2056728991137553\n",
      "---------------------\n",
      "Iteration Number: 4861\n",
      "Loss: 14.048563508617127\n",
      "l2 norm of gradients: 0.12161963795481608\n",
      "l2 norm of weights: 3.2056436443285636\n",
      "---------------------\n",
      "Iteration Number: 4862\n",
      "Loss: 14.046582849900192\n",
      "l2 norm of gradients: 0.12157271273154749\n",
      "l2 norm of weights: 3.205614486628522\n",
      "---------------------\n",
      "Iteration Number: 4863\n",
      "Loss: 14.044603688426756\n",
      "l2 norm of gradients: 0.12152580621643674\n",
      "l2 norm of weights: 3.2055854259432883\n",
      "---------------------\n",
      "Iteration Number: 4864\n",
      "Loss: 14.04262602328425\n",
      "l2 norm of gradients: 0.1214789184040428\n",
      "l2 norm of weights: 3.205556462202559\n",
      "---------------------\n",
      "Iteration Number: 4865\n",
      "Loss: 14.040649853560295\n",
      "l2 norm of gradients: 0.12143204928899472\n",
      "l2 norm of weights: 3.2055275953360742\n",
      "---------------------\n",
      "Iteration Number: 4866\n",
      "Loss: 14.038675178342528\n",
      "l2 norm of gradients: 0.12138519886599121\n",
      "l2 norm of weights: 3.205498825273615\n",
      "---------------------\n",
      "Iteration Number: 4867\n",
      "Loss: 14.036701996718747\n",
      "l2 norm of gradients: 0.1213383671298006\n",
      "l2 norm of weights: 3.205470151945003\n",
      "---------------------\n",
      "Iteration Number: 4868\n",
      "Loss: 14.034730307776805\n",
      "l2 norm of gradients: 0.12129155407526075\n",
      "l2 norm of weights: 3.2054415752800995\n",
      "---------------------\n",
      "Iteration Number: 4869\n",
      "Loss: 14.032760110604675\n",
      "l2 norm of gradients: 0.12124475969727866\n",
      "l2 norm of weights: 3.2054130952088102\n",
      "---------------------\n",
      "Iteration Number: 4870\n",
      "Loss: 14.030791404290431\n",
      "l2 norm of gradients: 0.12119798399083064\n",
      "l2 norm of weights: 3.205384711661079\n",
      "---------------------\n",
      "Iteration Number: 4871\n",
      "Loss: 14.028824187922229\n",
      "l2 norm of gradients: 0.12115122695096177\n",
      "l2 norm of weights: 3.2053564245668915\n",
      "---------------------\n",
      "Iteration Number: 4872\n",
      "Loss: 14.026858460588324\n",
      "l2 norm of gradients: 0.12110448857278608\n",
      "l2 norm of weights: 3.205328233856275\n",
      "---------------------\n",
      "Iteration Number: 4873\n",
      "Loss: 14.024894221377016\n",
      "l2 norm of gradients: 0.12105776885148618\n",
      "l2 norm of weights: 3.2053001394592964\n",
      "---------------------\n",
      "Iteration Number: 4874\n",
      "Loss: 14.02293146937679\n",
      "l2 norm of gradients: 0.12101106778231319\n",
      "l2 norm of weights: 3.2052721413060645\n",
      "---------------------\n",
      "Iteration Number: 4875\n",
      "Loss: 14.020970203676121\n",
      "l2 norm of gradients: 0.12096438536058648\n",
      "l2 norm of weights: 3.2052442393267286\n",
      "---------------------\n",
      "Iteration Number: 4876\n",
      "Loss: 14.019010423363607\n",
      "l2 norm of gradients: 0.12091772158169359\n",
      "l2 norm of weights: 3.205216433451478\n",
      "---------------------\n",
      "Iteration Number: 4877\n",
      "Loss: 14.01705212752798\n",
      "l2 norm of gradients: 0.12087107644109012\n",
      "l2 norm of weights: 3.2051887236105436\n",
      "---------------------\n",
      "Iteration Number: 4878\n",
      "Loss: 14.015095315257978\n",
      "l2 norm of gradients: 0.12082444993429939\n",
      "l2 norm of weights: 3.205161109734196\n",
      "---------------------\n",
      "Iteration Number: 4879\n",
      "Loss: 14.013139985642438\n",
      "l2 norm of gradients: 0.1207778420569125\n",
      "l2 norm of weights: 3.205133591752747\n",
      "---------------------\n",
      "Iteration Number: 4880\n",
      "Loss: 14.011186137770327\n",
      "l2 norm of gradients: 0.12073125280458791\n",
      "l2 norm of weights: 3.205106169596548\n",
      "---------------------\n",
      "Iteration Number: 4881\n",
      "Loss: 14.009233770730635\n",
      "l2 norm of gradients: 0.12068468217305148\n",
      "l2 norm of weights: 3.205078843195993\n",
      "---------------------\n",
      "Iteration Number: 4882\n",
      "Loss: 14.007282883612449\n",
      "l2 norm of gradients: 0.12063813015809623\n",
      "l2 norm of weights: 3.205051612481514\n",
      "---------------------\n",
      "Iteration Number: 4883\n",
      "Loss: 14.00533347550492\n",
      "l2 norm of gradients: 0.1205915967555822\n",
      "l2 norm of weights: 3.205024477383584\n",
      "---------------------\n",
      "Iteration Number: 4884\n",
      "Loss: 14.003385545497299\n",
      "l2 norm of gradients: 0.12054508196143622\n",
      "l2 norm of weights: 3.2049974378327173\n",
      "---------------------\n",
      "Iteration Number: 4885\n",
      "Loss: 14.001439092678893\n",
      "l2 norm of gradients: 0.12049858577165173\n",
      "l2 norm of weights: 3.2049704937594674\n",
      "---------------------\n",
      "Iteration Number: 4886\n",
      "Loss: 13.99949411613908\n",
      "l2 norm of gradients: 0.12045210818228881\n",
      "l2 norm of weights: 3.204943645094428\n",
      "---------------------\n",
      "Iteration Number: 4887\n",
      "Loss: 13.997550614967306\n",
      "l2 norm of gradients: 0.12040564918947373\n",
      "l2 norm of weights: 3.2049168917682342\n",
      "---------------------\n",
      "Iteration Number: 4888\n",
      "Loss: 13.995608588253095\n",
      "l2 norm of gradients: 0.12035920878939899\n",
      "l2 norm of weights: 3.2048902337115592\n",
      "---------------------\n",
      "Iteration Number: 4889\n",
      "Loss: 13.993668035086012\n",
      "l2 norm of gradients: 0.12031278697832304\n",
      "l2 norm of weights: 3.204863670855118\n",
      "---------------------\n",
      "Iteration Number: 4890\n",
      "Loss: 13.991728954555736\n",
      "l2 norm of gradients: 0.12026638375257022\n",
      "l2 norm of weights: 3.204837203129665\n",
      "---------------------\n",
      "Iteration Number: 4891\n",
      "Loss: 13.98979134575196\n",
      "l2 norm of gradients: 0.12021999910853041\n",
      "l2 norm of weights: 3.204810830465993\n",
      "---------------------\n",
      "Iteration Number: 4892\n",
      "Loss: 13.987855207764484\n",
      "l2 norm of gradients: 0.12017363304265909\n",
      "l2 norm of weights: 3.204784552794938\n",
      "---------------------\n",
      "Iteration Number: 4893\n",
      "Loss: 13.985920539683118\n",
      "l2 norm of gradients: 0.12012728555147696\n",
      "l2 norm of weights: 3.2047583700473727\n",
      "---------------------\n",
      "Iteration Number: 4894\n",
      "Loss: 13.983987340597787\n",
      "l2 norm of gradients: 0.12008095663156985\n",
      "l2 norm of weights: 3.204732282154212\n",
      "---------------------\n",
      "Iteration Number: 4895\n",
      "Loss: 13.982055609598417\n",
      "l2 norm of gradients: 0.12003464627958871\n",
      "l2 norm of weights: 3.2047062890464075\n",
      "---------------------\n",
      "Iteration Number: 4896\n",
      "Loss: 13.980125345775074\n",
      "l2 norm of gradients: 0.11998835449224912\n",
      "l2 norm of weights: 3.204680390654954\n",
      "---------------------\n",
      "Iteration Number: 4897\n",
      "Loss: 13.978196548217767\n",
      "l2 norm of gradients: 0.11994208126633134\n",
      "l2 norm of weights: 3.2046545869108836\n",
      "---------------------\n",
      "Iteration Number: 4898\n",
      "Loss: 13.976269216016673\n",
      "l2 norm of gradients: 0.1198958265986801\n",
      "l2 norm of weights: 3.204628877745268\n",
      "---------------------\n",
      "Iteration Number: 4899\n",
      "Loss: 13.974343348261943\n",
      "l2 norm of gradients: 0.11984959048620444\n",
      "l2 norm of weights: 3.2046032630892207\n",
      "---------------------\n",
      "Iteration Number: 4900\n",
      "Loss: 13.972418944043827\n",
      "l2 norm of gradients: 0.11980337292587742\n",
      "l2 norm of weights: 3.2045777428738913\n",
      "---------------------\n",
      "Iteration Number: 4901\n",
      "Loss: 13.970496002452592\n",
      "l2 norm of gradients: 0.11975717391473614\n",
      "l2 norm of weights: 3.2045523170304717\n",
      "---------------------\n",
      "Iteration Number: 4902\n",
      "Loss: 13.968574522578571\n",
      "l2 norm of gradients: 0.11971099344988134\n",
      "l2 norm of weights: 3.204526985490191\n",
      "---------------------\n",
      "Iteration Number: 4903\n",
      "Loss: 13.966654503512153\n",
      "l2 norm of gradients: 0.11966483152847747\n",
      "l2 norm of weights: 3.2045017481843194\n",
      "---------------------\n",
      "Iteration Number: 4904\n",
      "Loss: 13.964735944343781\n",
      "l2 norm of gradients: 0.11961868814775237\n",
      "l2 norm of weights: 3.204476605044165\n",
      "---------------------\n",
      "Iteration Number: 4905\n",
      "Loss: 13.962818844163897\n",
      "l2 norm of gradients: 0.11957256330499699\n",
      "l2 norm of weights: 3.2044515560010765\n",
      "---------------------\n",
      "Iteration Number: 4906\n",
      "Loss: 13.960903202063049\n",
      "l2 norm of gradients: 0.11952645699756555\n",
      "l2 norm of weights: 3.20442660098644\n",
      "---------------------\n",
      "Iteration Number: 4907\n",
      "Loss: 13.958989017131772\n",
      "l2 norm of gradients: 0.11948036922287496\n",
      "l2 norm of weights: 3.2044017399316824\n",
      "---------------------\n",
      "Iteration Number: 4908\n",
      "Loss: 13.957076288460678\n",
      "l2 norm of gradients: 0.11943429997840502\n",
      "l2 norm of weights: 3.204376972768268\n",
      "---------------------\n",
      "Iteration Number: 4909\n",
      "Loss: 13.955165015140425\n",
      "l2 norm of gradients: 0.11938824926169796\n",
      "l2 norm of weights: 3.2043522994277014\n",
      "---------------------\n",
      "Iteration Number: 4910\n",
      "Loss: 13.953255196261697\n",
      "l2 norm of gradients: 0.11934221707035836\n",
      "l2 norm of weights: 3.2043277198415256\n",
      "---------------------\n",
      "Iteration Number: 4911\n",
      "Loss: 13.951346830915202\n",
      "l2 norm of gradients: 0.11929620340205306\n",
      "l2 norm of weights: 3.2043032339413235\n",
      "---------------------\n",
      "Iteration Number: 4912\n",
      "Loss: 13.949439918191716\n",
      "l2 norm of gradients: 0.11925020825451088\n",
      "l2 norm of weights: 3.2042788416587142\n",
      "---------------------\n",
      "Iteration Number: 4913\n",
      "Loss: 13.947534457182035\n",
      "l2 norm of gradients: 0.11920423162552245\n",
      "l2 norm of weights: 3.204254542925358\n",
      "---------------------\n",
      "Iteration Number: 4914\n",
      "Loss: 13.945630446976992\n",
      "l2 norm of gradients: 0.11915827351294002\n",
      "l2 norm of weights: 3.204230337672954\n",
      "---------------------\n",
      "Iteration Number: 4915\n",
      "Loss: 13.94372788666745\n",
      "l2 norm of gradients: 0.11911233391467745\n",
      "l2 norm of weights: 3.2042062258332384\n",
      "---------------------\n",
      "Iteration Number: 4916\n",
      "Loss: 13.94182677534429\n",
      "l2 norm of gradients: 0.11906641282870971\n",
      "l2 norm of weights: 3.2041822073379866\n",
      "---------------------\n",
      "Iteration Number: 4917\n",
      "Loss: 13.939927112098488\n",
      "l2 norm of gradients: 0.11902051025307304\n",
      "l2 norm of weights: 3.204158282119014\n",
      "---------------------\n",
      "Iteration Number: 4918\n",
      "Loss: 13.93802889602096\n",
      "l2 norm of gradients: 0.11897462618586459\n",
      "l2 norm of weights: 3.2041344501081714\n",
      "---------------------\n",
      "Iteration Number: 4919\n",
      "Loss: 13.936132126202743\n",
      "l2 norm of gradients: 0.1189287606252422\n",
      "l2 norm of weights: 3.2041107112373517\n",
      "---------------------\n",
      "Iteration Number: 4920\n",
      "Loss: 13.934236801734816\n",
      "l2 norm of gradients: 0.11888291356942432\n",
      "l2 norm of weights: 3.204087065438484\n",
      "---------------------\n",
      "Iteration Number: 4921\n",
      "Loss: 13.932342921708248\n",
      "l2 norm of gradients: 0.11883708501668988\n",
      "l2 norm of weights: 3.204063512643535\n",
      "---------------------\n",
      "Iteration Number: 4922\n",
      "Loss: 13.930450485214115\n",
      "l2 norm of gradients: 0.11879127496537799\n",
      "l2 norm of weights: 3.2040400527845123\n",
      "---------------------\n",
      "Iteration Number: 4923\n",
      "Loss: 13.928559491343497\n",
      "l2 norm of gradients: 0.11874548341388769\n",
      "l2 norm of weights: 3.20401668579346\n",
      "---------------------\n",
      "Iteration Number: 4924\n",
      "Loss: 13.92666993918754\n",
      "l2 norm of gradients: 0.1186997103606781\n",
      "l2 norm of weights: 3.20399341160246\n",
      "---------------------\n",
      "Iteration Number: 4925\n",
      "Loss: 13.924781827837377\n",
      "l2 norm of gradients: 0.1186539558042678\n",
      "l2 norm of weights: 3.2039702301436344\n",
      "---------------------\n",
      "Iteration Number: 4926\n",
      "Loss: 13.922895156384172\n",
      "l2 norm of gradients: 0.118608219743235\n",
      "l2 norm of weights: 3.2039471413491403\n",
      "---------------------\n",
      "Iteration Number: 4927\n",
      "Loss: 13.921009923919135\n",
      "l2 norm of gradients: 0.11856250217621718\n",
      "l2 norm of weights: 3.2039241451511757\n",
      "---------------------\n",
      "Iteration Number: 4928\n",
      "Loss: 13.91912612953345\n",
      "l2 norm of gradients: 0.11851680310191105\n",
      "l2 norm of weights: 3.2039012414819754\n",
      "---------------------\n",
      "Iteration Number: 4929\n",
      "Loss: 13.917243772318358\n",
      "l2 norm of gradients: 0.11847112251907209\n",
      "l2 norm of weights: 3.2038784302738117\n",
      "---------------------\n",
      "Iteration Number: 4930\n",
      "Loss: 13.915362851365106\n",
      "l2 norm of gradients: 0.11842546042651476\n",
      "l2 norm of weights: 3.203855711458995\n",
      "---------------------\n",
      "Iteration Number: 4931\n",
      "Loss: 13.91348336576495\n",
      "l2 norm of gradients: 0.118379816823112\n",
      "l2 norm of weights: 3.2038330849698746\n",
      "---------------------\n",
      "Iteration Number: 4932\n",
      "Loss: 13.911605314609167\n",
      "l2 norm of gradients: 0.11833419170779506\n",
      "l2 norm of weights: 3.2038105507388357\n",
      "---------------------\n",
      "Iteration Number: 4933\n",
      "Loss: 13.909728696989065\n",
      "l2 norm of gradients: 0.11828858507955363\n",
      "l2 norm of weights: 3.2037881086983027\n",
      "---------------------\n",
      "Iteration Number: 4934\n",
      "Loss: 13.907853511995949\n",
      "l2 norm of gradients: 0.11824299693743533\n",
      "l2 norm of weights: 3.2037657587807367\n",
      "---------------------\n",
      "Iteration Number: 4935\n",
      "Loss: 13.905979758721125\n",
      "l2 norm of gradients: 0.11819742728054558\n",
      "l2 norm of weights: 3.2037435009186366\n",
      "---------------------\n",
      "Iteration Number: 4936\n",
      "Loss: 13.904107436255938\n",
      "l2 norm of gradients: 0.11815187610804757\n",
      "l2 norm of weights: 3.2037213350445395\n",
      "---------------------\n",
      "Iteration Number: 4937\n",
      "Loss: 13.902236543691725\n",
      "l2 norm of gradients: 0.11810634341916201\n",
      "l2 norm of weights: 3.2036992610910193\n",
      "---------------------\n",
      "Iteration Number: 4938\n",
      "Loss: 13.900367080119857\n",
      "l2 norm of gradients: 0.11806082921316682\n",
      "l2 norm of weights: 3.203677278990687\n",
      "---------------------\n",
      "Iteration Number: 4939\n",
      "Loss: 13.89849904463167\n",
      "l2 norm of gradients: 0.11801533348939702\n",
      "l2 norm of weights: 3.2036553886761916\n",
      "---------------------\n",
      "Iteration Number: 4940\n",
      "Loss: 13.896632436318558\n",
      "l2 norm of gradients: 0.11796985624724471\n",
      "l2 norm of weights: 3.2036335900802198\n",
      "---------------------\n",
      "Iteration Number: 4941\n",
      "Loss: 13.89476725427189\n",
      "l2 norm of gradients: 0.11792439748615859\n",
      "l2 norm of weights: 3.203611883135494\n",
      "---------------------\n",
      "Iteration Number: 4942\n",
      "Loss: 13.89290349758306\n",
      "l2 norm of gradients: 0.11787895720564401\n",
      "l2 norm of weights: 3.2035902677747754\n",
      "---------------------\n",
      "Iteration Number: 4943\n",
      "Loss: 13.89104116534345\n",
      "l2 norm of gradients: 0.11783353540526273\n",
      "l2 norm of weights: 3.2035687439308616\n",
      "---------------------\n",
      "Iteration Number: 4944\n",
      "Loss: 13.889180256644453\n",
      "l2 norm of gradients: 0.1177881320846326\n",
      "l2 norm of weights: 3.2035473115365867\n",
      "---------------------\n",
      "Iteration Number: 4945\n",
      "Loss: 13.887320770577503\n",
      "l2 norm of gradients: 0.11774274724342756\n",
      "l2 norm of weights: 3.203525970524824\n",
      "---------------------\n",
      "Iteration Number: 4946\n",
      "Loss: 13.885462706233957\n",
      "l2 norm of gradients: 0.11769738088137738\n",
      "l2 norm of weights: 3.2035047208284806\n",
      "---------------------\n",
      "Iteration Number: 4947\n",
      "Loss: 13.883606062705258\n",
      "l2 norm of gradients: 0.1176520329982674\n",
      "l2 norm of weights: 3.203483562380503\n",
      "---------------------\n",
      "Iteration Number: 4948\n",
      "Loss: 13.881750839082795\n",
      "l2 norm of gradients: 0.11760670359393856\n",
      "l2 norm of weights: 3.203462495113874\n",
      "---------------------\n",
      "Iteration Number: 4949\n",
      "Loss: 13.879897034457981\n",
      "l2 norm of gradients: 0.11756139266828682\n",
      "l2 norm of weights: 3.2034415189616126\n",
      "---------------------\n",
      "Iteration Number: 4950\n",
      "Loss: 13.878044647922238\n",
      "l2 norm of gradients: 0.11751610022126341\n",
      "l2 norm of weights: 3.2034206338567746\n",
      "---------------------\n",
      "Iteration Number: 4951\n",
      "Loss: 13.876193678566935\n",
      "l2 norm of gradients: 0.1174708262528744\n",
      "l2 norm of weights: 3.203399839732453\n",
      "---------------------\n",
      "Iteration Number: 4952\n",
      "Loss: 13.874344125483502\n",
      "l2 norm of gradients: 0.11742557076318058\n",
      "l2 norm of weights: 3.2033791365217774\n",
      "---------------------\n",
      "Iteration Number: 4953\n",
      "Loss: 13.87249598776334\n",
      "l2 norm of gradients: 0.11738033375229723\n",
      "l2 norm of weights: 3.2033585241579132\n",
      "---------------------\n",
      "Iteration Number: 4954\n",
      "Loss: 13.87064926449784\n",
      "l2 norm of gradients: 0.1173351152203939\n",
      "l2 norm of weights: 3.2033380025740636\n",
      "---------------------\n",
      "Iteration Number: 4955\n",
      "Loss: 13.86880395477841\n",
      "l2 norm of gradients: 0.1172899151676944\n",
      "l2 norm of weights: 3.203317571703467\n",
      "---------------------\n",
      "Iteration Number: 4956\n",
      "Loss: 13.866960057696431\n",
      "l2 norm of gradients: 0.11724473359447643\n",
      "l2 norm of weights: 3.2032972314793993\n",
      "---------------------\n",
      "Iteration Number: 4957\n",
      "Loss: 13.865117572343284\n",
      "l2 norm of gradients: 0.1171995705010714\n",
      "l2 norm of weights: 3.203276981835172\n",
      "---------------------\n",
      "Iteration Number: 4958\n",
      "Loss: 13.863276497810356\n",
      "l2 norm of gradients: 0.11715442588786439\n",
      "l2 norm of weights: 3.2032568227041325\n",
      "---------------------\n",
      "Iteration Number: 4959\n",
      "Loss: 13.861436833188996\n",
      "l2 norm of gradients: 0.1171092997552938\n",
      "l2 norm of weights: 3.2032367540196662\n",
      "---------------------\n",
      "Iteration Number: 4960\n",
      "Loss: 13.859598577570573\n",
      "l2 norm of gradients: 0.11706419210385129\n",
      "l2 norm of weights: 3.203216775715192\n",
      "---------------------\n",
      "Iteration Number: 4961\n",
      "Loss: 13.85776173004647\n",
      "l2 norm of gradients: 0.11701910293408142\n",
      "l2 norm of weights: 3.203196887724167\n",
      "---------------------\n",
      "Iteration Number: 4962\n",
      "Loss: 13.855926289707991\n",
      "l2 norm of gradients: 0.11697403224658169\n",
      "l2 norm of weights: 3.2031770899800853\n",
      "---------------------\n",
      "Iteration Number: 4963\n",
      "Loss: 13.854092255646476\n",
      "l2 norm of gradients: 0.11692898004200213\n",
      "l2 norm of weights: 3.203157382416473\n",
      "---------------------\n",
      "Iteration Number: 4964\n",
      "Loss: 13.852259626953273\n",
      "l2 norm of gradients: 0.11688394632104525\n",
      "l2 norm of weights: 3.203137764966897\n",
      "---------------------\n",
      "Iteration Number: 4965\n",
      "Loss: 13.850428402719668\n",
      "l2 norm of gradients: 0.1168389310844658\n",
      "l2 norm of weights: 3.203118237564956\n",
      "---------------------\n",
      "Iteration Number: 4966\n",
      "Loss: 13.848598582036972\n",
      "l2 norm of gradients: 0.11679393433307063\n",
      "l2 norm of weights: 3.2030988001442866\n",
      "---------------------\n",
      "Iteration Number: 4967\n",
      "Loss: 13.846770163996469\n",
      "l2 norm of gradients: 0.11674895606771837\n",
      "l2 norm of weights: 3.203079452638561\n",
      "---------------------\n",
      "Iteration Number: 4968\n",
      "Loss: 13.84494314768942\n",
      "l2 norm of gradients: 0.11670399628931936\n",
      "l2 norm of weights: 3.203060194981488\n",
      "---------------------\n",
      "Iteration Number: 4969\n",
      "Loss: 13.843117532207094\n",
      "l2 norm of gradients: 0.1166590549988355\n",
      "l2 norm of weights: 3.203041027106809\n",
      "---------------------\n",
      "Iteration Number: 4970\n",
      "Loss: 13.841293316640744\n",
      "l2 norm of gradients: 0.11661413219727988\n",
      "l2 norm of weights: 3.2030219489483045\n",
      "---------------------\n",
      "Iteration Number: 4971\n",
      "Loss: 13.839470500081598\n",
      "l2 norm of gradients: 0.11656922788571675\n",
      "l2 norm of weights: 3.2030029604397887\n",
      "---------------------\n",
      "Iteration Number: 4972\n",
      "Loss: 13.837649081620823\n",
      "l2 norm of gradients: 0.11652434206526127\n",
      "l2 norm of weights: 3.2029840615151115\n",
      "---------------------\n",
      "Iteration Number: 4973\n",
      "Loss: 13.835829060349665\n",
      "l2 norm of gradients: 0.11647947473707929\n",
      "l2 norm of weights: 3.2029652521081586\n",
      "---------------------\n",
      "Iteration Number: 4974\n",
      "Loss: 13.83401043535929\n",
      "l2 norm of gradients: 0.11643462590238722\n",
      "l2 norm of weights: 3.202946532152851\n",
      "---------------------\n",
      "Iteration Number: 4975\n",
      "Loss: 13.832193205740833\n",
      "l2 norm of gradients: 0.11638979556245181\n",
      "l2 norm of weights: 3.2029279015831444\n",
      "---------------------\n",
      "Iteration Number: 4976\n",
      "Loss: 13.83037737058548\n",
      "l2 norm of gradients: 0.11634498371858991\n",
      "l2 norm of weights: 3.202909360333031\n",
      "---------------------\n",
      "Iteration Number: 4977\n",
      "Loss: 13.828562928984313\n",
      "l2 norm of gradients: 0.11630019037216842\n",
      "l2 norm of weights: 3.2028909083365362\n",
      "---------------------\n",
      "Iteration Number: 4978\n",
      "Loss: 13.826749880028435\n",
      "l2 norm of gradients: 0.11625541552460388\n",
      "l2 norm of weights: 3.2028725455277227\n",
      "---------------------\n",
      "Iteration Number: 4979\n",
      "Loss: 13.824938222808955\n",
      "l2 norm of gradients: 0.11621065917736245\n",
      "l2 norm of weights: 3.2028542718406876\n",
      "---------------------\n",
      "Iteration Number: 4980\n",
      "Loss: 13.823127956416922\n",
      "l2 norm of gradients: 0.11616592133195974\n",
      "l2 norm of weights: 3.202836087209562\n",
      "---------------------\n",
      "Iteration Number: 4981\n",
      "Loss: 13.821319079943358\n",
      "l2 norm of gradients: 0.1161212019899604\n",
      "l2 norm of weights: 3.202817991568513\n",
      "---------------------\n",
      "Iteration Number: 4982\n",
      "Loss: 13.819511592479296\n",
      "l2 norm of gradients: 0.1160765011529782\n",
      "l2 norm of weights: 3.202799984851743\n",
      "---------------------\n",
      "Iteration Number: 4983\n",
      "Loss: 13.81770549311573\n",
      "l2 norm of gradients: 0.11603181882267567\n",
      "l2 norm of weights: 3.2027820669934877\n",
      "---------------------\n",
      "Iteration Number: 4984\n",
      "Loss: 13.815900780943638\n",
      "l2 norm of gradients: 0.11598715500076387\n",
      "l2 norm of weights: 3.2027642379280192\n",
      "---------------------\n",
      "Iteration Number: 4985\n",
      "Loss: 13.814097455053977\n",
      "l2 norm of gradients: 0.11594250968900237\n",
      "l2 norm of weights: 3.202746497589643\n",
      "---------------------\n",
      "Iteration Number: 4986\n",
      "Loss: 13.81229551453764\n",
      "l2 norm of gradients: 0.11589788288919892\n",
      "l2 norm of weights: 3.202728845912701\n",
      "---------------------\n",
      "Iteration Number: 4987\n",
      "Loss: 13.810494958485545\n",
      "l2 norm of gradients: 0.1158532746032093\n",
      "l2 norm of weights: 3.2027112828315674\n",
      "---------------------\n",
      "Iteration Number: 4988\n",
      "Loss: 13.808695785988554\n",
      "l2 norm of gradients: 0.11580868483293709\n",
      "l2 norm of weights: 3.202693808280653\n",
      "---------------------\n",
      "Iteration Number: 4989\n",
      "Loss: 13.806897996137538\n",
      "l2 norm of gradients: 0.11576411358033356\n",
      "l2 norm of weights: 3.202676422194402\n",
      "---------------------\n",
      "Iteration Number: 4990\n",
      "Loss: 13.805101588023323\n",
      "l2 norm of gradients: 0.11571956084739739\n",
      "l2 norm of weights: 3.2026591245072935\n",
      "---------------------\n",
      "Iteration Number: 4991\n",
      "Loss: 13.80330656073667\n",
      "l2 norm of gradients: 0.11567502663617452\n",
      "l2 norm of weights: 3.2026419151538406\n",
      "---------------------\n",
      "Iteration Number: 4992\n",
      "Loss: 13.801512913368377\n",
      "l2 norm of gradients: 0.11563051094875786\n",
      "l2 norm of weights: 3.202624794068591\n",
      "---------------------\n",
      "Iteration Number: 4993\n",
      "Loss: 13.799720645009184\n",
      "l2 norm of gradients: 0.11558601378728738\n",
      "l2 norm of weights: 3.2026077611861266\n",
      "---------------------\n",
      "Iteration Number: 4994\n",
      "Loss: 13.797929754749802\n",
      "l2 norm of gradients: 0.11554153515394947\n",
      "l2 norm of weights: 3.202590816441064\n",
      "---------------------\n",
      "Iteration Number: 4995\n",
      "Loss: 13.79614024168092\n",
      "l2 norm of gradients: 0.11549707505097717\n",
      "l2 norm of weights: 3.2025739597680536\n",
      "---------------------\n",
      "Iteration Number: 4996\n",
      "Loss: 13.794352104893184\n",
      "l2 norm of gradients: 0.11545263348064977\n",
      "l2 norm of weights: 3.202557191101779\n",
      "---------------------\n",
      "Iteration Number: 4997\n",
      "Loss: 13.792565343477253\n",
      "l2 norm of gradients: 0.11540821044529258\n",
      "l2 norm of weights: 3.2025405103769584\n",
      "---------------------\n",
      "Iteration Number: 4998\n",
      "Loss: 13.790779956523682\n",
      "l2 norm of gradients: 0.11536380594727677\n",
      "l2 norm of weights: 3.202523917528345\n",
      "---------------------\n",
      "Iteration Number: 4999\n",
      "Loss: 13.78899594312308\n",
      "l2 norm of gradients: 0.11531941998901934\n",
      "l2 norm of weights: 3.202507412490725\n",
      "---------------------\n",
      "Iteration Number: 5000\n",
      "Loss: 13.787213302365938\n",
      "l2 norm of gradients: 0.11527505257298261\n",
      "l2 norm of weights: 3.2024909951989184\n",
      "---------------------\n",
      "Iteration Number: 5001\n",
      "Loss: 13.78543203334282\n",
      "l2 norm of gradients: 0.11523070370167438\n",
      "l2 norm of weights: 3.202474665587779\n",
      "---------------------\n",
      "Iteration Number: 5002\n",
      "Loss: 13.783652135144155\n",
      "l2 norm of gradients: 0.11518637337764738\n",
      "l2 norm of weights: 3.202458423592195\n",
      "---------------------\n",
      "Iteration Number: 5003\n",
      "Loss: 13.78187360686043\n",
      "l2 norm of gradients: 0.11514206160349942\n",
      "l2 norm of weights: 3.2024422691470877\n",
      "---------------------\n",
      "Iteration Number: 5004\n",
      "Loss: 13.780096447582027\n",
      "l2 norm of gradients: 0.11509776838187287\n",
      "l2 norm of weights: 3.2024262021874117\n",
      "---------------------\n",
      "Iteration Number: 5005\n",
      "Loss: 13.778320656399327\n",
      "l2 norm of gradients: 0.11505349371545469\n",
      "l2 norm of weights: 3.202410222648156\n",
      "---------------------\n",
      "Iteration Number: 5006\n",
      "Loss: 13.776546232402714\n",
      "l2 norm of gradients: 0.11500923760697618\n",
      "l2 norm of weights: 3.2023943304643416\n",
      "---------------------\n",
      "Iteration Number: 5007\n",
      "Loss: 13.77477317468245\n",
      "l2 norm of gradients: 0.11496500005921266\n",
      "l2 norm of weights: 3.2023785255710258\n",
      "---------------------\n",
      "Iteration Number: 5008\n",
      "Loss: 13.773001482328867\n",
      "l2 norm of gradients: 0.11492078107498353\n",
      "l2 norm of weights: 3.202362807903296\n",
      "---------------------\n",
      "Iteration Number: 5009\n",
      "Loss: 13.771231154432193\n",
      "l2 norm of gradients: 0.11487658065715182\n",
      "l2 norm of weights: 3.202347177396275\n",
      "---------------------\n",
      "Iteration Number: 5010\n",
      "Loss: 13.769462190082628\n",
      "l2 norm of gradients: 0.11483239880862414\n",
      "l2 norm of weights: 3.2023316339851196\n",
      "---------------------\n",
      "Iteration Number: 5011\n",
      "Loss: 13.767694588370379\n",
      "l2 norm of gradients: 0.1147882355323504\n",
      "l2 norm of weights: 3.2023161776050157\n",
      "---------------------\n",
      "Iteration Number: 5012\n",
      "Loss: 13.76592834838557\n",
      "l2 norm of gradients: 0.11474409083132364\n",
      "l2 norm of weights: 3.2023008081911875\n",
      "---------------------\n",
      "Iteration Number: 5013\n",
      "Loss: 13.764163469218326\n",
      "l2 norm of gradients: 0.11469996470857995\n",
      "l2 norm of weights: 3.202285525678889\n",
      "---------------------\n",
      "Iteration Number: 5014\n",
      "Loss: 13.762399949958722\n",
      "l2 norm of gradients: 0.11465585716719805\n",
      "l2 norm of weights: 3.2022703300034085\n",
      "---------------------\n",
      "Iteration Number: 5015\n",
      "Loss: 13.760637789696801\n",
      "l2 norm of gradients: 0.11461176821029928\n",
      "l2 norm of weights: 3.2022552211000668\n",
      "---------------------\n",
      "Iteration Number: 5016\n",
      "Loss: 13.758876987522559\n",
      "l2 norm of gradients: 0.11456769784104728\n",
      "l2 norm of weights: 3.2022401989042177\n",
      "---------------------\n",
      "Iteration Number: 5017\n",
      "Loss: 13.757117542525993\n",
      "l2 norm of gradients: 0.11452364606264792\n",
      "l2 norm of weights: 3.2022252633512482\n",
      "---------------------\n",
      "Iteration Number: 5018\n",
      "Loss: 13.755359453796986\n",
      "l2 norm of gradients: 0.11447961287834897\n",
      "l2 norm of weights: 3.202210414376578\n",
      "---------------------\n",
      "Iteration Number: 5019\n",
      "Loss: 13.753602720425498\n",
      "l2 norm of gradients: 0.11443559829143998\n",
      "l2 norm of weights: 3.2021956519156594\n",
      "---------------------\n",
      "Iteration Number: 5020\n",
      "Loss: 13.751847341501332\n",
      "l2 norm of gradients: 0.11439160230525211\n",
      "l2 norm of weights: 3.202180975903976\n",
      "---------------------\n",
      "Iteration Number: 5021\n",
      "Loss: 13.750093316114349\n",
      "l2 norm of gradients: 0.11434762492315781\n",
      "l2 norm of weights: 3.2021663862770477\n",
      "---------------------\n",
      "Iteration Number: 5022\n",
      "Loss: 13.748340643354314\n",
      "l2 norm of gradients: 0.11430366614857074\n",
      "l2 norm of weights: 3.202151882970423\n",
      "---------------------\n",
      "Iteration Number: 5023\n",
      "Loss: 13.746589322310992\n",
      "l2 norm of gradients: 0.11425972598494553\n",
      "l2 norm of weights: 3.202137465919685\n",
      "---------------------\n",
      "Iteration Number: 5024\n",
      "Loss: 13.74483935207409\n",
      "l2 norm of gradients: 0.11421580443577761\n",
      "l2 norm of weights: 3.202123135060449\n",
      "---------------------\n",
      "Iteration Number: 5025\n",
      "Loss: 13.743090731733284\n",
      "l2 norm of gradients: 0.11417190150460295\n",
      "l2 norm of weights: 3.2021088903283617\n",
      "---------------------\n",
      "Iteration Number: 5026\n",
      "Loss: 13.741343460378214\n",
      "l2 norm of gradients: 0.11412801719499786\n",
      "l2 norm of weights: 3.2020947316591037\n",
      "---------------------\n",
      "Iteration Number: 5027\n",
      "Loss: 13.739597537098476\n",
      "l2 norm of gradients: 0.11408415151057898\n",
      "l2 norm of weights: 3.2020806589883875\n",
      "---------------------\n",
      "Iteration Number: 5028\n",
      "Loss: 13.737852960983613\n",
      "l2 norm of gradients: 0.11404030445500277\n",
      "l2 norm of weights: 3.202066672251956\n",
      "---------------------\n",
      "Iteration Number: 5029\n",
      "Loss: 13.736109731123166\n",
      "l2 norm of gradients: 0.11399647603196553\n",
      "l2 norm of weights: 3.202052771385587\n",
      "---------------------\n",
      "Iteration Number: 5030\n",
      "Loss: 13.734367846606613\n",
      "l2 norm of gradients: 0.11395266624520319\n",
      "l2 norm of weights: 3.2020389563250884\n",
      "---------------------\n",
      "Iteration Number: 5031\n",
      "Loss: 13.732627306523405\n",
      "l2 norm of gradients: 0.11390887509849106\n",
      "l2 norm of weights: 3.2020252270063003\n",
      "---------------------\n",
      "Iteration Number: 5032\n",
      "Loss: 13.730888109962935\n",
      "l2 norm of gradients: 0.11386510259564353\n",
      "l2 norm of weights: 3.202011583365096\n",
      "---------------------\n",
      "Iteration Number: 5033\n",
      "Loss: 13.729150256014586\n",
      "l2 norm of gradients: 0.11382134874051414\n",
      "l2 norm of weights: 3.2019980253373794\n",
      "---------------------\n",
      "Iteration Number: 5034\n",
      "Loss: 13.727413743767675\n",
      "l2 norm of gradients: 0.11377761353699507\n",
      "l2 norm of weights: 3.2019845528590873\n",
      "---------------------\n",
      "Iteration Number: 5035\n",
      "Loss: 13.725678572311494\n",
      "l2 norm of gradients: 0.11373389698901727\n",
      "l2 norm of weights: 3.2019711658661874\n",
      "---------------------\n",
      "Iteration Number: 5036\n",
      "Loss: 13.723944740735282\n",
      "l2 norm of gradients: 0.11369019910054994\n",
      "l2 norm of weights: 3.20195786429468\n",
      "---------------------\n",
      "Iteration Number: 5037\n",
      "Loss: 13.72221224812827\n",
      "l2 norm of gradients: 0.11364651987560047\n",
      "l2 norm of weights: 3.201944648080596\n",
      "---------------------\n",
      "Iteration Number: 5038\n",
      "Loss: 13.720481093579615\n",
      "l2 norm of gradients: 0.11360285931821437\n",
      "l2 norm of weights: 3.201931517159999\n",
      "---------------------\n",
      "Iteration Number: 5039\n",
      "Loss: 13.718751276178448\n",
      "l2 norm of gradients: 0.11355921743247487\n",
      "l2 norm of weights: 3.2019184714689835\n",
      "---------------------\n",
      "Iteration Number: 5040\n",
      "Loss: 13.717022795013865\n",
      "l2 norm of gradients: 0.11351559422250276\n",
      "l2 norm of weights: 3.2019055109436754\n",
      "---------------------\n",
      "Iteration Number: 5041\n",
      "Loss: 13.71529564917492\n",
      "l2 norm of gradients: 0.11347198969245631\n",
      "l2 norm of weights: 3.201892635520233\n",
      "---------------------\n",
      "Iteration Number: 5042\n",
      "Loss: 13.7135698377506\n",
      "l2 norm of gradients: 0.11342840384653097\n",
      "l2 norm of weights: 3.2018798451348447\n",
      "---------------------\n",
      "Iteration Number: 5043\n",
      "Loss: 13.711845359829898\n",
      "l2 norm of gradients: 0.11338483668895914\n",
      "l2 norm of weights: 3.2018671397237313\n",
      "---------------------\n",
      "Iteration Number: 5044\n",
      "Loss: 13.710122214501741\n",
      "l2 norm of gradients: 0.11334128822401011\n",
      "l2 norm of weights: 3.2018545192231445\n",
      "---------------------\n",
      "Iteration Number: 5045\n",
      "Loss: 13.70840040085502\n",
      "l2 norm of gradients: 0.11329775845598972\n",
      "l2 norm of weights: 3.201841983569366\n",
      "---------------------\n",
      "Iteration Number: 5046\n",
      "Loss: 13.706679917978587\n",
      "l2 norm of gradients: 0.11325424738924017\n",
      "l2 norm of weights: 3.2018295326987114\n",
      "---------------------\n",
      "Iteration Number: 5047\n",
      "Loss: 13.70496076496124\n",
      "l2 norm of gradients: 0.11321075502813994\n",
      "l2 norm of weights: 3.2018171665475244\n",
      "---------------------\n",
      "Iteration Number: 5048\n",
      "Loss: 13.70324294089177\n",
      "l2 norm of gradients: 0.11316728137710355\n",
      "l2 norm of weights: 3.201804885052182\n",
      "---------------------\n",
      "Iteration Number: 5049\n",
      "Loss: 13.701526444858905\n",
      "l2 norm of gradients: 0.11312382644058117\n",
      "l2 norm of weights: 3.201792688149091\n",
      "---------------------\n",
      "Iteration Number: 5050\n",
      "Loss: 13.699811275951308\n",
      "l2 norm of gradients: 0.11308039022305873\n",
      "l2 norm of weights: 3.2017805757746887\n",
      "---------------------\n",
      "Iteration Number: 5051\n",
      "Loss: 13.69809743325767\n",
      "l2 norm of gradients: 0.11303697272905747\n",
      "l2 norm of weights: 3.2017685478654454\n",
      "---------------------\n",
      "Iteration Number: 5052\n",
      "Loss: 13.696384915866581\n",
      "l2 norm of gradients: 0.11299357396313393\n",
      "l2 norm of weights: 3.2017566043578594\n",
      "---------------------\n",
      "Iteration Number: 5053\n",
      "Loss: 13.694673722866604\n",
      "l2 norm of gradients: 0.11295019392987955\n",
      "l2 norm of weights: 3.2017447451884617\n",
      "---------------------\n",
      "Iteration Number: 5054\n",
      "Loss: 13.692963853346274\n",
      "l2 norm of gradients: 0.11290683263392062\n",
      "l2 norm of weights: 3.201732970293813\n",
      "---------------------\n",
      "Iteration Number: 5055\n",
      "Loss: 13.691255306394103\n",
      "l2 norm of gradients: 0.11286349007991808\n",
      "l2 norm of weights: 3.2017212796105055\n",
      "---------------------\n",
      "Iteration Number: 5056\n",
      "Loss: 13.68954808109849\n",
      "l2 norm of gradients: 0.1128201662725672\n",
      "l2 norm of weights: 3.201709673075161\n",
      "---------------------\n",
      "Iteration Number: 5057\n",
      "Loss: 13.6878421765479\n",
      "l2 norm of gradients: 0.11277686121659754\n",
      "l2 norm of weights: 3.2016981506244324\n",
      "---------------------\n",
      "Iteration Number: 5058\n",
      "Loss: 13.686137591830647\n",
      "l2 norm of gradients: 0.11273357491677258\n",
      "l2 norm of weights: 3.2016867121950034\n",
      "---------------------\n",
      "Iteration Number: 5059\n",
      "Loss: 13.684434326035099\n",
      "l2 norm of gradients: 0.11269030737788968\n",
      "l2 norm of weights: 3.2016753577235866\n",
      "---------------------\n",
      "Iteration Number: 5060\n",
      "Loss: 13.68273237824954\n",
      "l2 norm of gradients: 0.1126470586047798\n",
      "l2 norm of weights: 3.201664087146926\n",
      "---------------------\n",
      "Iteration Number: 5061\n",
      "Loss: 13.68103174756222\n",
      "l2 norm of gradients: 0.11260382860230723\n",
      "l2 norm of weights: 3.2016529004017973\n",
      "---------------------\n",
      "Iteration Number: 5062\n",
      "Loss: 13.679332433061337\n",
      "l2 norm of gradients: 0.11256061737536956\n",
      "l2 norm of weights: 3.2016417974250033\n",
      "---------------------\n",
      "Iteration Number: 5063\n",
      "Loss: 13.677634433835076\n",
      "l2 norm of gradients: 0.11251742492889731\n",
      "l2 norm of weights: 3.201630778153379\n",
      "---------------------\n",
      "Iteration Number: 5064\n",
      "Loss: 13.675937748971553\n",
      "l2 norm of gradients: 0.11247425126785392\n",
      "l2 norm of weights: 3.2016198425237894\n",
      "---------------------\n",
      "Iteration Number: 5065\n",
      "Loss: 13.674242377558858\n",
      "l2 norm of gradients: 0.11243109639723525\n",
      "l2 norm of weights: 3.201608990473129\n",
      "---------------------\n",
      "Iteration Number: 5066\n",
      "Loss: 13.67254831868506\n",
      "l2 norm of gradients: 0.11238796032206975\n",
      "l2 norm of weights: 3.2015982219383226\n",
      "---------------------\n",
      "Iteration Number: 5067\n",
      "Loss: 13.670855571438155\n",
      "l2 norm of gradients: 0.11234484304741797\n",
      "l2 norm of weights: 3.2015875368563242\n",
      "---------------------\n",
      "Iteration Number: 5068\n",
      "Loss: 13.669164134906104\n",
      "l2 norm of gradients: 0.1123017445783725\n",
      "l2 norm of weights: 3.2015769351641192\n",
      "---------------------\n",
      "Iteration Number: 5069\n",
      "Loss: 13.667474008176875\n",
      "l2 norm of gradients: 0.11225866492005769\n",
      "l2 norm of weights: 3.2015664167987214\n",
      "---------------------\n",
      "Iteration Number: 5070\n",
      "Loss: 13.665785190338346\n",
      "l2 norm of gradients: 0.11221560407762962\n",
      "l2 norm of weights: 3.2015559816971755\n",
      "---------------------\n",
      "Iteration Number: 5071\n",
      "Loss: 13.664097680478354\n",
      "l2 norm of gradients: 0.11217256205627558\n",
      "l2 norm of weights: 3.2015456297965543\n",
      "---------------------\n",
      "Iteration Number: 5072\n",
      "Loss: 13.66241147768476\n",
      "l2 norm of gradients: 0.11212953886121427\n",
      "l2 norm of weights: 3.2015353610339616\n",
      "---------------------\n",
      "Iteration Number: 5073\n",
      "Loss: 13.660726581045296\n",
      "l2 norm of gradients: 0.11208653449769522\n",
      "l2 norm of weights: 3.201525175346531\n",
      "---------------------\n",
      "Iteration Number: 5074\n",
      "Loss: 13.659042989647723\n",
      "l2 norm of gradients: 0.11204354897099883\n",
      "l2 norm of weights: 3.201515072671423\n",
      "---------------------\n",
      "Iteration Number: 5075\n",
      "Loss: 13.65736070257976\n",
      "l2 norm of gradients: 0.11200058228643611\n",
      "l2 norm of weights: 3.2015050529458327\n",
      "---------------------\n",
      "Iteration Number: 5076\n",
      "Loss: 13.655679718929047\n",
      "l2 norm of gradients: 0.11195763444934846\n",
      "l2 norm of weights: 3.2014951161069782\n",
      "---------------------\n",
      "Iteration Number: 5077\n",
      "Loss: 13.654000037783195\n",
      "l2 norm of gradients: 0.11191470546510748\n",
      "l2 norm of weights: 3.2014852620921124\n",
      "---------------------\n",
      "Iteration Number: 5078\n",
      "Loss: 13.65232165822984\n",
      "l2 norm of gradients: 0.1118717953391148\n",
      "l2 norm of weights: 3.201475490838515\n",
      "---------------------\n",
      "Iteration Number: 5079\n",
      "Loss: 13.650644579356491\n",
      "l2 norm of gradients: 0.11182890407680174\n",
      "l2 norm of weights: 3.201465802283495\n",
      "---------------------\n",
      "Iteration Number: 5080\n",
      "Loss: 13.648968800250694\n",
      "l2 norm of gradients: 0.11178603168362936\n",
      "l2 norm of weights: 3.201456196364391\n",
      "---------------------\n",
      "Iteration Number: 5081\n",
      "Loss: 13.647294319999888\n",
      "l2 norm of gradients: 0.11174317816508798\n",
      "l2 norm of weights: 3.2014466730185696\n",
      "---------------------\n",
      "Iteration Number: 5082\n",
      "Loss: 13.645621137691545\n",
      "l2 norm of gradients: 0.1117003435266972\n",
      "l2 norm of weights: 3.2014372321834283\n",
      "---------------------\n",
      "Iteration Number: 5083\n",
      "Loss: 13.643949252413055\n",
      "l2 norm of gradients: 0.11165752777400567\n",
      "l2 norm of weights: 3.2014278737963933\n",
      "---------------------\n",
      "Iteration Number: 5084\n",
      "Loss: 13.642278663251773\n",
      "l2 norm of gradients: 0.11161473091259073\n",
      "l2 norm of weights: 3.2014185977949183\n",
      "---------------------\n",
      "Iteration Number: 5085\n",
      "Loss: 13.640609369295046\n",
      "l2 norm of gradients: 0.11157195294805825\n",
      "l2 norm of weights: 3.201409404116487\n",
      "---------------------\n",
      "Iteration Number: 5086\n",
      "Loss: 13.638941369630174\n",
      "l2 norm of gradients: 0.1115291938860427\n",
      "l2 norm of weights: 3.201400292698611\n",
      "---------------------\n",
      "Iteration Number: 5087\n",
      "Loss: 13.637274663344398\n",
      "l2 norm of gradients: 0.11148645373220657\n",
      "l2 norm of weights: 3.201391263478833\n",
      "---------------------\n",
      "Iteration Number: 5088\n",
      "Loss: 13.635609249524938\n",
      "l2 norm of gradients: 0.11144373249224046\n",
      "l2 norm of weights: 3.2013823163947217\n",
      "---------------------\n",
      "Iteration Number: 5089\n",
      "Loss: 13.633945127259008\n",
      "l2 norm of gradients: 0.11140103017186259\n",
      "l2 norm of weights: 3.2013734513838763\n",
      "---------------------\n",
      "Iteration Number: 5090\n",
      "Loss: 13.632282295633733\n",
      "l2 norm of gradients: 0.11135834677681898\n",
      "l2 norm of weights: 3.2013646683839228\n",
      "---------------------\n",
      "Iteration Number: 5091\n",
      "Loss: 13.630620753736231\n",
      "l2 norm of gradients: 0.11131568231288286\n",
      "l2 norm of weights: 3.2013559673325185\n",
      "---------------------\n",
      "Iteration Number: 5092\n",
      "Loss: 13.628960500653607\n",
      "l2 norm of gradients: 0.11127303678585475\n",
      "l2 norm of weights: 3.2013473481673462\n",
      "---------------------\n",
      "Iteration Number: 5093\n",
      "Loss: 13.627301535472894\n",
      "l2 norm of gradients: 0.1112304102015621\n",
      "l2 norm of weights: 3.2013388108261194\n",
      "---------------------\n",
      "Iteration Number: 5094\n",
      "Loss: 13.625643857281103\n",
      "l2 norm of gradients: 0.1111878025658592\n",
      "l2 norm of weights: 3.2013303552465784\n",
      "---------------------\n",
      "Iteration Number: 5095\n",
      "Loss: 13.623987465165234\n",
      "l2 norm of gradients: 0.11114521388462685\n",
      "l2 norm of weights: 3.201321981366493\n",
      "---------------------\n",
      "Iteration Number: 5096\n",
      "Loss: 13.622332358212228\n",
      "l2 norm of gradients: 0.11110264416377236\n",
      "l2 norm of weights: 3.201313689123661\n",
      "---------------------\n",
      "Iteration Number: 5097\n",
      "Loss: 13.620678535508985\n",
      "l2 norm of gradients: 0.11106009340922902\n",
      "l2 norm of weights: 3.2013054784559083\n",
      "---------------------\n",
      "Iteration Number: 5098\n",
      "Loss: 13.619025996142406\n",
      "l2 norm of gradients: 0.1110175616269563\n",
      "l2 norm of weights: 3.2012973493010883\n",
      "---------------------\n",
      "Iteration Number: 5099\n",
      "Loss: 13.617374739199326\n",
      "l2 norm of gradients: 0.11097504882293933\n",
      "l2 norm of weights: 3.201289301597084\n",
      "---------------------\n",
      "Iteration Number: 5100\n",
      "Loss: 13.615724763766577\n",
      "l2 norm of gradients: 0.11093255500318885\n",
      "l2 norm of weights: 3.201281335281805\n",
      "---------------------\n",
      "Iteration Number: 5101\n",
      "Loss: 13.614076068930947\n",
      "l2 norm of gradients: 0.110890080173741\n",
      "l2 norm of weights: 3.2012734502931894\n",
      "---------------------\n",
      "Iteration Number: 5102\n",
      "Loss: 13.612428653779169\n",
      "l2 norm of gradients: 0.11084762434065708\n",
      "l2 norm of weights: 3.2012656465692038\n",
      "---------------------\n",
      "Iteration Number: 5103\n",
      "Loss: 13.610782517397984\n",
      "l2 norm of gradients: 0.11080518751002333\n",
      "l2 norm of weights: 3.2012579240478423\n",
      "---------------------\n",
      "Iteration Number: 5104\n",
      "Loss: 13.60913765887409\n",
      "l2 norm of gradients: 0.11076276968795085\n",
      "l2 norm of weights: 3.2012502826671265\n",
      "---------------------\n",
      "Iteration Number: 5105\n",
      "Loss: 13.607494077294128\n",
      "l2 norm of gradients: 0.11072037088057522\n",
      "l2 norm of weights: 3.2012427223651057\n",
      "---------------------\n",
      "Iteration Number: 5106\n",
      "Loss: 13.605851771744753\n",
      "l2 norm of gradients: 0.11067799109405642\n",
      "l2 norm of weights: 3.2012352430798585\n",
      "---------------------\n",
      "Iteration Number: 5107\n",
      "Loss: 13.60421074131256\n",
      "l2 norm of gradients: 0.11063563033457868\n",
      "l2 norm of weights: 3.201227844749489\n",
      "---------------------\n",
      "Iteration Number: 5108\n",
      "Loss: 13.602570985084116\n",
      "l2 norm of gradients: 0.11059328860835008\n",
      "l2 norm of weights: 3.2012205273121297\n",
      "---------------------\n",
      "Iteration Number: 5109\n",
      "Loss: 13.600932502145987\n",
      "l2 norm of gradients: 0.11055096592160256\n",
      "l2 norm of weights: 3.2012132907059416\n",
      "---------------------\n",
      "Iteration Number: 5110\n",
      "Loss: 13.599295291584674\n",
      "l2 norm of gradients: 0.11050866228059157\n",
      "l2 norm of weights: 3.2012061348691123\n",
      "---------------------\n",
      "Iteration Number: 5111\n",
      "Loss: 13.597659352486659\n",
      "l2 norm of gradients: 0.11046637769159595\n",
      "l2 norm of weights: 3.201199059739857\n",
      "---------------------\n",
      "Iteration Number: 5112\n",
      "Loss: 13.596024683938406\n",
      "l2 norm of gradients: 0.11042411216091774\n",
      "l2 norm of weights: 3.201192065256418\n",
      "---------------------\n",
      "Iteration Number: 5113\n",
      "Loss: 13.594391285026356\n",
      "l2 norm of gradients: 0.1103818656948819\n",
      "l2 norm of weights: 3.2011851513570657\n",
      "---------------------\n",
      "Iteration Number: 5114\n",
      "Loss: 13.592759154836925\n",
      "l2 norm of gradients: 0.11033963829983616\n",
      "l2 norm of weights: 3.201178317980097\n",
      "---------------------\n",
      "Iteration Number: 5115\n",
      "Loss: 13.591128292456471\n",
      "l2 norm of gradients: 0.11029742998215085\n",
      "l2 norm of weights: 3.201171565063836\n",
      "---------------------\n",
      "Iteration Number: 5116\n",
      "Loss: 13.589498696971354\n",
      "l2 norm of gradients: 0.11025524074821859\n",
      "l2 norm of weights: 3.201164892546636\n",
      "---------------------\n",
      "Iteration Number: 5117\n",
      "Loss: 13.587870367467907\n",
      "l2 norm of gradients: 0.11021307060445427\n",
      "l2 norm of weights: 3.2011583003668744\n",
      "---------------------\n",
      "Iteration Number: 5118\n",
      "Loss: 13.586243303032436\n",
      "l2 norm of gradients: 0.11017091955729463\n",
      "l2 norm of weights: 3.2011517884629574\n",
      "---------------------\n",
      "Iteration Number: 5119\n",
      "Loss: 13.58461750275123\n",
      "l2 norm of gradients: 0.11012878761319826\n",
      "l2 norm of weights: 3.201145356773318\n",
      "---------------------\n",
      "Iteration Number: 5120\n",
      "Loss: 13.582992965710515\n",
      "l2 norm of gradients: 0.11008667477864532\n",
      "l2 norm of weights: 3.201139005236416\n",
      "---------------------\n",
      "Iteration Number: 5121\n",
      "Loss: 13.581369690996551\n",
      "l2 norm of gradients: 0.11004458106013716\n",
      "l2 norm of weights: 3.201132733790739\n",
      "---------------------\n",
      "Iteration Number: 5122\n",
      "Loss: 13.579747677695515\n",
      "l2 norm of gradients: 0.11000250646419649\n",
      "l2 norm of weights: 3.2011265423747997\n",
      "---------------------\n",
      "Iteration Number: 5123\n",
      "Loss: 13.578126924893613\n",
      "l2 norm of gradients: 0.10996045099736691\n",
      "l2 norm of weights: 3.2011204309271393\n",
      "---------------------\n",
      "Iteration Number: 5124\n",
      "Loss: 13.576507431677005\n",
      "l2 norm of gradients: 0.10991841466621274\n",
      "l2 norm of weights: 3.2011143993863245\n",
      "---------------------\n",
      "Iteration Number: 5125\n",
      "Loss: 13.57488919713183\n",
      "l2 norm of gradients: 0.10987639747731888\n",
      "l2 norm of weights: 3.2011084476909493\n",
      "---------------------\n",
      "Iteration Number: 5126\n",
      "Loss: 13.573272220344201\n",
      "l2 norm of gradients: 0.10983439943729059\n",
      "l2 norm of weights: 3.201102575779635\n",
      "---------------------\n",
      "Iteration Number: 5127\n",
      "Loss: 13.571656500400215\n",
      "l2 norm of gradients: 0.10979242055275334\n",
      "l2 norm of weights: 3.2010967835910287\n",
      "---------------------\n",
      "Iteration Number: 5128\n",
      "Loss: 13.570042036385955\n",
      "l2 norm of gradients: 0.1097504608303524\n",
      "l2 norm of weights: 3.2010910710638036\n",
      "---------------------\n",
      "Iteration Number: 5129\n",
      "Loss: 13.568428827387468\n",
      "l2 norm of gradients: 0.109708520276753\n",
      "l2 norm of weights: 3.2010854381366602\n",
      "---------------------\n",
      "Iteration Number: 5130\n",
      "Loss: 13.566816872490802\n",
      "l2 norm of gradients: 0.10966659889863974\n",
      "l2 norm of weights: 3.2010798847483257\n",
      "---------------------\n",
      "Iteration Number: 5131\n",
      "Loss: 13.565206170781982\n",
      "l2 norm of gradients: 0.1096246967027167\n",
      "l2 norm of weights: 3.201074410837553\n",
      "---------------------\n",
      "Iteration Number: 5132\n",
      "Loss: 13.563596721346991\n",
      "l2 norm of gradients: 0.10958281369570703\n",
      "l2 norm of weights: 3.2010690163431224\n",
      "---------------------\n",
      "Iteration Number: 5133\n",
      "Loss: 13.561988523271813\n",
      "l2 norm of gradients: 0.10954094988435288\n",
      "l2 norm of weights: 3.201063701203838\n",
      "---------------------\n",
      "Iteration Number: 5134\n",
      "Loss: 13.560381575642426\n",
      "l2 norm of gradients: 0.10949910527541515\n",
      "l2 norm of weights: 3.2010584653585332\n",
      "---------------------\n",
      "Iteration Number: 5135\n",
      "Loss: 13.55877587754476\n",
      "l2 norm of gradients: 0.10945727987567327\n",
      "l2 norm of weights: 3.2010533087460655\n",
      "---------------------\n",
      "Iteration Number: 5136\n",
      "Loss: 13.557171428064763\n",
      "l2 norm of gradients: 0.10941547369192504\n",
      "l2 norm of weights: 3.2010482313053195\n",
      "---------------------\n",
      "Iteration Number: 5137\n",
      "Loss: 13.555568226288344\n",
      "l2 norm of gradients: 0.10937368673098638\n",
      "l2 norm of weights: 3.2010432329752057\n",
      "---------------------\n",
      "Iteration Number: 5138\n",
      "Loss: 13.553966271301395\n",
      "l2 norm of gradients: 0.10933191899969126\n",
      "l2 norm of weights: 3.20103831369466\n",
      "---------------------\n",
      "Iteration Number: 5139\n",
      "Loss: 13.552365562189804\n",
      "l2 norm of gradients: 0.10929017050489122\n",
      "l2 norm of weights: 3.2010334734026467\n",
      "---------------------\n",
      "Iteration Number: 5140\n",
      "Loss: 13.550766098039462\n",
      "l2 norm of gradients: 0.10924844125345552\n",
      "l2 norm of weights: 3.2010287120381524\n",
      "---------------------\n",
      "Iteration Number: 5141\n",
      "Loss: 13.549167877936194\n",
      "l2 norm of gradients: 0.10920673125227073\n",
      "l2 norm of weights: 3.2010240295401915\n",
      "---------------------\n",
      "Iteration Number: 5142\n",
      "Loss: 13.54757090096589\n",
      "l2 norm of gradients: 0.10916504050824054\n",
      "l2 norm of weights: 3.2010194258478055\n",
      "---------------------\n",
      "Iteration Number: 5143\n",
      "Loss: 13.54597516621432\n",
      "l2 norm of gradients: 0.10912336902828555\n",
      "l2 norm of weights: 3.201014900900059\n",
      "---------------------\n",
      "Iteration Number: 5144\n",
      "Loss: 13.544380672767378\n",
      "l2 norm of gradients: 0.10908171681934321\n",
      "l2 norm of weights: 3.201010454636044\n",
      "---------------------\n",
      "Iteration Number: 5145\n",
      "Loss: 13.542787419710816\n",
      "l2 norm of gradients: 0.10904008388836746\n",
      "l2 norm of weights: 3.2010060869948784\n",
      "---------------------\n",
      "Iteration Number: 5146\n",
      "Loss: 13.541195406130456\n",
      "l2 norm of gradients: 0.10899847024232866\n",
      "l2 norm of weights: 3.201001797915705\n",
      "---------------------\n",
      "Iteration Number: 5147\n",
      "Loss: 13.539604631112066\n",
      "l2 norm of gradients: 0.10895687588821316\n",
      "l2 norm of weights: 3.200997587337692\n",
      "---------------------\n",
      "Iteration Number: 5148\n",
      "Loss: 13.538015093741464\n",
      "l2 norm of gradients: 0.10891530083302345\n",
      "l2 norm of weights: 3.2009934552000336\n",
      "---------------------\n",
      "Iteration Number: 5149\n",
      "Loss: 13.53642679310439\n",
      "l2 norm of gradients: 0.1088737450837777\n",
      "l2 norm of weights: 3.2009894014419498\n",
      "---------------------\n",
      "Iteration Number: 5150\n",
      "Loss: 13.53483972828661\n",
      "l2 norm of gradients: 0.10883220864750955\n",
      "l2 norm of weights: 3.2009854260026853\n",
      "---------------------\n",
      "Iteration Number: 5151\n",
      "Loss: 13.533253898373891\n",
      "l2 norm of gradients: 0.10879069153126811\n",
      "l2 norm of weights: 3.2009815288215115\n",
      "---------------------\n",
      "Iteration Number: 5152\n",
      "Loss: 13.531669302451963\n",
      "l2 norm of gradients: 0.10874919374211764\n",
      "l2 norm of weights: 3.2009777098377223\n",
      "---------------------\n",
      "Iteration Number: 5153\n",
      "Loss: 13.53008593960658\n",
      "l2 norm of gradients: 0.1087077152871373\n",
      "l2 norm of weights: 3.2009739689906405\n",
      "---------------------\n",
      "Iteration Number: 5154\n",
      "Loss: 13.52850380892348\n",
      "l2 norm of gradients: 0.108666256173421\n",
      "l2 norm of weights: 3.2009703062196118\n",
      "---------------------\n",
      "Iteration Number: 5155\n",
      "Loss: 13.526922909488377\n",
      "l2 norm of gradients: 0.10862481640807725\n",
      "l2 norm of weights: 3.200966721464008\n",
      "---------------------\n",
      "Iteration Number: 5156\n",
      "Loss: 13.52534324038701\n",
      "l2 norm of gradients: 0.10858339599822892\n",
      "l2 norm of weights: 3.200963214663225\n",
      "---------------------\n",
      "Iteration Number: 5157\n",
      "Loss: 13.523764800705102\n",
      "l2 norm of gradients: 0.10854199495101295\n",
      "l2 norm of weights: 3.2009597857566856\n",
      "---------------------\n",
      "Iteration Number: 5158\n",
      "Loss: 13.522187589528365\n",
      "l2 norm of gradients: 0.10850061327358039\n",
      "l2 norm of weights: 3.200956434683836\n",
      "---------------------\n",
      "Iteration Number: 5159\n",
      "Loss: 13.520611605942532\n",
      "l2 norm of gradients: 0.10845925097309587\n",
      "l2 norm of weights: 3.2009531613841484\n",
      "---------------------\n",
      "Iteration Number: 5160\n",
      "Loss: 13.519036849033288\n",
      "l2 norm of gradients: 0.10841790805673783\n",
      "l2 norm of weights: 3.20094996579712\n",
      "---------------------\n",
      "Iteration Number: 5161\n",
      "Loss: 13.51746331788636\n",
      "l2 norm of gradients: 0.10837658453169774\n",
      "l2 norm of weights: 3.200946847862271\n",
      "---------------------\n",
      "Iteration Number: 5162\n",
      "Loss: 13.515891011587485\n",
      "l2 norm of gradients: 0.10833528040518055\n",
      "l2 norm of weights: 3.20094380751915\n",
      "---------------------\n",
      "Iteration Number: 5163\n",
      "Loss: 13.514319929222337\n",
      "l2 norm of gradients: 0.10829399568440395\n",
      "l2 norm of weights: 3.200940844707327\n",
      "---------------------\n",
      "Iteration Number: 5164\n",
      "Loss: 13.512750069876663\n",
      "l2 norm of gradients: 0.10825273037659859\n",
      "l2 norm of weights: 3.2009379593663985\n",
      "---------------------\n",
      "Iteration Number: 5165\n",
      "Loss: 13.511181432636167\n",
      "l2 norm of gradients: 0.10821148448900748\n",
      "l2 norm of weights: 3.200935151435986\n",
      "---------------------\n",
      "Iteration Number: 5166\n",
      "Loss: 13.509614016586553\n",
      "l2 norm of gradients: 0.10817025802888615\n",
      "l2 norm of weights: 3.200932420855734\n",
      "---------------------\n",
      "Iteration Number: 5167\n",
      "Loss: 13.508047820813562\n",
      "l2 norm of gradients: 0.10812905100350229\n",
      "l2 norm of weights: 3.200929767565314\n",
      "---------------------\n",
      "Iteration Number: 5168\n",
      "Loss: 13.506482844402905\n",
      "l2 norm of gradients: 0.10808786342013542\n",
      "l2 norm of weights: 3.2009271915044204\n",
      "---------------------\n",
      "Iteration Number: 5169\n",
      "Loss: 13.504919086440344\n",
      "l2 norm of gradients: 0.10804669528607708\n",
      "l2 norm of weights: 3.2009246926127717\n",
      "---------------------\n",
      "Iteration Number: 5170\n",
      "Loss: 13.503356546011556\n",
      "l2 norm of gradients: 0.10800554660863021\n",
      "l2 norm of weights: 3.200922270830113\n",
      "---------------------\n",
      "Iteration Number: 5171\n",
      "Loss: 13.501795222202333\n",
      "l2 norm of gradients: 0.10796441739510913\n",
      "l2 norm of weights: 3.200919926096212\n",
      "---------------------\n",
      "Iteration Number: 5172\n",
      "Loss: 13.500235114098407\n",
      "l2 norm of gradients: 0.10792330765283943\n",
      "l2 norm of weights: 3.200917658350861\n",
      "---------------------\n",
      "Iteration Number: 5173\n",
      "Loss: 13.498676220785537\n",
      "l2 norm of gradients: 0.1078822173891577\n",
      "l2 norm of weights: 3.2009154675338776\n",
      "---------------------\n",
      "Iteration Number: 5174\n",
      "Loss: 13.49711854134946\n",
      "l2 norm of gradients: 0.10784114661141109\n",
      "l2 norm of weights: 3.2009133535851033\n",
      "---------------------\n",
      "Iteration Number: 5175\n",
      "Loss: 13.495562074875993\n",
      "l2 norm of gradients: 0.1078000953269576\n",
      "l2 norm of weights: 3.200911316444403\n",
      "---------------------\n",
      "Iteration Number: 5176\n",
      "Loss: 13.494006820450878\n",
      "l2 norm of gradients: 0.10775906354316554\n",
      "l2 norm of weights: 3.200909356051668\n",
      "---------------------\n",
      "Iteration Number: 5177\n",
      "Loss: 13.492452777159912\n",
      "l2 norm of gradients: 0.10771805126741339\n",
      "l2 norm of weights: 3.200907472346811\n",
      "---------------------\n",
      "Iteration Number: 5178\n",
      "Loss: 13.490899944088913\n",
      "l2 norm of gradients: 0.10767705850708963\n",
      "l2 norm of weights: 3.200905665269771\n",
      "---------------------\n",
      "Iteration Number: 5179\n",
      "Loss: 13.489348320323696\n",
      "l2 norm of gradients: 0.10763608526959252\n",
      "l2 norm of weights: 3.2009039347605097\n",
      "---------------------\n",
      "Iteration Number: 5180\n",
      "Loss: 13.487797904950073\n",
      "l2 norm of gradients: 0.10759513156232998\n",
      "l2 norm of weights: 3.200902280759014\n",
      "---------------------\n",
      "Iteration Number: 5181\n",
      "Loss: 13.486248697053899\n",
      "l2 norm of gradients: 0.10755419739271928\n",
      "l2 norm of weights: 3.200900703205294\n",
      "---------------------\n",
      "Iteration Number: 5182\n",
      "Loss: 13.484700695721017\n",
      "l2 norm of gradients: 0.10751328276818695\n",
      "l2 norm of weights: 3.2008992020393836\n",
      "---------------------\n",
      "Iteration Number: 5183\n",
      "Loss: 13.483153900037303\n",
      "l2 norm of gradients: 0.10747238769616849\n",
      "l2 norm of weights: 3.200897777201342\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 5184\n",
      "Loss: 13.481608309088658\n",
      "l2 norm of gradients: 0.10743151218410826\n",
      "l2 norm of weights: 3.2008964286312502\n",
      "---------------------\n",
      "Iteration Number: 5185\n",
      "Loss: 13.480063921960951\n",
      "l2 norm of gradients: 0.10739065623945919\n",
      "l2 norm of weights: 3.200895156269215\n",
      "---------------------\n",
      "Iteration Number: 5186\n",
      "Loss: 13.478520737740128\n",
      "l2 norm of gradients: 0.10734981986968263\n",
      "l2 norm of weights: 3.200893960055365\n",
      "---------------------\n",
      "Iteration Number: 5187\n",
      "Loss: 13.476978755512121\n",
      "l2 norm of gradients: 0.10730900308224824\n",
      "l2 norm of weights: 3.2008928399298555\n",
      "---------------------\n",
      "Iteration Number: 5188\n",
      "Loss: 13.475437974362881\n",
      "l2 norm of gradients: 0.10726820588463357\n",
      "l2 norm of weights: 3.200891795832862\n",
      "---------------------\n",
      "Iteration Number: 5189\n",
      "Loss: 13.473898393378388\n",
      "l2 norm of gradients: 0.10722742828432413\n",
      "l2 norm of weights: 3.2008908277045864\n",
      "---------------------\n",
      "Iteration Number: 5190\n",
      "Loss: 13.472360011644655\n",
      "l2 norm of gradients: 0.10718667028881304\n",
      "l2 norm of weights: 3.200889935485253\n",
      "---------------------\n",
      "Iteration Number: 5191\n",
      "Loss: 13.470822828247666\n",
      "l2 norm of gradients: 0.10714593190560083\n",
      "l2 norm of weights: 3.2008891191151094\n",
      "---------------------\n",
      "Iteration Number: 5192\n",
      "Loss: 13.469286842273483\n",
      "l2 norm of gradients: 0.10710521314219529\n",
      "l2 norm of weights: 3.2008883785344273\n",
      "---------------------\n",
      "Iteration Number: 5193\n",
      "Loss: 13.467752052808178\n",
      "l2 norm of gradients: 0.10706451400611125\n",
      "l2 norm of weights: 3.2008877136835023\n",
      "---------------------\n",
      "Iteration Number: 5194\n",
      "Loss: 13.466218458937819\n",
      "l2 norm of gradients: 0.10702383450487049\n",
      "l2 norm of weights: 3.2008871245026524\n",
      "---------------------\n",
      "Iteration Number: 5195\n",
      "Loss: 13.464686059748535\n",
      "l2 norm of gradients: 0.10698317464600128\n",
      "l2 norm of weights: 3.200886610932219\n",
      "---------------------\n",
      "Iteration Number: 5196\n",
      "Loss: 13.463154854326438\n",
      "l2 norm of gradients: 0.10694253443703848\n",
      "l2 norm of weights: 3.200886172912569\n",
      "---------------------\n",
      "Iteration Number: 5197\n",
      "Loss: 13.461624841757736\n",
      "l2 norm of gradients: 0.10690191388552324\n",
      "l2 norm of weights: 3.20088581038409\n",
      "---------------------\n",
      "Iteration Number: 5198\n",
      "Loss: 13.460096021128589\n",
      "l2 norm of gradients: 0.10686131299900269\n",
      "l2 norm of weights: 3.200885523287194\n",
      "---------------------\n",
      "Iteration Number: 5199\n",
      "Loss: 13.45856839152521\n",
      "l2 norm of gradients: 0.10682073178502989\n",
      "l2 norm of weights: 3.200885311562316\n",
      "---------------------\n",
      "Iteration Number: 5200\n",
      "Loss: 13.457041952033881\n",
      "l2 norm of gradients: 0.10678017025116364\n",
      "l2 norm of weights: 3.200885175149915\n",
      "---------------------\n",
      "Iteration Number: 5201\n",
      "Loss: 13.45551670174086\n",
      "l2 norm of gradients: 0.10673962840496809\n",
      "l2 norm of weights: 3.2008851139904726\n",
      "---------------------\n",
      "Iteration Number: 5202\n",
      "Loss: 13.453992639732464\n",
      "l2 norm of gradients: 0.10669910625401287\n",
      "l2 norm of weights: 3.200885128024493\n",
      "---------------------\n",
      "Iteration Number: 5203\n",
      "Loss: 13.452469765095017\n",
      "l2 norm of gradients: 0.10665860380587261\n",
      "l2 norm of weights: 3.2008852171925044\n",
      "---------------------\n",
      "Iteration Number: 5204\n",
      "Loss: 13.45094807691491\n",
      "l2 norm of gradients: 0.10661812106812683\n",
      "l2 norm of weights: 3.2008853814350577\n",
      "---------------------\n",
      "Iteration Number: 5205\n",
      "Loss: 13.449427574278541\n",
      "l2 norm of gradients: 0.10657765804835982\n",
      "l2 norm of weights: 3.2008856206927265\n",
      "---------------------\n",
      "Iteration Number: 5206\n",
      "Loss: 13.447908256272369\n",
      "l2 norm of gradients: 0.10653721475416042\n",
      "l2 norm of weights: 3.2008859349061076\n",
      "---------------------\n",
      "Iteration Number: 5207\n",
      "Loss: 13.44639012198284\n",
      "l2 norm of gradients: 0.10649679119312166\n",
      "l2 norm of weights: 3.200886324015821\n",
      "---------------------\n",
      "Iteration Number: 5208\n",
      "Loss: 13.444873170496495\n",
      "l2 norm of gradients: 0.10645638737284091\n",
      "l2 norm of weights: 3.2008867879625096\n",
      "---------------------\n",
      "Iteration Number: 5209\n",
      "Loss: 13.443357400899869\n",
      "l2 norm of gradients: 0.1064160033009193\n",
      "l2 norm of weights: 3.200887326686839\n",
      "---------------------\n",
      "Iteration Number: 5210\n",
      "Loss: 13.441842812279555\n",
      "l2 norm of gradients: 0.10637563898496184\n",
      "l2 norm of weights: 3.2008879401294976\n",
      "---------------------\n",
      "Iteration Number: 5211\n",
      "Loss: 13.440329403722158\n",
      "l2 norm of gradients: 0.10633529443257701\n",
      "l2 norm of weights: 3.2008886282311955\n",
      "---------------------\n",
      "Iteration Number: 5212\n",
      "Loss: 13.43881717431438\n",
      "l2 norm of gradients: 0.10629496965137673\n",
      "l2 norm of weights: 3.200889390932668\n",
      "---------------------\n",
      "Iteration Number: 5213\n",
      "Loss: 13.437306123142891\n",
      "l2 norm of gradients: 0.10625466464897604\n",
      "l2 norm of weights: 3.200890228174672\n",
      "---------------------\n",
      "Iteration Number: 5214\n",
      "Loss: 13.435796249294473\n",
      "l2 norm of gradients: 0.10621437943299296\n",
      "l2 norm of weights: 3.200891139897985\n",
      "---------------------\n",
      "Iteration Number: 5215\n",
      "Loss: 13.434287551855892\n",
      "l2 norm of gradients: 0.1061741140110483\n",
      "l2 norm of weights: 3.2008921260434104\n",
      "---------------------\n",
      "Iteration Number: 5216\n",
      "Loss: 13.432780029913992\n",
      "l2 norm of gradients: 0.10613386839076554\n",
      "l2 norm of weights: 3.2008931865517725\n",
      "---------------------\n",
      "Iteration Number: 5217\n",
      "Loss: 13.43127368255565\n",
      "l2 norm of gradients: 0.10609364257977046\n",
      "l2 norm of weights: 3.2008943213639185\n",
      "---------------------\n",
      "Iteration Number: 5218\n",
      "Loss: 13.429768508867772\n",
      "l2 norm of gradients: 0.10605343658569112\n",
      "l2 norm of weights: 3.2008955304207176\n",
      "---------------------\n",
      "Iteration Number: 5219\n",
      "Loss: 13.428264507937358\n",
      "l2 norm of gradients: 0.10601325041615756\n",
      "l2 norm of weights: 3.200896813663062\n",
      "---------------------\n",
      "Iteration Number: 5220\n",
      "Loss: 13.426761678851417\n",
      "l2 norm of gradients: 0.10597308407880167\n",
      "l2 norm of weights: 3.200898171031867\n",
      "---------------------\n",
      "Iteration Number: 5221\n",
      "Loss: 13.425260020696978\n",
      "l2 norm of gradients: 0.105932937581257\n",
      "l2 norm of weights: 3.200899602468069\n",
      "---------------------\n",
      "Iteration Number: 5222\n",
      "Loss: 13.423759532561196\n",
      "l2 norm of gradients: 0.10589281093115854\n",
      "l2 norm of weights: 3.2009011079126277\n",
      "---------------------\n",
      "Iteration Number: 5223\n",
      "Loss: 13.422260213531219\n",
      "l2 norm of gradients: 0.10585270413614246\n",
      "l2 norm of weights: 3.2009026873065243\n",
      "---------------------\n",
      "Iteration Number: 5224\n",
      "Loss: 13.420762062694259\n",
      "l2 norm of gradients: 0.10581261720384613\n",
      "l2 norm of weights: 3.2009043405907636\n",
      "---------------------\n",
      "Iteration Number: 5225\n",
      "Loss: 13.419265079137592\n",
      "l2 norm of gradients: 0.1057725501419077\n",
      "l2 norm of weights: 3.200906067706372\n",
      "---------------------\n",
      "Iteration Number: 5226\n",
      "Loss: 13.417769261948525\n",
      "l2 norm of gradients: 0.10573250295796602\n",
      "l2 norm of weights: 3.2009078685943972\n",
      "---------------------\n",
      "Iteration Number: 5227\n",
      "Loss: 13.416274610214439\n",
      "l2 norm of gradients: 0.1056924756596605\n",
      "l2 norm of weights: 3.2009097431959113\n",
      "---------------------\n",
      "Iteration Number: 5228\n",
      "Loss: 13.41478112302276\n",
      "l2 norm of gradients: 0.10565246825463079\n",
      "l2 norm of weights: 3.2009116914520064\n",
      "---------------------\n",
      "Iteration Number: 5229\n",
      "Loss: 13.413288799460965\n",
      "l2 norm of gradients: 0.10561248075051669\n",
      "l2 norm of weights: 3.200913713303798\n",
      "---------------------\n",
      "Iteration Number: 5230\n",
      "Loss: 13.411797638616603\n",
      "l2 norm of gradients: 0.10557251315495789\n",
      "l2 norm of weights: 3.200915808692423\n",
      "---------------------\n",
      "Iteration Number: 5231\n",
      "Loss: 13.41030763957727\n",
      "l2 norm of gradients: 0.10553256547559385\n",
      "l2 norm of weights: 3.2009179775590413\n",
      "---------------------\n",
      "Iteration Number: 5232\n",
      "Loss: 13.408818801430623\n",
      "l2 norm of gradients: 0.10549263772006362\n",
      "l2 norm of weights: 3.200920219844834\n",
      "---------------------\n",
      "Iteration Number: 5233\n",
      "Loss: 13.407331123264363\n",
      "l2 norm of gradients: 0.10545272989600551\n",
      "l2 norm of weights: 3.2009225354910047\n",
      "---------------------\n",
      "Iteration Number: 5234\n",
      "Loss: 13.40584460416629\n",
      "l2 norm of gradients: 0.10541284201105709\n",
      "l2 norm of weights: 3.2009249244387785\n",
      "---------------------\n",
      "Iteration Number: 5235\n",
      "Loss: 13.40435924322424\n",
      "l2 norm of gradients: 0.10537297407285491\n",
      "l2 norm of weights: 3.200927386629403\n",
      "---------------------\n",
      "Iteration Number: 5236\n",
      "Loss: 13.40287503952612\n",
      "l2 norm of gradients: 0.10533312608903427\n",
      "l2 norm of weights: 3.200929922004147\n",
      "---------------------\n",
      "Iteration Number: 5237\n",
      "Loss: 13.401391992159866\n",
      "l2 norm of gradients: 0.10529329806722912\n",
      "l2 norm of weights: 3.200932530504302\n",
      "---------------------\n",
      "Iteration Number: 5238\n",
      "Loss: 13.399910100213539\n",
      "l2 norm of gradients: 0.10525349001507181\n",
      "l2 norm of weights: 3.200935212071181\n",
      "---------------------\n",
      "Iteration Number: 5239\n",
      "Loss: 13.398429362775232\n",
      "l2 norm of gradients: 0.1052137019401929\n",
      "l2 norm of weights: 3.2009379666461193\n",
      "---------------------\n",
      "Iteration Number: 5240\n",
      "Loss: 13.396949778933077\n",
      "l2 norm of gradients: 0.1051739338502211\n",
      "l2 norm of weights: 3.200940794170472\n",
      "---------------------\n",
      "Iteration Number: 5241\n",
      "Loss: 13.395471347775354\n",
      "l2 norm of gradients: 0.10513418575278283\n",
      "l2 norm of weights: 3.2009436945856193\n",
      "---------------------\n",
      "Iteration Number: 5242\n",
      "Loss: 13.393994068390326\n",
      "l2 norm of gradients: 0.1050944576555023\n",
      "l2 norm of weights: 3.20094666783296\n",
      "---------------------\n",
      "Iteration Number: 5243\n",
      "Loss: 13.392517939866366\n",
      "l2 norm of gradients: 0.10505474956600118\n",
      "l2 norm of weights: 3.200949713853917\n",
      "---------------------\n",
      "Iteration Number: 5244\n",
      "Loss: 13.391042961291934\n",
      "l2 norm of gradients: 0.10501506149189839\n",
      "l2 norm of weights: 3.200952832589932\n",
      "---------------------\n",
      "Iteration Number: 5245\n",
      "Loss: 13.389569131755522\n",
      "l2 norm of gradients: 0.10497539344081004\n",
      "l2 norm of weights: 3.200956023982472\n",
      "---------------------\n",
      "Iteration Number: 5246\n",
      "Loss: 13.388096450345735\n",
      "l2 norm of gradients: 0.10493574542034918\n",
      "l2 norm of weights: 3.2009592879730224\n",
      "---------------------\n",
      "Iteration Number: 5247\n",
      "Loss: 13.386624916151224\n",
      "l2 norm of gradients: 0.10489611743812546\n",
      "l2 norm of weights: 3.2009626245030924\n",
      "---------------------\n",
      "Iteration Number: 5248\n",
      "Loss: 13.38515452826072\n",
      "l2 norm of gradients: 0.10485650950174523\n",
      "l2 norm of weights: 3.200966033514211\n",
      "---------------------\n",
      "Iteration Number: 5249\n",
      "Loss: 13.38368528576304\n",
      "l2 norm of gradients: 0.10481692161881118\n",
      "l2 norm of weights: 3.2009695149479303\n",
      "---------------------\n",
      "Iteration Number: 5250\n",
      "Loss: 13.382217187747067\n",
      "l2 norm of gradients: 0.10477735379692214\n",
      "l2 norm of weights: 3.2009730687458227\n",
      "---------------------\n",
      "Iteration Number: 5251\n",
      "Loss: 13.380750233301773\n",
      "l2 norm of gradients: 0.10473780604367304\n",
      "l2 norm of weights: 3.200976694849482\n",
      "---------------------\n",
      "Iteration Number: 5252\n",
      "Loss: 13.379284421516209\n",
      "l2 norm of gradients: 0.10469827836665455\n",
      "l2 norm of weights: 3.2009803932005254\n",
      "---------------------\n",
      "Iteration Number: 5253\n",
      "Loss: 13.377819751479478\n",
      "l2 norm of gradients: 0.104658770773453\n",
      "l2 norm of weights: 3.2009841637405883\n",
      "---------------------\n",
      "Iteration Number: 5254\n",
      "Loss: 13.376356222280826\n",
      "l2 norm of gradients: 0.10461928327165017\n",
      "l2 norm of weights: 3.2009880064113303\n",
      "---------------------\n",
      "Iteration Number: 5255\n",
      "Loss: 13.374893833009514\n",
      "l2 norm of gradients: 0.10457981586882309\n",
      "l2 norm of weights: 3.2009919211544315\n",
      "---------------------\n",
      "Iteration Number: 5256\n",
      "Loss: 13.37343258275492\n",
      "l2 norm of gradients: 0.10454036857254392\n",
      "l2 norm of weights: 3.200995907911592\n",
      "---------------------\n",
      "Iteration Number: 5257\n",
      "Loss: 13.371972470606517\n",
      "l2 norm of gradients: 0.10450094139037969\n",
      "l2 norm of weights: 3.2009999666245355\n",
      "---------------------\n",
      "Iteration Number: 5258\n",
      "Loss: 13.370513495653837\n",
      "l2 norm of gradients: 0.10446153432989216\n",
      "l2 norm of weights: 3.2010040972350047\n",
      "---------------------\n",
      "Iteration Number: 5259\n",
      "Loss: 13.369055656986522\n",
      "l2 norm of gradients: 0.10442214739863764\n",
      "l2 norm of weights: 3.201008299684765\n",
      "---------------------\n",
      "Iteration Number: 5260\n",
      "Loss: 13.367598953694278\n",
      "l2 norm of gradients: 0.10438278060416678\n",
      "l2 norm of weights: 3.2010125739156026\n",
      "---------------------\n",
      "Iteration Number: 5261\n",
      "Loss: 13.366143384866934\n",
      "l2 norm of gradients: 0.10434343395402436\n",
      "l2 norm of weights: 3.201016919869325\n",
      "---------------------\n",
      "Iteration Number: 5262\n",
      "Loss: 13.36468894959436\n",
      "l2 norm of gradients: 0.10430410745574925\n",
      "l2 norm of weights: 3.2010213374877607\n",
      "---------------------\n",
      "Iteration Number: 5263\n",
      "Loss: 13.363235646966574\n",
      "l2 norm of gradients: 0.10426480111687404\n",
      "l2 norm of weights: 3.2010258267127596\n",
      "---------------------\n",
      "Iteration Number: 5264\n",
      "Loss: 13.361783476073644\n",
      "l2 norm of gradients: 0.10422551494492503\n",
      "l2 norm of weights: 3.201030387486192\n",
      "---------------------\n",
      "Iteration Number: 5265\n",
      "Loss: 13.360332436005763\n",
      "l2 norm of gradients: 0.10418624894742184\n",
      "l2 norm of weights: 3.2010350197499498\n",
      "---------------------\n",
      "Iteration Number: 5266\n",
      "Loss: 13.358882525853197\n",
      "l2 norm of gradients: 0.10414700313187754\n",
      "l2 norm of weights: 3.2010397234459464\n",
      "---------------------\n",
      "Iteration Number: 5267\n",
      "Loss: 13.357433744706285\n",
      "l2 norm of gradients: 0.10410777750579817\n",
      "l2 norm of weights: 3.2010444985161155\n",
      "---------------------\n",
      "Iteration Number: 5268\n",
      "Loss: 13.355986091655526\n",
      "l2 norm of gradients: 0.10406857207668263\n",
      "l2 norm of weights: 3.2010493449024127\n",
      "---------------------\n",
      "Iteration Number: 5269\n",
      "Loss: 13.35453956579146\n",
      "l2 norm of gradients: 0.10402938685202275\n",
      "l2 norm of weights: 3.201054262546813\n",
      "---------------------\n",
      "Iteration Number: 5270\n",
      "Loss: 13.353094166204757\n",
      "l2 norm of gradients: 0.10399022183930265\n",
      "l2 norm of weights: 3.201059251391314\n",
      "---------------------\n",
      "Iteration Number: 5271\n",
      "Loss: 13.351649891986167\n",
      "l2 norm of gradients: 0.10395107704599897\n",
      "l2 norm of weights: 3.201064311377933\n",
      "---------------------\n",
      "Iteration Number: 5272\n",
      "Loss: 13.350206742226568\n",
      "l2 norm of gradients: 0.10391195247958059\n",
      "l2 norm of weights: 3.2010694424487087\n",
      "---------------------\n",
      "Iteration Number: 5273\n",
      "Loss: 13.348764716016928\n",
      "l2 norm of gradients: 0.10387284814750827\n",
      "l2 norm of weights: 3.201074644545702\n",
      "---------------------\n",
      "Iteration Number: 5274\n",
      "Loss: 13.347323812448293\n",
      "l2 norm of gradients: 0.10383376405723467\n",
      "l2 norm of weights: 3.2010799176109925\n",
      "---------------------\n",
      "Iteration Number: 5275\n",
      "Loss: 13.345884030611845\n",
      "l2 norm of gradients: 0.10379470021620414\n",
      "l2 norm of weights: 3.201085261586682\n",
      "---------------------\n",
      "Iteration Number: 5276\n",
      "Loss: 13.344445369598876\n",
      "l2 norm of gradients: 0.10375565663185246\n",
      "l2 norm of weights: 3.2010906764148923\n",
      "---------------------\n",
      "Iteration Number: 5277\n",
      "Loss: 13.343007828500754\n",
      "l2 norm of gradients: 0.1037166333116067\n",
      "l2 norm of weights: 3.2010961620377665\n",
      "---------------------\n",
      "Iteration Number: 5278\n",
      "Loss: 13.341571406408985\n",
      "l2 norm of gradients: 0.10367763026288512\n",
      "l2 norm of weights: 3.2011017183974686\n",
      "---------------------\n",
      "Iteration Number: 5279\n",
      "Loss: 13.340136102415173\n",
      "l2 norm of gradients: 0.10363864749309691\n",
      "l2 norm of weights: 3.2011073454361827\n",
      "---------------------\n",
      "Iteration Number: 5280\n",
      "Loss: 13.338701915611026\n",
      "l2 norm of gradients: 0.10359968500964196\n",
      "l2 norm of weights: 3.201113043096115\n",
      "---------------------\n",
      "Iteration Number: 5281\n",
      "Loss: 13.337268845088362\n",
      "l2 norm of gradients: 0.10356074281991086\n",
      "l2 norm of weights: 3.2011188113194904\n",
      "---------------------\n",
      "Iteration Number: 5282\n",
      "Loss: 13.335836889939126\n",
      "l2 norm of gradients: 0.10352182093128452\n",
      "l2 norm of weights: 3.2011246500485564\n",
      "---------------------\n",
      "Iteration Number: 5283\n",
      "Loss: 13.334406049255366\n",
      "l2 norm of gradients: 0.10348291935113423\n",
      "l2 norm of weights: 3.2011305592255797\n",
      "---------------------\n",
      "Iteration Number: 5284\n",
      "Loss: 13.332976322129268\n",
      "l2 norm of gradients: 0.10344403808682122\n",
      "l2 norm of weights: 3.2011365387928485\n",
      "---------------------\n",
      "Iteration Number: 5285\n",
      "Loss: 13.331547707653105\n",
      "l2 norm of gradients: 0.10340517714569668\n",
      "l2 norm of weights: 3.2011425886926723\n",
      "---------------------\n",
      "Iteration Number: 5286\n",
      "Loss: 13.330120204919272\n",
      "l2 norm of gradients: 0.1033663365351015\n",
      "l2 norm of weights: 3.201148708867379\n",
      "---------------------\n",
      "Iteration Number: 5287\n",
      "Loss: 13.32869381302028\n",
      "l2 norm of gradients: 0.10332751626236616\n",
      "l2 norm of weights: 3.2011548992593193\n",
      "---------------------\n",
      "Iteration Number: 5288\n",
      "Loss: 13.32726853104879\n",
      "l2 norm of gradients: 0.10328871633481046\n",
      "l2 norm of weights: 3.201161159810863\n",
      "---------------------\n",
      "Iteration Number: 5289\n",
      "Loss: 13.325844358097548\n",
      "l2 norm of gradients: 0.10324993675974345\n",
      "l2 norm of weights: 3.2011674904644014\n",
      "---------------------\n",
      "Iteration Number: 5290\n",
      "Loss: 13.324421293259423\n",
      "l2 norm of gradients: 0.10321117754446318\n",
      "l2 norm of weights: 3.2011738911623464\n",
      "---------------------\n",
      "Iteration Number: 5291\n",
      "Loss: 13.322999335627467\n",
      "l2 norm of gradients: 0.10317243869625656\n",
      "l2 norm of weights: 3.201180361847129\n",
      "---------------------\n",
      "Iteration Number: 5292\n",
      "Loss: 13.321578484294767\n",
      "l2 norm of gradients: 0.10313372022239917\n",
      "l2 norm of weights: 3.2011869024612025\n",
      "---------------------\n",
      "Iteration Number: 5293\n",
      "Loss: 13.320158738354587\n",
      "l2 norm of gradients: 0.10309502213015516\n",
      "l2 norm of weights: 3.20119351294704\n",
      "---------------------\n",
      "Iteration Number: 5294\n",
      "Loss: 13.31874009690031\n",
      "l2 norm of gradients: 0.10305634442677697\n",
      "l2 norm of weights: 3.2012001932471335\n",
      "---------------------\n",
      "Iteration Number: 5295\n",
      "Loss: 13.317322559025454\n",
      "l2 norm of gradients: 0.10301768711950522\n",
      "l2 norm of weights: 3.2012069433039985\n",
      "---------------------\n",
      "Iteration Number: 5296\n",
      "Loss: 13.315906123823664\n",
      "l2 norm of gradients: 0.10297905021556851\n",
      "l2 norm of weights: 3.201213763060168\n",
      "---------------------\n",
      "Iteration Number: 5297\n",
      "Loss: 13.314490790388696\n",
      "l2 norm of gradients: 0.1029404337221833\n",
      "l2 norm of weights: 3.2012206524581974\n",
      "---------------------\n",
      "Iteration Number: 5298\n",
      "Loss: 13.313076557814465\n",
      "l2 norm of gradients: 0.10290183764655374\n",
      "l2 norm of weights: 3.2012276114406615\n",
      "---------------------\n",
      "Iteration Number: 5299\n",
      "Loss: 13.311663425194988\n",
      "l2 norm of gradients: 0.10286326199587137\n",
      "l2 norm of weights: 3.201234639950156\n",
      "---------------------\n",
      "Iteration Number: 5300\n",
      "Loss: 13.310251391624455\n",
      "l2 norm of gradients: 0.10282470677731512\n",
      "l2 norm of weights: 3.201241737929296\n",
      "---------------------\n",
      "Iteration Number: 5301\n",
      "Loss: 13.308840456197156\n",
      "l2 norm of gradients: 0.1027861719980511\n",
      "l2 norm of weights: 3.2012489053207185\n",
      "---------------------\n",
      "Iteration Number: 5302\n",
      "Loss: 13.30743061800753\n",
      "l2 norm of gradients: 0.10274765766523229\n",
      "l2 norm of weights: 3.201256142067079\n",
      "---------------------\n",
      "Iteration Number: 5303\n",
      "Loss: 13.306021876150147\n",
      "l2 norm of gradients: 0.10270916378599865\n",
      "l2 norm of weights: 3.201263448111055\n",
      "---------------------\n",
      "Iteration Number: 5304\n",
      "Loss: 13.304614229719757\n",
      "l2 norm of gradients: 0.10267069036747661\n",
      "l2 norm of weights: 3.2012708233953426\n",
      "---------------------\n",
      "Iteration Number: 5305\n",
      "Loss: 13.303207677811173\n",
      "l2 norm of gradients: 0.10263223741677922\n",
      "l2 norm of weights: 3.20127826786266\n",
      "---------------------\n",
      "Iteration Number: 5306\n",
      "Loss: 13.301802219519434\n",
      "l2 norm of gradients: 0.10259380494100573\n",
      "l2 norm of weights: 3.2012857814557436\n",
      "---------------------\n",
      "Iteration Number: 5307\n",
      "Loss: 13.300397853939648\n",
      "l2 norm of gradients: 0.10255539294724161\n",
      "l2 norm of weights: 3.2012933641173524\n",
      "---------------------\n",
      "Iteration Number: 5308\n",
      "Loss: 13.298994580167117\n",
      "l2 norm of gradients: 0.10251700144255826\n",
      "l2 norm of weights: 3.2013010157902637\n",
      "---------------------\n",
      "Iteration Number: 5309\n",
      "Loss: 13.29759239729727\n",
      "l2 norm of gradients: 0.10247863043401297\n",
      "l2 norm of weights: 3.201308736417276\n",
      "---------------------\n",
      "Iteration Number: 5310\n",
      "Loss: 13.296191304425669\n",
      "l2 norm of gradients: 0.10244027992864858\n",
      "l2 norm of weights: 3.2013165259412073\n",
      "---------------------\n",
      "Iteration Number: 5311\n",
      "Loss: 13.294791300648054\n",
      "l2 norm of gradients: 0.1024019499334935\n",
      "l2 norm of weights: 3.201324384304896\n",
      "---------------------\n",
      "Iteration Number: 5312\n",
      "Loss: 13.2933923850603\n",
      "l2 norm of gradients: 0.10236364045556136\n",
      "l2 norm of weights: 3.2013323114512016\n",
      "---------------------\n",
      "Iteration Number: 5313\n",
      "Loss: 13.291994556758382\n",
      "l2 norm of gradients: 0.10232535150185107\n",
      "l2 norm of weights: 3.2013403073230027\n",
      "---------------------\n",
      "Iteration Number: 5314\n",
      "Loss: 13.290597814838536\n",
      "l2 norm of gradients: 0.10228708307934642\n",
      "l2 norm of weights: 3.201348371863198\n",
      "---------------------\n",
      "Iteration Number: 5315\n",
      "Loss: 13.289202158397055\n",
      "l2 norm of gradients: 0.10224883519501608\n",
      "l2 norm of weights: 3.2013565050147066\n",
      "---------------------\n",
      "Iteration Number: 5316\n",
      "Loss: 13.287807586530409\n",
      "l2 norm of gradients: 0.10221060785581336\n",
      "l2 norm of weights: 3.2013647067204682\n",
      "---------------------\n",
      "Iteration Number: 5317\n",
      "Loss: 13.286414098335246\n",
      "l2 norm of gradients: 0.10217240106867607\n",
      "l2 norm of weights: 3.201372976923442\n",
      "---------------------\n",
      "Iteration Number: 5318\n",
      "Loss: 13.285021692908364\n",
      "l2 norm of gradients: 0.1021342148405264\n",
      "l2 norm of weights: 3.2013813155666075\n",
      "---------------------\n",
      "Iteration Number: 5319\n",
      "Loss: 13.283630369346682\n",
      "l2 norm of gradients: 0.10209604917827067\n",
      "l2 norm of weights: 3.201389722592964\n",
      "---------------------\n",
      "Iteration Number: 5320\n",
      "Loss: 13.282240126747322\n",
      "l2 norm of gradients: 0.10205790408879922\n",
      "l2 norm of weights: 3.2013981979455313\n",
      "---------------------\n",
      "Iteration Number: 5321\n",
      "Loss: 13.280850964207543\n",
      "l2 norm of gradients: 0.10201977957898624\n",
      "l2 norm of weights: 3.201406741567349\n",
      "---------------------\n",
      "Iteration Number: 5322\n",
      "Loss: 13.27946288082477\n",
      "l2 norm of gradients: 0.10198167565568965\n",
      "l2 norm of weights: 3.2014153534014764\n",
      "---------------------\n",
      "Iteration Number: 5323\n",
      "Loss: 13.27807587569659\n",
      "l2 norm of gradients: 0.10194359232575088\n",
      "l2 norm of weights: 3.201424033390994\n",
      "---------------------\n",
      "Iteration Number: 5324\n",
      "Loss: 13.276689947920778\n",
      "l2 norm of gradients: 0.10190552959599476\n",
      "l2 norm of weights: 3.2014327814790007\n",
      "---------------------\n",
      "Iteration Number: 5325\n",
      "Loss: 13.275305096595213\n",
      "l2 norm of gradients: 0.10186748747322924\n",
      "l2 norm of weights: 3.201441597608617\n",
      "---------------------\n",
      "Iteration Number: 5326\n",
      "Loss: 13.273921320817982\n",
      "l2 norm of gradients: 0.10182946596424546\n",
      "l2 norm of weights: 3.2014504817229814\n",
      "---------------------\n",
      "Iteration Number: 5327\n",
      "Loss: 13.272538619687365\n",
      "l2 norm of gradients: 0.10179146507581732\n",
      "l2 norm of weights: 3.201459433765255\n",
      "---------------------\n",
      "Iteration Number: 5328\n",
      "Loss: 13.27115699230177\n",
      "l2 norm of gradients: 0.10175348481470163\n",
      "l2 norm of weights: 3.2014684536786175\n",
      "---------------------\n",
      "Iteration Number: 5329\n",
      "Loss: 13.269776437759754\n",
      "l2 norm of gradients: 0.10171552518763757\n",
      "l2 norm of weights: 3.2014775414062675\n",
      "---------------------\n",
      "Iteration Number: 5330\n",
      "Loss: 13.268396955160103\n",
      "l2 norm of gradients: 0.10167758620134695\n",
      "l2 norm of weights: 3.201486696891426\n",
      "---------------------\n",
      "Iteration Number: 5331\n",
      "Loss: 13.267018543601738\n",
      "l2 norm of gradients: 0.1016396678625337\n",
      "l2 norm of weights: 3.201495920077331\n",
      "---------------------\n",
      "Iteration Number: 5332\n",
      "Loss: 13.265641202183781\n",
      "l2 norm of gradients: 0.10160177017788391\n",
      "l2 norm of weights: 3.201505210907243\n",
      "---------------------\n",
      "Iteration Number: 5333\n",
      "Loss: 13.264264930005481\n",
      "l2 norm of gradients: 0.10156389315406568\n",
      "l2 norm of weights: 3.201514569324441\n",
      "---------------------\n",
      "Iteration Number: 5334\n",
      "Loss: 13.262889726166321\n",
      "l2 norm of gradients: 0.10152603679772879\n",
      "l2 norm of weights: 3.2015239952722245\n",
      "---------------------\n",
      "Iteration Number: 5335\n",
      "Loss: 13.261515589765928\n",
      "l2 norm of gradients: 0.1014882011155048\n",
      "l2 norm of weights: 3.2015334886939133\n",
      "---------------------\n",
      "Iteration Number: 5336\n",
      "Loss: 13.260142519904102\n",
      "l2 norm of gradients: 0.10145038611400664\n",
      "l2 norm of weights: 3.201543049532846\n",
      "---------------------\n",
      "Iteration Number: 5337\n",
      "Loss: 13.258770515680833\n",
      "l2 norm of gradients: 0.10141259179982871\n",
      "l2 norm of weights: 3.2015526777323817\n",
      "---------------------\n",
      "Iteration Number: 5338\n",
      "Loss: 13.25739957619632\n",
      "l2 norm of gradients: 0.10137481817954647\n",
      "l2 norm of weights: 3.201562373235899\n",
      "---------------------\n",
      "Iteration Number: 5339\n",
      "Loss: 13.256029700550883\n",
      "l2 norm of gradients: 0.10133706525971643\n",
      "l2 norm of weights: 3.2015721359867984\n",
      "---------------------\n",
      "Iteration Number: 5340\n",
      "Loss: 13.25466088784509\n",
      "l2 norm of gradients: 0.10129933304687608\n",
      "l2 norm of weights: 3.2015819659284963\n",
      "---------------------\n",
      "Iteration Number: 5341\n",
      "Loss: 13.253293137179657\n",
      "l2 norm of gradients: 0.10126162154754349\n",
      "l2 norm of weights: 3.2015918630044333\n",
      "---------------------\n",
      "Iteration Number: 5342\n",
      "Loss: 13.251926447655475\n",
      "l2 norm of gradients: 0.10122393076821745\n",
      "l2 norm of weights: 3.201601827158066\n",
      "---------------------\n",
      "Iteration Number: 5343\n",
      "Loss: 13.250560818373676\n",
      "l2 norm of gradients: 0.10118626071537705\n",
      "l2 norm of weights: 3.201611858332875\n",
      "---------------------\n",
      "Iteration Number: 5344\n",
      "Loss: 13.249196248435506\n",
      "l2 norm of gradients: 0.10114861139548172\n",
      "l2 norm of weights: 3.2016219564723563\n",
      "---------------------\n",
      "Iteration Number: 5345\n",
      "Loss: 13.24783273694249\n",
      "l2 norm of gradients: 0.10111098281497101\n",
      "l2 norm of weights: 3.201632121520029\n",
      "---------------------\n",
      "Iteration Number: 5346\n",
      "Loss: 13.246470282996258\n",
      "l2 norm of gradients: 0.10107337498026445\n",
      "l2 norm of weights: 3.201642353419431\n",
      "---------------------\n",
      "Iteration Number: 5347\n",
      "Loss: 13.245108885698688\n",
      "l2 norm of gradients: 0.10103578789776127\n",
      "l2 norm of weights: 3.2016526521141198\n",
      "---------------------\n",
      "Iteration Number: 5348\n",
      "Loss: 13.243748544151838\n",
      "l2 norm of gradients: 0.1009982215738406\n",
      "l2 norm of weights: 3.201663017547672\n",
      "---------------------\n",
      "Iteration Number: 5349\n",
      "Loss: 13.242389257457942\n",
      "l2 norm of gradients: 0.10096067601486093\n",
      "l2 norm of weights: 3.2016734496636863\n",
      "---------------------\n",
      "Iteration Number: 5350\n",
      "Loss: 13.241031024719453\n",
      "l2 norm of gradients: 0.10092315122716017\n",
      "l2 norm of weights: 3.2016839484057793\n",
      "---------------------\n",
      "Iteration Number: 5351\n",
      "Loss: 13.239673845039036\n",
      "l2 norm of gradients: 0.10088564721705548\n",
      "l2 norm of weights: 3.201694513717588\n",
      "---------------------\n",
      "Iteration Number: 5352\n",
      "Loss: 13.23831771751952\n",
      "l2 norm of gradients: 0.10084816399084306\n",
      "l2 norm of weights: 3.2017051455427685\n",
      "---------------------\n",
      "Iteration Number: 5353\n",
      "Loss: 13.236962641263931\n",
      "l2 norm of gradients: 0.10081070155479814\n",
      "l2 norm of weights: 3.2017158438249984\n",
      "---------------------\n",
      "Iteration Number: 5354\n",
      "Loss: 13.23560861537556\n",
      "l2 norm of gradients: 0.10077325991517458\n",
      "l2 norm of weights: 3.201726608507973\n",
      "---------------------\n",
      "Iteration Number: 5355\n",
      "Loss: 13.234255638957816\n",
      "l2 norm of gradients: 0.10073583907820509\n",
      "l2 norm of weights: 3.20173743953541\n",
      "---------------------\n",
      "Iteration Number: 5356\n",
      "Loss: 13.232903711114368\n",
      "l2 norm of gradients: 0.10069843905010074\n",
      "l2 norm of weights: 3.2017483368510438\n",
      "---------------------\n",
      "Iteration Number: 5357\n",
      "Loss: 13.231552830949067\n",
      "l2 norm of gradients: 0.100661059837051\n",
      "l2 norm of weights: 3.2017593003986313\n",
      "---------------------\n",
      "Iteration Number: 5358\n",
      "Loss: 13.230202997566007\n",
      "l2 norm of gradients: 0.10062370144522356\n",
      "l2 norm of weights: 3.201770330121947\n",
      "---------------------\n",
      "Iteration Number: 5359\n",
      "Loss: 13.228854210069432\n",
      "l2 norm of gradients: 0.10058636388076418\n",
      "l2 norm of weights: 3.2017814259647865\n",
      "---------------------\n",
      "Iteration Number: 5360\n",
      "Loss: 13.22750646756386\n",
      "l2 norm of gradients: 0.10054904714979652\n",
      "l2 norm of weights: 3.201792587870966\n",
      "---------------------\n",
      "Iteration Number: 5361\n",
      "Loss: 13.226159769153943\n",
      "l2 norm of gradients: 0.10051175125842209\n",
      "l2 norm of weights: 3.2018038157843187\n",
      "---------------------\n",
      "Iteration Number: 5362\n",
      "Loss: 13.224814113944621\n",
      "l2 norm of gradients: 0.10047447621271999\n",
      "l2 norm of weights: 3.2018151096487\n",
      "---------------------\n",
      "Iteration Number: 5363\n",
      "Loss: 13.223469501041016\n",
      "l2 norm of gradients: 0.10043722201874684\n",
      "l2 norm of weights: 3.201826469407985\n",
      "---------------------\n",
      "Iteration Number: 5364\n",
      "Loss: 13.222125929548442\n",
      "l2 norm of gradients: 0.10039998868253662\n",
      "l2 norm of weights: 3.2018378950060664\n",
      "---------------------\n",
      "Iteration Number: 5365\n",
      "Loss: 13.220783398572472\n",
      "l2 norm of gradients: 0.10036277621010051\n",
      "l2 norm of weights: 3.2018493863868596\n",
      "---------------------\n",
      "Iteration Number: 5366\n",
      "Loss: 13.219441907218865\n",
      "l2 norm of gradients: 0.10032558460742685\n",
      "l2 norm of weights: 3.2018609434942977\n",
      "---------------------\n",
      "Iteration Number: 5367\n",
      "Loss: 13.218101454593613\n",
      "l2 norm of gradients: 0.10028841388048083\n",
      "l2 norm of weights: 3.201872566272334\n",
      "---------------------\n",
      "Iteration Number: 5368\n",
      "Loss: 13.216762039802909\n",
      "l2 norm of gradients: 0.10025126403520451\n",
      "l2 norm of weights: 3.201884254664942\n",
      "---------------------\n",
      "Iteration Number: 5369\n",
      "Loss: 13.21542366195322\n",
      "l2 norm of gradients: 0.10021413507751663\n",
      "l2 norm of weights: 3.2018960086161155\n",
      "---------------------\n",
      "Iteration Number: 5370\n",
      "Loss: 13.214086320151157\n",
      "l2 norm of gradients: 0.10017702701331231\n",
      "l2 norm of weights: 3.2019078280698667\n",
      "---------------------\n",
      "Iteration Number: 5371\n",
      "Loss: 13.212750013503632\n",
      "l2 norm of gradients: 0.1001399398484633\n",
      "l2 norm of weights: 3.201919712970228\n",
      "---------------------\n",
      "Iteration Number: 5372\n",
      "Loss: 13.211414741117714\n",
      "l2 norm of gradients: 0.10010287358881738\n",
      "l2 norm of weights: 3.201931663261251\n",
      "---------------------\n",
      "Iteration Number: 5373\n",
      "Loss: 13.210080502100759\n",
      "l2 norm of gradients: 0.10006582824019865\n",
      "l2 norm of weights: 3.2019436788870097\n",
      "---------------------\n",
      "Iteration Number: 5374\n",
      "Loss: 13.208747295560297\n",
      "l2 norm of gradients: 0.10002880380840702\n",
      "l2 norm of weights: 3.2019557597915944\n",
      "---------------------\n",
      "Iteration Number: 5375\n",
      "Loss: 13.207415120604129\n",
      "l2 norm of gradients: 0.09999180029921835\n",
      "l2 norm of weights: 3.201967905919118\n",
      "---------------------\n",
      "Iteration Number: 5376\n",
      "Loss: 13.206083976340294\n",
      "l2 norm of gradients: 0.09995481771838421\n",
      "l2 norm of weights: 3.201980117213712\n",
      "---------------------\n",
      "Iteration Number: 5377\n",
      "Loss: 13.204753861876991\n",
      "l2 norm of gradients: 0.09991785607163171\n",
      "l2 norm of weights: 3.201992393619526\n",
      "---------------------\n",
      "Iteration Number: 5378\n",
      "Loss: 13.203424776322734\n",
      "l2 norm of gradients: 0.09988091536466348\n",
      "l2 norm of weights: 3.202004735080733\n",
      "---------------------\n",
      "Iteration Number: 5379\n",
      "Loss: 13.20209671878623\n",
      "l2 norm of gradients: 0.09984399560315735\n",
      "l2 norm of weights: 3.202017141541522\n",
      "---------------------\n",
      "Iteration Number: 5380\n",
      "Loss: 13.200769688376438\n",
      "l2 norm of gradients: 0.09980709679276649\n",
      "l2 norm of weights: 3.202029612946105\n",
      "---------------------\n",
      "Iteration Number: 5381\n",
      "Loss: 13.199443684202526\n",
      "l2 norm of gradients: 0.09977021893911898\n",
      "l2 norm of weights: 3.2020421492387117\n",
      "---------------------\n",
      "Iteration Number: 5382\n",
      "Loss: 13.198118705373947\n",
      "l2 norm of gradients: 0.09973336204781787\n",
      "l2 norm of weights: 3.202054750363593\n",
      "---------------------\n",
      "Iteration Number: 5383\n",
      "Loss: 13.196794751000342\n",
      "l2 norm of gradients: 0.09969652612444103\n",
      "l2 norm of weights: 3.2020674162650176\n",
      "---------------------\n",
      "Iteration Number: 5384\n",
      "Loss: 13.19547182019165\n",
      "l2 norm of gradients: 0.09965971117454099\n",
      "l2 norm of weights: 3.2020801468872753\n",
      "---------------------\n",
      "Iteration Number: 5385\n",
      "Loss: 13.194149912057984\n",
      "l2 norm of gradients: 0.09962291720364475\n",
      "l2 norm of weights: 3.2020929421746764\n",
      "---------------------\n",
      "Iteration Number: 5386\n",
      "Loss: 13.192829025709774\n",
      "l2 norm of gradients: 0.09958614421725377\n",
      "l2 norm of weights: 3.20210580207155\n",
      "---------------------\n",
      "Iteration Number: 5387\n",
      "Loss: 13.191509160257636\n",
      "l2 norm of gradients: 0.09954939222084382\n",
      "l2 norm of weights: 3.202118726522245\n",
      "---------------------\n",
      "Iteration Number: 5388\n",
      "Loss: 13.190190314812455\n",
      "l2 norm of gradients: 0.0995126612198647\n",
      "l2 norm of weights: 3.2021317154711304\n",
      "---------------------\n",
      "Iteration Number: 5389\n",
      "Loss: 13.188872488485377\n",
      "l2 norm of gradients: 0.09947595121974036\n",
      "l2 norm of weights: 3.202144768862595\n",
      "---------------------\n",
      "Iteration Number: 5390\n",
      "Loss: 13.18755568038778\n",
      "l2 norm of gradients: 0.0994392622258685\n",
      "l2 norm of weights: 3.202157886641048\n",
      "---------------------\n",
      "Iteration Number: 5391\n",
      "Loss: 13.186239889631302\n",
      "l2 norm of gradients: 0.09940259424362073\n",
      "l2 norm of weights: 3.202171068750916\n",
      "---------------------\n",
      "Iteration Number: 5392\n",
      "Loss: 13.184925115327813\n",
      "l2 norm of gradients: 0.09936594727834223\n",
      "l2 norm of weights: 3.2021843151366487\n",
      "---------------------\n",
      "Iteration Number: 5393\n",
      "Loss: 13.183611356589468\n",
      "l2 norm of gradients: 0.09932932133535173\n",
      "l2 norm of weights: 3.2021976257427136\n",
      "---------------------\n",
      "Iteration Number: 5394\n",
      "Loss: 13.182298612528637\n",
      "l2 norm of gradients: 0.09929271641994132\n",
      "l2 norm of weights: 3.202211000513599\n",
      "---------------------\n",
      "Iteration Number: 5395\n",
      "Loss: 13.180986882257987\n",
      "l2 norm of gradients: 0.09925613253737635\n",
      "l2 norm of weights: 3.2022244393938117\n",
      "---------------------\n",
      "Iteration Number: 5396\n",
      "Loss: 13.179676164890404\n",
      "l2 norm of gradients: 0.09921956969289537\n",
      "l2 norm of weights: 3.20223794232788\n",
      "---------------------\n",
      "Iteration Number: 5397\n",
      "Loss: 13.17836645953906\n",
      "l2 norm of gradients: 0.09918302789170994\n",
      "l2 norm of weights: 3.2022515092603503\n",
      "---------------------\n",
      "Iteration Number: 5398\n",
      "Loss: 13.177057765317379\n",
      "l2 norm of gradients: 0.09914650713900447\n",
      "l2 norm of weights: 3.202265140135791\n",
      "---------------------\n",
      "Iteration Number: 5399\n",
      "Loss: 13.175750081339043\n",
      "l2 norm of gradients: 0.09911000743993621\n",
      "l2 norm of weights: 3.202278834898789\n",
      "---------------------\n",
      "Iteration Number: 5400\n",
      "Loss: 13.174443406718009\n",
      "l2 norm of gradients: 0.09907352879963507\n",
      "l2 norm of weights: 3.202292593493951\n",
      "---------------------\n",
      "Iteration Number: 5401\n",
      "Loss: 13.173137740568452\n",
      "l2 norm of gradients: 0.09903707122320349\n",
      "l2 norm of weights: 3.202306415865903\n",
      "---------------------\n",
      "Iteration Number: 5402\n",
      "Loss: 13.171833082004879\n",
      "l2 norm of gradients: 0.0990006347157163\n",
      "l2 norm of weights: 3.202320301959293\n",
      "---------------------\n",
      "Iteration Number: 5403\n",
      "Loss: 13.170529430142011\n",
      "l2 norm of gradients: 0.09896421928222066\n",
      "l2 norm of weights: 3.2023342517187867\n",
      "---------------------\n",
      "Iteration Number: 5404\n",
      "Loss: 13.169226784094871\n",
      "l2 norm of gradients: 0.09892782492773589\n",
      "l2 norm of weights: 3.2023482650890713\n",
      "---------------------\n",
      "Iteration Number: 5405\n",
      "Loss: 13.167925142978728\n",
      "l2 norm of gradients: 0.09889145165725344\n",
      "l2 norm of weights: 3.2023623420148524\n",
      "---------------------\n",
      "Iteration Number: 5406\n",
      "Loss: 13.166624505909125\n",
      "l2 norm of gradients: 0.09885509947573665\n",
      "l2 norm of weights: 3.202376482440858\n",
      "---------------------\n",
      "Iteration Number: 5407\n",
      "Loss: 13.16532487200188\n",
      "l2 norm of gradients: 0.09881876838812072\n",
      "l2 norm of weights: 3.202390686311832\n",
      "---------------------\n",
      "Iteration Number: 5408\n",
      "Loss: 13.16402624037309\n",
      "l2 norm of gradients: 0.09878245839931253\n",
      "l2 norm of weights: 3.2024049535725423\n",
      "---------------------\n",
      "Iteration Number: 5409\n",
      "Loss: 13.162728610139125\n",
      "l2 norm of gradients: 0.09874616951419063\n",
      "l2 norm of weights: 3.2024192841677737\n",
      "---------------------\n",
      "Iteration Number: 5410\n",
      "Loss: 13.161431980416614\n",
      "l2 norm of gradients: 0.09870990173760502\n",
      "l2 norm of weights: 3.2024336780423335\n",
      "---------------------\n",
      "Iteration Number: 5411\n",
      "Loss: 13.160136350322478\n",
      "l2 norm of gradients: 0.09867365507437707\n",
      "l2 norm of weights: 3.2024481351410468\n",
      "---------------------\n",
      "Iteration Number: 5412\n",
      "Loss: 13.15884171897391\n",
      "l2 norm of gradients: 0.09863742952929934\n",
      "l2 norm of weights: 3.2024626554087594\n",
      "---------------------\n",
      "Iteration Number: 5413\n",
      "Loss: 13.15754808548839\n",
      "l2 norm of gradients: 0.0986012251071357\n",
      "l2 norm of weights: 3.2024772387903386\n",
      "---------------------\n",
      "Iteration Number: 5414\n",
      "Loss: 13.156255448983673\n",
      "l2 norm of gradients: 0.09856504181262092\n",
      "l2 norm of weights: 3.202491885230669\n",
      "---------------------\n",
      "Iteration Number: 5415\n",
      "Loss: 13.154963808577792\n",
      "l2 norm of gradients: 0.09852887965046075\n",
      "l2 norm of weights: 3.2025065946746567\n",
      "---------------------\n",
      "Iteration Number: 5416\n",
      "Loss: 13.153673163389085\n",
      "l2 norm of gradients: 0.09849273862533171\n",
      "l2 norm of weights: 3.2025213670672277\n",
      "---------------------\n",
      "Iteration Number: 5417\n",
      "Loss: 13.15238351253615\n",
      "l2 norm of gradients: 0.09845661874188112\n",
      "l2 norm of weights: 3.202536202353328\n",
      "---------------------\n",
      "Iteration Number: 5418\n",
      "Loss: 13.151094855137858\n",
      "l2 norm of gradients: 0.09842052000472669\n",
      "l2 norm of weights: 3.2025511004779235\n",
      "---------------------\n",
      "Iteration Number: 5419\n",
      "Loss: 13.149807190313423\n",
      "l2 norm of gradients: 0.09838444241845691\n",
      "l2 norm of weights: 3.202566061386\n",
      "---------------------\n",
      "Iteration Number: 5420\n",
      "Loss: 13.148520517182284\n",
      "l2 norm of gradients: 0.09834838598763036\n",
      "l2 norm of weights: 3.202581085022563\n",
      "---------------------\n",
      "Iteration Number: 5421\n",
      "Loss: 13.147234834864227\n",
      "l2 norm of gradients: 0.09831235071677606\n",
      "l2 norm of weights: 3.2025961713326394\n",
      "---------------------\n",
      "Iteration Number: 5422\n",
      "Loss: 13.145950142479295\n",
      "l2 norm of gradients: 0.0982763366103931\n",
      "l2 norm of weights: 3.202611320261275\n",
      "---------------------\n",
      "Iteration Number: 5423\n",
      "Loss: 13.144666439147802\n",
      "l2 norm of gradients: 0.09824034367295068\n",
      "l2 norm of weights: 3.202626531753536\n",
      "---------------------\n",
      "Iteration Number: 5424\n",
      "Loss: 13.143383723990418\n",
      "l2 norm of gradients: 0.0982043719088879\n",
      "l2 norm of weights: 3.202641805754508\n",
      "---------------------\n",
      "Iteration Number: 5425\n",
      "Loss: 13.142101996128078\n",
      "l2 norm of gradients: 0.09816842132261376\n",
      "l2 norm of weights: 3.2026571422092975\n",
      "---------------------\n",
      "Iteration Number: 5426\n",
      "Loss: 13.140821254681988\n",
      "l2 norm of gradients: 0.09813249191850693\n",
      "l2 norm of weights: 3.202672541063032\n",
      "---------------------\n",
      "Iteration Number: 5427\n",
      "Loss: 13.13954149877368\n",
      "l2 norm of gradients: 0.09809658370091573\n",
      "l2 norm of weights: 3.2026880022608566\n",
      "---------------------\n",
      "Iteration Number: 5428\n",
      "Loss: 13.138262727524992\n",
      "l2 norm of gradients: 0.09806069667415805\n",
      "l2 norm of weights: 3.2027035257479395\n",
      "---------------------\n",
      "Iteration Number: 5429\n",
      "Loss: 13.136984940058033\n",
      "l2 norm of gradients: 0.09802483084252119\n",
      "l2 norm of weights: 3.2027191114694658\n",
      "---------------------\n",
      "Iteration Number: 5430\n",
      "Loss: 13.135708135495257\n",
      "l2 norm of gradients: 0.0979889862102617\n",
      "l2 norm of weights: 3.202734759370644\n",
      "---------------------\n",
      "Iteration Number: 5431\n",
      "Loss: 13.134432312959373\n",
      "l2 norm of gradients: 0.09795316278160547\n",
      "l2 norm of weights: 3.2027504693967\n",
      "---------------------\n",
      "Iteration Number: 5432\n",
      "Loss: 13.133157471573425\n",
      "l2 norm of gradients: 0.0979173605607474\n",
      "l2 norm of weights: 3.202766241492882\n",
      "---------------------\n",
      "Iteration Number: 5433\n",
      "Loss: 13.131883610460761\n",
      "l2 norm of gradients: 0.09788157955185152\n",
      "l2 norm of weights: 3.2027820756044574\n",
      "---------------------\n",
      "Iteration Number: 5434\n",
      "Loss: 13.13061072874502\n",
      "l2 norm of gradients: 0.09784581975905067\n",
      "l2 norm of weights: 3.202797971676713\n",
      "---------------------\n",
      "Iteration Number: 5435\n",
      "Loss: 13.12933882555017\n",
      "l2 norm of gradients: 0.09781008118644659\n",
      "l2 norm of weights: 3.202813929654958\n",
      "---------------------\n",
      "Iteration Number: 5436\n",
      "Loss: 13.12806790000047\n",
      "l2 norm of gradients: 0.09777436383810971\n",
      "l2 norm of weights: 3.20282994948452\n",
      "---------------------\n",
      "Iteration Number: 5437\n",
      "Loss: 13.126797951220507\n",
      "l2 norm of gradients: 0.09773866771807913\n",
      "l2 norm of weights: 3.202846031110747\n",
      "---------------------\n",
      "Iteration Number: 5438\n",
      "Loss: 13.125528978335169\n",
      "l2 norm of gradients: 0.0977029928303624\n",
      "l2 norm of weights: 3.202862174479008\n",
      "---------------------\n",
      "Iteration Number: 5439\n",
      "Loss: 13.12426098046967\n",
      "l2 norm of gradients: 0.09766733917893554\n",
      "l2 norm of weights: 3.202878379534692\n",
      "---------------------\n",
      "Iteration Number: 5440\n",
      "Loss: 13.122993956749518\n",
      "l2 norm of gradients: 0.09763170676774291\n",
      "l2 norm of weights: 3.202894646223208\n",
      "---------------------\n",
      "Iteration Number: 5441\n",
      "Loss: 13.121727906300547\n",
      "l2 norm of gradients: 0.09759609560069711\n",
      "l2 norm of weights: 3.2029109744899857\n",
      "---------------------\n",
      "Iteration Number: 5442\n",
      "Loss: 13.120462828248943\n",
      "l2 norm of gradients: 0.09756050568167894\n",
      "l2 norm of weights: 3.202927364280475\n",
      "---------------------\n",
      "Iteration Number: 5443\n",
      "Loss: 13.119198721721144\n",
      "l2 norm of gradients: 0.09752493701453716\n",
      "l2 norm of weights: 3.2029438155401455\n",
      "---------------------\n",
      "Iteration Number: 5444\n",
      "Loss: 13.117935585843952\n",
      "l2 norm of gradients: 0.09748938960308849\n",
      "l2 norm of weights: 3.202960328214489\n",
      "---------------------\n",
      "Iteration Number: 5445\n",
      "Loss: 13.116673419744483\n",
      "l2 norm of gradients: 0.09745386345111758\n",
      "l2 norm of weights: 3.2029769022490147\n",
      "---------------------\n",
      "Iteration Number: 5446\n",
      "Loss: 13.115412222550182\n",
      "l2 norm of gradients: 0.09741835856237682\n",
      "l2 norm of weights: 3.2029935375892555\n",
      "---------------------\n",
      "Iteration Number: 5447\n",
      "Loss: 13.114151993388813\n",
      "l2 norm of gradients: 0.09738287494058631\n",
      "l2 norm of weights: 3.2030102341807614\n",
      "---------------------\n",
      "Iteration Number: 5448\n",
      "Loss: 13.112892731388463\n",
      "l2 norm of gradients: 0.0973474125894337\n",
      "l2 norm of weights: 3.2030269919691055\n",
      "---------------------\n",
      "Iteration Number: 5449\n",
      "Loss: 13.111634435677542\n",
      "l2 norm of gradients: 0.09731197151257417\n",
      "l2 norm of weights: 3.2030438108998793\n",
      "---------------------\n",
      "Iteration Number: 5450\n",
      "Loss: 13.11037710538479\n",
      "l2 norm of gradients: 0.09727655171363037\n",
      "l2 norm of weights: 3.203060690918697\n",
      "---------------------\n",
      "Iteration Number: 5451\n",
      "Loss: 13.109120739639295\n",
      "l2 norm of gradients: 0.09724115319619209\n",
      "l2 norm of weights: 3.2030776319711913\n",
      "---------------------\n",
      "Iteration Number: 5452\n",
      "Loss: 13.107865337570445\n",
      "l2 norm of gradients: 0.09720577596381658\n",
      "l2 norm of weights: 3.2030946340030155\n",
      "---------------------\n",
      "Iteration Number: 5453\n",
      "Loss: 13.106610898307984\n",
      "l2 norm of gradients: 0.09717042002002813\n",
      "l2 norm of weights: 3.2031116969598448\n",
      "---------------------\n",
      "Iteration Number: 5454\n",
      "Loss: 13.105357420981967\n",
      "l2 norm of gradients: 0.09713508536831801\n",
      "l2 norm of weights: 3.2031288207873736\n",
      "---------------------\n",
      "Iteration Number: 5455\n",
      "Loss: 13.104104904722826\n",
      "l2 norm of gradients: 0.09709977201214465\n",
      "l2 norm of weights: 3.203146005431317\n",
      "---------------------\n",
      "Iteration Number: 5456\n",
      "Loss: 13.102853348661277\n",
      "l2 norm of gradients: 0.09706447995493322\n",
      "l2 norm of weights: 3.2031632508374113\n",
      "---------------------\n",
      "Iteration Number: 5457\n",
      "Loss: 13.101602751928393\n",
      "l2 norm of gradients: 0.09702920920007581\n",
      "l2 norm of weights: 3.2031805569514122\n",
      "---------------------\n",
      "Iteration Number: 5458\n",
      "Loss: 13.100353113655615\n",
      "l2 norm of gradients: 0.09699395975093111\n",
      "l2 norm of weights: 3.203197923719097\n",
      "---------------------\n",
      "Iteration Number: 5459\n",
      "Loss: 13.099104432974691\n",
      "l2 norm of gradients: 0.09695873161082456\n",
      "l2 norm of weights: 3.2032153510862638\n",
      "---------------------\n",
      "Iteration Number: 5460\n",
      "Loss: 13.097856709017691\n",
      "l2 norm of gradients: 0.0969235247830481\n",
      "l2 norm of weights: 3.2032328389987303\n",
      "---------------------\n",
      "Iteration Number: 5461\n",
      "Loss: 13.096609940917077\n",
      "l2 norm of gradients: 0.09688833927086012\n",
      "l2 norm of weights: 3.2032503874023353\n",
      "---------------------\n",
      "Iteration Number: 5462\n",
      "Loss: 13.095364127805626\n",
      "l2 norm of gradients: 0.09685317507748545\n",
      "l2 norm of weights: 3.203267996242938\n",
      "---------------------\n",
      "Iteration Number: 5463\n",
      "Loss: 13.094119268816462\n",
      "l2 norm of gradients: 0.09681803220611528\n",
      "l2 norm of weights: 3.2032856654664186\n",
      "---------------------\n",
      "Iteration Number: 5464\n",
      "Loss: 13.092875363083067\n",
      "l2 norm of gradients: 0.09678291065990688\n",
      "l2 norm of weights: 3.2033033950186782\n",
      "---------------------\n",
      "Iteration Number: 5465\n",
      "Loss: 13.09163240973926\n",
      "l2 norm of gradients: 0.0967478104419838\n",
      "l2 norm of weights: 3.2033211848456378\n",
      "---------------------\n",
      "Iteration Number: 5466\n",
      "Loss: 13.090390407919209\n",
      "l2 norm of gradients: 0.0967127315554356\n",
      "l2 norm of weights: 3.20333903489324\n",
      "---------------------\n",
      "Iteration Number: 5467\n",
      "Loss: 13.089149356757433\n",
      "l2 norm of gradients: 0.09667767400331786\n",
      "l2 norm of weights: 3.2033569451074473\n",
      "---------------------\n",
      "Iteration Number: 5468\n",
      "Loss: 13.087909255388794\n",
      "l2 norm of gradients: 0.09664263778865212\n",
      "l2 norm of weights: 3.203374915434244\n",
      "---------------------\n",
      "Iteration Number: 5469\n",
      "Loss: 13.086670102948533\n",
      "l2 norm of gradients: 0.09660762291442564\n",
      "l2 norm of weights: 3.2033929458196333\n",
      "---------------------\n",
      "Iteration Number: 5470\n",
      "Loss: 13.085431898572232\n",
      "l2 norm of gradients: 0.09657262938359162\n",
      "l2 norm of weights: 3.203411036209642\n",
      "---------------------\n",
      "Iteration Number: 5471\n",
      "Loss: 13.084194641395806\n",
      "l2 norm of gradients: 0.09653765719906873\n",
      "l2 norm of weights: 3.203429186550314\n",
      "---------------------\n",
      "Iteration Number: 5472\n",
      "Loss: 13.082958330555554\n",
      "l2 norm of gradients: 0.09650270636374148\n",
      "l2 norm of weights: 3.203447396787719\n",
      "---------------------\n",
      "Iteration Number: 5473\n",
      "Loss: 13.081722965188131\n",
      "l2 norm of gradients: 0.09646777688045975\n",
      "l2 norm of weights: 3.2034656668679418\n",
      "---------------------\n",
      "Iteration Number: 5474\n",
      "Loss: 13.080488544430521\n",
      "l2 norm of gradients: 0.09643286875203896\n",
      "l2 norm of weights: 3.2034839967370927\n",
      "---------------------\n",
      "Iteration Number: 5475\n",
      "Loss: 13.079255067420132\n",
      "l2 norm of gradients: 0.09639798198125994\n",
      "l2 norm of weights: 3.2035023863413006\n",
      "---------------------\n",
      "Iteration Number: 5476\n",
      "Loss: 13.078022533294648\n",
      "l2 norm of gradients: 0.0963631165708688\n",
      "l2 norm of weights: 3.203520835626716\n",
      "---------------------\n",
      "Iteration Number: 5477\n",
      "Loss: 13.076790941192193\n",
      "l2 norm of gradients: 0.09632827252357692\n",
      "l2 norm of weights: 3.2035393445395104\n",
      "---------------------\n",
      "Iteration Number: 5478\n",
      "Loss: 13.075560290251193\n",
      "l2 norm of gradients: 0.09629344984206083\n",
      "l2 norm of weights: 3.2035579130258753\n",
      "---------------------\n",
      "Iteration Number: 5479\n",
      "Loss: 13.074330579610498\n",
      "l2 norm of gradients: 0.09625864852896227\n",
      "l2 norm of weights: 3.2035765410320245\n",
      "---------------------\n",
      "Iteration Number: 5480\n",
      "Loss: 13.073101808409296\n",
      "l2 norm of gradients: 0.09622386858688789\n",
      "l2 norm of weights: 3.2035952285041915\n",
      "---------------------\n",
      "Iteration Number: 5481\n",
      "Loss: 13.07187397578712\n",
      "l2 norm of gradients: 0.0961891100184094\n",
      "l2 norm of weights: 3.203613975388632\n",
      "---------------------\n",
      "Iteration Number: 5482\n",
      "Loss: 13.070647080883914\n",
      "l2 norm of gradients: 0.09615437282606334\n",
      "l2 norm of weights: 3.203632781631623\n",
      "---------------------\n",
      "Iteration Number: 5483\n",
      "Loss: 13.069421122839964\n",
      "l2 norm of gradients: 0.09611965701235117\n",
      "l2 norm of weights: 3.2036516471794596\n",
      "---------------------\n",
      "Iteration Number: 5484\n",
      "Loss: 13.068196100795937\n",
      "l2 norm of gradients: 0.09608496257973911\n",
      "l2 norm of weights: 3.2036705719784617\n",
      "---------------------\n",
      "Iteration Number: 5485\n",
      "Loss: 13.066972013892881\n",
      "l2 norm of gradients: 0.096050289530658\n",
      "l2 norm of weights: 3.203689555974969\n",
      "---------------------\n",
      "Iteration Number: 5486\n",
      "Loss: 13.06574886127221\n",
      "l2 norm of gradients: 0.09601563786750346\n",
      "l2 norm of weights: 3.2037085991153407\n",
      "---------------------\n",
      "Iteration Number: 5487\n",
      "Loss: 13.06452664207572\n",
      "l2 norm of gradients: 0.09598100759263546\n",
      "l2 norm of weights: 3.2037277013459597\n",
      "---------------------\n",
      "Iteration Number: 5488\n",
      "Loss: 13.06330535544556\n",
      "l2 norm of gradients: 0.0959463987083787\n",
      "l2 norm of weights: 3.203746862613228\n",
      "---------------------\n",
      "Iteration Number: 5489\n",
      "Loss: 13.062085000524277\n",
      "l2 norm of gradients: 0.09591181121702226\n",
      "l2 norm of weights: 3.2037660828635697\n",
      "---------------------\n",
      "Iteration Number: 5490\n",
      "Loss: 13.06086557645482\n",
      "l2 norm of gradients: 0.09587724512081956\n",
      "l2 norm of weights: 3.2037853620434302\n",
      "---------------------\n",
      "Iteration Number: 5491\n",
      "Loss: 13.059647082380465\n",
      "l2 norm of gradients: 0.09584270042198835\n",
      "l2 norm of weights: 3.2038047000992766\n",
      "---------------------\n",
      "Iteration Number: 5492\n",
      "Loss: 13.058429517444921\n",
      "l2 norm of gradients: 0.09580817712271072\n",
      "l2 norm of weights: 3.203824096977595\n",
      "---------------------\n",
      "Iteration Number: 5493\n",
      "Loss: 13.057212880792239\n",
      "l2 norm of gradients: 0.09577367522513286\n",
      "l2 norm of weights: 3.2038435526248956\n",
      "---------------------\n",
      "Iteration Number: 5494\n",
      "Loss: 13.055997171566876\n",
      "l2 norm of gradients: 0.09573919473136512\n",
      "l2 norm of weights: 3.2038630669877075\n",
      "---------------------\n",
      "Iteration Number: 5495\n",
      "Loss: 13.054782388913694\n",
      "l2 norm of gradients: 0.09570473564348199\n",
      "l2 norm of weights: 3.2038826400125835\n",
      "---------------------\n",
      "Iteration Number: 5496\n",
      "Loss: 13.053568531977886\n",
      "l2 norm of gradients: 0.09567029796352194\n",
      "l2 norm of weights: 3.203902271646096\n",
      "---------------------\n",
      "Iteration Number: 5497\n",
      "Loss: 13.052355599905084\n",
      "l2 norm of gradients: 0.0956358816934874\n",
      "l2 norm of weights: 3.2039219618348387\n",
      "---------------------\n",
      "Iteration Number: 5498\n",
      "Loss: 13.051143591841285\n",
      "l2 norm of gradients: 0.0956014868353447\n",
      "l2 norm of weights: 3.203941710525427\n",
      "---------------------\n",
      "Iteration Number: 5499\n",
      "Loss: 13.04993250693289\n",
      "l2 norm of gradients: 0.09556711339102406\n",
      "l2 norm of weights: 3.203961517664499\n",
      "---------------------\n",
      "Iteration Number: 5500\n",
      "Loss: 13.04872234432667\n",
      "l2 norm of gradients: 0.09553276136241945\n",
      "l2 norm of weights: 3.203981383198712\n",
      "---------------------\n",
      "Iteration Number: 5501\n",
      "Loss: 13.047513103169836\n",
      "l2 norm of gradients: 0.0954984307513886\n",
      "l2 norm of weights: 3.2040013070747473\n",
      "---------------------\n",
      "Iteration Number: 5502\n",
      "Loss: 13.04630478260991\n",
      "l2 norm of gradients: 0.09546412155975297\n",
      "l2 norm of weights: 3.2040212892393045\n",
      "---------------------\n",
      "Iteration Number: 5503\n",
      "Loss: 13.045097381794891\n",
      "l2 norm of gradients: 0.09542983378929755\n",
      "l2 norm of weights: 3.204041329639108\n",
      "---------------------\n",
      "Iteration Number: 5504\n",
      "Loss: 13.043890899873158\n",
      "l2 norm of gradients: 0.09539556744177095\n",
      "l2 norm of weights: 3.2040614282209003\n",
      "---------------------\n",
      "Iteration Number: 5505\n",
      "Loss: 13.04268533599342\n",
      "l2 norm of gradients: 0.09536132251888539\n",
      "l2 norm of weights: 3.204081584931449\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 5506\n",
      "Loss: 13.041480689304892\n",
      "l2 norm of gradients: 0.09532709902231648\n",
      "l2 norm of weights: 3.204101799717541\n",
      "---------------------\n",
      "Iteration Number: 5507\n",
      "Loss: 13.040276958957111\n",
      "l2 norm of gradients: 0.0952928969537032\n",
      "l2 norm of weights: 3.2041220725259856\n",
      "---------------------\n",
      "Iteration Number: 5508\n",
      "Loss: 13.039074144100047\n",
      "l2 norm of gradients: 0.095258716314648\n",
      "l2 norm of weights: 3.2041424033036123\n",
      "---------------------\n",
      "Iteration Number: 5509\n",
      "Loss: 13.037872243884035\n",
      "l2 norm of gradients: 0.09522455710671662\n",
      "l2 norm of weights: 3.2041627919972746\n",
      "---------------------\n",
      "Iteration Number: 5510\n",
      "Loss: 13.036671257459892\n",
      "l2 norm of gradients: 0.0951904193314381\n",
      "l2 norm of weights: 3.204183238553846\n",
      "---------------------\n",
      "Iteration Number: 5511\n",
      "Loss: 13.035471183978757\n",
      "l2 norm of gradients: 0.09515630299030467\n",
      "l2 norm of weights: 3.204203742920223\n",
      "---------------------\n",
      "Iteration Number: 5512\n",
      "Loss: 13.034272022592216\n",
      "l2 norm of gradients: 0.09512220808477166\n",
      "l2 norm of weights: 3.204224305043321\n",
      "---------------------\n",
      "Iteration Number: 5513\n",
      "Loss: 13.033073772452278\n",
      "l2 norm of gradients: 0.09508813461625772\n",
      "l2 norm of weights: 3.204244924870081\n",
      "---------------------\n",
      "Iteration Number: 5514\n",
      "Loss: 13.031876432711307\n",
      "l2 norm of gradients: 0.09505408258614445\n",
      "l2 norm of weights: 3.204265602347463\n",
      "---------------------\n",
      "Iteration Number: 5515\n",
      "Loss: 13.030680002522123\n",
      "l2 norm of gradients: 0.09502005199577647\n",
      "l2 norm of weights: 3.2042863374224497\n",
      "---------------------\n",
      "Iteration Number: 5516\n",
      "Loss: 13.029484481037949\n",
      "l2 norm of gradients: 0.09498604284646148\n",
      "l2 norm of weights: 3.2043071300420456\n",
      "---------------------\n",
      "Iteration Number: 5517\n",
      "Loss: 13.028289867412413\n",
      "l2 norm of gradients: 0.09495205513947005\n",
      "l2 norm of weights: 3.2043279801532774\n",
      "---------------------\n",
      "Iteration Number: 5518\n",
      "Loss: 13.027096160799553\n",
      "l2 norm of gradients: 0.0949180888760357\n",
      "l2 norm of weights: 3.2043488877031927\n",
      "---------------------\n",
      "Iteration Number: 5519\n",
      "Loss: 13.025903360353823\n",
      "l2 norm of gradients: 0.0948841440573548\n",
      "l2 norm of weights: 3.2043698526388624\n",
      "---------------------\n",
      "Iteration Number: 5520\n",
      "Loss: 13.024711465230123\n",
      "l2 norm of gradients: 0.09485022068458658\n",
      "l2 norm of weights: 3.204390874907377\n",
      "---------------------\n",
      "Iteration Number: 5521\n",
      "Loss: 13.02352047458369\n",
      "l2 norm of gradients: 0.09481631875885292\n",
      "l2 norm of weights: 3.204411954455851\n",
      "---------------------\n",
      "Iteration Number: 5522\n",
      "Loss: 13.022330387570278\n",
      "l2 norm of gradients: 0.09478243828123858\n",
      "l2 norm of weights: 3.204433091231421\n",
      "---------------------\n",
      "Iteration Number: 5523\n",
      "Loss: 13.021141203346001\n",
      "l2 norm of gradients: 0.09474857925279095\n",
      "l2 norm of weights: 3.204454285181244\n",
      "---------------------\n",
      "Iteration Number: 5524\n",
      "Loss: 13.019952921067393\n",
      "l2 norm of gradients: 0.09471474167452006\n",
      "l2 norm of weights: 3.2044755362525\n",
      "---------------------\n",
      "Iteration Number: 5525\n",
      "Loss: 13.018765539891461\n",
      "l2 norm of gradients: 0.0946809255473986\n",
      "l2 norm of weights: 3.2044968443923914\n",
      "---------------------\n",
      "Iteration Number: 5526\n",
      "Loss: 13.017579058975569\n",
      "l2 norm of gradients: 0.0946471308723618\n",
      "l2 norm of weights: 3.2045182095481417\n",
      "---------------------\n",
      "Iteration Number: 5527\n",
      "Loss: 13.016393477477527\n",
      "l2 norm of gradients: 0.09461335765030748\n",
      "l2 norm of weights: 3.204539631666997\n",
      "---------------------\n",
      "Iteration Number: 5528\n",
      "Loss: 13.015208794555598\n",
      "l2 norm of gradients: 0.09457960588209587\n",
      "l2 norm of weights: 3.2045611106962255\n",
      "---------------------\n",
      "Iteration Number: 5529\n",
      "Loss: 13.014025009368437\n",
      "l2 norm of gradients: 0.09454587556854976\n",
      "l2 norm of weights: 3.2045826465831175\n",
      "---------------------\n",
      "Iteration Number: 5530\n",
      "Loss: 13.01284212107515\n",
      "l2 norm of gradients: 0.0945121667104544\n",
      "l2 norm of weights: 3.2046042392749854\n",
      "---------------------\n",
      "Iteration Number: 5531\n",
      "Loss: 13.011660128835253\n",
      "l2 norm of gradients: 0.09447847930855728\n",
      "l2 norm of weights: 3.204625888719164\n",
      "---------------------\n",
      "Iteration Number: 5532\n",
      "Loss: 13.010479031808725\n",
      "l2 norm of gradients: 0.09444481336356843\n",
      "l2 norm of weights: 3.2046475948630104\n",
      "---------------------\n",
      "Iteration Number: 5533\n",
      "Loss: 13.009298829155917\n",
      "l2 norm of gradients: 0.09441116887616002\n",
      "l2 norm of weights: 3.2046693576539034\n",
      "---------------------\n",
      "Iteration Number: 5534\n",
      "Loss: 13.008119520037665\n",
      "l2 norm of gradients: 0.09437754584696671\n",
      "l2 norm of weights: 3.204691177039245\n",
      "---------------------\n",
      "Iteration Number: 5535\n",
      "Loss: 13.006941103615222\n",
      "l2 norm of gradients: 0.09434394427658531\n",
      "l2 norm of weights: 3.204713052966458\n",
      "---------------------\n",
      "Iteration Number: 5536\n",
      "Loss: 13.005763579050265\n",
      "l2 norm of gradients: 0.09431036416557488\n",
      "l2 norm of weights: 3.2047349853829887\n",
      "---------------------\n",
      "Iteration Number: 5537\n",
      "Loss: 13.004586945504924\n",
      "l2 norm of gradients: 0.09427680551445669\n",
      "l2 norm of weights: 3.2047569742363065\n",
      "---------------------\n",
      "Iteration Number: 5538\n",
      "Loss: 13.003411202141763\n",
      "l2 norm of gradients: 0.09424326832371421\n",
      "l2 norm of weights: 3.2047790194739014\n",
      "---------------------\n",
      "Iteration Number: 5539\n",
      "Loss: 13.00223634812376\n",
      "l2 norm of gradients: 0.09420975259379298\n",
      "l2 norm of weights: 3.2048011210432867\n",
      "---------------------\n",
      "Iteration Number: 5540\n",
      "Loss: 13.001062382614379\n",
      "l2 norm of gradients: 0.09417625832510065\n",
      "l2 norm of weights: 3.204823278891998\n",
      "---------------------\n",
      "Iteration Number: 5541\n",
      "Loss: 12.999889304777469\n",
      "l2 norm of gradients: 0.09414278551800703\n",
      "l2 norm of weights: 3.2048454929675936\n",
      "---------------------\n",
      "Iteration Number: 5542\n",
      "Loss: 12.998717113777348\n",
      "l2 norm of gradients: 0.09410933417284398\n",
      "l2 norm of weights: 3.204867763217654\n",
      "---------------------\n",
      "Iteration Number: 5543\n",
      "Loss: 12.99754580877879\n",
      "l2 norm of gradients: 0.09407590428990527\n",
      "l2 norm of weights: 3.2048900895897825\n",
      "---------------------\n",
      "Iteration Number: 5544\n",
      "Loss: 12.996375388947015\n",
      "l2 norm of gradients: 0.09404249586944678\n",
      "l2 norm of weights: 3.2049124720316047\n",
      "---------------------\n",
      "Iteration Number: 5545\n",
      "Loss: 12.995205853447622\n",
      "l2 norm of gradients: 0.09400910891168637\n",
      "l2 norm of weights: 3.2049349104907687\n",
      "---------------------\n",
      "Iteration Number: 5546\n",
      "Loss: 12.994037201446744\n",
      "l2 norm of gradients: 0.09397574341680381\n",
      "l2 norm of weights: 3.204957404914946\n",
      "---------------------\n",
      "Iteration Number: 5547\n",
      "Loss: 12.992869432110922\n",
      "l2 norm of gradients: 0.09394239938494078\n",
      "l2 norm of weights: 3.204979955251829\n",
      "---------------------\n",
      "Iteration Number: 5548\n",
      "Loss: 12.991702544607127\n",
      "l2 norm of gradients: 0.09390907681620089\n",
      "l2 norm of weights: 3.205002561449135\n",
      "---------------------\n",
      "Iteration Number: 5549\n",
      "Loss: 12.990536538102798\n",
      "l2 norm of gradients: 0.09387577571064962\n",
      "l2 norm of weights: 3.205025223454602\n",
      "---------------------\n",
      "Iteration Number: 5550\n",
      "Loss: 12.989371411765838\n",
      "l2 norm of gradients: 0.09384249606831431\n",
      "l2 norm of weights: 3.2050479412159922\n",
      "---------------------\n",
      "Iteration Number: 5551\n",
      "Loss: 12.98820716476459\n",
      "l2 norm of gradients: 0.09380923788918412\n",
      "l2 norm of weights: 3.2050707146810895\n",
      "---------------------\n",
      "Iteration Number: 5552\n",
      "Loss: 12.98704379626782\n",
      "l2 norm of gradients: 0.09377600117321004\n",
      "l2 norm of weights: 3.2050935437977013\n",
      "---------------------\n",
      "Iteration Number: 5553\n",
      "Loss: 12.985881305444812\n",
      "l2 norm of gradients: 0.09374278592030486\n",
      "l2 norm of weights: 3.2051164285136573\n",
      "---------------------\n",
      "Iteration Number: 5554\n",
      "Loss: 12.984719691465235\n",
      "l2 norm of gradients: 0.09370959213034313\n",
      "l2 norm of weights: 3.20513936877681\n",
      "---------------------\n",
      "Iteration Number: 5555\n",
      "Loss: 12.983558953499266\n",
      "l2 norm of gradients: 0.09367641980316112\n",
      "l2 norm of weights: 3.205162364535035\n",
      "---------------------\n",
      "Iteration Number: 5556\n",
      "Loss: 12.982399090717504\n",
      "l2 norm of gradients: 0.0936432689385569\n",
      "l2 norm of weights: 3.205185415736232\n",
      "---------------------\n",
      "Iteration Number: 5557\n",
      "Loss: 12.981240102291052\n",
      "l2 norm of gradients: 0.09361013953629024\n",
      "l2 norm of weights: 3.20520852232832\n",
      "---------------------\n",
      "Iteration Number: 5558\n",
      "Loss: 12.98008198739141\n",
      "l2 norm of gradients: 0.09357703159608255\n",
      "l2 norm of weights: 3.2052316842592448\n",
      "---------------------\n",
      "Iteration Number: 5559\n",
      "Loss: 12.97892474519058\n",
      "l2 norm of gradients: 0.09354394511761698\n",
      "l2 norm of weights: 3.2052549014769736\n",
      "---------------------\n",
      "Iteration Number: 5560\n",
      "Loss: 12.977768374861016\n",
      "l2 norm of gradients: 0.0935108801005383\n",
      "l2 norm of weights: 3.2052781739294964\n",
      "---------------------\n",
      "Iteration Number: 5561\n",
      "Loss: 12.976612875575643\n",
      "l2 norm of gradients: 0.09347783654445303\n",
      "l2 norm of weights: 3.205301501564827\n",
      "---------------------\n",
      "Iteration Number: 5562\n",
      "Loss: 12.97545824650782\n",
      "l2 norm of gradients: 0.0934448144489292\n",
      "l2 norm of weights: 3.2053248843310014\n",
      "---------------------\n",
      "Iteration Number: 5563\n",
      "Loss: 12.974304486831409\n",
      "l2 norm of gradients: 0.09341181381349652\n",
      "l2 norm of weights: 3.2053483221760786\n",
      "---------------------\n",
      "Iteration Number: 5564\n",
      "Loss: 12.973151595720724\n",
      "l2 norm of gradients: 0.0933788346376463\n",
      "l2 norm of weights: 3.2053718150481414\n",
      "---------------------\n",
      "Iteration Number: 5565\n",
      "Loss: 12.971999572350523\n",
      "l2 norm of gradients: 0.0933458769208315\n",
      "l2 norm of weights: 3.2053953628952963\n",
      "---------------------\n",
      "Iteration Number: 5566\n",
      "Loss: 12.970848415896059\n",
      "l2 norm of gradients: 0.09331294066246658\n",
      "l2 norm of weights: 3.205418965665671\n",
      "---------------------\n",
      "Iteration Number: 5567\n",
      "Loss: 12.969698125533057\n",
      "l2 norm of gradients: 0.09328002586192757\n",
      "l2 norm of weights: 3.2054426233074182\n",
      "---------------------\n",
      "Iteration Number: 5568\n",
      "Loss: 12.96854870043767\n",
      "l2 norm of gradients: 0.09324713251855214\n",
      "l2 norm of weights: 3.205466335768713\n",
      "---------------------\n",
      "Iteration Number: 5569\n",
      "Loss: 12.967400139786584\n",
      "l2 norm of gradients: 0.09321426063163944\n",
      "l2 norm of weights: 3.2054901029977545\n",
      "---------------------\n",
      "Iteration Number: 5570\n",
      "Loss: 12.966252442756907\n",
      "l2 norm of gradients: 0.09318141020045023\n",
      "l2 norm of weights: 3.205513924942764\n",
      "---------------------\n",
      "Iteration Number: 5571\n",
      "Loss: 12.965105608526267\n",
      "l2 norm of gradients: 0.09314858122420672\n",
      "l2 norm of weights: 3.2055378015519866\n",
      "---------------------\n",
      "Iteration Number: 5572\n",
      "Loss: 12.963959636272685\n",
      "l2 norm of gradients: 0.09311577370209265\n",
      "l2 norm of weights: 3.205561732773691\n",
      "---------------------\n",
      "Iteration Number: 5573\n",
      "Loss: 12.96281452517476\n",
      "l2 norm of gradients: 0.09308298763325334\n",
      "l2 norm of weights: 3.2055857185561694\n",
      "---------------------\n",
      "Iteration Number: 5574\n",
      "Loss: 12.961670274411487\n",
      "l2 norm of gradients: 0.09305022301679555\n",
      "l2 norm of weights: 3.2056097588477357\n",
      "---------------------\n",
      "Iteration Number: 5575\n",
      "Loss: 12.960526883162398\n",
      "l2 norm of gradients: 0.0930174798517876\n",
      "l2 norm of weights: 3.2056338535967304\n",
      "---------------------\n",
      "Iteration Number: 5576\n",
      "Loss: 12.959384350607436\n",
      "l2 norm of gradients: 0.09298475813725926\n",
      "l2 norm of weights: 3.205658002751515\n",
      "---------------------\n",
      "Iteration Number: 5577\n",
      "Loss: 12.958242675927085\n",
      "l2 norm of gradients: 0.0929520578722018\n",
      "l2 norm of weights: 3.205682206260475\n",
      "---------------------\n",
      "Iteration Number: 5578\n",
      "Loss: 12.957101858302293\n",
      "l2 norm of gradients: 0.09291937905556796\n",
      "l2 norm of weights: 3.2057064640720196\n",
      "---------------------\n",
      "Iteration Number: 5579\n",
      "Loss: 12.955961896914477\n",
      "l2 norm of gradients: 0.09288672168627206\n",
      "l2 norm of weights: 3.2057307761345823\n",
      "---------------------\n",
      "Iteration Number: 5580\n",
      "Loss: 12.954822790945515\n",
      "l2 norm of gradients: 0.0928540857631897\n",
      "l2 norm of weights: 3.2057551423966184\n",
      "---------------------\n",
      "Iteration Number: 5581\n",
      "Loss: 12.953684539577832\n",
      "l2 norm of gradients: 0.09282147128515815\n",
      "l2 norm of weights: 3.2057795628066086\n",
      "---------------------\n",
      "Iteration Number: 5582\n",
      "Loss: 12.952547141994295\n",
      "l2 norm of gradients: 0.09278887825097604\n",
      "l2 norm of weights: 3.205804037313056\n",
      "---------------------\n",
      "Iteration Number: 5583\n",
      "Loss: 12.95141059737825\n",
      "l2 norm of gradients: 0.09275630665940349\n",
      "l2 norm of weights: 3.205828565864488\n",
      "---------------------\n",
      "Iteration Number: 5584\n",
      "Loss: 12.95027490491355\n",
      "l2 norm of gradients: 0.09272375650916212\n",
      "l2 norm of weights: 3.2058531484094557\n",
      "---------------------\n",
      "Iteration Number: 5585\n",
      "Loss: 12.949140063784517\n",
      "l2 norm of gradients: 0.09269122779893497\n",
      "l2 norm of weights: 3.205877784896534\n",
      "---------------------\n",
      "Iteration Number: 5586\n",
      "Loss: 12.948006073175982\n",
      "l2 norm of gradients: 0.09265872052736665\n",
      "l2 norm of weights: 3.2059024752743213\n",
      "---------------------\n",
      "Iteration Number: 5587\n",
      "Loss: 12.94687293227327\n",
      "l2 norm of gradients: 0.09262623469306308\n",
      "l2 norm of weights: 3.205927219491439\n",
      "---------------------\n",
      "Iteration Number: 5588\n",
      "Loss: 12.945740640262159\n",
      "l2 norm of gradients: 0.09259377029459172\n",
      "l2 norm of weights: 3.205952017496535\n",
      "---------------------\n",
      "Iteration Number: 5589\n",
      "Loss: 12.944609196328944\n",
      "l2 norm of gradients: 0.0925613273304816\n",
      "l2 norm of weights: 3.205976869238277\n",
      "---------------------\n",
      "Iteration Number: 5590\n",
      "Loss: 12.943478599660423\n",
      "l2 norm of gradients: 0.09252890579922311\n",
      "l2 norm of weights: 3.2060017746653604\n",
      "---------------------\n",
      "Iteration Number: 5591\n",
      "Loss: 12.942348849443887\n",
      "l2 norm of gradients: 0.09249650569926811\n",
      "l2 norm of weights: 3.2060267337265023\n",
      "---------------------\n",
      "Iteration Number: 5592\n",
      "Loss: 12.94121994486707\n",
      "l2 norm of gradients: 0.09246412702903\n",
      "l2 norm of weights: 3.2060517463704445\n",
      "---------------------\n",
      "Iteration Number: 5593\n",
      "Loss: 12.940091885118292\n",
      "l2 norm of gradients: 0.09243176978688365\n",
      "l2 norm of weights: 3.206076812545953\n",
      "---------------------\n",
      "Iteration Number: 5594\n",
      "Loss: 12.93896466938629\n",
      "l2 norm of gradients: 0.09239943397116547\n",
      "l2 norm of weights: 3.206101932201817\n",
      "---------------------\n",
      "Iteration Number: 5595\n",
      "Loss: 12.937838296860331\n",
      "l2 norm of gradients: 0.09236711958017321\n",
      "l2 norm of weights: 3.2061271052868503\n",
      "---------------------\n",
      "Iteration Number: 5596\n",
      "Loss: 12.93671276673018\n",
      "l2 norm of gradients: 0.09233482661216634\n",
      "l2 norm of weights: 3.2061523317498906\n",
      "---------------------\n",
      "Iteration Number: 5597\n",
      "Loss: 12.935588078186111\n",
      "l2 norm of gradients: 0.09230255506536564\n",
      "l2 norm of weights: 3.2061776115397995\n",
      "---------------------\n",
      "Iteration Number: 5598\n",
      "Loss: 12.934464230418865\n",
      "l2 norm of gradients: 0.09227030493795353\n",
      "l2 norm of weights: 3.2062029446054634\n",
      "---------------------\n",
      "Iteration Number: 5599\n",
      "Loss: 12.933341222619731\n",
      "l2 norm of gradients: 0.09223807622807392\n",
      "l2 norm of weights: 3.206228330895792\n",
      "---------------------\n",
      "Iteration Number: 5600\n",
      "Loss: 12.932219053980447\n",
      "l2 norm of gradients: 0.09220586893383224\n",
      "l2 norm of weights: 3.2062537703597203\n",
      "---------------------\n",
      "Iteration Number: 5601\n",
      "Loss: 12.931097723693316\n",
      "l2 norm of gradients: 0.09217368305329547\n",
      "l2 norm of weights: 3.2062792629462065\n",
      "---------------------\n",
      "Iteration Number: 5602\n",
      "Loss: 12.929977230951092\n",
      "l2 norm of gradients: 0.09214151858449214\n",
      "l2 norm of weights: 3.2063048086042323\n",
      "---------------------\n",
      "Iteration Number: 5603\n",
      "Loss: 12.928857574947056\n",
      "l2 norm of gradients: 0.09210937552541236\n",
      "l2 norm of weights: 3.2063304072828065\n",
      "---------------------\n",
      "Iteration Number: 5604\n",
      "Loss: 12.927738754874996\n",
      "l2 norm of gradients: 0.09207725387400778\n",
      "l2 norm of weights: 3.2063560589309588\n",
      "---------------------\n",
      "Iteration Number: 5605\n",
      "Loss: 12.926620769929206\n",
      "l2 norm of gradients: 0.09204515362819167\n",
      "l2 norm of weights: 3.206381763497746\n",
      "---------------------\n",
      "Iteration Number: 5606\n",
      "Loss: 12.925503619304495\n",
      "l2 norm of gradients: 0.09201307478583888\n",
      "l2 norm of weights: 3.2064075209322476\n",
      "---------------------\n",
      "Iteration Number: 5607\n",
      "Loss: 12.924387302196171\n",
      "l2 norm of gradients: 0.09198101734478586\n",
      "l2 norm of weights: 3.206433331183568\n",
      "---------------------\n",
      "Iteration Number: 5608\n",
      "Loss: 12.923271817800059\n",
      "l2 norm of gradients: 0.09194898130283063\n",
      "l2 norm of weights: 3.2064591942008365\n",
      "---------------------\n",
      "Iteration Number: 5609\n",
      "Loss: 12.92215716531247\n",
      "l2 norm of gradients: 0.09191696665773298\n",
      "l2 norm of weights: 3.2064851099332063\n",
      "---------------------\n",
      "Iteration Number: 5610\n",
      "Loss: 12.921043343930274\n",
      "l2 norm of gradients: 0.09188497340721426\n",
      "l2 norm of weights: 3.2065110783298545\n",
      "---------------------\n",
      "Iteration Number: 5611\n",
      "Loss: 12.919930352850805\n",
      "l2 norm of gradients: 0.09185300154895744\n",
      "l2 norm of weights: 3.2065370993399847\n",
      "---------------------\n",
      "Iteration Number: 5612\n",
      "Loss: 12.918818191271962\n",
      "l2 norm of gradients: 0.09182105108060729\n",
      "l2 norm of weights: 3.2065631729128223\n",
      "---------------------\n",
      "Iteration Number: 5613\n",
      "Loss: 12.917706858392096\n",
      "l2 norm of gradients: 0.09178912199977016\n",
      "l2 norm of weights: 3.20658929899762\n",
      "---------------------\n",
      "Iteration Number: 5614\n",
      "Loss: 12.916596353410135\n",
      "l2 norm of gradients: 0.09175721430401416\n",
      "l2 norm of weights: 3.2066154775436533\n",
      "---------------------\n",
      "Iteration Number: 5615\n",
      "Loss: 12.915486675525491\n",
      "l2 norm of gradients: 0.09172532799086916\n",
      "l2 norm of weights: 3.206641708500223\n",
      "---------------------\n",
      "Iteration Number: 5616\n",
      "Loss: 12.914377823938098\n",
      "l2 norm of gradients: 0.09169346305782676\n",
      "l2 norm of weights: 3.206667991816654\n",
      "---------------------\n",
      "Iteration Number: 5617\n",
      "Loss: 12.91326979784839\n",
      "l2 norm of gradients: 0.09166161950234028\n",
      "l2 norm of weights: 3.2066943274422974\n",
      "---------------------\n",
      "Iteration Number: 5618\n",
      "Loss: 12.912162596457376\n",
      "l2 norm of gradients: 0.09162979732182487\n",
      "l2 norm of weights: 3.206720715326527\n",
      "---------------------\n",
      "Iteration Number: 5619\n",
      "Loss: 12.911056218966518\n",
      "l2 norm of gradients: 0.0915979965136574\n",
      "l2 norm of weights: 3.2067471554187432\n",
      "---------------------\n",
      "Iteration Number: 5620\n",
      "Loss: 12.909950664577845\n",
      "l2 norm of gradients: 0.09156621707517672\n",
      "l2 norm of weights: 3.2067736476683697\n",
      "---------------------\n",
      "Iteration Number: 5621\n",
      "Loss: 12.9088459324939\n",
      "l2 norm of gradients: 0.09153445900368337\n",
      "l2 norm of weights: 3.2068001920248563\n",
      "---------------------\n",
      "Iteration Number: 5622\n",
      "Loss: 12.907742021917723\n",
      "l2 norm of gradients: 0.09150272229643985\n",
      "l2 norm of weights: 3.206826788437677\n",
      "---------------------\n",
      "Iteration Number: 5623\n",
      "Loss: 12.906638932052916\n",
      "l2 norm of gradients: 0.09147100695067047\n",
      "l2 norm of weights: 3.20685343685633\n",
      "---------------------\n",
      "Iteration Number: 5624\n",
      "Loss: 12.905536662103561\n",
      "l2 norm of gradients: 0.09143931296356152\n",
      "l2 norm of weights: 3.20688013723034\n",
      "---------------------\n",
      "Iteration Number: 5625\n",
      "Loss: 12.90443521127431\n",
      "l2 norm of gradients: 0.09140764033226115\n",
      "l2 norm of weights: 3.206906889509255\n",
      "---------------------\n",
      "Iteration Number: 5626\n",
      "Loss: 12.903334578770323\n",
      "l2 norm of gradients: 0.09137598905387949\n",
      "l2 norm of weights: 3.20693369364265\n",
      "---------------------\n",
      "Iteration Number: 5627\n",
      "Loss: 12.902234763797281\n",
      "l2 norm of gradients: 0.09134435912548867\n",
      "l2 norm of weights: 3.2069605495801223\n",
      "---------------------\n",
      "Iteration Number: 5628\n",
      "Loss: 12.901135765561405\n",
      "l2 norm of gradients: 0.09131275054412284\n",
      "l2 norm of weights: 3.2069874572712966\n",
      "---------------------\n",
      "Iteration Number: 5629\n",
      "Loss: 12.900037583269432\n",
      "l2 norm of gradients: 0.09128116330677806\n",
      "l2 norm of weights: 3.2070144166658223\n",
      "---------------------\n",
      "Iteration Number: 5630\n",
      "Loss: 12.89894021612864\n",
      "l2 norm of gradients: 0.09124959741041255\n",
      "l2 norm of weights: 3.2070414277133725\n",
      "---------------------\n",
      "Iteration Number: 5631\n",
      "Loss: 12.897843663346823\n",
      "l2 norm of gradients: 0.09121805285194659\n",
      "l2 norm of weights: 3.2070684903636466\n",
      "---------------------\n",
      "Iteration Number: 5632\n",
      "Loss: 12.896747924132335\n",
      "l2 norm of gradients: 0.09118652962826251\n",
      "l2 norm of weights: 3.2070956045663688\n",
      "---------------------\n",
      "Iteration Number: 5633\n",
      "Loss: 12.89565299769404\n",
      "l2 norm of gradients: 0.09115502773620485\n",
      "l2 norm of weights: 3.2071227702712886\n",
      "---------------------\n",
      "Iteration Number: 5634\n",
      "Loss: 12.894558883241324\n",
      "l2 norm of gradients: 0.0911235471725802\n",
      "l2 norm of weights: 3.207149987428181\n",
      "---------------------\n",
      "Iteration Number: 5635\n",
      "Loss: 12.893465579984149\n",
      "l2 norm of gradients: 0.09109208793415738\n",
      "l2 norm of weights: 3.207177255986846\n",
      "---------------------\n",
      "Iteration Number: 5636\n",
      "Loss: 12.892373087132963\n",
      "l2 norm of gradients: 0.09106065001766749\n",
      "l2 norm of weights: 3.207204575897108\n",
      "---------------------\n",
      "Iteration Number: 5637\n",
      "Loss: 12.89128140389879\n",
      "l2 norm of gradients: 0.09102923341980376\n",
      "l2 norm of weights: 3.2072319471088186\n",
      "---------------------\n",
      "Iteration Number: 5638\n",
      "Loss: 12.890190529493168\n",
      "l2 norm of gradients: 0.09099783813722175\n",
      "l2 norm of weights: 3.2072593695718536\n",
      "---------------------\n",
      "Iteration Number: 5639\n",
      "Loss: 12.889100463128177\n",
      "l2 norm of gradients: 0.09096646416653932\n",
      "l2 norm of weights: 3.207286843236114\n",
      "---------------------\n",
      "Iteration Number: 5640\n",
      "Loss: 12.888011204016436\n",
      "l2 norm of gradients: 0.09093511150433665\n",
      "l2 norm of weights: 3.207314368051527\n",
      "---------------------\n",
      "Iteration Number: 5641\n",
      "Loss: 12.886922751371104\n",
      "l2 norm of gradients: 0.0909037801471563\n",
      "l2 norm of weights: 3.2073419439680433\n",
      "---------------------\n",
      "Iteration Number: 5642\n",
      "Loss: 12.885835104405889\n",
      "l2 norm of gradients: 0.09087247009150316\n",
      "l2 norm of weights: 3.207369570935642\n",
      "---------------------\n",
      "Iteration Number: 5643\n",
      "Loss: 12.884748262335014\n",
      "l2 norm of gradients: 0.0908411813338446\n",
      "l2 norm of weights: 3.207397248904326\n",
      "---------------------\n",
      "Iteration Number: 5644\n",
      "Loss: 12.883662224373264\n",
      "l2 norm of gradients: 0.09080991387061046\n",
      "l2 norm of weights: 3.207424977824123\n",
      "---------------------\n",
      "Iteration Number: 5645\n",
      "Loss: 12.882576989735995\n",
      "l2 norm of gradients: 0.09077866769819307\n",
      "l2 norm of weights: 3.207452757645088\n",
      "---------------------\n",
      "Iteration Number: 5646\n",
      "Loss: 12.881492557639032\n",
      "l2 norm of gradients: 0.09074744281294715\n",
      "l2 norm of weights: 3.207480588317301\n",
      "---------------------\n",
      "Iteration Number: 5647\n",
      "Loss: 12.880408927298818\n",
      "l2 norm of gradients: 0.09071623921119018\n",
      "l2 norm of weights: 3.207508469790867\n",
      "---------------------\n",
      "Iteration Number: 5648\n",
      "Loss: 12.879326097932275\n",
      "l2 norm of gradients: 0.0906850568892021\n",
      "l2 norm of weights: 3.2075364020159167\n",
      "---------------------\n",
      "Iteration Number: 5649\n",
      "Loss: 12.878244068756942\n",
      "l2 norm of gradients: 0.0906538958432255\n",
      "l2 norm of weights: 3.2075643849426076\n",
      "---------------------\n",
      "Iteration Number: 5650\n",
      "Loss: 12.877162838990845\n",
      "l2 norm of gradients: 0.09062275606946567\n",
      "l2 norm of weights: 3.2075924185211218\n",
      "---------------------\n",
      "Iteration Number: 5651\n",
      "Loss: 12.8760824078526\n",
      "l2 norm of gradients: 0.09059163756409057\n",
      "l2 norm of weights: 3.2076205027016673\n",
      "---------------------\n",
      "Iteration Number: 5652\n",
      "Loss: 12.875002774561324\n",
      "l2 norm of gradients: 0.09056054032323083\n",
      "l2 norm of weights: 3.2076486374344784\n",
      "---------------------\n",
      "Iteration Number: 5653\n",
      "Loss: 12.873923938336718\n",
      "l2 norm of gradients: 0.09052946434297994\n",
      "l2 norm of weights: 3.2076768226698147\n",
      "---------------------\n",
      "Iteration Number: 5654\n",
      "Loss: 12.872845898399053\n",
      "l2 norm of gradients: 0.09049840961939422\n",
      "l2 norm of weights: 3.207705058357962\n",
      "---------------------\n",
      "Iteration Number: 5655\n",
      "Loss: 12.871768653969076\n",
      "l2 norm of gradients: 0.0904673761484927\n",
      "l2 norm of weights: 3.2077333444492315\n",
      "---------------------\n",
      "Iteration Number: 5656\n",
      "Loss: 12.870692204268169\n",
      "l2 norm of gradients: 0.09043636392625747\n",
      "l2 norm of weights: 3.207761680893961\n",
      "---------------------\n",
      "Iteration Number: 5657\n",
      "Loss: 12.869616548518207\n",
      "l2 norm of gradients: 0.09040537294863335\n",
      "l2 norm of weights: 3.207790067642513\n",
      "---------------------\n",
      "Iteration Number: 5658\n",
      "Loss: 12.868541685941654\n",
      "l2 norm of gradients: 0.0903744032115283\n",
      "l2 norm of weights: 3.2078185046452776\n",
      "---------------------\n",
      "Iteration Number: 5659\n",
      "Loss: 12.867467615761505\n",
      "l2 norm of gradients: 0.09034345471081313\n",
      "l2 norm of weights: 3.20784699185267\n",
      "---------------------\n",
      "Iteration Number: 5660\n",
      "Loss: 12.866394337201308\n",
      "l2 norm of gradients: 0.09031252744232185\n",
      "l2 norm of weights: 3.207875529215131\n",
      "---------------------\n",
      "Iteration Number: 5661\n",
      "Loss: 12.865321849485184\n",
      "l2 norm of gradients: 0.0902816214018514\n",
      "l2 norm of weights: 3.2079041166831277\n",
      "---------------------\n",
      "Iteration Number: 5662\n",
      "Loss: 12.864250151837798\n",
      "l2 norm of gradients: 0.0902507365851619\n",
      "l2 norm of weights: 3.2079327542071545\n",
      "---------------------\n",
      "Iteration Number: 5663\n",
      "Loss: 12.863179243484389\n",
      "l2 norm of gradients: 0.09021987298797667\n",
      "l2 norm of weights: 3.2079614417377296\n",
      "---------------------\n",
      "Iteration Number: 5664\n",
      "Loss: 12.862109123650715\n",
      "l2 norm of gradients: 0.09018903060598223\n",
      "l2 norm of weights: 3.2079901792253995\n",
      "---------------------\n",
      "Iteration Number: 5665\n",
      "Loss: 12.861039791563124\n",
      "l2 norm of gradients: 0.09015820943482826\n",
      "l2 norm of weights: 3.208018966620736\n",
      "---------------------\n",
      "Iteration Number: 5666\n",
      "Loss: 12.859971246448518\n",
      "l2 norm of gradients: 0.09012740947012779\n",
      "l2 norm of weights: 3.208047803874337\n",
      "---------------------\n",
      "Iteration Number: 5667\n",
      "Loss: 12.858903487534356\n",
      "l2 norm of gradients: 0.09009663070745727\n",
      "l2 norm of weights: 3.208076690936826\n",
      "---------------------\n",
      "Iteration Number: 5668\n",
      "Loss: 12.857836514048662\n",
      "l2 norm of gradients: 0.09006587314235638\n",
      "l2 norm of weights: 3.2081056277588544\n",
      "---------------------\n",
      "Iteration Number: 5669\n",
      "Loss: 12.856770325219992\n",
      "l2 norm of gradients: 0.09003513677032832\n",
      "l2 norm of weights: 3.2081346142910983\n",
      "---------------------\n",
      "Iteration Number: 5670\n",
      "Loss: 12.8557049202775\n",
      "l2 norm of gradients: 0.09000442158683974\n",
      "l2 norm of weights: 3.208163650484261\n",
      "---------------------\n",
      "Iteration Number: 5671\n",
      "Loss: 12.854640298450885\n",
      "l2 norm of gradients: 0.08997372758732076\n",
      "l2 norm of weights: 3.2081927362890723\n",
      "---------------------\n",
      "Iteration Number: 5672\n",
      "Loss: 12.853576458970428\n",
      "l2 norm of gradients: 0.08994305476716512\n",
      "l2 norm of weights: 3.208221871656287\n",
      "---------------------\n",
      "Iteration Number: 5673\n",
      "Loss: 12.852513401066945\n",
      "l2 norm of gradients: 0.08991240312173009\n",
      "l2 norm of weights: 3.2082510565366884\n",
      "---------------------\n",
      "Iteration Number: 5674\n",
      "Loss: 12.851451123971827\n",
      "l2 norm of gradients: 0.08988177264633666\n",
      "l2 norm of weights: 3.2082802908810852\n",
      "---------------------\n",
      "Iteration Number: 5675\n",
      "Loss: 12.850389626917039\n",
      "l2 norm of gradients: 0.0898511633362695\n",
      "l2 norm of weights: 3.2083095746403116\n",
      "---------------------\n",
      "Iteration Number: 5676\n",
      "Loss: 12.849328909135103\n",
      "l2 norm of gradients: 0.08982057518677697\n",
      "l2 norm of weights: 3.2083389077652296\n",
      "---------------------\n",
      "Iteration Number: 5677\n",
      "Loss: 12.848268969859129\n",
      "l2 norm of gradients: 0.08979000819307129\n",
      "l2 norm of weights: 3.2083682902067276\n",
      "---------------------\n",
      "Iteration Number: 5678\n",
      "Loss: 12.847209808322765\n",
      "l2 norm of gradients: 0.0897594623503285\n",
      "l2 norm of weights: 3.20839772191572\n",
      "---------------------\n",
      "Iteration Number: 5679\n",
      "Loss: 12.846151423760217\n",
      "l2 norm of gradients: 0.08972893765368856\n",
      "l2 norm of weights: 3.208427202843148\n",
      "---------------------\n",
      "Iteration Number: 5680\n",
      "Loss: 12.845093815406303\n",
      "l2 norm of gradients: 0.08969843409825529\n",
      "l2 norm of weights: 3.2084567329399794\n",
      "---------------------\n",
      "Iteration Number: 5681\n",
      "Loss: 12.8440369824964\n",
      "l2 norm of gradients: 0.08966795167909652\n",
      "l2 norm of weights: 3.208486312157209\n",
      "---------------------\n",
      "Iteration Number: 5682\n",
      "Loss: 12.842980924266417\n",
      "l2 norm of gradients: 0.08963749039124418\n",
      "l2 norm of weights: 3.2085159404458583\n",
      "---------------------\n",
      "Iteration Number: 5683\n",
      "Loss: 12.84192563995288\n",
      "l2 norm of gradients: 0.08960705022969419\n",
      "l2 norm of weights: 3.2085456177569744\n",
      "---------------------\n",
      "Iteration Number: 5684\n",
      "Loss: 12.840871128792847\n",
      "l2 norm of gradients: 0.08957663118940673\n",
      "l2 norm of weights: 3.2085753440416327\n",
      "---------------------\n",
      "Iteration Number: 5685\n",
      "Loss: 12.839817390023995\n",
      "l2 norm of gradients: 0.08954623326530607\n",
      "l2 norm of weights: 3.2086051192509344\n",
      "---------------------\n",
      "Iteration Number: 5686\n",
      "Loss: 12.838764422884529\n",
      "l2 norm of gradients: 0.0895158564522807\n",
      "l2 norm of weights: 3.208634943336007\n",
      "---------------------\n",
      "Iteration Number: 5687\n",
      "Loss: 12.837712226613235\n",
      "l2 norm of gradients: 0.08948550074518352\n",
      "l2 norm of weights: 3.2086648162480067\n",
      "---------------------\n",
      "Iteration Number: 5688\n",
      "Loss: 12.83666080044951\n",
      "l2 norm of gradients: 0.0894551661388317\n",
      "l2 norm of weights: 3.208694737938115\n",
      "---------------------\n",
      "Iteration Number: 5689\n",
      "Loss: 12.835610143633282\n",
      "l2 norm of gradients: 0.08942485262800677\n",
      "l2 norm of weights: 3.2087247083575394\n",
      "---------------------\n",
      "Iteration Number: 5690\n",
      "Loss: 12.834560255405075\n",
      "l2 norm of gradients: 0.08939456020745479\n",
      "l2 norm of weights: 3.208754727457517\n",
      "---------------------\n",
      "Iteration Number: 5691\n",
      "Loss: 12.833511135005983\n",
      "l2 norm of gradients: 0.08936428887188627\n",
      "l2 norm of weights: 3.2087847951893105\n",
      "---------------------\n",
      "Iteration Number: 5692\n",
      "Loss: 12.832462781677684\n",
      "l2 norm of gradients: 0.08933403861597632\n",
      "l2 norm of weights: 3.208814911504208\n",
      "---------------------\n",
      "Iteration Number: 5693\n",
      "Loss: 12.83141519466242\n",
      "l2 norm of gradients: 0.08930380943436458\n",
      "l2 norm of weights: 3.2088450763535272\n",
      "---------------------\n",
      "Iteration Number: 5694\n",
      "Loss: 12.830368373203036\n",
      "l2 norm of gradients: 0.08927360132165547\n",
      "l2 norm of weights: 3.2088752896886112\n",
      "---------------------\n",
      "Iteration Number: 5695\n",
      "Loss: 12.829322316542912\n",
      "l2 norm of gradients: 0.08924341427241807\n",
      "l2 norm of weights: 3.2089055514608313\n",
      "---------------------\n",
      "Iteration Number: 5696\n",
      "Loss: 12.828277023926061\n",
      "l2 norm of gradients: 0.08921324828118621\n",
      "l2 norm of weights: 3.2089358616215846\n",
      "---------------------\n",
      "Iteration Number: 5697\n",
      "Loss: 12.827232494597032\n",
      "l2 norm of gradients: 0.08918310334245862\n",
      "l2 norm of weights: 3.2089662201222957\n",
      "---------------------\n",
      "Iteration Number: 5698\n",
      "Loss: 12.826188727800977\n",
      "l2 norm of gradients: 0.08915297945069883\n",
      "l2 norm of weights: 3.208996626914417\n",
      "---------------------\n",
      "Iteration Number: 5699\n",
      "Loss: 12.825145722783628\n",
      "l2 norm of gradients: 0.08912287660033544\n",
      "l2 norm of weights: 3.2090270819494275\n",
      "---------------------\n",
      "Iteration Number: 5700\n",
      "Loss: 12.824103478791281\n",
      "l2 norm of gradients: 0.08909279478576192\n",
      "l2 norm of weights: 3.2090575851788334\n",
      "---------------------\n",
      "Iteration Number: 5701\n",
      "Loss: 12.823061995070837\n",
      "l2 norm of gradients: 0.08906273400133688\n",
      "l2 norm of weights: 3.209088136554169\n",
      "---------------------\n",
      "Iteration Number: 5702\n",
      "Loss: 12.822021270869776\n",
      "l2 norm of gradients: 0.08903269424138403\n",
      "l2 norm of weights: 3.2091187360269937\n",
      "---------------------\n",
      "Iteration Number: 5703\n",
      "Loss: 12.82098130543615\n",
      "l2 norm of gradients: 0.0890026755001922\n",
      "l2 norm of weights: 3.2091493835488967\n",
      "---------------------\n",
      "Iteration Number: 5704\n",
      "Loss: 12.819942098018595\n",
      "l2 norm of gradients: 0.08897267777201559\n",
      "l2 norm of weights: 3.209180079071493\n",
      "---------------------\n",
      "Iteration Number: 5705\n",
      "Loss: 12.818903647866346\n",
      "l2 norm of gradients: 0.08894270105107357\n",
      "l2 norm of weights: 3.2092108225464253\n",
      "---------------------\n",
      "Iteration Number: 5706\n",
      "Loss: 12.817865954229212\n",
      "l2 norm of gradients: 0.08891274533155087\n",
      "l2 norm of weights: 3.209241613925364\n",
      "---------------------\n",
      "Iteration Number: 5707\n",
      "Loss: 12.816829016357582\n",
      "l2 norm of gradients: 0.08888281060759773\n",
      "l2 norm of weights: 3.2092724531600063\n",
      "---------------------\n",
      "Iteration Number: 5708\n",
      "Loss: 12.81579283350245\n",
      "l2 norm of gradients: 0.08885289687332971\n",
      "l2 norm of weights: 3.209303340202077\n",
      "---------------------\n",
      "Iteration Number: 5709\n",
      "Loss: 12.814757404915396\n",
      "l2 norm of gradients: 0.08882300412282806\n",
      "l2 norm of weights: 3.209334275003329\n",
      "---------------------\n",
      "Iteration Number: 5710\n",
      "Loss: 12.813722729848548\n",
      "l2 norm of gradients: 0.08879313235013948\n",
      "l2 norm of weights: 3.209365257515542\n",
      "---------------------\n",
      "Iteration Number: 5711\n",
      "Loss: 12.812688807554697\n",
      "l2 norm of gradients: 0.08876328154927642\n",
      "l2 norm of weights: 3.209396287690523\n",
      "---------------------\n",
      "Iteration Number: 5712\n",
      "Loss: 12.811655637287137\n",
      "l2 norm of gradients: 0.08873345171421697\n",
      "l2 norm of weights: 3.2094273654801064\n",
      "---------------------\n",
      "Iteration Number: 5713\n",
      "Loss: 12.81062321829983\n",
      "l2 norm of gradients: 0.0887036428389051\n",
      "l2 norm of weights: 3.2094584908361563\n",
      "---------------------\n",
      "Iteration Number: 5714\n",
      "Loss: 12.80959154984726\n",
      "l2 norm of gradients: 0.08867385491725051\n",
      "l2 norm of weights: 3.209489663710561\n",
      "---------------------\n",
      "Iteration Number: 5715\n",
      "Loss: 12.808560631184545\n",
      "l2 norm of gradients: 0.08864408794312885\n",
      "l2 norm of weights: 3.2095208840552396\n",
      "---------------------\n",
      "Iteration Number: 5716\n",
      "Loss: 12.807530461567392\n",
      "l2 norm of gradients: 0.08861434191038171\n",
      "l2 norm of weights: 3.209552151822137\n",
      "---------------------\n",
      "Iteration Number: 5717\n",
      "Loss: 12.806501040252076\n",
      "l2 norm of gradients: 0.08858461681281672\n",
      "l2 norm of weights: 3.2095834669632257\n",
      "---------------------\n",
      "Iteration Number: 5718\n",
      "Loss: 12.805472366495485\n",
      "l2 norm of gradients: 0.08855491264420765\n",
      "l2 norm of weights: 3.2096148294305067\n",
      "---------------------\n",
      "Iteration Number: 5719\n",
      "Loss: 12.804444439555088\n",
      "l2 norm of gradients: 0.0885252293982943\n",
      "l2 norm of weights: 3.209646239176008\n",
      "---------------------\n",
      "Iteration Number: 5720\n",
      "Loss: 12.803417258688949\n",
      "l2 norm of gradients: 0.08849556706878282\n",
      "l2 norm of weights: 3.209677696151787\n",
      "---------------------\n",
      "Iteration Number: 5721\n",
      "Loss: 12.802390823155726\n",
      "l2 norm of gradients: 0.08846592564934547\n",
      "l2 norm of weights: 3.2097092003099266\n",
      "---------------------\n",
      "Iteration Number: 5722\n",
      "Loss: 12.80136513221468\n",
      "l2 norm of gradients: 0.0884363051336211\n",
      "l2 norm of weights: 3.2097407516025385\n",
      "---------------------\n",
      "Iteration Number: 5723\n",
      "Loss: 12.800340185125652\n",
      "l2 norm of gradients: 0.08840670551521473\n",
      "l2 norm of weights: 3.2097723499817636\n",
      "---------------------\n",
      "Iteration Number: 5724\n",
      "Loss: 12.7993159811491\n",
      "l2 norm of gradients: 0.08837712678769799\n",
      "l2 norm of weights: 3.209803995399768\n",
      "---------------------\n",
      "Iteration Number: 5725\n",
      "Loss: 12.798292519546063\n",
      "l2 norm of gradients: 0.08834756894460902\n",
      "l2 norm of weights: 3.2098356878087473\n",
      "---------------------\n",
      "Iteration Number: 5726\n",
      "Loss: 12.797269799578157\n",
      "l2 norm of gradients: 0.08831803197945255\n",
      "l2 norm of weights: 3.2098674271609253\n",
      "---------------------\n",
      "Iteration Number: 5727\n",
      "Loss: 12.796247820507627\n",
      "l2 norm of gradients: 0.0882885158857\n",
      "l2 norm of weights: 3.209899213408553\n",
      "---------------------\n",
      "Iteration Number: 5728\n",
      "Loss: 12.79522658159732\n",
      "l2 norm of gradients: 0.08825902065678952\n",
      "l2 norm of weights: 3.20993104650391\n",
      "---------------------\n",
      "Iteration Number: 5729\n",
      "Loss: 12.794206082110641\n",
      "l2 norm of gradients: 0.08822954628612606\n",
      "l2 norm of weights: 3.2099629263993035\n",
      "---------------------\n",
      "Iteration Number: 5730\n",
      "Loss: 12.79318632131163\n",
      "l2 norm of gradients: 0.08820009276708142\n",
      "l2 norm of weights: 3.209994853047068\n",
      "---------------------\n",
      "Iteration Number: 5731\n",
      "Loss: 12.7921672984649\n",
      "l2 norm of gradients: 0.08817066009299444\n",
      "l2 norm of weights: 3.2100268263995675\n",
      "---------------------\n",
      "Iteration Number: 5732\n",
      "Loss: 12.7911490128357\n",
      "l2 norm of gradients: 0.08814124825717089\n",
      "l2 norm of weights: 3.210058846409193\n",
      "---------------------\n",
      "Iteration Number: 5733\n",
      "Loss: 12.790131463689823\n",
      "l2 norm of gradients: 0.08811185725288362\n",
      "l2 norm of weights: 3.2100909130283646\n",
      "---------------------\n",
      "Iteration Number: 5734\n",
      "Loss: 12.78911465029373\n",
      "l2 norm of gradients: 0.08808248707337256\n",
      "l2 norm of weights: 3.21012302620953\n",
      "---------------------\n",
      "Iteration Number: 5735\n",
      "Loss: 12.788098571914425\n",
      "l2 norm of gradients: 0.08805313771184503\n",
      "l2 norm of weights: 3.210155185905164\n",
      "---------------------\n",
      "Iteration Number: 5736\n",
      "Loss: 12.78708322781954\n",
      "l2 norm of gradients: 0.08802380916147542\n",
      "l2 norm of weights: 3.2101873920677715\n",
      "---------------------\n",
      "Iteration Number: 5737\n",
      "Loss: 12.786068617277312\n",
      "l2 norm of gradients: 0.08799450141540567\n",
      "l2 norm of weights: 3.2102196446498845\n",
      "---------------------\n",
      "Iteration Number: 5738\n",
      "Loss: 12.78505473955655\n",
      "l2 norm of gradients: 0.08796521446674496\n",
      "l2 norm of weights: 3.2102519436040633\n",
      "---------------------\n",
      "Iteration Number: 5739\n",
      "Loss: 12.78404159392672\n",
      "l2 norm of gradients: 0.0879359483085701\n",
      "l2 norm of weights: 3.2102842888828964\n",
      "---------------------\n",
      "Iteration Number: 5740\n",
      "Loss: 12.783029179657841\n",
      "l2 norm of gradients: 0.08790670293392538\n",
      "l2 norm of weights: 3.210316680439002\n",
      "---------------------\n",
      "Iteration Number: 5741\n",
      "Loss: 12.782017496020561\n",
      "l2 norm of gradients: 0.08787747833582278\n",
      "l2 norm of weights: 3.2103491182250243\n",
      "---------------------\n",
      "Iteration Number: 5742\n",
      "Loss: 12.781006542286125\n",
      "l2 norm of gradients: 0.0878482745072419\n",
      "l2 norm of weights: 3.210381602193637\n",
      "---------------------\n",
      "Iteration Number: 5743\n",
      "Loss: 12.779996317726381\n",
      "l2 norm of gradients: 0.08781909144113019\n",
      "l2 norm of weights: 3.2104141322975432\n",
      "---------------------\n",
      "Iteration Number: 5744\n",
      "Loss: 12.778986821613783\n",
      "l2 norm of gradients: 0.08778992913040286\n",
      "l2 norm of weights: 3.2104467084894717\n",
      "---------------------\n",
      "Iteration Number: 5745\n",
      "Loss: 12.777978053221398\n",
      "l2 norm of gradients: 0.08776078756794313\n",
      "l2 norm of weights: 3.210479330722183\n",
      "---------------------\n",
      "Iteration Number: 5746\n",
      "Loss: 12.776970011822893\n",
      "l2 norm of gradients: 0.08773166674660215\n",
      "l2 norm of weights: 3.2105119989484634\n",
      "---------------------\n",
      "Iteration Number: 5747\n",
      "Loss: 12.775962696692536\n",
      "l2 norm of gradients: 0.08770256665919914\n",
      "l2 norm of weights: 3.2105447131211293\n",
      "---------------------\n",
      "Iteration Number: 5748\n",
      "Loss: 12.774956107105208\n",
      "l2 norm of gradients: 0.08767348729852142\n",
      "l2 norm of weights: 3.210577473193024\n",
      "---------------------\n",
      "Iteration Number: 5749\n",
      "Loss: 12.773950242336408\n",
      "l2 norm of gradients: 0.08764442865732457\n",
      "l2 norm of weights: 3.210610279117022\n",
      "---------------------\n",
      "Iteration Number: 5750\n",
      "Loss: 12.77294510166221\n",
      "l2 norm of gradients: 0.08761539072833241\n",
      "l2 norm of weights: 3.2106431308460235\n",
      "---------------------\n",
      "Iteration Number: 5751\n",
      "Loss: 12.77194068435933\n",
      "l2 norm of gradients: 0.08758637350423708\n",
      "l2 norm of weights: 3.2106760283329585\n",
      "---------------------\n",
      "Iteration Number: 5752\n",
      "Loss: 12.770936989705081\n",
      "l2 norm of gradients: 0.08755737697769919\n",
      "l2 norm of weights: 3.210708971530786\n",
      "---------------------\n",
      "Iteration Number: 5753\n",
      "Loss: 12.76993401697739\n",
      "l2 norm of gradients: 0.08752840114134784\n",
      "l2 norm of weights: 3.2107419603924923\n",
      "---------------------\n",
      "Iteration Number: 5754\n",
      "Loss: 12.768931765454777\n",
      "l2 norm of gradients: 0.08749944598778067\n",
      "l2 norm of weights: 3.210774994871094\n",
      "---------------------\n",
      "Iteration Number: 5755\n",
      "Loss: 12.767930234416385\n",
      "l2 norm of gradients: 0.08747051150956393\n",
      "l2 norm of weights: 3.210808074919636\n",
      "---------------------\n",
      "Iteration Number: 5756\n",
      "Loss: 12.766929423141951\n",
      "l2 norm of gradients: 0.08744159769923274\n",
      "l2 norm of weights: 3.2108412004911906\n",
      "---------------------\n",
      "Iteration Number: 5757\n",
      "Loss: 12.765929330911854\n",
      "l2 norm of gradients: 0.08741270454929083\n",
      "l2 norm of weights: 3.2108743715388606\n",
      "---------------------\n",
      "Iteration Number: 5758\n",
      "Loss: 12.764929957007075\n",
      "l2 norm of gradients: 0.08738383205221087\n",
      "l2 norm of weights: 3.210907588015776\n",
      "---------------------\n",
      "Iteration Number: 5759\n",
      "Loss: 12.763931300709173\n",
      "l2 norm of gradients: 0.08735498020043449\n",
      "l2 norm of weights: 3.2109408498750964\n",
      "---------------------\n",
      "Iteration Number: 5760\n",
      "Loss: 12.762933361300373\n",
      "l2 norm of gradients: 0.08732614898637227\n",
      "l2 norm of weights: 3.21097415707001\n",
      "---------------------\n",
      "Iteration Number: 5761\n",
      "Loss: 12.761936138063447\n",
      "l2 norm of gradients: 0.08729733840240396\n",
      "l2 norm of weights: 3.2110075095537343\n",
      "---------------------\n",
      "Iteration Number: 5762\n",
      "Loss: 12.760939630281824\n",
      "l2 norm of gradients: 0.08726854844087843\n",
      "l2 norm of weights: 3.2110409072795147\n",
      "---------------------\n",
      "Iteration Number: 5763\n",
      "Loss: 12.759943837239556\n",
      "l2 norm of gradients: 0.08723977909411382\n",
      "l2 norm of weights: 3.2110743502006263\n",
      "---------------------\n",
      "Iteration Number: 5764\n",
      "Loss: 12.758948758221273\n",
      "l2 norm of gradients: 0.08721103035439752\n",
      "l2 norm of weights: 3.211107838270373\n",
      "---------------------\n",
      "Iteration Number: 5765\n",
      "Loss: 12.75795439251225\n",
      "l2 norm of gradients: 0.08718230221398642\n",
      "l2 norm of weights: 3.2111413714420873\n",
      "---------------------\n",
      "Iteration Number: 5766\n",
      "Loss: 12.756960739398355\n",
      "l2 norm of gradients: 0.08715359466510679\n",
      "l2 norm of weights: 3.21117494966913\n",
      "---------------------\n",
      "Iteration Number: 5767\n",
      "Loss: 12.755967798166061\n",
      "l2 norm of gradients: 0.08712490769995451\n",
      "l2 norm of weights: 3.211208572904893\n",
      "---------------------\n",
      "Iteration Number: 5768\n",
      "Loss: 12.754975568102493\n",
      "l2 norm of gradients: 0.08709624131069504\n",
      "l2 norm of weights: 3.211242241102795\n",
      "---------------------\n",
      "Iteration Number: 5769\n",
      "Loss: 12.75398404849535\n",
      "l2 norm of gradients: 0.08706759548946354\n",
      "l2 norm of weights: 3.2112759542162843\n",
      "---------------------\n",
      "Iteration Number: 5770\n",
      "Loss: 12.752993238633001\n",
      "l2 norm of gradients: 0.08703897022836501\n",
      "l2 norm of weights: 3.2113097121988394\n",
      "---------------------\n",
      "Iteration Number: 5771\n",
      "Loss: 12.75200313780435\n",
      "l2 norm of gradients: 0.08701036551947422\n",
      "l2 norm of weights: 3.2113435150039664\n",
      "---------------------\n",
      "Iteration Number: 5772\n",
      "Loss: 12.751013745298998\n",
      "l2 norm of gradients: 0.08698178135483593\n",
      "l2 norm of weights: 3.211377362585201\n",
      "---------------------\n",
      "Iteration Number: 5773\n",
      "Loss: 12.750025060407129\n",
      "l2 norm of gradients: 0.0869532177264649\n",
      "l2 norm of weights: 3.211411254896108\n",
      "---------------------\n",
      "Iteration Number: 5774\n",
      "Loss: 12.749037082419509\n",
      "l2 norm of gradients: 0.08692467462634602\n",
      "l2 norm of weights: 3.2114451918902818\n",
      "---------------------\n",
      "Iteration Number: 5775\n",
      "Loss: 12.748049810627597\n",
      "l2 norm of gradients: 0.08689615204643424\n",
      "l2 norm of weights: 3.211479173521345\n",
      "---------------------\n",
      "Iteration Number: 5776\n",
      "Loss: 12.74706324432341\n",
      "l2 norm of gradients: 0.08686764997865486\n",
      "l2 norm of weights: 3.2115131997429502\n",
      "---------------------\n",
      "Iteration Number: 5777\n",
      "Loss: 12.74607738279959\n",
      "l2 norm of gradients: 0.08683916841490347\n",
      "l2 norm of weights: 3.2115472705087793\n",
      "---------------------\n",
      "Iteration Number: 5778\n",
      "Loss: 12.74509222534943\n",
      "l2 norm of gradients: 0.08681070734704607\n",
      "l2 norm of weights: 3.2115813857725426\n",
      "---------------------\n",
      "Iteration Number: 5779\n",
      "Loss: 12.74410777126681\n",
      "l2 norm of gradients: 0.08678226676691914\n",
      "l2 norm of weights: 3.21161554548798\n",
      "---------------------\n",
      "Iteration Number: 5780\n",
      "Loss: 12.74312401984624\n",
      "l2 norm of gradients: 0.0867538466663297\n",
      "l2 norm of weights: 3.211649749608861\n",
      "---------------------\n",
      "Iteration Number: 5781\n",
      "Loss: 12.742140970382845\n",
      "l2 norm of gradients: 0.08672544703705547\n",
      "l2 norm of weights: 3.211683998088984\n",
      "---------------------\n",
      "Iteration Number: 5782\n",
      "Loss: 12.741158622172373\n",
      "l2 norm of gradients: 0.08669706787084483\n",
      "l2 norm of weights: 3.211718290882178\n",
      "---------------------\n",
      "Iteration Number: 5783\n",
      "Loss: 12.740176974511195\n",
      "l2 norm of gradients: 0.08666870915941702\n",
      "l2 norm of weights: 3.2117526279422988\n",
      "---------------------\n",
      "Iteration Number: 5784\n",
      "Loss: 12.739196026696312\n",
      "l2 norm of gradients: 0.0866403708944621\n",
      "l2 norm of weights: 3.211787009223233\n",
      "---------------------\n",
      "Iteration Number: 5785\n",
      "Loss: 12.738215778025324\n",
      "l2 norm of gradients: 0.08661205306764115\n",
      "l2 norm of weights: 3.211821434678898\n",
      "---------------------\n",
      "Iteration Number: 5786\n",
      "Loss: 12.73723622779644\n",
      "l2 norm of gradients: 0.08658375567058624\n",
      "l2 norm of weights: 3.211855904263238\n",
      "---------------------\n",
      "Iteration Number: 5787\n",
      "Loss: 12.73625737530854\n",
      "l2 norm of gradients: 0.08655547869490063\n",
      "l2 norm of weights: 3.2118904179302277\n",
      "---------------------\n",
      "Iteration Number: 5788\n",
      "Loss: 12.735279219861098\n",
      "l2 norm of gradients: 0.08652722213215876\n",
      "l2 norm of weights: 3.211924975633872\n",
      "---------------------\n",
      "Iteration Number: 5789\n",
      "Loss: 12.734301760754175\n",
      "l2 norm of gradients: 0.0864989859739063\n",
      "l2 norm of weights: 3.2119595773282046\n",
      "---------------------\n",
      "Iteration Number: 5790\n",
      "Loss: 12.73332499728852\n",
      "l2 norm of gradients: 0.08647077021166034\n",
      "l2 norm of weights: 3.2119942229672884\n",
      "---------------------\n",
      "Iteration Number: 5791\n",
      "Loss: 12.732348928765447\n",
      "l2 norm of gradients: 0.08644257483690945\n",
      "l2 norm of weights: 3.2120289125052164\n",
      "---------------------\n",
      "Iteration Number: 5792\n",
      "Loss: 12.73137355448694\n",
      "l2 norm of gradients: 0.08641439984111361\n",
      "l2 norm of weights: 3.2120636458961114\n",
      "---------------------\n",
      "Iteration Number: 5793\n",
      "Loss: 12.730398873755572\n",
      "l2 norm of gradients: 0.08638624521570457\n",
      "l2 norm of weights: 3.212098423094124\n",
      "---------------------\n",
      "Iteration Number: 5794\n",
      "Loss: 12.729424885874549\n",
      "l2 norm of gradients: 0.08635811095208562\n",
      "l2 norm of weights: 3.212133244053437\n",
      "---------------------\n",
      "Iteration Number: 5795\n",
      "Loss: 12.728451590147708\n",
      "l2 norm of gradients: 0.08632999704163191\n",
      "l2 norm of weights: 3.21216810872826\n",
      "---------------------\n",
      "Iteration Number: 5796\n",
      "Loss: 12.727478985879488\n",
      "l2 norm of gradients: 0.08630190347569047\n",
      "l2 norm of weights: 3.2122030170728353\n",
      "---------------------\n",
      "Iteration Number: 5797\n",
      "Loss: 12.726507072374998\n",
      "l2 norm of gradients: 0.0862738302455802\n",
      "l2 norm of weights: 3.212237969041432\n",
      "---------------------\n",
      "Iteration Number: 5798\n",
      "Loss: 12.725535848939906\n",
      "l2 norm of gradients: 0.08624577734259205\n",
      "l2 norm of weights: 3.2122729645883514\n",
      "---------------------\n",
      "Iteration Number: 5799\n",
      "Loss: 12.72456531488055\n",
      "l2 norm of gradients: 0.08621774475798906\n",
      "l2 norm of weights: 3.2123080036679212\n",
      "---------------------\n",
      "Iteration Number: 5800\n",
      "Loss: 12.723595469503895\n",
      "l2 norm of gradients: 0.08618973248300651\n",
      "l2 norm of weights: 3.212343086234503\n",
      "---------------------\n",
      "Iteration Number: 5801\n",
      "Loss: 12.722626312117507\n",
      "l2 norm of gradients: 0.08616174050885186\n",
      "l2 norm of weights: 3.2123782122424838\n",
      "---------------------\n",
      "Iteration Number: 5802\n",
      "Loss: 12.721657842029574\n",
      "l2 norm of gradients: 0.08613376882670504\n",
      "l2 norm of weights: 3.2124133816462845\n",
      "---------------------\n",
      "Iteration Number: 5803\n",
      "Loss: 12.720690058548959\n",
      "l2 norm of gradients: 0.08610581742771829\n",
      "l2 norm of weights: 3.2124485944003522\n",
      "---------------------\n",
      "Iteration Number: 5804\n",
      "Loss: 12.719722960985075\n",
      "l2 norm of gradients: 0.08607788630301644\n",
      "l2 norm of weights: 3.212483850459166\n",
      "---------------------\n",
      "Iteration Number: 5805\n",
      "Loss: 12.718756548648036\n",
      "l2 norm of gradients: 0.08604997544369691\n",
      "l2 norm of weights: 3.212519149777234\n",
      "---------------------\n",
      "Iteration Number: 5806\n",
      "Loss: 12.717790820848528\n",
      "l2 norm of gradients: 0.0860220848408298\n",
      "l2 norm of weights: 3.2125544923090947\n",
      "---------------------\n",
      "Iteration Number: 5807\n",
      "Loss: 12.716825776897878\n",
      "l2 norm of gradients: 0.08599421448545795\n",
      "l2 norm of weights: 3.2125898780093154\n",
      "---------------------\n",
      "Iteration Number: 5808\n",
      "Loss: 12.715861416108059\n",
      "l2 norm of gradients: 0.08596636436859716\n",
      "l2 norm of weights: 3.2126253068324946\n",
      "---------------------\n",
      "Iteration Number: 5809\n",
      "Loss: 12.714897737791649\n",
      "l2 norm of gradients: 0.08593853448123598\n",
      "l2 norm of weights: 3.2126607787332597\n",
      "---------------------\n",
      "Iteration Number: 5810\n",
      "Loss: 12.713934741261859\n",
      "l2 norm of gradients: 0.08591072481433618\n",
      "l2 norm of weights: 3.212696293666268\n",
      "---------------------\n",
      "Iteration Number: 5811\n",
      "Loss: 12.712972425832543\n",
      "l2 norm of gradients: 0.08588293535883247\n",
      "l2 norm of weights: 3.2127318515862076\n",
      "---------------------\n",
      "Iteration Number: 5812\n",
      "Loss: 12.712010790818145\n",
      "l2 norm of gradients: 0.08585516610563289\n",
      "l2 norm of weights: 3.2127674524477956\n",
      "---------------------\n",
      "Iteration Number: 5813\n",
      "Loss: 12.711049835533776\n",
      "l2 norm of gradients: 0.08582741704561866\n",
      "l2 norm of weights: 3.2128030962057794\n",
      "---------------------\n",
      "Iteration Number: 5814\n",
      "Loss: 12.710089559295154\n",
      "l2 norm of gradients: 0.08579968816964437\n",
      "l2 norm of weights: 3.2128387828149374\n",
      "---------------------\n",
      "Iteration Number: 5815\n",
      "Loss: 12.70912996141863\n",
      "l2 norm of gradients: 0.08577197946853803\n",
      "l2 norm of weights: 3.2128745122300755\n",
      "---------------------\n",
      "Iteration Number: 5816\n",
      "Loss: 12.708171041221181\n",
      "l2 norm of gradients: 0.08574429093310128\n",
      "l2 norm of weights: 3.2129102844060333\n",
      "---------------------\n",
      "Iteration Number: 5817\n",
      "Loss: 12.707212798020429\n",
      "l2 norm of gradients: 0.08571662255410925\n",
      "l2 norm of weights: 3.212946099297677\n",
      "---------------------\n",
      "Iteration Number: 5818\n",
      "Loss: 12.706255231134588\n",
      "l2 norm of gradients: 0.08568897432231085\n",
      "l2 norm of weights: 3.212981956859904\n",
      "---------------------\n",
      "Iteration Number: 5819\n",
      "Loss: 12.705298339882525\n",
      "l2 norm of gradients: 0.08566134622842866\n",
      "l2 norm of weights: 3.213017857047643\n",
      "---------------------\n",
      "Iteration Number: 5820\n",
      "Loss: 12.704342123583775\n",
      "l2 norm of gradients: 0.08563373826315933\n",
      "l2 norm of weights: 3.2130537998158513\n",
      "---------------------\n",
      "Iteration Number: 5821\n",
      "Loss: 12.703386581558409\n",
      "l2 norm of gradients: 0.08560615041717326\n",
      "l2 norm of weights: 3.2130897851195175\n",
      "---------------------\n",
      "Iteration Number: 5822\n",
      "Loss: 12.702431713127217\n",
      "l2 norm of gradients: 0.08557858268111496\n",
      "l2 norm of weights: 3.2131258129136584\n",
      "---------------------\n",
      "Iteration Number: 5823\n",
      "Loss: 12.701477517611572\n",
      "l2 norm of gradients: 0.0855510350456031\n",
      "l2 norm of weights: 3.2131618831533237\n",
      "---------------------\n",
      "Iteration Number: 5824\n",
      "Loss: 12.70052399433348\n",
      "l2 norm of gradients: 0.0855235075012305\n",
      "l2 norm of weights: 3.213197995793591\n",
      "---------------------\n",
      "Iteration Number: 5825\n",
      "Loss: 12.699571142615596\n",
      "l2 norm of gradients: 0.0854960000385643\n",
      "l2 norm of weights: 3.2132341507895696\n",
      "---------------------\n",
      "Iteration Number: 5826\n",
      "Loss: 12.698618961781168\n",
      "l2 norm of gradients: 0.08546851264814602\n",
      "l2 norm of weights: 3.213270348096398\n",
      "---------------------\n",
      "Iteration Number: 5827\n",
      "Loss: 12.69766745115414\n",
      "l2 norm of gradients: 0.08544104532049164\n",
      "l2 norm of weights: 3.2133065876692455\n",
      "---------------------\n",
      "Iteration Number: 5828\n",
      "Loss: 12.696716610059019\n",
      "l2 norm of gradients: 0.08541359804609168\n",
      "l2 norm of weights: 3.2133428694633115\n",
      "---------------------\n",
      "Iteration Number: 5829\n",
      "Loss: 12.695766437820971\n",
      "l2 norm of gradients: 0.08538617081541135\n",
      "l2 norm of weights: 3.2133791934338256\n",
      "---------------------\n",
      "Iteration Number: 5830\n",
      "Loss: 12.694816933765809\n",
      "l2 norm of gradients: 0.08535876361889048\n",
      "l2 norm of weights: 3.213415559536048\n",
      "---------------------\n",
      "Iteration Number: 5831\n",
      "Loss: 12.69386809721994\n",
      "l2 norm of gradients: 0.08533137644694379\n",
      "l2 norm of weights: 3.213451967725269\n",
      "---------------------\n",
      "Iteration Number: 5832\n",
      "Loss: 12.692919927510431\n",
      "l2 norm of gradients: 0.08530400928996092\n",
      "l2 norm of weights: 3.2134884179568086\n",
      "---------------------\n",
      "Iteration Number: 5833\n",
      "Loss: 12.691972423964973\n",
      "l2 norm of gradients: 0.0852766621383064\n",
      "l2 norm of weights: 3.2135249101860186\n",
      "---------------------\n",
      "Iteration Number: 5834\n",
      "Loss: 12.69102558591189\n",
      "l2 norm of gradients: 0.08524933498231989\n",
      "l2 norm of weights: 3.21356144436828\n",
      "---------------------\n",
      "Iteration Number: 5835\n",
      "Loss: 12.690079412680118\n",
      "l2 norm of gradients: 0.08522202781231619\n",
      "l2 norm of weights: 3.213598020459004\n",
      "---------------------\n",
      "Iteration Number: 5836\n",
      "Loss: 12.689133903599275\n",
      "l2 norm of gradients: 0.08519474061858531\n",
      "l2 norm of weights: 3.2136346384136334\n",
      "---------------------\n",
      "Iteration Number: 5837\n",
      "Loss: 12.688189057999537\n",
      "l2 norm of gradients: 0.08516747339139268\n",
      "l2 norm of weights: 3.2136712981876405\n",
      "---------------------\n",
      "Iteration Number: 5838\n",
      "Loss: 12.687244875211768\n",
      "l2 norm of gradients: 0.08514022612097902\n",
      "l2 norm of weights: 3.2137079997365285\n",
      "---------------------\n",
      "Iteration Number: 5839\n",
      "Loss: 12.686301354567458\n",
      "l2 norm of gradients: 0.08511299879756065\n",
      "l2 norm of weights: 3.2137447430158304\n",
      "---------------------\n",
      "Iteration Number: 5840\n",
      "Loss: 12.685358495398717\n",
      "l2 norm of gradients: 0.08508579141132942\n",
      "l2 norm of weights: 3.2137815279811104\n",
      "---------------------\n",
      "Iteration Number: 5841\n",
      "Loss: 12.684416297038267\n",
      "l2 norm of gradients: 0.08505860395245285\n",
      "l2 norm of weights: 3.213818354587963\n",
      "---------------------\n",
      "Iteration Number: 5842\n",
      "Loss: 12.683474758819521\n",
      "l2 norm of gradients: 0.08503143641107432\n",
      "l2 norm of weights: 3.2138552227920125\n",
      "---------------------\n",
      "Iteration Number: 5843\n",
      "Loss: 12.682533880076464\n",
      "l2 norm of gradients: 0.0850042887773129\n",
      "l2 norm of weights: 3.2138921325489145\n",
      "---------------------\n",
      "Iteration Number: 5844\n",
      "Loss: 12.68159366014374\n",
      "l2 norm of gradients: 0.08497716104126371\n",
      "l2 norm of weights: 3.213929083814355\n",
      "---------------------\n",
      "Iteration Number: 5845\n",
      "Loss: 12.68065409835662\n",
      "l2 norm of gradients: 0.08495005319299789\n",
      "l2 norm of weights: 3.213966076544051\n",
      "---------------------\n",
      "Iteration Number: 5846\n",
      "Loss: 12.67971519405104\n",
      "l2 norm of gradients: 0.08492296522256265\n",
      "l2 norm of weights: 3.2140031106937488\n",
      "---------------------\n",
      "Iteration Number: 5847\n",
      "Loss: 12.678776946563511\n",
      "l2 norm of gradients: 0.08489589711998138\n",
      "l2 norm of weights: 3.214040186219226\n",
      "---------------------\n",
      "Iteration Number: 5848\n",
      "Loss: 12.677839355231217\n",
      "l2 norm of gradients: 0.08486884887525378\n",
      "l2 norm of weights: 3.214077303076292\n",
      "---------------------\n",
      "Iteration Number: 5849\n",
      "Loss: 12.676902419391984\n",
      "l2 norm of gradients: 0.08484182047835594\n",
      "l2 norm of weights: 3.2141144612207846\n",
      "---------------------\n",
      "Iteration Number: 5850\n",
      "Loss: 12.675966138384226\n",
      "l2 norm of gradients: 0.08481481191924037\n",
      "l2 norm of weights: 3.2141516606085734\n",
      "---------------------\n",
      "Iteration Number: 5851\n",
      "Loss: 12.675030511547021\n",
      "l2 norm of gradients: 0.08478782318783618\n",
      "l2 norm of weights: 3.2141889011955587\n",
      "---------------------\n",
      "Iteration Number: 5852\n",
      "Loss: 12.67409553822008\n",
      "l2 norm of gradients: 0.08476085427404907\n",
      "l2 norm of weights: 3.2142261829376713\n",
      "---------------------\n",
      "Iteration Number: 5853\n",
      "Loss: 12.673161217743756\n",
      "l2 norm of gradients: 0.08473390516776143\n",
      "l2 norm of weights: 3.214263505790872\n",
      "---------------------\n",
      "Iteration Number: 5854\n",
      "Loss: 12.672227549459011\n",
      "l2 norm of gradients: 0.08470697585883252\n",
      "l2 norm of weights: 3.214300869711155\n",
      "---------------------\n",
      "Iteration Number: 5855\n",
      "Loss: 12.671294532707465\n",
      "l2 norm of gradients: 0.08468006633709853\n",
      "l2 norm of weights: 3.2143382746545406\n",
      "---------------------\n",
      "Iteration Number: 5856\n",
      "Loss: 12.670362166831351\n",
      "l2 norm of gradients: 0.08465317659237252\n",
      "l2 norm of weights: 3.2143757205770838\n",
      "---------------------\n",
      "Iteration Number: 5857\n",
      "Loss: 12.669430451173536\n",
      "l2 norm of gradients: 0.08462630661444467\n",
      "l2 norm of weights: 3.2144132074348692\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 5858\n",
      "Loss: 12.668499385077556\n",
      "l2 norm of gradients: 0.08459945639308235\n",
      "l2 norm of weights: 3.2144507351840117\n",
      "---------------------\n",
      "Iteration Number: 5859\n",
      "Loss: 12.66756896788753\n",
      "l2 norm of gradients: 0.08457262591803015\n",
      "l2 norm of weights: 3.2144883037806564\n",
      "---------------------\n",
      "Iteration Number: 5860\n",
      "Loss: 12.666639198948266\n",
      "l2 norm of gradients: 0.08454581517901\n",
      "l2 norm of weights: 3.214525913180981\n",
      "---------------------\n",
      "Iteration Number: 5861\n",
      "Loss: 12.665710077605137\n",
      "l2 norm of gradients: 0.08451902416572123\n",
      "l2 norm of weights: 3.214563563341192\n",
      "---------------------\n",
      "Iteration Number: 5862\n",
      "Loss: 12.664781603204219\n",
      "l2 norm of gradients: 0.08449225286784073\n",
      "l2 norm of weights: 3.2146012542175293\n",
      "---------------------\n",
      "Iteration Number: 5863\n",
      "Loss: 12.663853775092184\n",
      "l2 norm of gradients: 0.08446550127502295\n",
      "l2 norm of weights: 3.21463898576626\n",
      "---------------------\n",
      "Iteration Number: 5864\n",
      "Loss: 12.662926592616348\n",
      "l2 norm of gradients: 0.08443876937690001\n",
      "l2 norm of weights: 3.2146767579436855\n",
      "---------------------\n",
      "Iteration Number: 5865\n",
      "Loss: 12.662000055124658\n",
      "l2 norm of gradients: 0.0844120571630818\n",
      "l2 norm of weights: 3.214714570706136\n",
      "---------------------\n",
      "Iteration Number: 5866\n",
      "Loss: 12.661074161965711\n",
      "l2 norm of gradients: 0.08438536462315617\n",
      "l2 norm of weights: 3.2147524240099736\n",
      "---------------------\n",
      "Iteration Number: 5867\n",
      "Loss: 12.66014891248871\n",
      "l2 norm of gradients: 0.08435869174668884\n",
      "l2 norm of weights: 3.2147903178115906\n",
      "---------------------\n",
      "Iteration Number: 5868\n",
      "Loss: 12.659224306043512\n",
      "l2 norm of gradients: 0.08433203852322348\n",
      "l2 norm of weights: 3.2148282520674103\n",
      "---------------------\n",
      "Iteration Number: 5869\n",
      "Loss: 12.658300341980596\n",
      "l2 norm of gradients: 0.08430540494228204\n",
      "l2 norm of weights: 3.214866226733888\n",
      "---------------------\n",
      "Iteration Number: 5870\n",
      "Loss: 12.657377019651104\n",
      "l2 norm of gradients: 0.0842787909933646\n",
      "l2 norm of weights: 3.214904241767508\n",
      "---------------------\n",
      "Iteration Number: 5871\n",
      "Loss: 12.656454338406768\n",
      "l2 norm of gradients: 0.08425219666594957\n",
      "l2 norm of weights: 3.214942297124787\n",
      "---------------------\n",
      "Iteration Number: 5872\n",
      "Loss: 12.655532297600006\n",
      "l2 norm of gradients: 0.08422562194949373\n",
      "l2 norm of weights: 3.214980392762272\n",
      "---------------------\n",
      "Iteration Number: 5873\n",
      "Loss: 12.65461089658383\n",
      "l2 norm of gradients: 0.08419906683343235\n",
      "l2 norm of weights: 3.2150185286365422\n",
      "---------------------\n",
      "Iteration Number: 5874\n",
      "Loss: 12.653690134711889\n",
      "l2 norm of gradients: 0.08417253130717925\n",
      "l2 norm of weights: 3.215056704704206\n",
      "---------------------\n",
      "Iteration Number: 5875\n",
      "Loss: 12.652770011338495\n",
      "l2 norm of gradients: 0.08414601536012688\n",
      "l2 norm of weights: 3.2150949209219033\n",
      "---------------------\n",
      "Iteration Number: 5876\n",
      "Loss: 12.651850525818565\n",
      "l2 norm of gradients: 0.08411951898164649\n",
      "l2 norm of weights: 3.215133177246306\n",
      "---------------------\n",
      "Iteration Number: 5877\n",
      "Loss: 12.650931677507682\n",
      "l2 norm of gradients: 0.08409304216108811\n",
      "l2 norm of weights: 3.2151714736341157\n",
      "---------------------\n",
      "Iteration Number: 5878\n",
      "Loss: 12.65001346576203\n",
      "l2 norm of gradients: 0.08406658488778072\n",
      "l2 norm of weights: 3.2152098100420665\n",
      "---------------------\n",
      "Iteration Number: 5879\n",
      "Loss: 12.649095889938444\n",
      "l2 norm of gradients: 0.08404014715103224\n",
      "l2 norm of weights: 3.215248186426922\n",
      "---------------------\n",
      "Iteration Number: 5880\n",
      "Loss: 12.648178949394381\n",
      "l2 norm of gradients: 0.08401372894012978\n",
      "l2 norm of weights: 3.2152866027454774\n",
      "---------------------\n",
      "Iteration Number: 5881\n",
      "Loss: 12.647262643487974\n",
      "l2 norm of gradients: 0.08398733024433953\n",
      "l2 norm of weights: 3.21532505895456\n",
      "---------------------\n",
      "Iteration Number: 5882\n",
      "Loss: 12.646346971577943\n",
      "l2 norm of gradients: 0.08396095105290705\n",
      "l2 norm of weights: 3.215363555011027\n",
      "---------------------\n",
      "Iteration Number: 5883\n",
      "Loss: 12.645431933023655\n",
      "l2 norm of gradients: 0.08393459135505713\n",
      "l2 norm of weights: 3.215402090871767\n",
      "---------------------\n",
      "Iteration Number: 5884\n",
      "Loss: 12.644517527185128\n",
      "l2 norm of gradients: 0.08390825113999416\n",
      "l2 norm of weights: 3.2154406664937\n",
      "---------------------\n",
      "Iteration Number: 5885\n",
      "Loss: 12.643603753422994\n",
      "l2 norm of gradients: 0.08388193039690188\n",
      "l2 norm of weights: 3.215479281833776\n",
      "---------------------\n",
      "Iteration Number: 5886\n",
      "Loss: 12.642690611098544\n",
      "l2 norm of gradients: 0.0838556291149438\n",
      "l2 norm of weights: 3.2155179368489777\n",
      "---------------------\n",
      "Iteration Number: 5887\n",
      "Loss: 12.641778099573669\n",
      "l2 norm of gradients: 0.08382934728326308\n",
      "l2 norm of weights: 3.215556631496319\n",
      "---------------------\n",
      "Iteration Number: 5888\n",
      "Loss: 12.640866218210943\n",
      "l2 norm of gradients: 0.08380308489098263\n",
      "l2 norm of weights: 3.215595365732843\n",
      "---------------------\n",
      "Iteration Number: 5889\n",
      "Loss: 12.639954966373523\n",
      "l2 norm of gradients: 0.08377684192720532\n",
      "l2 norm of weights: 3.215634139515626\n",
      "---------------------\n",
      "Iteration Number: 5890\n",
      "Loss: 12.639044343425233\n",
      "l2 norm of gradients: 0.08375061838101397\n",
      "l2 norm of weights: 3.2156729528017745\n",
      "---------------------\n",
      "Iteration Number: 5891\n",
      "Loss: 12.638134348730512\n",
      "l2 norm of gradients: 0.08372441424147144\n",
      "l2 norm of weights: 3.215711805548426\n",
      "---------------------\n",
      "Iteration Number: 5892\n",
      "Loss: 12.637224981654462\n",
      "l2 norm of gradients: 0.08369822949762071\n",
      "l2 norm of weights: 3.21575069771275\n",
      "---------------------\n",
      "Iteration Number: 5893\n",
      "Loss: 12.636316241562783\n",
      "l2 norm of gradients: 0.08367206413848505\n",
      "l2 norm of weights: 3.215789629251947\n",
      "---------------------\n",
      "Iteration Number: 5894\n",
      "Loss: 12.63540812782185\n",
      "l2 norm of gradients: 0.08364591815306804\n",
      "l2 norm of weights: 3.215828600123248\n",
      "---------------------\n",
      "Iteration Number: 5895\n",
      "Loss: 12.63450063979864\n",
      "l2 norm of gradients: 0.08361979153035363\n",
      "l2 norm of weights: 3.2158676102839165\n",
      "---------------------\n",
      "Iteration Number: 5896\n",
      "Loss: 12.633593776860756\n",
      "l2 norm of gradients: 0.08359368425930633\n",
      "l2 norm of weights: 3.215906659691246\n",
      "---------------------\n",
      "Iteration Number: 5897\n",
      "Loss: 12.632687538376492\n",
      "l2 norm of gradients: 0.08356759632887117\n",
      "l2 norm of weights: 3.215945748302562\n",
      "---------------------\n",
      "Iteration Number: 5898\n",
      "Loss: 12.63178192371471\n",
      "l2 norm of gradients: 0.08354152772797388\n",
      "l2 norm of weights: 3.2159848760752214\n",
      "---------------------\n",
      "Iteration Number: 5899\n",
      "Loss: 12.630876932244941\n",
      "l2 norm of gradients: 0.08351547844552099\n",
      "l2 norm of weights: 3.216024042966611\n",
      "---------------------\n",
      "Iteration Number: 5900\n",
      "Loss: 12.629972563337363\n",
      "l2 norm of gradients: 0.0834894484703998\n",
      "l2 norm of weights: 3.216063248934151\n",
      "---------------------\n",
      "Iteration Number: 5901\n",
      "Loss: 12.62906881636274\n",
      "l2 norm of gradients: 0.08346343779147858\n",
      "l2 norm of weights: 3.2161024939352925\n",
      "---------------------\n",
      "Iteration Number: 5902\n",
      "Loss: 12.628165690692514\n",
      "l2 norm of gradients: 0.08343744639760661\n",
      "l2 norm of weights: 3.2161417779275157\n",
      "---------------------\n",
      "Iteration Number: 5903\n",
      "Loss: 12.62726318569875\n",
      "l2 norm of gradients: 0.08341147427761438\n",
      "l2 norm of weights: 3.2161811008683343\n",
      "---------------------\n",
      "Iteration Number: 5904\n",
      "Loss: 12.626361300754137\n",
      "l2 norm of gradients: 0.0833855214203134\n",
      "l2 norm of weights: 3.2162204627152935\n",
      "---------------------\n",
      "Iteration Number: 5905\n",
      "Loss: 12.625460035232006\n",
      "l2 norm of gradients: 0.08335958781449662\n",
      "l2 norm of weights: 3.216259863425968\n",
      "---------------------\n",
      "Iteration Number: 5906\n",
      "Loss: 12.624559388506322\n",
      "l2 norm of gradients: 0.08333367344893823\n",
      "l2 norm of weights: 3.216299302957966\n",
      "---------------------\n",
      "Iteration Number: 5907\n",
      "Loss: 12.62365935995168\n",
      "l2 norm of gradients: 0.08330777831239401\n",
      "l2 norm of weights: 3.2163387812689255\n",
      "---------------------\n",
      "Iteration Number: 5908\n",
      "Loss: 12.622759948943306\n",
      "l2 norm of gradients: 0.08328190239360121\n",
      "l2 norm of weights: 3.2163782983165166\n",
      "---------------------\n",
      "Iteration Number: 5909\n",
      "Loss: 12.621861154857093\n",
      "l2 norm of gradients: 0.08325604568127873\n",
      "l2 norm of weights: 3.2164178540584403\n",
      "---------------------\n",
      "Iteration Number: 5910\n",
      "Loss: 12.62096297706949\n",
      "l2 norm of gradients: 0.08323020816412716\n",
      "l2 norm of weights: 3.2164574484524295\n",
      "---------------------\n",
      "Iteration Number: 5911\n",
      "Loss: 12.620065414957676\n",
      "l2 norm of gradients: 0.08320438983082895\n",
      "l2 norm of weights: 3.2164970814562484\n",
      "---------------------\n",
      "Iteration Number: 5912\n",
      "Loss: 12.619168467899378\n",
      "l2 norm of gradients: 0.0831785906700484\n",
      "l2 norm of weights: 3.2165367530276927\n",
      "---------------------\n",
      "Iteration Number: 5913\n",
      "Loss: 12.618272135273019\n",
      "l2 norm of gradients: 0.08315281067043187\n",
      "l2 norm of weights: 3.216576463124589\n",
      "---------------------\n",
      "Iteration Number: 5914\n",
      "Loss: 12.617376416457612\n",
      "l2 norm of gradients: 0.08312704982060766\n",
      "l2 norm of weights: 3.216616211704796\n",
      "---------------------\n",
      "Iteration Number: 5915\n",
      "Loss: 12.616481310832837\n",
      "l2 norm of gradients: 0.08310130810918631\n",
      "l2 norm of weights: 3.216655998726203\n",
      "---------------------\n",
      "Iteration Number: 5916\n",
      "Loss: 12.615586817778995\n",
      "l2 norm of gradients: 0.08307558552476067\n",
      "l2 norm of weights: 3.2166958241467323\n",
      "---------------------\n",
      "Iteration Number: 5917\n",
      "Loss: 12.61469293667701\n",
      "l2 norm of gradients: 0.08304988205590574\n",
      "l2 norm of weights: 3.2167356879243356\n",
      "---------------------\n",
      "Iteration Number: 5918\n",
      "Loss: 12.613799666908427\n",
      "l2 norm of gradients: 0.08302419769117908\n",
      "l2 norm of weights: 3.2167755900169976\n",
      "---------------------\n",
      "Iteration Number: 5919\n",
      "Loss: 12.61290700785546\n",
      "l2 norm of gradients: 0.08299853241912068\n",
      "l2 norm of weights: 3.216815530382734\n",
      "---------------------\n",
      "Iteration Number: 5920\n",
      "Loss: 12.612014958900941\n",
      "l2 norm of gradients: 0.08297288622825316\n",
      "l2 norm of weights: 3.216855508979592\n",
      "---------------------\n",
      "Iteration Number: 5921\n",
      "Loss: 12.611123519428329\n",
      "l2 norm of gradients: 0.08294725910708182\n",
      "l2 norm of weights: 3.2168955257656506\n",
      "---------------------\n",
      "Iteration Number: 5922\n",
      "Loss: 12.610232688821705\n",
      "l2 norm of gradients: 0.0829216510440947\n",
      "l2 norm of weights: 3.216935580699019\n",
      "---------------------\n",
      "Iteration Number: 5923\n",
      "Loss: 12.609342466465817\n",
      "l2 norm of gradients: 0.08289606202776269\n",
      "l2 norm of weights: 3.21697567373784\n",
      "---------------------\n",
      "Iteration Number: 5924\n",
      "Loss: 12.608452851746003\n",
      "l2 norm of gradients: 0.08287049204653957\n",
      "l2 norm of weights: 3.2170158048402864\n",
      "---------------------\n",
      "Iteration Number: 5925\n",
      "Loss: 12.607563844048265\n",
      "l2 norm of gradients: 0.08284494108886226\n",
      "l2 norm of weights: 3.2170559739645626\n",
      "---------------------\n",
      "Iteration Number: 5926\n",
      "Loss: 12.606675442759212\n",
      "l2 norm of gradients: 0.08281940914315065\n",
      "l2 norm of weights: 3.2170961810689054\n",
      "---------------------\n",
      "Iteration Number: 5927\n",
      "Loss: 12.605787647266121\n",
      "l2 norm of gradients: 0.08279389619780791\n",
      "l2 norm of weights: 3.217136426111583\n",
      "---------------------\n",
      "Iteration Number: 5928\n",
      "Loss: 12.604900456956855\n",
      "l2 norm of gradients: 0.08276840224122052\n",
      "l2 norm of weights: 3.217176709050894\n",
      "---------------------\n",
      "Iteration Number: 5929\n",
      "Loss: 12.604013871219948\n",
      "l2 norm of gradients: 0.0827429272617582\n",
      "l2 norm of weights: 3.217217029845169\n",
      "---------------------\n",
      "Iteration Number: 5930\n",
      "Loss: 12.603127889444545\n",
      "l2 norm of gradients: 0.08271747124777425\n",
      "l2 norm of weights: 3.217257388452772\n",
      "---------------------\n",
      "Iteration Number: 5931\n",
      "Loss: 12.60224251102041\n",
      "l2 norm of gradients: 0.08269203418760541\n",
      "l2 norm of weights: 3.2172977848320956\n",
      "---------------------\n",
      "Iteration Number: 5932\n",
      "Loss: 12.601357735337972\n",
      "l2 norm of gradients: 0.08266661606957212\n",
      "l2 norm of weights: 3.217338218941567\n",
      "---------------------\n",
      "Iteration Number: 5933\n",
      "Loss: 12.600473561788279\n",
      "l2 norm of gradients: 0.08264121688197848\n",
      "l2 norm of weights: 3.217378690739642\n",
      "---------------------\n",
      "Iteration Number: 5934\n",
      "Loss: 12.599589989762984\n",
      "l2 norm of gradients: 0.08261583661311239\n",
      "l2 norm of weights: 3.21741920018481\n",
      "---------------------\n",
      "Iteration Number: 5935\n",
      "Loss: 12.598707018654409\n",
      "l2 norm of gradients: 0.08259047525124566\n",
      "l2 norm of weights: 3.2174597472355924\n",
      "---------------------\n",
      "Iteration Number: 5936\n",
      "Loss: 12.59782464785548\n",
      "l2 norm of gradients: 0.08256513278463405\n",
      "l2 norm of weights: 3.2175003318505397\n",
      "---------------------\n",
      "Iteration Number: 5937\n",
      "Loss: 12.596942876759757\n",
      "l2 norm of gradients: 0.08253980920151732\n",
      "l2 norm of weights: 3.217540953988237\n",
      "---------------------\n",
      "Iteration Number: 5938\n",
      "Loss: 12.596061704761455\n",
      "l2 norm of gradients: 0.08251450449011942\n",
      "l2 norm of weights: 3.217581613607298\n",
      "---------------------\n",
      "Iteration Number: 5939\n",
      "Loss: 12.595181131255389\n",
      "l2 norm of gradients: 0.08248921863864853\n",
      "l2 norm of weights: 3.217622310666371\n",
      "---------------------\n",
      "Iteration Number: 5940\n",
      "Loss: 12.594301155637003\n",
      "l2 norm of gradients: 0.08246395163529713\n",
      "l2 norm of weights: 3.217663045124134\n",
      "---------------------\n",
      "Iteration Number: 5941\n",
      "Loss: 12.593421777302412\n",
      "l2 norm of gradients: 0.08243870346824206\n",
      "l2 norm of weights: 3.217703816939298\n",
      "---------------------\n",
      "Iteration Number: 5942\n",
      "Loss: 12.592542995648305\n",
      "l2 norm of gradients: 0.08241347412564469\n",
      "l2 norm of weights: 3.2177446260706044\n",
      "---------------------\n",
      "Iteration Number: 5943\n",
      "Loss: 12.591664810072036\n",
      "l2 norm of gradients: 0.08238826359565085\n",
      "l2 norm of weights: 3.2177854724768262\n",
      "---------------------\n",
      "Iteration Number: 5944\n",
      "Loss: 12.590787219971574\n",
      "l2 norm of gradients: 0.08236307186639112\n",
      "l2 norm of weights: 3.217826356116769\n",
      "---------------------\n",
      "Iteration Number: 5945\n",
      "Loss: 12.589910224745548\n",
      "l2 norm of gradients: 0.0823378989259808\n",
      "l2 norm of weights: 3.2178672769492698\n",
      "---------------------\n",
      "Iteration Number: 5946\n",
      "Loss: 12.589033823793157\n",
      "l2 norm of gradients: 0.08231274476251992\n",
      "l2 norm of weights: 3.2179082349331973\n",
      "---------------------\n",
      "Iteration Number: 5947\n",
      "Loss: 12.588158016514294\n",
      "l2 norm of gradients: 0.08228760936409352\n",
      "l2 norm of weights: 3.2179492300274513\n",
      "---------------------\n",
      "Iteration Number: 5948\n",
      "Loss: 12.587282802309431\n",
      "l2 norm of gradients: 0.08226249271877156\n",
      "l2 norm of weights: 3.2179902621909644\n",
      "---------------------\n",
      "Iteration Number: 5949\n",
      "Loss: 12.586408180579689\n",
      "l2 norm of gradients: 0.08223739481460905\n",
      "l2 norm of weights: 3.218031331382699\n",
      "---------------------\n",
      "Iteration Number: 5950\n",
      "Loss: 12.585534150726815\n",
      "l2 norm of gradients: 0.08221231563964622\n",
      "l2 norm of weights: 3.218072437561651\n",
      "---------------------\n",
      "Iteration Number: 5951\n",
      "Loss: 12.5846607121532\n",
      "l2 norm of gradients: 0.08218725518190852\n",
      "l2 norm of weights: 3.218113580686848\n",
      "---------------------\n",
      "Iteration Number: 5952\n",
      "Loss: 12.583787864261838\n",
      "l2 norm of gradients: 0.08216221342940663\n",
      "l2 norm of weights: 3.218154760717348\n",
      "---------------------\n",
      "Iteration Number: 5953\n",
      "Loss: 12.58291560645637\n",
      "l2 norm of gradients: 0.08213719037013674\n",
      "l2 norm of weights: 3.218195977612242\n",
      "---------------------\n",
      "Iteration Number: 5954\n",
      "Loss: 12.582043938141023\n",
      "l2 norm of gradients: 0.08211218599208055\n",
      "l2 norm of weights: 3.218237231330651\n",
      "---------------------\n",
      "Iteration Number: 5955\n",
      "Loss: 12.581172858720725\n",
      "l2 norm of gradients: 0.08208720028320518\n",
      "l2 norm of weights: 3.2182785218317305\n",
      "---------------------\n",
      "Iteration Number: 5956\n",
      "Loss: 12.580302367600973\n",
      "l2 norm of gradients: 0.08206223323146355\n",
      "l2 norm of weights: 3.218319849074664\n",
      "---------------------\n",
      "Iteration Number: 5957\n",
      "Loss: 12.579432464187914\n",
      "l2 norm of gradients: 0.0820372848247943\n",
      "l2 norm of weights: 3.2183612130186705\n",
      "---------------------\n",
      "Iteration Number: 5958\n",
      "Loss: 12.578563147888309\n",
      "l2 norm of gradients: 0.08201235505112185\n",
      "l2 norm of weights: 3.218402613622999\n",
      "---------------------\n",
      "Iteration Number: 5959\n",
      "Loss: 12.57769441810956\n",
      "l2 norm of gradients: 0.08198744389835652\n",
      "l2 norm of weights: 3.2184440508469288\n",
      "---------------------\n",
      "Iteration Number: 5960\n",
      "Loss: 12.57682627425969\n",
      "l2 norm of gradients: 0.08196255135439469\n",
      "l2 norm of weights: 3.218485524649774\n",
      "---------------------\n",
      "Iteration Number: 5961\n",
      "Loss: 12.575958715747333\n",
      "l2 norm of gradients: 0.08193767740711867\n",
      "l2 norm of weights: 3.2185270349908777\n",
      "---------------------\n",
      "Iteration Number: 5962\n",
      "Loss: 12.57509174198179\n",
      "l2 norm of gradients: 0.08191282204439711\n",
      "l2 norm of weights: 3.2185685818296172\n",
      "---------------------\n",
      "Iteration Number: 5963\n",
      "Loss: 12.574225352372943\n",
      "l2 norm of gradients: 0.08188798525408478\n",
      "l2 norm of weights: 3.218610165125399\n",
      "---------------------\n",
      "Iteration Number: 5964\n",
      "Loss: 12.573359546331321\n",
      "l2 norm of gradients: 0.08186316702402281\n",
      "l2 norm of weights: 3.2186517848376637\n",
      "---------------------\n",
      "Iteration Number: 5965\n",
      "Loss: 12.572494323268074\n",
      "l2 norm of gradients: 0.08183836734203873\n",
      "l2 norm of weights: 3.2186934409258816\n",
      "---------------------\n",
      "Iteration Number: 5966\n",
      "Loss: 12.571629682594995\n",
      "l2 norm of gradients: 0.08181358619594653\n",
      "l2 norm of weights: 3.218735133349557\n",
      "---------------------\n",
      "Iteration Number: 5967\n",
      "Loss: 12.570765623724476\n",
      "l2 norm of gradients: 0.08178882357354678\n",
      "l2 norm of weights: 3.2187768620682236\n",
      "---------------------\n",
      "Iteration Number: 5968\n",
      "Loss: 12.56990214606953\n",
      "l2 norm of gradients: 0.08176407946262675\n",
      "l2 norm of weights: 3.2188186270414487\n",
      "---------------------\n",
      "Iteration Number: 5969\n",
      "Loss: 12.569039249043827\n",
      "l2 norm of gradients: 0.08173935385096039\n",
      "l2 norm of weights: 3.21886042822883\n",
      "---------------------\n",
      "Iteration Number: 5970\n",
      "Loss: 12.568176932061636\n",
      "l2 norm of gradients: 0.08171464672630847\n",
      "l2 norm of weights: 3.218902265589998\n",
      "---------------------\n",
      "Iteration Number: 5971\n",
      "Loss: 12.567315194537862\n",
      "l2 norm of gradients: 0.0816899580764187\n",
      "l2 norm of weights: 3.218944139084616\n",
      "---------------------\n",
      "Iteration Number: 5972\n",
      "Loss: 12.566454035888025\n",
      "l2 norm of gradients: 0.08166528788902569\n",
      "l2 norm of weights: 3.218986048672375\n",
      "---------------------\n",
      "Iteration Number: 5973\n",
      "Loss: 12.565593455528283\n",
      "l2 norm of gradients: 0.08164063615185123\n",
      "l2 norm of weights: 3.2190279943130036\n",
      "---------------------\n",
      "Iteration Number: 5974\n",
      "Loss: 12.5647334528754\n",
      "l2 norm of gradients: 0.08161600285260413\n",
      "l2 norm of weights: 3.2190699759662564\n",
      "---------------------\n",
      "Iteration Number: 5975\n",
      "Loss: 12.563874027346776\n",
      "l2 norm of gradients: 0.0815913879789805\n",
      "l2 norm of weights: 3.2191119935919246\n",
      "---------------------\n",
      "Iteration Number: 5976\n",
      "Loss: 12.563015178360432\n",
      "l2 norm of gradients: 0.08156679151866371\n",
      "l2 norm of weights: 3.2191540471498286\n",
      "---------------------\n",
      "Iteration Number: 5977\n",
      "Loss: 12.562156905334987\n",
      "l2 norm of gradients: 0.08154221345932461\n",
      "l2 norm of weights: 3.21919613659982\n",
      "---------------------\n",
      "Iteration Number: 5978\n",
      "Loss: 12.561299207689746\n",
      "l2 norm of gradients: 0.08151765378862136\n",
      "l2 norm of weights: 3.219238261901785\n",
      "---------------------\n",
      "Iteration Number: 5979\n",
      "Loss: 12.560442084844551\n",
      "l2 norm of gradients: 0.08149311249419985\n",
      "l2 norm of weights: 3.219280423015639\n",
      "---------------------\n",
      "Iteration Number: 5980\n",
      "Loss: 12.559585536219942\n",
      "l2 norm of gradients: 0.08146858956369346\n",
      "l2 norm of weights: 3.2193226199013303\n",
      "---------------------\n",
      "Iteration Number: 5981\n",
      "Loss: 12.558729561237046\n",
      "l2 norm of gradients: 0.08144408498472333\n",
      "l2 norm of weights: 3.219364852518839\n",
      "---------------------\n",
      "Iteration Number: 5982\n",
      "Loss: 12.557874159317592\n",
      "l2 norm of gradients: 0.08141959874489842\n",
      "l2 norm of weights: 3.219407120828177\n",
      "---------------------\n",
      "Iteration Number: 5983\n",
      "Loss: 12.55701932988398\n",
      "l2 norm of gradients: 0.08139513083181554\n",
      "l2 norm of weights: 3.219449424789388\n",
      "---------------------\n",
      "Iteration Number: 5984\n",
      "Loss: 12.556165072359201\n",
      "l2 norm of gradients: 0.08137068123305945\n",
      "l2 norm of weights: 3.219491764362547\n",
      "---------------------\n",
      "Iteration Number: 5985\n",
      "Loss: 12.555311386166862\n",
      "l2 norm of gradients: 0.08134624993620293\n",
      "l2 norm of weights: 3.2195341395077617\n",
      "---------------------\n",
      "Iteration Number: 5986\n",
      "Loss: 12.554458270731198\n",
      "l2 norm of gradients: 0.08132183692880691\n",
      "l2 norm of weights: 3.219576550185171\n",
      "---------------------\n",
      "Iteration Number: 5987\n",
      "Loss: 12.553605725477079\n",
      "l2 norm of gradients: 0.08129744219842046\n",
      "l2 norm of weights: 3.2196189963549458\n",
      "---------------------\n",
      "Iteration Number: 5988\n",
      "Loss: 12.552753749829982\n",
      "l2 norm of gradients: 0.08127306573258099\n",
      "l2 norm of weights: 3.2196614779772883\n",
      "---------------------\n",
      "Iteration Number: 5989\n",
      "Loss: 12.551902343215989\n",
      "l2 norm of gradients: 0.08124870751881424\n",
      "l2 norm of weights: 3.219703995012434\n",
      "---------------------\n",
      "Iteration Number: 5990\n",
      "Loss: 12.551051505061828\n",
      "l2 norm of gradients: 0.08122436754463434\n",
      "l2 norm of weights: 3.2197465474206486\n",
      "---------------------\n",
      "Iteration Number: 5991\n",
      "Loss: 12.550201234794827\n",
      "l2 norm of gradients: 0.08120004579754399\n",
      "l2 norm of weights: 3.219789135162231\n",
      "---------------------\n",
      "Iteration Number: 5992\n",
      "Loss: 12.549351531842962\n",
      "l2 norm of gradients: 0.08117574226503449\n",
      "l2 norm of weights: 3.2198317581975107\n",
      "---------------------\n",
      "Iteration Number: 5993\n",
      "Loss: 12.548502395634792\n",
      "l2 norm of gradients: 0.08115145693458573\n",
      "l2 norm of weights: 3.2198744164868494\n",
      "---------------------\n",
      "Iteration Number: 5994\n",
      "Loss: 12.547653825599495\n",
      "l2 norm of gradients: 0.08112718979366641\n",
      "l2 norm of weights: 3.2199171099906416\n",
      "---------------------\n",
      "Iteration Number: 5995\n",
      "Loss: 12.546805821166917\n",
      "l2 norm of gradients: 0.0811029408297341\n",
      "l2 norm of weights: 3.2199598386693116\n",
      "---------------------\n",
      "Iteration Number: 5996\n",
      "Loss: 12.545958381767457\n",
      "l2 norm of gradients: 0.08107871003023516\n",
      "l2 norm of weights: 3.220002602483318\n",
      "---------------------\n",
      "Iteration Number: 5997\n",
      "Loss: 12.545111506832182\n",
      "l2 norm of gradients: 0.08105449738260505\n",
      "l2 norm of weights: 3.2200454013931497\n",
      "---------------------\n",
      "Iteration Number: 5998\n",
      "Loss: 12.544265195792745\n",
      "l2 norm of gradients: 0.08103030287426827\n",
      "l2 norm of weights: 3.220088235359327\n",
      "---------------------\n",
      "Iteration Number: 5999\n",
      "Loss: 12.543419448081446\n",
      "l2 norm of gradients: 0.08100612649263844\n",
      "l2 norm of weights: 3.2201311043424043\n",
      "---------------------\n",
      "Iteration Number: 6000\n",
      "Loss: 12.542574263131156\n",
      "l2 norm of gradients: 0.08098196822511837\n",
      "l2 norm of weights: 3.2201740083029646\n",
      "---------------------\n",
      "Iteration Number: 6001\n",
      "Loss: 12.541729640375415\n",
      "l2 norm of gradients: 0.08095782805910023\n",
      "l2 norm of weights: 3.220216947201626\n",
      "---------------------\n",
      "Iteration Number: 6002\n",
      "Loss: 12.540885579248332\n",
      "l2 norm of gradients: 0.0809337059819656\n",
      "l2 norm of weights: 3.220259920999036\n",
      "---------------------\n",
      "Iteration Number: 6003\n",
      "Loss: 12.540042079184687\n",
      "l2 norm of gradients: 0.08090960198108543\n",
      "l2 norm of weights: 3.2203029296558747\n",
      "---------------------\n",
      "Iteration Number: 6004\n",
      "Loss: 12.53919913961982\n",
      "l2 norm of gradients: 0.08088551604382026\n",
      "l2 norm of weights: 3.220345973132855\n",
      "---------------------\n",
      "Iteration Number: 6005\n",
      "Loss: 12.538356759989718\n",
      "l2 norm of gradients: 0.08086144815752018\n",
      "l2 norm of weights: 3.22038905139072\n",
      "---------------------\n",
      "Iteration Number: 6006\n",
      "Loss: 12.537514939730986\n",
      "l2 norm of gradients: 0.08083739830952506\n",
      "l2 norm of weights: 3.220432164390246\n",
      "---------------------\n",
      "Iteration Number: 6007\n",
      "Loss: 12.53667367828081\n",
      "l2 norm of gradients: 0.0808133664871645\n",
      "l2 norm of weights: 3.22047531209224\n",
      "---------------------\n",
      "Iteration Number: 6008\n",
      "Loss: 12.535832975077035\n",
      "l2 norm of gradients: 0.08078935267775793\n",
      "l2 norm of weights: 3.2205184944575422\n",
      "---------------------\n",
      "Iteration Number: 6009\n",
      "Loss: 12.534992829558098\n",
      "l2 norm of gradients: 0.0807653568686147\n",
      "l2 norm of weights: 3.220561711447023\n",
      "---------------------\n",
      "Iteration Number: 6010\n",
      "Loss: 12.534153241163043\n",
      "l2 norm of gradients: 0.08074137904703416\n",
      "l2 norm of weights: 3.2206049630215867\n",
      "---------------------\n",
      "Iteration Number: 6011\n",
      "Loss: 12.53331420933156\n",
      "l2 norm of gradients: 0.08071741920030577\n",
      "l2 norm of weights: 3.220648249142167\n",
      "---------------------\n",
      "Iteration Number: 6012\n",
      "Loss: 12.532475733503905\n",
      "l2 norm of gradients: 0.0806934773157091\n",
      "l2 norm of weights: 3.2206915697697314\n",
      "---------------------\n",
      "Iteration Number: 6013\n",
      "Loss: 12.53163781312098\n",
      "l2 norm of gradients: 0.08066955338051399\n",
      "l2 norm of weights: 3.220734924865278\n",
      "---------------------\n",
      "Iteration Number: 6014\n",
      "Loss: 12.530800447624289\n",
      "l2 norm of gradients: 0.08064564738198052\n",
      "l2 norm of weights: 3.2207783143898374\n",
      "---------------------\n",
      "Iteration Number: 6015\n",
      "Loss: 12.529963636455966\n",
      "l2 norm of gradients: 0.08062175930735922\n",
      "l2 norm of weights: 3.220821738304472\n",
      "---------------------\n",
      "Iteration Number: 6016\n",
      "Loss: 12.529127379058727\n",
      "l2 norm of gradients: 0.08059788914389103\n",
      "l2 norm of weights: 3.220865196570277\n",
      "---------------------\n",
      "Iteration Number: 6017\n",
      "Loss: 12.528291674875918\n",
      "l2 norm of gradients: 0.08057403687880749\n",
      "l2 norm of weights: 3.220908689148376\n",
      "---------------------\n",
      "Iteration Number: 6018\n",
      "Loss: 12.527456523351509\n",
      "l2 norm of gradients: 0.0805502024993306\n",
      "l2 norm of weights: 3.220952215999928\n",
      "---------------------\n",
      "Iteration Number: 6019\n",
      "Loss: 12.52662192393005\n",
      "l2 norm of gradients: 0.08052638599267326\n",
      "l2 norm of weights: 3.2209957770861233\n",
      "---------------------\n",
      "Iteration Number: 6020\n",
      "Loss: 12.525787876056725\n",
      "l2 norm of gradients: 0.08050258734603892\n",
      "l2 norm of weights: 3.221039372368182\n",
      "---------------------\n",
      "Iteration Number: 6021\n",
      "Loss: 12.52495437917733\n",
      "l2 norm of gradients: 0.08047880654662203\n",
      "l2 norm of weights: 3.221083001807358\n",
      "---------------------\n",
      "Iteration Number: 6022\n",
      "Loss: 12.524121432738255\n",
      "l2 norm of gradients: 0.08045504358160785\n",
      "l2 norm of weights: 3.2211266653649364\n",
      "---------------------\n",
      "Iteration Number: 6023\n",
      "Loss: 12.523289036186512\n",
      "l2 norm of gradients: 0.08043129843817268\n",
      "l2 norm of weights: 3.2211703630022335\n",
      "---------------------\n",
      "Iteration Number: 6024\n",
      "Loss: 12.522457188969721\n",
      "l2 norm of gradients: 0.08040757110348382\n",
      "l2 norm of weights: 3.221214094680599\n",
      "---------------------\n",
      "Iteration Number: 6025\n",
      "Loss: 12.521625890536106\n",
      "l2 norm of gradients: 0.08038386156469975\n",
      "l2 norm of weights: 3.2212578603614124\n",
      "---------------------\n",
      "Iteration Number: 6026\n",
      "Loss: 12.520795140334519\n",
      "l2 norm of gradients: 0.08036016980897019\n",
      "l2 norm of weights: 3.2213016600060866\n",
      "---------------------\n",
      "Iteration Number: 6027\n",
      "Loss: 12.519964937814384\n",
      "l2 norm of gradients: 0.08033649582343605\n",
      "l2 norm of weights: 3.2213454935760657\n",
      "---------------------\n",
      "Iteration Number: 6028\n",
      "Loss: 12.519135282425779\n",
      "l2 norm of gradients: 0.0803128395952297\n",
      "l2 norm of weights: 3.2213893610328252\n",
      "---------------------\n",
      "Iteration Number: 6029\n",
      "Loss: 12.518306173619369\n",
      "l2 norm of gradients: 0.08028920111147486\n",
      "l2 norm of weights: 3.2214332623378734\n",
      "---------------------\n",
      "Iteration Number: 6030\n",
      "Loss: 12.5174776108464\n",
      "l2 norm of gradients: 0.0802655803592868\n",
      "l2 norm of weights: 3.22147719745275\n",
      "---------------------\n",
      "Iteration Number: 6031\n",
      "Loss: 12.51664959355877\n",
      "l2 norm of gradients: 0.08024197732577242\n",
      "l2 norm of weights: 3.2215211663390257\n",
      "---------------------\n",
      "Iteration Number: 6032\n",
      "Loss: 12.515822121208961\n",
      "l2 norm of gradients: 0.08021839199803014\n",
      "l2 norm of weights: 3.221565168958304\n",
      "---------------------\n",
      "Iteration Number: 6033\n",
      "Loss: 12.514995193250071\n",
      "l2 norm of gradients: 0.08019482436315023\n",
      "l2 norm of weights: 3.2216092052722196\n",
      "---------------------\n",
      "Iteration Number: 6034\n",
      "Loss: 12.514168809135787\n",
      "l2 norm of gradients: 0.08017127440821471\n",
      "l2 norm of weights: 3.22165327524244\n",
      "---------------------\n",
      "Iteration Number: 6035\n",
      "Loss: 12.513342968320424\n",
      "l2 norm of gradients: 0.08014774212029749\n",
      "l2 norm of weights: 3.2216973788306627\n",
      "---------------------\n",
      "Iteration Number: 6036\n",
      "Loss: 12.512517670258896\n",
      "l2 norm of gradients: 0.08012422748646437\n",
      "l2 norm of weights: 3.2217415159986187\n",
      "---------------------\n",
      "Iteration Number: 6037\n",
      "Loss: 12.511692914406712\n",
      "l2 norm of gradients: 0.08010073049377327\n",
      "l2 norm of weights: 3.2217856867080705\n",
      "---------------------\n",
      "Iteration Number: 6038\n",
      "Loss: 12.510868700220001\n",
      "l2 norm of gradients: 0.08007725112927411\n",
      "l2 norm of weights: 3.221829890920811\n",
      "---------------------\n",
      "Iteration Number: 6039\n",
      "Loss: 12.51004502715548\n",
      "l2 norm of gradients: 0.08005378938000901\n",
      "l2 norm of weights: 3.2218741285986665\n",
      "---------------------\n",
      "Iteration Number: 6040\n",
      "Loss: 12.509221894670503\n",
      "l2 norm of gradients: 0.08003034523301239\n",
      "l2 norm of weights: 3.2219183997034944\n",
      "---------------------\n",
      "Iteration Number: 6041\n",
      "Loss: 12.50839930222298\n",
      "l2 norm of gradients: 0.0800069186753109\n",
      "l2 norm of weights: 3.2219627041971837\n",
      "---------------------\n",
      "Iteration Number: 6042\n",
      "Loss: 12.507577249271469\n",
      "l2 norm of gradients: 0.07998350969392352\n",
      "l2 norm of weights: 3.222007042041656\n",
      "---------------------\n",
      "Iteration Number: 6043\n",
      "Loss: 12.506755735275096\n",
      "l2 norm of gradients: 0.07996011827586184\n",
      "l2 norm of weights: 3.2220514131988636\n",
      "---------------------\n",
      "Iteration Number: 6044\n",
      "Loss: 12.505934759693634\n",
      "l2 norm of gradients: 0.07993674440812987\n",
      "l2 norm of weights: 3.2220958176307906\n",
      "---------------------\n",
      "Iteration Number: 6045\n",
      "Loss: 12.505114321987396\n",
      "l2 norm of gradients: 0.07991338807772425\n",
      "l2 norm of weights: 3.222140255299454\n",
      "---------------------\n",
      "Iteration Number: 6046\n",
      "Loss: 12.50429442161737\n",
      "l2 norm of gradients: 0.07989004927163423\n",
      "l2 norm of weights: 3.222184726166902\n",
      "---------------------\n",
      "Iteration Number: 6047\n",
      "Loss: 12.503475058045085\n",
      "l2 norm of gradients: 0.07986672797684191\n",
      "l2 norm of weights: 3.2222292301952136\n",
      "---------------------\n",
      "Iteration Number: 6048\n",
      "Loss: 12.502656230732697\n",
      "l2 norm of gradients: 0.0798434241803221\n",
      "l2 norm of weights: 3.222273767346501\n",
      "---------------------\n",
      "Iteration Number: 6049\n",
      "Loss: 12.501837939142973\n",
      "l2 norm of gradients: 0.07982013786904253\n",
      "l2 norm of weights: 3.2223183375829074\n",
      "---------------------\n",
      "Iteration Number: 6050\n",
      "Loss: 12.501020182739257\n",
      "l2 norm of gradients: 0.07979686902996391\n",
      "l2 norm of weights: 3.2223629408666072\n",
      "---------------------\n",
      "Iteration Number: 6051\n",
      "Loss: 12.50020296098553\n",
      "l2 norm of gradients: 0.07977361765003994\n",
      "l2 norm of weights: 3.2224075771598084\n",
      "---------------------\n",
      "Iteration Number: 6052\n",
      "Loss: 12.499386273346325\n",
      "l2 norm of gradients: 0.07975038371621737\n",
      "l2 norm of weights: 3.222452246424748\n",
      "---------------------\n",
      "Iteration Number: 6053\n",
      "Loss: 12.498570119286802\n",
      "l2 norm of gradients: 0.07972716721543621\n",
      "l2 norm of weights: 3.2224969486236974\n",
      "---------------------\n",
      "Iteration Number: 6054\n",
      "Loss: 12.497754498272748\n",
      "l2 norm of gradients: 0.07970396813462964\n",
      "l2 norm of weights: 3.222541683718958\n",
      "---------------------\n",
      "Iteration Number: 6055\n",
      "Loss: 12.496939409770482\n",
      "l2 norm of gradients: 0.0796807864607242\n",
      "l2 norm of weights: 3.222586451672863\n",
      "---------------------\n",
      "Iteration Number: 6056\n",
      "Loss: 12.49612485324698\n",
      "l2 norm of gradients: 0.07965762218063971\n",
      "l2 norm of weights: 3.222631252447779\n",
      "---------------------\n",
      "Iteration Number: 6057\n",
      "Loss: 12.495310828169796\n",
      "l2 norm of gradients: 0.07963447528128954\n",
      "l2 norm of weights: 3.222676086006102\n",
      "---------------------\n",
      "Iteration Number: 6058\n",
      "Loss: 12.494497334007077\n",
      "l2 norm of gradients: 0.07961134574958048\n",
      "l2 norm of weights: 3.222720952310261\n",
      "---------------------\n",
      "Iteration Number: 6059\n",
      "Loss: 12.493684370227573\n",
      "l2 norm of gradients: 0.07958823357241299\n",
      "l2 norm of weights: 3.222765851322717\n",
      "---------------------\n",
      "Iteration Number: 6060\n",
      "Loss: 12.492871936300636\n",
      "l2 norm of gradients: 0.07956513873668108\n",
      "l2 norm of weights: 3.222810783005962\n",
      "---------------------\n",
      "Iteration Number: 6061\n",
      "Loss: 12.4920600316962\n",
      "l2 norm of gradients: 0.07954206122927258\n",
      "l2 norm of weights: 3.222855747322519\n",
      "---------------------\n",
      "Iteration Number: 6062\n",
      "Loss: 12.491248655884819\n",
      "l2 norm of gradients: 0.07951900103706906\n",
      "l2 norm of weights: 3.2229007442349444\n",
      "---------------------\n",
      "Iteration Number: 6063\n",
      "Loss: 12.49043780833762\n",
      "l2 norm of gradients: 0.07949595814694592\n",
      "l2 norm of weights: 3.222945773705825\n",
      "---------------------\n",
      "Iteration Number: 6064\n",
      "Loss: 12.489627488526335\n",
      "l2 norm of gradients: 0.07947293254577255\n",
      "l2 norm of weights: 3.22299083569778\n",
      "---------------------\n",
      "Iteration Number: 6065\n",
      "Loss: 12.488817695923288\n",
      "l2 norm of gradients: 0.07944992422041229\n",
      "l2 norm of weights: 3.22303593017346\n",
      "---------------------\n",
      "Iteration Number: 6066\n",
      "Loss: 12.488008430001422\n",
      "l2 norm of gradients: 0.07942693315772256\n",
      "l2 norm of weights: 3.2230810570955466\n",
      "---------------------\n",
      "Iteration Number: 6067\n",
      "Loss: 12.48719969023423\n",
      "l2 norm of gradients: 0.07940395934455494\n",
      "l2 norm of weights: 3.223126216426754\n",
      "---------------------\n",
      "Iteration Number: 6068\n",
      "Loss: 12.486391476095847\n",
      "l2 norm of gradients: 0.07938100276775513\n",
      "l2 norm of weights: 3.223171408129828\n",
      "---------------------\n",
      "Iteration Number: 6069\n",
      "Loss: 12.48558378706095\n",
      "l2 norm of gradients: 0.07935806341416317\n",
      "l2 norm of weights: 3.2232166321675457\n",
      "---------------------\n",
      "Iteration Number: 6070\n",
      "Loss: 12.484776622604853\n",
      "l2 norm of gradients: 0.07933514127061338\n",
      "l2 norm of weights: 3.223261888502715\n",
      "---------------------\n",
      "Iteration Number: 6071\n",
      "Loss: 12.483969982203472\n",
      "l2 norm of gradients: 0.07931223632393454\n",
      "l2 norm of weights: 3.2233071770981776\n",
      "---------------------\n",
      "Iteration Number: 6072\n",
      "Loss: 12.483163865333257\n",
      "l2 norm of gradients: 0.07928934856094985\n",
      "l2 norm of weights: 3.2233524979168044\n",
      "---------------------\n",
      "Iteration Number: 6073\n",
      "Loss: 12.482358271471298\n",
      "l2 norm of gradients: 0.07926647796847708\n",
      "l2 norm of weights: 3.2233978509214998\n",
      "---------------------\n",
      "Iteration Number: 6074\n",
      "Loss: 12.481553200095261\n",
      "l2 norm of gradients: 0.07924362453332852\n",
      "l2 norm of weights: 3.223443236075199\n",
      "---------------------\n",
      "Iteration Number: 6075\n",
      "Loss: 12.48074865068342\n",
      "l2 norm of gradients: 0.07922078824231127\n",
      "l2 norm of weights: 3.223488653340869\n",
      "---------------------\n",
      "Iteration Number: 6076\n",
      "Loss: 12.479944622714605\n",
      "l2 norm of gradients: 0.07919796908222702\n",
      "l2 norm of weights: 3.2235341026815076\n",
      "---------------------\n",
      "Iteration Number: 6077\n",
      "Loss: 12.479141115668284\n",
      "l2 norm of gradients: 0.07917516703987236\n",
      "l2 norm of weights: 3.2235795840601456\n",
      "---------------------\n",
      "Iteration Number: 6078\n",
      "Loss: 12.478338129024465\n",
      "l2 norm of gradients: 0.07915238210203873\n",
      "l2 norm of weights: 3.2236250974398444\n",
      "---------------------\n",
      "Iteration Number: 6079\n",
      "Loss: 12.477535662263787\n",
      "l2 norm of gradients: 0.07912961425551247\n",
      "l2 norm of weights: 3.2236706427836967\n",
      "---------------------\n",
      "Iteration Number: 6080\n",
      "Loss: 12.476733714867459\n",
      "l2 norm of gradients: 0.07910686348707494\n",
      "l2 norm of weights: 3.2237162200548286\n",
      "---------------------\n",
      "Iteration Number: 6081\n",
      "Loss: 12.475932286317297\n",
      "l2 norm of gradients: 0.07908412978350253\n",
      "l2 norm of weights: 3.223761829216395\n",
      "---------------------\n",
      "Iteration Number: 6082\n",
      "Loss: 12.475131376095668\n",
      "l2 norm of gradients: 0.07906141313156684\n",
      "l2 norm of weights: 3.2238074702315846\n",
      "---------------------\n",
      "Iteration Number: 6083\n",
      "Loss: 12.47433098368556\n",
      "l2 norm of gradients: 0.07903871351803457\n",
      "l2 norm of weights: 3.223853143063617\n",
      "---------------------\n",
      "Iteration Number: 6084\n",
      "Loss: 12.473531108570535\n",
      "l2 norm of gradients: 0.07901603092966773\n",
      "l2 norm of weights: 3.223898847675744\n",
      "---------------------\n",
      "Iteration Number: 6085\n",
      "Loss: 12.472731750234757\n",
      "l2 norm of gradients: 0.07899336535322371\n",
      "l2 norm of weights: 3.223944584031246\n",
      "---------------------\n",
      "Iteration Number: 6086\n",
      "Loss: 12.471932908162968\n",
      "l2 norm of gradients: 0.07897071677545518\n",
      "l2 norm of weights: 3.223990352093439\n",
      "---------------------\n",
      "Iteration Number: 6087\n",
      "Loss: 12.471134581840486\n",
      "l2 norm of gradients: 0.07894808518311033\n",
      "l2 norm of weights: 3.224036151825668\n",
      "---------------------\n",
      "Iteration Number: 6088\n",
      "Loss: 12.470336770753239\n",
      "l2 norm of gradients: 0.07892547056293284\n",
      "l2 norm of weights: 3.22408198319131\n",
      "---------------------\n",
      "Iteration Number: 6089\n",
      "Loss: 12.469539474387707\n",
      "l2 norm of gradients: 0.07890287290166202\n",
      "l2 norm of weights: 3.224127846153774\n",
      "---------------------\n",
      "Iteration Number: 6090\n",
      "Loss: 12.468742692231004\n",
      "l2 norm of gradients: 0.07888029218603278\n",
      "l2 norm of weights: 3.2241737406765\n",
      "---------------------\n",
      "Iteration Number: 6091\n",
      "Loss: 12.467946423770778\n",
      "l2 norm of gradients: 0.07885772840277576\n",
      "l2 norm of weights: 3.22421966672296\n",
      "---------------------\n",
      "Iteration Number: 6092\n",
      "Loss: 12.4671506684953\n",
      "l2 norm of gradients: 0.07883518153861735\n",
      "l2 norm of weights: 3.2242656242566565\n",
      "---------------------\n",
      "Iteration Number: 6093\n",
      "Loss: 12.466355425893402\n",
      "l2 norm of gradients: 0.07881265158027985\n",
      "l2 norm of weights: 3.2243116132411247\n",
      "---------------------\n",
      "Iteration Number: 6094\n",
      "Loss: 12.465560695454508\n",
      "l2 norm of gradients: 0.07879013851448137\n",
      "l2 norm of weights: 3.22435763363993\n",
      "---------------------\n",
      "Iteration Number: 6095\n",
      "Loss: 12.464766476668636\n",
      "l2 norm of gradients: 0.07876764232793604\n",
      "l2 norm of weights: 3.224403685416671\n",
      "---------------------\n",
      "Iteration Number: 6096\n",
      "Loss: 12.463972769026373\n",
      "l2 norm of gradients: 0.078745163007354\n",
      "l2 norm of weights: 3.224449768534976\n",
      "---------------------\n",
      "Iteration Number: 6097\n",
      "Loss: 12.463179572018873\n",
      "l2 norm of gradients: 0.07872270053944147\n",
      "l2 norm of weights: 3.2244958829585055\n",
      "---------------------\n",
      "Iteration Number: 6098\n",
      "Loss: 12.462386885137912\n",
      "l2 norm of gradients: 0.0787002549109009\n",
      "l2 norm of weights: 3.2245420286509514\n",
      "---------------------\n",
      "Iteration Number: 6099\n",
      "Loss: 12.461594707875825\n",
      "l2 norm of gradients: 0.07867782610843084\n",
      "l2 norm of weights: 3.224588205576038\n",
      "---------------------\n",
      "Iteration Number: 6100\n",
      "Loss: 12.46080303972553\n",
      "l2 norm of gradients: 0.07865541411872619\n",
      "l2 norm of weights: 3.2246344136975185\n",
      "---------------------\n",
      "Iteration Number: 6101\n",
      "Loss: 12.46001188018052\n",
      "l2 norm of gradients: 0.07863301892847817\n",
      "l2 norm of weights: 3.2246806529791803\n",
      "---------------------\n",
      "Iteration Number: 6102\n",
      "Loss: 12.459221228734865\n",
      "l2 norm of gradients: 0.07861064052437443\n",
      "l2 norm of weights: 3.22472692338484\n",
      "---------------------\n",
      "Iteration Number: 6103\n",
      "Loss: 12.458431084883255\n",
      "l2 norm of gradients: 0.07858827889309904\n",
      "l2 norm of weights: 3.224773224878348\n",
      "---------------------\n",
      "Iteration Number: 6104\n",
      "Loss: 12.457641448120915\n",
      "l2 norm of gradients: 0.07856593402133258\n",
      "l2 norm of weights: 3.2248195574235834\n",
      "---------------------\n",
      "Iteration Number: 6105\n",
      "Loss: 12.456852317943655\n",
      "l2 norm of gradients: 0.07854360589575235\n",
      "l2 norm of weights: 3.224865920984459\n",
      "---------------------\n",
      "Iteration Number: 6106\n",
      "Loss: 12.456063693847886\n",
      "l2 norm of gradients: 0.07852129450303207\n",
      "l2 norm of weights: 3.2249123155249166\n",
      "---------------------\n",
      "Iteration Number: 6107\n",
      "Loss: 12.455275575330571\n",
      "l2 norm of gradients: 0.07849899982984242\n",
      "l2 norm of weights: 3.224958741008931\n",
      "---------------------\n",
      "Iteration Number: 6108\n",
      "Loss: 12.45448796188928\n",
      "l2 norm of gradients: 0.07847672186285067\n",
      "l2 norm of weights: 3.2250051974005096\n",
      "---------------------\n",
      "Iteration Number: 6109\n",
      "Loss: 12.453700853022127\n",
      "l2 norm of gradients: 0.078454460588721\n",
      "l2 norm of weights: 3.225051684663688\n",
      "---------------------\n",
      "Iteration Number: 6110\n",
      "Loss: 12.452914248227838\n",
      "l2 norm of gradients: 0.07843221599411444\n",
      "l2 norm of weights: 3.2250982027625352\n",
      "---------------------\n",
      "Iteration Number: 6111\n",
      "Loss: 12.452128147005686\n",
      "l2 norm of gradients: 0.07840998806568908\n",
      "l2 norm of weights: 3.225144751661151\n",
      "---------------------\n",
      "Iteration Number: 6112\n",
      "Loss: 12.451342548855534\n",
      "l2 norm of gradients: 0.07838777679009984\n",
      "l2 norm of weights: 3.2251913313236673\n",
      "---------------------\n",
      "Iteration Number: 6113\n",
      "Loss: 12.450557453277824\n",
      "l2 norm of gradients: 0.07836558215399886\n",
      "l2 norm of weights: 3.225237941714245\n",
      "---------------------\n",
      "Iteration Number: 6114\n",
      "Loss: 12.449772859773573\n",
      "l2 norm of gradients: 0.07834340414403534\n",
      "l2 norm of weights: 3.2252845827970797\n",
      "---------------------\n",
      "Iteration Number: 6115\n",
      "Loss: 12.448988767844348\n",
      "l2 norm of gradients: 0.07832124274685572\n",
      "l2 norm of weights: 3.2253312545363952\n",
      "---------------------\n",
      "Iteration Number: 6116\n",
      "Loss: 12.44820517699234\n",
      "l2 norm of gradients: 0.07829909794910367\n",
      "l2 norm of weights: 3.2253779568964487\n",
      "---------------------\n",
      "Iteration Number: 6117\n",
      "Loss: 12.447422086720264\n",
      "l2 norm of gradients: 0.07827696973742014\n",
      "l2 norm of weights: 3.2254246898415277\n",
      "---------------------\n",
      "Iteration Number: 6118\n",
      "Loss: 12.44663949653143\n",
      "l2 norm of gradients: 0.07825485809844351\n",
      "l2 norm of weights: 3.2254714533359503\n",
      "---------------------\n",
      "Iteration Number: 6119\n",
      "Loss: 12.445857405929736\n",
      "l2 norm of gradients: 0.07823276301880953\n",
      "l2 norm of weights: 3.225518247344067\n",
      "---------------------\n",
      "Iteration Number: 6120\n",
      "Loss: 12.445075814419619\n",
      "l2 norm of gradients: 0.07821068448515149\n",
      "l2 norm of weights: 3.2255650718302604\n",
      "---------------------\n",
      "Iteration Number: 6121\n",
      "Loss: 12.444294721506129\n",
      "l2 norm of gradients: 0.07818862248410022\n",
      "l2 norm of weights: 3.2256119267589414\n",
      "---------------------\n",
      "Iteration Number: 6122\n",
      "Loss: 12.443514126694833\n",
      "l2 norm of gradients: 0.07816657700228406\n",
      "l2 norm of weights: 3.2256588120945553\n",
      "---------------------\n",
      "Iteration Number: 6123\n",
      "Loss: 12.442734029491936\n",
      "l2 norm of gradients: 0.07814454802632916\n",
      "l2 norm of weights: 3.225705727801576\n",
      "---------------------\n",
      "Iteration Number: 6124\n",
      "Loss: 12.441954429404163\n",
      "l2 norm of gradients: 0.07812253554285932\n",
      "l2 norm of weights: 3.2257526738445104\n",
      "---------------------\n",
      "Iteration Number: 6125\n",
      "Loss: 12.441175325938834\n",
      "l2 norm of gradients: 0.07810053953849608\n",
      "l2 norm of weights: 3.225799650187896\n",
      "---------------------\n",
      "Iteration Number: 6126\n",
      "Loss: 12.440396718603823\n",
      "l2 norm of gradients: 0.07807855999985885\n",
      "l2 norm of weights: 3.225846656796301\n",
      "---------------------\n",
      "Iteration Number: 6127\n",
      "Loss: 12.439618606907578\n",
      "l2 norm of gradients: 0.07805659691356504\n",
      "l2 norm of weights: 3.2258936936343257\n",
      "---------------------\n",
      "Iteration Number: 6128\n",
      "Loss: 12.438840990359136\n",
      "l2 norm of gradients: 0.07803465026622983\n",
      "l2 norm of weights: 3.225940760666601\n",
      "---------------------\n",
      "Iteration Number: 6129\n",
      "Loss: 12.43806386846807\n",
      "l2 norm of gradients: 0.07801272004446648\n",
      "l2 norm of weights: 3.2259878578577887\n",
      "---------------------\n",
      "Iteration Number: 6130\n",
      "Loss: 12.437287240744556\n",
      "l2 norm of gradients: 0.07799080623488637\n",
      "l2 norm of weights: 3.2260349851725825\n",
      "---------------------\n",
      "Iteration Number: 6131\n",
      "Loss: 12.436511106699305\n",
      "l2 norm of gradients: 0.07796890882409895\n",
      "l2 norm of weights: 3.2260821425757067\n",
      "---------------------\n",
      "Iteration Number: 6132\n",
      "Loss: 12.43573546584362\n",
      "l2 norm of gradients: 0.07794702779871192\n",
      "l2 norm of weights: 3.226129330031917\n",
      "---------------------\n",
      "Iteration Number: 6133\n",
      "Loss: 12.434960317689363\n",
      "l2 norm of gradients: 0.07792516314533106\n",
      "l2 norm of weights: 3.2261765475059994\n",
      "---------------------\n",
      "Iteration Number: 6134\n",
      "Loss: 12.434185661748947\n",
      "l2 norm of gradients: 0.07790331485056057\n",
      "l2 norm of weights: 3.226223794962773\n",
      "---------------------\n",
      "Iteration Number: 6135\n",
      "Loss: 12.433411497535365\n",
      "l2 norm of gradients: 0.07788148290100302\n",
      "l2 norm of weights: 3.226271072367085\n",
      "---------------------\n",
      "Iteration Number: 6136\n",
      "Loss: 12.432637824562192\n",
      "l2 norm of gradients: 0.07785966728325931\n",
      "l2 norm of weights: 3.226318379683816\n",
      "---------------------\n",
      "Iteration Number: 6137\n",
      "Loss: 12.431864642343548\n",
      "l2 norm of gradients: 0.0778378679839288\n",
      "l2 norm of weights: 3.2263657168778774\n",
      "---------------------\n",
      "Iteration Number: 6138\n",
      "Loss: 12.43109195039409\n",
      "l2 norm of gradients: 0.07781608498960942\n",
      "l2 norm of weights: 3.226413083914211\n",
      "---------------------\n",
      "Iteration Number: 6139\n",
      "Loss: 12.430319748229117\n",
      "l2 norm of gradients: 0.0777943182868977\n",
      "l2 norm of weights: 3.22646048075779\n",
      "---------------------\n",
      "Iteration Number: 6140\n",
      "Loss: 12.429548035364414\n",
      "l2 norm of gradients: 0.07777256786238863\n",
      "l2 norm of weights: 3.2265079073736183\n",
      "---------------------\n",
      "Iteration Number: 6141\n",
      "Loss: 12.428776811316366\n",
      "l2 norm of gradients: 0.07775083370267605\n",
      "l2 norm of weights: 3.226555363726731\n",
      "---------------------\n",
      "Iteration Number: 6142\n",
      "Loss: 12.428006075601914\n",
      "l2 norm of gradients: 0.0777291157943525\n",
      "l2 norm of weights: 3.226602849782194\n",
      "---------------------\n",
      "Iteration Number: 6143\n",
      "Loss: 12.427235827738574\n",
      "l2 norm of gradients: 0.07770741412400926\n",
      "l2 norm of weights: 3.226650365505105\n",
      "---------------------\n",
      "Iteration Number: 6144\n",
      "Loss: 12.42646606724439\n",
      "l2 norm of gradients: 0.07768572867823652\n",
      "l2 norm of weights: 3.2266979108605924\n",
      "---------------------\n",
      "Iteration Number: 6145\n",
      "Loss: 12.425696793638005\n",
      "l2 norm of gradients: 0.07766405944362333\n",
      "l2 norm of weights: 3.2267454858138147\n",
      "---------------------\n",
      "Iteration Number: 6146\n",
      "Loss: 12.424928006438615\n",
      "l2 norm of gradients: 0.07764240640675772\n",
      "l2 norm of weights: 3.226793090329962\n",
      "---------------------\n",
      "Iteration Number: 6147\n",
      "Loss: 12.42415970516594\n",
      "l2 norm of gradients: 0.07762076955422668\n",
      "l2 norm of weights: 3.226840724374256\n",
      "---------------------\n",
      "Iteration Number: 6148\n",
      "Loss: 12.423391889340312\n",
      "l2 norm of gradients: 0.07759914887261636\n",
      "l2 norm of weights: 3.226888387911948\n",
      "---------------------\n",
      "Iteration Number: 6149\n",
      "Loss: 12.42262455848259\n",
      "l2 norm of gradients: 0.07757754434851191\n",
      "l2 norm of weights: 3.226936080908321\n",
      "---------------------\n",
      "Iteration Number: 6150\n",
      "Loss: 12.421857712114216\n",
      "l2 norm of gradients: 0.07755595596849774\n",
      "l2 norm of weights: 3.226983803328689\n",
      "---------------------\n",
      "Iteration Number: 6151\n",
      "Loss: 12.421091349757148\n",
      "l2 norm of gradients: 0.07753438371915743\n",
      "l2 norm of weights: 3.2270315551383972\n",
      "---------------------\n",
      "Iteration Number: 6152\n",
      "Loss: 12.420325470933953\n",
      "l2 norm of gradients: 0.0775128275870739\n",
      "l2 norm of weights: 3.227079336302821\n",
      "---------------------\n",
      "Iteration Number: 6153\n",
      "Loss: 12.419560075167725\n",
      "l2 norm of gradients: 0.07749128755882931\n",
      "l2 norm of weights: 3.2271271467873666\n",
      "---------------------\n",
      "Iteration Number: 6154\n",
      "Loss: 12.418795161982121\n",
      "l2 norm of gradients: 0.07746976362100529\n",
      "l2 norm of weights: 3.2271749865574715\n",
      "---------------------\n",
      "Iteration Number: 6155\n",
      "Loss: 12.418030730901354\n",
      "l2 norm of gradients: 0.07744825576018287\n",
      "l2 norm of weights: 3.227222855578604\n",
      "---------------------\n",
      "Iteration Number: 6156\n",
      "Loss: 12.417266781450204\n",
      "l2 norm of gradients: 0.07742676396294251\n",
      "l2 norm of weights: 3.227270753816264\n",
      "---------------------\n",
      "Iteration Number: 6157\n",
      "Loss: 12.41650331315399\n",
      "l2 norm of gradients: 0.07740528821586433\n",
      "l2 norm of weights: 3.2273186812359804\n",
      "---------------------\n",
      "Iteration Number: 6158\n",
      "Loss: 12.4157403255386\n",
      "l2 norm of gradients: 0.07738382850552797\n",
      "l2 norm of weights: 3.2273666378033146\n",
      "---------------------\n",
      "Iteration Number: 6159\n",
      "Loss: 12.414977818130481\n",
      "l2 norm of gradients: 0.07736238481851268\n",
      "l2 norm of weights: 3.2274146234838574\n",
      "---------------------\n",
      "Iteration Number: 6160\n",
      "Loss: 12.414215790456604\n",
      "l2 norm of gradients: 0.07734095714139752\n",
      "l2 norm of weights: 3.227462638243232\n",
      "---------------------\n",
      "Iteration Number: 6161\n",
      "Loss: 12.413454242044526\n",
      "l2 norm of gradients: 0.07731954546076117\n",
      "l2 norm of weights: 3.227510682047092\n",
      "---------------------\n",
      "Iteration Number: 6162\n",
      "Loss: 12.412693172422355\n",
      "l2 norm of gradients: 0.07729814976318218\n",
      "l2 norm of weights: 3.22755875486112\n",
      "---------------------\n",
      "Iteration Number: 6163\n",
      "Loss: 12.411932581118721\n",
      "l2 norm of gradients: 0.07727677003523896\n",
      "l2 norm of weights: 3.2276068566510316\n",
      "---------------------\n",
      "Iteration Number: 6164\n",
      "Loss: 12.411172467662844\n",
      "l2 norm of gradients: 0.07725540626350982\n",
      "l2 norm of weights: 3.2276549873825724\n",
      "---------------------\n",
      "Iteration Number: 6165\n",
      "Loss: 12.410412831584477\n",
      "l2 norm of gradients: 0.07723405843457293\n",
      "l2 norm of weights: 3.2277031470215185\n",
      "---------------------\n",
      "Iteration Number: 6166\n",
      "Loss: 12.409653672413945\n",
      "l2 norm of gradients: 0.0772127265350066\n",
      "l2 norm of weights: 3.2277513355336764\n",
      "---------------------\n",
      "Iteration Number: 6167\n",
      "Loss: 12.408894989682079\n",
      "l2 norm of gradients: 0.07719141055138912\n",
      "l2 norm of weights: 3.2277995528848837\n",
      "---------------------\n",
      "Iteration Number: 6168\n",
      "Loss: 12.408136782920291\n",
      "l2 norm of gradients: 0.07717011047029888\n",
      "l2 norm of weights: 3.22784779904101\n",
      "---------------------\n",
      "Iteration Number: 6169\n",
      "Loss: 12.407379051660568\n",
      "l2 norm of gradients: 0.07714882627831447\n",
      "l2 norm of weights: 3.2278960739679525\n",
      "---------------------\n",
      "Iteration Number: 6170\n",
      "Loss: 12.406621795435399\n",
      "l2 norm of gradients: 0.07712755796201465\n",
      "l2 norm of weights: 3.2279443776316423\n",
      "---------------------\n",
      "Iteration Number: 6171\n",
      "Loss: 12.405865013777849\n",
      "l2 norm of gradients: 0.07710630550797841\n",
      "l2 norm of weights: 3.2279927099980394\n",
      "---------------------\n",
      "Iteration Number: 6172\n",
      "Loss: 12.405108706221506\n",
      "l2 norm of gradients: 0.0770850689027851\n",
      "l2 norm of weights: 3.2280410710331346\n",
      "---------------------\n",
      "Iteration Number: 6173\n",
      "Loss: 12.404352872300555\n",
      "l2 norm of gradients: 0.07706384813301438\n",
      "l2 norm of weights: 3.2280894607029493\n",
      "---------------------\n",
      "Iteration Number: 6174\n",
      "Loss: 12.403597511549682\n",
      "l2 norm of gradients: 0.0770426431852464\n",
      "l2 norm of weights: 3.2281378789735364\n",
      "---------------------\n",
      "Iteration Number: 6175\n",
      "Loss: 12.40284262350413\n",
      "l2 norm of gradients: 0.07702145404606162\n",
      "l2 norm of weights: 3.2281863258109786\n",
      "---------------------\n",
      "Iteration Number: 6176\n",
      "Loss: 12.402088207699716\n",
      "l2 norm of gradients: 0.07700028070204114\n",
      "l2 norm of weights: 3.228234801181389\n",
      "---------------------\n",
      "Iteration Number: 6177\n",
      "Loss: 12.401334263672771\n",
      "l2 norm of gradients: 0.07697912313976649\n",
      "l2 norm of weights: 3.2282833050509123\n",
      "---------------------\n",
      "Iteration Number: 6178\n",
      "Loss: 12.400580790960188\n",
      "l2 norm of gradients: 0.07695798134581988\n",
      "l2 norm of weights: 3.2283318373857224\n",
      "---------------------\n",
      "Iteration Number: 6179\n",
      "Loss: 12.399827789099405\n",
      "l2 norm of gradients: 0.07693685530678417\n",
      "l2 norm of weights: 3.2283803981520247\n",
      "---------------------\n",
      "Iteration Number: 6180\n",
      "Loss: 12.399075257628377\n",
      "l2 norm of gradients: 0.07691574500924284\n",
      "l2 norm of weights: 3.228428987316055\n",
      "---------------------\n",
      "Iteration Number: 6181\n",
      "Loss: 12.39832319608566\n",
      "l2 norm of gradients: 0.07689465043978021\n",
      "l2 norm of weights: 3.22847760484408\n",
      "---------------------\n",
      "Iteration Number: 6182\n",
      "Loss: 12.397571604010285\n",
      "l2 norm of gradients: 0.07687357158498137\n",
      "l2 norm of weights: 3.2285262507023957\n",
      "---------------------\n",
      "Iteration Number: 6183\n",
      "Loss: 12.396820480941903\n",
      "l2 norm of gradients: 0.07685250843143214\n",
      "l2 norm of weights: 3.22857492485733\n",
      "---------------------\n",
      "Iteration Number: 6184\n",
      "Loss: 12.396069826420632\n",
      "l2 norm of gradients: 0.07683146096571944\n",
      "l2 norm of weights: 3.22862362727524\n",
      "---------------------\n",
      "Iteration Number: 6185\n",
      "Loss: 12.395319639987182\n",
      "l2 norm of gradients: 0.0768104291744309\n",
      "l2 norm of weights: 3.228672357922514\n",
      "---------------------\n",
      "Iteration Number: 6186\n",
      "Loss: 12.394569921182793\n",
      "l2 norm of gradients: 0.07678941304415522\n",
      "l2 norm of weights: 3.228721116765571\n",
      "---------------------\n",
      "Iteration Number: 6187\n",
      "Loss: 12.39382066954924\n",
      "l2 norm of gradients: 0.07676841256148227\n",
      "l2 norm of weights: 3.22876990377086\n",
      "---------------------\n",
      "Iteration Number: 6188\n",
      "Loss: 12.393071884628839\n",
      "l2 norm of gradients: 0.07674742771300276\n",
      "l2 norm of weights: 3.228818718904861\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 6189\n",
      "Loss: 12.392323565964448\n",
      "l2 norm of gradients: 0.07672645848530868\n",
      "l2 norm of weights: 3.228867562134083\n",
      "---------------------\n",
      "Iteration Number: 6190\n",
      "Loss: 12.39157571309947\n",
      "l2 norm of gradients: 0.07670550486499314\n",
      "l2 norm of weights: 3.228916433425067\n",
      "---------------------\n",
      "Iteration Number: 6191\n",
      "Loss: 12.390828325577846\n",
      "l2 norm of gradients: 0.07668456683865045\n",
      "l2 norm of weights: 3.2289653327443837\n",
      "---------------------\n",
      "Iteration Number: 6192\n",
      "Loss: 12.390081402944054\n",
      "l2 norm of gradients: 0.07666364439287623\n",
      "l2 norm of weights: 3.2290142600586336\n",
      "---------------------\n",
      "Iteration Number: 6193\n",
      "Loss: 12.389334944743094\n",
      "l2 norm of gradients: 0.07664273751426737\n",
      "l2 norm of weights: 3.229063215334449\n",
      "---------------------\n",
      "Iteration Number: 6194\n",
      "Loss: 12.388588950520543\n",
      "l2 norm of gradients: 0.07662184618942215\n",
      "l2 norm of weights: 3.229112198538491\n",
      "---------------------\n",
      "Iteration Number: 6195\n",
      "Loss: 12.387843419822483\n",
      "l2 norm of gradients: 0.07660097040494022\n",
      "l2 norm of weights: 3.2291612096374527\n",
      "---------------------\n",
      "Iteration Number: 6196\n",
      "Loss: 12.38709835219554\n",
      "l2 norm of gradients: 0.07658011014742266\n",
      "l2 norm of weights: 3.2292102485980556\n",
      "---------------------\n",
      "Iteration Number: 6197\n",
      "Loss: 12.38635374718688\n",
      "l2 norm of gradients: 0.07655926540347209\n",
      "l2 norm of weights: 3.2292593153870537\n",
      "---------------------\n",
      "Iteration Number: 6198\n",
      "Loss: 12.385609604344198\n",
      "l2 norm of gradients: 0.07653843615969264\n",
      "l2 norm of weights: 3.2293084099712286\n",
      "---------------------\n",
      "Iteration Number: 6199\n",
      "Loss: 12.384865923215738\n",
      "l2 norm of gradients: 0.07651762240269003\n",
      "l2 norm of weights: 3.2293575323173944\n",
      "---------------------\n",
      "Iteration Number: 6200\n",
      "Loss: 12.384122703350267\n",
      "l2 norm of gradients: 0.0764968241190716\n",
      "l2 norm of weights: 3.2294066823923946\n",
      "---------------------\n",
      "Iteration Number: 6201\n",
      "Loss: 12.383379944297088\n",
      "l2 norm of gradients: 0.0764760412954464\n",
      "l2 norm of weights: 3.2294558601631027\n",
      "---------------------\n",
      "Iteration Number: 6202\n",
      "Loss: 12.382637645606039\n",
      "l2 norm of gradients: 0.07645527391842509\n",
      "l2 norm of weights: 3.2295050655964235\n",
      "---------------------\n",
      "Iteration Number: 6203\n",
      "Loss: 12.381895806827506\n",
      "l2 norm of gradients: 0.07643452197462025\n",
      "l2 norm of weights: 3.229554298659291\n",
      "---------------------\n",
      "Iteration Number: 6204\n",
      "Loss: 12.38115442751239\n",
      "l2 norm of gradients: 0.07641378545064612\n",
      "l2 norm of weights: 3.2296035593186687\n",
      "---------------------\n",
      "Iteration Number: 6205\n",
      "Loss: 12.380413507212113\n",
      "l2 norm of gradients: 0.07639306433311888\n",
      "l2 norm of weights: 3.2296528475415527\n",
      "---------------------\n",
      "Iteration Number: 6206\n",
      "Loss: 12.379673045478658\n",
      "l2 norm of gradients: 0.07637235860865654\n",
      "l2 norm of weights: 3.229702163294968\n",
      "---------------------\n",
      "Iteration Number: 6207\n",
      "Loss: 12.378933041864524\n",
      "l2 norm of gradients: 0.07635166826387912\n",
      "l2 norm of weights: 3.229751506545968\n",
      "---------------------\n",
      "Iteration Number: 6208\n",
      "Loss: 12.378193495922739\n",
      "l2 norm of gradients: 0.07633099328540856\n",
      "l2 norm of weights: 3.2298008772616384\n",
      "---------------------\n",
      "Iteration Number: 6209\n",
      "Loss: 12.377454407206866\n",
      "l2 norm of gradients: 0.07631033365986883\n",
      "l2 norm of weights: 3.229850275409095\n",
      "---------------------\n",
      "Iteration Number: 6210\n",
      "Loss: 12.376715775271006\n",
      "l2 norm of gradients: 0.07628968937388604\n",
      "l2 norm of weights: 3.2298997009554826\n",
      "---------------------\n",
      "Iteration Number: 6211\n",
      "Loss: 12.375977599669781\n",
      "l2 norm of gradients: 0.07626906041408828\n",
      "l2 norm of weights: 3.2299491538679774\n",
      "---------------------\n",
      "Iteration Number: 6212\n",
      "Loss: 12.375239879958333\n",
      "l2 norm of gradients: 0.0762484467671059\n",
      "l2 norm of weights: 3.229998634113784\n",
      "---------------------\n",
      "Iteration Number: 6213\n",
      "Loss: 12.374502615692343\n",
      "l2 norm of gradients: 0.07622784841957142\n",
      "l2 norm of weights: 3.2300481416601388\n",
      "---------------------\n",
      "Iteration Number: 6214\n",
      "Loss: 12.373765806428025\n",
      "l2 norm of gradients: 0.07620726535811959\n",
      "l2 norm of weights: 3.2300976764743066\n",
      "---------------------\n",
      "Iteration Number: 6215\n",
      "Loss: 12.373029451722108\n",
      "l2 norm of gradients: 0.07618669756938745\n",
      "l2 norm of weights: 3.2301472385235837\n",
      "---------------------\n",
      "Iteration Number: 6216\n",
      "Loss: 12.372293551131849\n",
      "l2 norm of gradients: 0.07616614504001434\n",
      "l2 norm of weights: 3.2301968277752957\n",
      "---------------------\n",
      "Iteration Number: 6217\n",
      "Loss: 12.371558104215055\n",
      "l2 norm of gradients: 0.07614560775664198\n",
      "l2 norm of weights: 3.230246444196798\n",
      "---------------------\n",
      "Iteration Number: 6218\n",
      "Loss: 12.370823110530004\n",
      "l2 norm of gradients: 0.07612508570591454\n",
      "l2 norm of weights: 3.2302960877554767\n",
      "---------------------\n",
      "Iteration Number: 6219\n",
      "Loss: 12.37008856963557\n",
      "l2 norm of gradients: 0.07610457887447857\n",
      "l2 norm of weights: 3.230345758418747\n",
      "---------------------\n",
      "Iteration Number: 6220\n",
      "Loss: 12.36935448109111\n",
      "l2 norm of gradients: 0.07608408724898315\n",
      "l2 norm of weights: 3.230395456154055\n",
      "---------------------\n",
      "Iteration Number: 6221\n",
      "Loss: 12.368620844456503\n",
      "l2 norm of gradients: 0.07606361081607985\n",
      "l2 norm of weights: 3.2304451809288763\n",
      "---------------------\n",
      "Iteration Number: 6222\n",
      "Loss: 12.367887659292162\n",
      "l2 norm of gradients: 0.07604314956242293\n",
      "l2 norm of weights: 3.2304949327107155\n",
      "---------------------\n",
      "Iteration Number: 6223\n",
      "Loss: 12.367154925159022\n",
      "l2 norm of gradients: 0.0760227034746691\n",
      "l2 norm of weights: 3.2305447114671084\n",
      "---------------------\n",
      "Iteration Number: 6224\n",
      "Loss: 12.366422641618554\n",
      "l2 norm of gradients: 0.07600227253947785\n",
      "l2 norm of weights: 3.230594517165621\n",
      "---------------------\n",
      "Iteration Number: 6225\n",
      "Loss: 12.365690808232726\n",
      "l2 norm of gradients: 0.07598185674351132\n",
      "l2 norm of weights: 3.2306443497738475\n",
      "---------------------\n",
      "Iteration Number: 6226\n",
      "Loss: 12.364959424564049\n",
      "l2 norm of gradients: 0.0759614560734344\n",
      "l2 norm of weights: 3.230694209259413\n",
      "---------------------\n",
      "Iteration Number: 6227\n",
      "Loss: 12.364228490175547\n",
      "l2 norm of gradients: 0.07594107051591477\n",
      "l2 norm of weights: 3.230744095589973\n",
      "---------------------\n",
      "Iteration Number: 6228\n",
      "Loss: 12.363498004630769\n",
      "l2 norm of gradients: 0.07592070005762286\n",
      "l2 norm of weights: 3.2307940087332114\n",
      "---------------------\n",
      "Iteration Number: 6229\n",
      "Loss: 12.362767967493758\n",
      "l2 norm of gradients: 0.07590034468523207\n",
      "l2 norm of weights: 3.2308439486568434\n",
      "---------------------\n",
      "Iteration Number: 6230\n",
      "Loss: 12.362038378329125\n",
      "l2 norm of gradients: 0.0758800043854186\n",
      "l2 norm of weights: 3.230893915328613\n",
      "---------------------\n",
      "Iteration Number: 6231\n",
      "Loss: 12.361309236701963\n",
      "l2 norm of gradients: 0.07585967914486164\n",
      "l2 norm of weights: 3.2309439087162937\n",
      "---------------------\n",
      "Iteration Number: 6232\n",
      "Loss: 12.360580542177905\n",
      "l2 norm of gradients: 0.07583936895024336\n",
      "l2 norm of weights: 3.23099392878769\n",
      "---------------------\n",
      "Iteration Number: 6233\n",
      "Loss: 12.35985229432307\n",
      "l2 norm of gradients: 0.0758190737882489\n",
      "l2 norm of weights: 3.231043975510636\n",
      "---------------------\n",
      "Iteration Number: 6234\n",
      "Loss: 12.359124492704138\n",
      "l2 norm of gradients: 0.0757987936455665\n",
      "l2 norm of weights: 3.231094048852994\n",
      "---------------------\n",
      "Iteration Number: 6235\n",
      "Loss: 12.358397136888291\n",
      "l2 norm of gradients: 0.07577852850888753\n",
      "l2 norm of weights: 3.2311441487826564\n",
      "---------------------\n",
      "Iteration Number: 6236\n",
      "Loss: 12.357670226443199\n",
      "l2 norm of gradients: 0.0757582783649064\n",
      "l2 norm of weights: 3.2311942752675478\n",
      "---------------------\n",
      "Iteration Number: 6237\n",
      "Loss: 12.356943760937082\n",
      "l2 norm of gradients: 0.07573804320032074\n",
      "l2 norm of weights: 3.2312444282756196\n",
      "---------------------\n",
      "Iteration Number: 6238\n",
      "Loss: 12.35621773993868\n",
      "l2 norm of gradients: 0.07571782300183146\n",
      "l2 norm of weights: 3.2312946077748537\n",
      "---------------------\n",
      "Iteration Number: 6239\n",
      "Loss: 12.355492163017214\n",
      "l2 norm of gradients: 0.0756976177561426\n",
      "l2 norm of weights: 3.231344813733262\n",
      "---------------------\n",
      "Iteration Number: 6240\n",
      "Loss: 12.35476702974246\n",
      "l2 norm of gradients: 0.07567742744996156\n",
      "l2 norm of weights: 3.231395046118886\n",
      "---------------------\n",
      "Iteration Number: 6241\n",
      "Loss: 12.354042339684662\n",
      "l2 norm of gradients: 0.0756572520699991\n",
      "l2 norm of weights: 3.231445304899797\n",
      "---------------------\n",
      "Iteration Number: 6242\n",
      "Loss: 12.353318092414629\n",
      "l2 norm of gradients: 0.07563709160296925\n",
      "l2 norm of weights: 3.2314955900440947\n",
      "---------------------\n",
      "Iteration Number: 6243\n",
      "Loss: 12.352594287503631\n",
      "l2 norm of gradients: 0.07561694603558952\n",
      "l2 norm of weights: 3.23154590151991\n",
      "---------------------\n",
      "Iteration Number: 6244\n",
      "Loss: 12.351870924523503\n",
      "l2 norm of gradients: 0.07559681535458088\n",
      "l2 norm of weights: 3.231596239295402\n",
      "---------------------\n",
      "Iteration Number: 6245\n",
      "Loss: 12.35114800304656\n",
      "l2 norm of gradients: 0.0755766995466677\n",
      "l2 norm of weights: 3.23164660333876\n",
      "---------------------\n",
      "Iteration Number: 6246\n",
      "Loss: 12.350425522645622\n",
      "l2 norm of gradients: 0.07555659859857788\n",
      "l2 norm of weights: 3.2316969936182036\n",
      "---------------------\n",
      "Iteration Number: 6247\n",
      "Loss: 12.349703482894045\n",
      "l2 norm of gradients: 0.07553651249704295\n",
      "l2 norm of weights: 3.2317474101019803\n",
      "---------------------\n",
      "Iteration Number: 6248\n",
      "Loss: 12.348981883365674\n",
      "l2 norm of gradients: 0.07551644122879798\n",
      "l2 norm of weights: 3.2317978527583686\n",
      "---------------------\n",
      "Iteration Number: 6249\n",
      "Loss: 12.3482607236349\n",
      "l2 norm of gradients: 0.07549638478058165\n",
      "l2 norm of weights: 3.2318483215556753\n",
      "---------------------\n",
      "Iteration Number: 6250\n",
      "Loss: 12.347540003276574\n",
      "l2 norm of gradients: 0.07547634313913634\n",
      "l2 norm of weights: 3.231898816462237\n",
      "---------------------\n",
      "Iteration Number: 6251\n",
      "Loss: 12.346819721866074\n",
      "l2 norm of gradients: 0.07545631629120811\n",
      "l2 norm of weights: 3.2319493374464203\n",
      "---------------------\n",
      "Iteration Number: 6252\n",
      "Loss: 12.34609987897932\n",
      "l2 norm of gradients: 0.07543630422354679\n",
      "l2 norm of weights: 3.231999884476621\n",
      "---------------------\n",
      "Iteration Number: 6253\n",
      "Loss: 12.345380474192678\n",
      "l2 norm of gradients: 0.07541630692290592\n",
      "l2 norm of weights: 3.2320504575212636\n",
      "---------------------\n",
      "Iteration Number: 6254\n",
      "Loss: 12.34466150708309\n",
      "l2 norm of gradients: 0.07539632437604292\n",
      "l2 norm of weights: 3.232101056548803\n",
      "---------------------\n",
      "Iteration Number: 6255\n",
      "Loss: 12.343942977227947\n",
      "l2 norm of gradients: 0.07537635656971906\n",
      "l2 norm of weights: 3.2321516815277227\n",
      "---------------------\n",
      "Iteration Number: 6256\n",
      "Loss: 12.343224884205183\n",
      "l2 norm of gradients: 0.0753564034906994\n",
      "l2 norm of weights: 3.2322023324265365\n",
      "---------------------\n",
      "Iteration Number: 6257\n",
      "Loss: 12.342507227593233\n",
      "l2 norm of gradients: 0.07533646512575305\n",
      "l2 norm of weights: 3.232253009213786\n",
      "---------------------\n",
      "Iteration Number: 6258\n",
      "Loss: 12.341790006971005\n",
      "l2 norm of gradients: 0.07531654146165292\n",
      "l2 norm of weights: 3.232303711858044\n",
      "---------------------\n",
      "Iteration Number: 6259\n",
      "Loss: 12.341073221917963\n",
      "l2 norm of gradients: 0.07529663248517611\n",
      "l2 norm of weights: 3.232354440327911\n",
      "---------------------\n",
      "Iteration Number: 6260\n",
      "Loss: 12.340356872014027\n",
      "l2 norm of gradients: 0.07527673818310354\n",
      "l2 norm of weights: 3.232405194592018\n",
      "---------------------\n",
      "Iteration Number: 6261\n",
      "Loss: 12.339640956839663\n",
      "l2 norm of gradients: 0.07525685854222032\n",
      "l2 norm of weights: 3.2324559746190245\n",
      "---------------------\n",
      "Iteration Number: 6262\n",
      "Loss: 12.338925475975799\n",
      "l2 norm of gradients: 0.07523699354931561\n",
      "l2 norm of weights: 3.2325067803776193\n",
      "---------------------\n",
      "Iteration Number: 6263\n",
      "Loss: 12.338210429003908\n",
      "l2 norm of gradients: 0.07521714319118271\n",
      "l2 norm of weights: 3.232557611836521\n",
      "---------------------\n",
      "Iteration Number: 6264\n",
      "Loss: 12.337495815505942\n",
      "l2 norm of gradients: 0.07519730745461908\n",
      "l2 norm of weights: 3.232608468964477\n",
      "---------------------\n",
      "Iteration Number: 6265\n",
      "Loss: 12.33678163506435\n",
      "l2 norm of gradients: 0.0751774863264264\n",
      "l2 norm of weights: 3.2326593517302635\n",
      "---------------------\n",
      "Iteration Number: 6266\n",
      "Loss: 12.33606788726211\n",
      "l2 norm of gradients: 0.07515767979341054\n",
      "l2 norm of weights: 3.2327102601026874\n",
      "---------------------\n",
      "Iteration Number: 6267\n",
      "Loss: 12.335354571682648\n",
      "l2 norm of gradients: 0.07513788784238169\n",
      "l2 norm of weights: 3.2327611940505827\n",
      "---------------------\n",
      "Iteration Number: 6268\n",
      "Loss: 12.334641687909953\n",
      "l2 norm of gradients: 0.07511811046015429\n",
      "l2 norm of weights: 3.2328121535428145\n",
      "---------------------\n",
      "Iteration Number: 6269\n",
      "Loss: 12.333929235528469\n",
      "l2 norm of gradients: 0.07509834763354716\n",
      "l2 norm of weights: 3.232863138548275\n",
      "---------------------\n",
      "Iteration Number: 6270\n",
      "Loss: 12.333217214123158\n",
      "l2 norm of gradients: 0.07507859934938345\n",
      "l2 norm of weights: 3.232914149035888\n",
      "---------------------\n",
      "Iteration Number: 6271\n",
      "Loss: 12.332505623279483\n",
      "l2 norm of gradients: 0.07505886559449075\n",
      "l2 norm of weights: 3.2329651849746037\n",
      "---------------------\n",
      "Iteration Number: 6272\n",
      "Loss: 12.331794462583403\n",
      "l2 norm of gradients: 0.0750391463557011\n",
      "l2 norm of weights: 3.233016246333404\n",
      "---------------------\n",
      "Iteration Number: 6273\n",
      "Loss: 12.331083731621366\n",
      "l2 norm of gradients: 0.07501944161985091\n",
      "l2 norm of weights: 3.2330673330812973\n",
      "---------------------\n",
      "Iteration Number: 6274\n",
      "Loss: 12.330373429980312\n",
      "l2 norm of gradients: 0.07499975137378123\n",
      "l2 norm of weights: 3.2331184451873236\n",
      "---------------------\n",
      "Iteration Number: 6275\n",
      "Loss: 12.329663557247702\n",
      "l2 norm of gradients: 0.07498007560433757\n",
      "l2 norm of weights: 3.2331695826205498\n",
      "---------------------\n",
      "Iteration Number: 6276\n",
      "Loss: 12.328954113011475\n",
      "l2 norm of gradients: 0.07496041429837001\n",
      "l2 norm of weights: 3.2332207453500734\n",
      "---------------------\n",
      "Iteration Number: 6277\n",
      "Loss: 12.328245096860064\n",
      "l2 norm of gradients: 0.07494076744273326\n",
      "l2 norm of weights: 3.2332719333450193\n",
      "---------------------\n",
      "Iteration Number: 6278\n",
      "Loss: 12.327536508382426\n",
      "l2 norm of gradients: 0.0749211350242866\n",
      "l2 norm of weights: 3.233323146574543\n",
      "---------------------\n",
      "Iteration Number: 6279\n",
      "Loss: 12.326828347167973\n",
      "l2 norm of gradients: 0.0749015170298941\n",
      "l2 norm of weights: 3.2333743850078274\n",
      "---------------------\n",
      "Iteration Number: 6280\n",
      "Loss: 12.326120612806626\n",
      "l2 norm of gradients: 0.07488191344642438\n",
      "l2 norm of weights: 3.2334256486140864\n",
      "---------------------\n",
      "Iteration Number: 6281\n",
      "Loss: 12.325413304888809\n",
      "l2 norm of gradients: 0.0748623242607509\n",
      "l2 norm of weights: 3.2334769373625605\n",
      "---------------------\n",
      "Iteration Number: 6282\n",
      "Loss: 12.32470642300543\n",
      "l2 norm of gradients: 0.07484274945975189\n",
      "l2 norm of weights: 3.2335282512225203\n",
      "---------------------\n",
      "Iteration Number: 6283\n",
      "Loss: 12.323999966747888\n",
      "l2 norm of gradients: 0.07482318903031028\n",
      "l2 norm of weights: 3.2335795901632656\n",
      "---------------------\n",
      "Iteration Number: 6284\n",
      "Loss: 12.323293935708076\n",
      "l2 norm of gradients: 0.07480364295931385\n",
      "l2 norm of weights: 3.233630954154124\n",
      "---------------------\n",
      "Iteration Number: 6285\n",
      "Loss: 12.3225883294784\n",
      "l2 norm of gradients: 0.07478411123365535\n",
      "l2 norm of weights: 3.2336823431644532\n",
      "---------------------\n",
      "Iteration Number: 6286\n",
      "Loss: 12.321883147651722\n",
      "l2 norm of gradients: 0.07476459384023233\n",
      "l2 norm of weights: 3.233733757163639\n",
      "---------------------\n",
      "Iteration Number: 6287\n",
      "Loss: 12.321178389821394\n",
      "l2 norm of gradients: 0.07474509076594722\n",
      "l2 norm of weights: 3.2337851961210955\n",
      "---------------------\n",
      "Iteration Number: 6288\n",
      "Loss: 12.3204740555813\n",
      "l2 norm of gradients: 0.0747256019977075\n",
      "l2 norm of weights: 3.2338366600062667\n",
      "---------------------\n",
      "Iteration Number: 6289\n",
      "Loss: 12.319770144525771\n",
      "l2 norm of gradients: 0.07470612752242554\n",
      "l2 norm of weights: 3.233888148788625\n",
      "---------------------\n",
      "Iteration Number: 6290\n",
      "Loss: 12.31906665624966\n",
      "l2 norm of gradients: 0.07468666732701881\n",
      "l2 norm of weights: 3.2339396624376713\n",
      "---------------------\n",
      "Iteration Number: 6291\n",
      "Loss: 12.31836359034827\n",
      "l2 norm of gradients: 0.07466722139840982\n",
      "l2 norm of weights: 3.233991200922935\n",
      "---------------------\n",
      "Iteration Number: 6292\n",
      "Loss: 12.317660946417428\n",
      "l2 norm of gradients: 0.07464778972352604\n",
      "l2 norm of weights: 3.234042764213975\n",
      "---------------------\n",
      "Iteration Number: 6293\n",
      "Loss: 12.316958724053428\n",
      "l2 norm of gradients: 0.07462837228930021\n",
      "l2 norm of weights: 3.234094352280378\n",
      "---------------------\n",
      "Iteration Number: 6294\n",
      "Loss: 12.316256922853054\n",
      "l2 norm of gradients: 0.07460896908267005\n",
      "l2 norm of weights: 3.2341459650917606\n",
      "---------------------\n",
      "Iteration Number: 6295\n",
      "Loss: 12.315555542413584\n",
      "l2 norm of gradients: 0.0745895800905786\n",
      "l2 norm of weights: 3.2341976026177672\n",
      "---------------------\n",
      "Iteration Number: 6296\n",
      "Loss: 12.314854582332778\n",
      "l2 norm of gradients: 0.07457020529997395\n",
      "l2 norm of weights: 3.2342492648280703\n",
      "---------------------\n",
      "Iteration Number: 6297\n",
      "Loss: 12.314154042208873\n",
      "l2 norm of gradients: 0.07455084469780951\n",
      "l2 norm of weights: 3.2343009516923726\n",
      "---------------------\n",
      "Iteration Number: 6298\n",
      "Loss: 12.313453921640603\n",
      "l2 norm of gradients: 0.07453149827104397\n",
      "l2 norm of weights: 3.2343526631804034\n",
      "---------------------\n",
      "Iteration Number: 6299\n",
      "Loss: 12.312754220227188\n",
      "l2 norm of gradients: 0.07451216600664115\n",
      "l2 norm of weights: 3.234404399261923\n",
      "---------------------\n",
      "Iteration Number: 6300\n",
      "Loss: 12.312054937568305\n",
      "l2 norm of gradients: 0.07449284789157037\n",
      "l2 norm of weights: 3.234456159906718\n",
      "---------------------\n",
      "Iteration Number: 6301\n",
      "Loss: 12.311356073264148\n",
      "l2 norm of gradients: 0.0744735439128062\n",
      "l2 norm of weights: 3.2345079450846046\n",
      "---------------------\n",
      "Iteration Number: 6302\n",
      "Loss: 12.310657626915383\n",
      "l2 norm of gradients: 0.07445425405732857\n",
      "l2 norm of weights: 3.234559754765428\n",
      "---------------------\n",
      "Iteration Number: 6303\n",
      "Loss: 12.309959598123143\n",
      "l2 norm of gradients: 0.07443497831212288\n",
      "l2 norm of weights: 3.2346115889190603\n",
      "---------------------\n",
      "Iteration Number: 6304\n",
      "Loss: 12.30926198648906\n",
      "l2 norm of gradients: 0.07441571666417988\n",
      "l2 norm of weights: 3.234663447515405\n",
      "---------------------\n",
      "Iteration Number: 6305\n",
      "Loss: 12.308564791615241\n",
      "l2 norm of gradients: 0.07439646910049588\n",
      "l2 norm of weights: 3.2347153305243905\n",
      "---------------------\n",
      "Iteration Number: 6306\n",
      "Loss: 12.307868013104272\n",
      "l2 norm of gradients: 0.07437723560807256\n",
      "l2 norm of weights: 3.234767237915976\n",
      "---------------------\n",
      "Iteration Number: 6307\n",
      "Loss: 12.307171650559233\n",
      "l2 norm of gradients: 0.07435801617391723\n",
      "l2 norm of weights: 3.234819169660149\n",
      "---------------------\n",
      "Iteration Number: 6308\n",
      "Loss: 12.306475703583665\n",
      "l2 norm of gradients: 0.07433881078504263\n",
      "l2 norm of weights: 3.234871125726924\n",
      "---------------------\n",
      "Iteration Number: 6309\n",
      "Loss: 12.30578017178158\n",
      "l2 norm of gradients: 0.07431961942846717\n",
      "l2 norm of weights: 3.2349231060863453\n",
      "---------------------\n",
      "Iteration Number: 6310\n",
      "Loss: 12.305085054757512\n",
      "l2 norm of gradients: 0.07430044209121485\n",
      "l2 norm of weights: 3.234975110708486\n",
      "---------------------\n",
      "Iteration Number: 6311\n",
      "Loss: 12.304390352116412\n",
      "l2 norm of gradients: 0.0742812787603152\n",
      "l2 norm of weights: 3.235027139563445\n",
      "---------------------\n",
      "Iteration Number: 6312\n",
      "Loss: 12.30369606346377\n",
      "l2 norm of gradients: 0.07426212942280358\n",
      "l2 norm of weights: 3.235079192621352\n",
      "---------------------\n",
      "Iteration Number: 6313\n",
      "Loss: 12.3030021884055\n",
      "l2 norm of gradients: 0.07424299406572087\n",
      "l2 norm of weights: 3.2351312698523644\n",
      "---------------------\n",
      "Iteration Number: 6314\n",
      "Loss: 12.302308726548038\n",
      "l2 norm of gradients: 0.07422387267611373\n",
      "l2 norm of weights: 3.235183371226668\n",
      "---------------------\n",
      "Iteration Number: 6315\n",
      "Loss: 12.301615677498265\n",
      "l2 norm of gradients: 0.07420476524103461\n",
      "l2 norm of weights: 3.235235496714476\n",
      "---------------------\n",
      "Iteration Number: 6316\n",
      "Loss: 12.300923040863536\n",
      "l2 norm of gradients: 0.07418567174754165\n",
      "l2 norm of weights: 3.2352876462860314\n",
      "---------------------\n",
      "Iteration Number: 6317\n",
      "Loss: 12.300230816251709\n",
      "l2 norm of gradients: 0.07416659218269882\n",
      "l2 norm of weights: 3.2353398199116037\n",
      "---------------------\n",
      "Iteration Number: 6318\n",
      "Loss: 12.299539003271086\n",
      "l2 norm of gradients: 0.0741475265335759\n",
      "l2 norm of weights: 3.235392017561492\n",
      "---------------------\n",
      "Iteration Number: 6319\n",
      "Loss: 12.29884760153047\n",
      "l2 norm of gradients: 0.07412847478724846\n",
      "l2 norm of weights: 3.2354442392060223\n",
      "---------------------\n",
      "Iteration Number: 6320\n",
      "Loss: 12.298156610639108\n",
      "l2 norm of gradients: 0.07410943693079809\n",
      "l2 norm of weights: 3.2354964848155507\n",
      "---------------------\n",
      "Iteration Number: 6321\n",
      "Loss: 12.29746603020674\n",
      "l2 norm of gradients: 0.07409041295131213\n",
      "l2 norm of weights: 3.2355487543604595\n",
      "---------------------\n",
      "Iteration Number: 6322\n",
      "Loss: 12.296775859843578\n",
      "l2 norm of gradients: 0.07407140283588395\n",
      "l2 norm of weights: 3.2356010478111608\n",
      "---------------------\n",
      "Iteration Number: 6323\n",
      "Loss: 12.296086099160291\n",
      "l2 norm of gradients: 0.07405240657161281\n",
      "l2 norm of weights: 3.235653365138093\n",
      "---------------------\n",
      "Iteration Number: 6324\n",
      "Loss: 12.295396747768049\n",
      "l2 norm of gradients: 0.074033424145604\n",
      "l2 norm of weights: 3.2357057063117245\n",
      "---------------------\n",
      "Iteration Number: 6325\n",
      "Loss: 12.294707805278442\n",
      "l2 norm of gradients: 0.07401445554496876\n",
      "l2 norm of weights: 3.23575807130255\n",
      "---------------------\n",
      "Iteration Number: 6326\n",
      "Loss: 12.294019271303593\n",
      "l2 norm of gradients: 0.07399550075682444\n",
      "l2 norm of weights: 3.2358104600810944\n",
      "---------------------\n",
      "Iteration Number: 6327\n",
      "Loss: 12.293331145456031\n",
      "l2 norm of gradients: 0.07397655976829433\n",
      "l2 norm of weights: 3.2358628726179086\n",
      "---------------------\n",
      "Iteration Number: 6328\n",
      "Loss: 12.292643427348816\n",
      "l2 norm of gradients: 0.07395763256650793\n",
      "l2 norm of weights: 3.235915308883573\n",
      "---------------------\n",
      "Iteration Number: 6329\n",
      "Loss: 12.291956116595436\n",
      "l2 norm of gradients: 0.07393871913860074\n",
      "l2 norm of weights: 3.2359677688486945\n",
      "---------------------\n",
      "Iteration Number: 6330\n",
      "Loss: 12.291269212809846\n",
      "l2 norm of gradients: 0.07391981947171453\n",
      "l2 norm of weights: 3.2360202524839097\n",
      "---------------------\n",
      "Iteration Number: 6331\n",
      "Loss: 12.290582715606506\n",
      "l2 norm of gradients: 0.07390093355299705\n",
      "l2 norm of weights: 3.236072759759882\n",
      "---------------------\n",
      "Iteration Number: 6332\n",
      "Loss: 12.289896624600296\n",
      "l2 norm of gradients: 0.07388206136960235\n",
      "l2 norm of weights: 3.236125290647304\n",
      "---------------------\n",
      "Iteration Number: 6333\n",
      "Loss: 12.289210939406578\n",
      "l2 norm of gradients: 0.07386320290869068\n",
      "l2 norm of weights: 3.236177845116894\n",
      "---------------------\n",
      "Iteration Number: 6334\n",
      "Loss: 12.288525659641206\n",
      "l2 norm of gradients: 0.07384435815742854\n",
      "l2 norm of weights: 3.2362304231394\n",
      "---------------------\n",
      "Iteration Number: 6335\n",
      "Loss: 12.287840784920478\n",
      "l2 norm of gradients: 0.07382552710298858\n",
      "l2 norm of weights: 3.236283024685598\n",
      "---------------------\n",
      "Iteration Number: 6336\n",
      "Loss: 12.287156314861146\n",
      "l2 norm of gradients: 0.0738067097325499\n",
      "l2 norm of weights: 3.2363356497262905\n",
      "---------------------\n",
      "Iteration Number: 6337\n",
      "Loss: 12.286472249080443\n",
      "l2 norm of gradients: 0.07378790603329773\n",
      "l2 norm of weights: 3.23638829823231\n",
      "---------------------\n",
      "Iteration Number: 6338\n",
      "Loss: 12.285788587196075\n",
      "l2 norm of gradients: 0.0737691159924238\n",
      "l2 norm of weights: 3.2364409701745136\n",
      "---------------------\n",
      "Iteration Number: 6339\n",
      "Loss: 12.285105328826184\n",
      "l2 norm of gradients: 0.07375033959712608\n",
      "l2 norm of weights: 3.23649366552379\n",
      "---------------------\n",
      "Iteration Number: 6340\n",
      "Loss: 12.284422473589386\n",
      "l2 norm of gradients: 0.07373157683460899\n",
      "l2 norm of weights: 3.2365463842510533\n",
      "---------------------\n",
      "Iteration Number: 6341\n",
      "Loss: 12.28374002110478\n",
      "l2 norm of gradients: 0.07371282769208326\n",
      "l2 norm of weights: 3.2365991263272456\n",
      "---------------------\n",
      "Iteration Number: 6342\n",
      "Loss: 12.283057970991903\n",
      "l2 norm of gradients: 0.07369409215676619\n",
      "l2 norm of weights: 3.236651891723337\n",
      "---------------------\n",
      "Iteration Number: 6343\n",
      "Loss: 12.282376322870752\n",
      "l2 norm of gradients: 0.07367537021588134\n",
      "l2 norm of weights: 3.236704680410326\n",
      "---------------------\n",
      "Iteration Number: 6344\n",
      "Loss: 12.281695076361792\n",
      "l2 norm of gradients: 0.07365666185665898\n",
      "l2 norm of weights: 3.236757492359238\n",
      "---------------------\n",
      "Iteration Number: 6345\n",
      "Loss: 12.281014231085965\n",
      "l2 norm of gradients: 0.07363796706633567\n",
      "l2 norm of weights: 3.236810327541126\n",
      "---------------------\n",
      "Iteration Number: 6346\n",
      "Loss: 12.280333786664656\n",
      "l2 norm of gradients: 0.07361928583215459\n",
      "l2 norm of weights: 3.2368631859270716\n",
      "---------------------\n",
      "Iteration Number: 6347\n",
      "Loss: 12.279653742719699\n",
      "l2 norm of gradients: 0.07360061814136547\n",
      "l2 norm of weights: 3.236916067488183\n",
      "---------------------\n",
      "Iteration Number: 6348\n",
      "Loss: 12.27897409887341\n",
      "l2 norm of gradients: 0.0735819639812246\n",
      "l2 norm of weights: 3.2369689721955965\n",
      "---------------------\n",
      "Iteration Number: 6349\n",
      "Loss: 12.278294854748532\n",
      "l2 norm of gradients: 0.07356332333899483\n",
      "l2 norm of weights: 3.2370219000204763\n",
      "---------------------\n",
      "Iteration Number: 6350\n",
      "Loss: 12.277616009968296\n",
      "l2 norm of gradients: 0.07354469620194565\n",
      "l2 norm of weights: 3.2370748509340137\n",
      "---------------------\n",
      "Iteration Number: 6351\n",
      "Loss: 12.276937564156386\n",
      "l2 norm of gradients: 0.07352608255735318\n",
      "l2 norm of weights: 3.2371278249074282\n",
      "---------------------\n",
      "Iteration Number: 6352\n",
      "Loss: 12.276259516936937\n",
      "l2 norm of gradients: 0.07350748239250024\n",
      "l2 norm of weights: 3.237180821911966\n",
      "---------------------\n",
      "Iteration Number: 6353\n",
      "Loss: 12.275581867934537\n",
      "l2 norm of gradients: 0.0734888956946762\n",
      "l2 norm of weights: 3.237233841918901\n",
      "---------------------\n",
      "Iteration Number: 6354\n",
      "Loss: 12.274904616774233\n",
      "l2 norm of gradients: 0.07347032245117732\n",
      "l2 norm of weights: 3.2372868848995364\n",
      "---------------------\n",
      "Iteration Number: 6355\n",
      "Loss: 12.27422776308151\n",
      "l2 norm of gradients: 0.07345176264930645\n",
      "l2 norm of weights: 3.2373399508252\n",
      "---------------------\n",
      "Iteration Number: 6356\n",
      "Loss: 12.27355130648235\n",
      "l2 norm of gradients: 0.0734332162763732\n",
      "l2 norm of weights: 3.237393039667248\n",
      "---------------------\n",
      "Iteration Number: 6357\n",
      "Loss: 12.27287524660314\n",
      "l2 norm of gradients: 0.07341468331969409\n",
      "l2 norm of weights: 3.2374461513970663\n",
      "---------------------\n",
      "Iteration Number: 6358\n",
      "Loss: 12.272199583070762\n",
      "l2 norm of gradients: 0.07339616376659218\n",
      "l2 norm of weights: 3.237499285986065\n",
      "---------------------\n",
      "Iteration Number: 6359\n",
      "Loss: 12.27152431551252\n",
      "l2 norm of gradients: 0.0733776576043976\n",
      "l2 norm of weights: 3.237552443405684\n",
      "---------------------\n",
      "Iteration Number: 6360\n",
      "Loss: 12.270849443556195\n",
      "l2 norm of gradients: 0.07335916482044716\n",
      "l2 norm of weights: 3.237605623627389\n",
      "---------------------\n",
      "Iteration Number: 6361\n",
      "Loss: 12.270174966829998\n",
      "l2 norm of gradients: 0.07334068540208454\n",
      "l2 norm of weights: 3.2376588266226745\n",
      "---------------------\n",
      "Iteration Number: 6362\n",
      "Loss: 12.269500884962616\n",
      "l2 norm of gradients: 0.07332221933666035\n",
      "l2 norm of weights: 3.237712052363061\n",
      "---------------------\n",
      "Iteration Number: 6363\n",
      "Loss: 12.268827197583157\n",
      "l2 norm of gradients: 0.0733037666115321\n",
      "l2 norm of weights: 3.237765300820097\n",
      "---------------------\n",
      "Iteration Number: 6364\n",
      "Loss: 12.268153904321208\n",
      "l2 norm of gradients: 0.07328532721406418\n",
      "l2 norm of weights: 3.237818571965359\n",
      "---------------------\n",
      "Iteration Number: 6365\n",
      "Loss: 12.267481004806784\n",
      "l2 norm of gradients: 0.07326690113162793\n",
      "l2 norm of weights: 3.237871865770449\n",
      "---------------------\n",
      "Iteration Number: 6366\n",
      "Loss: 12.266808498670361\n",
      "l2 norm of gradients: 0.07324848835160164\n",
      "l2 norm of weights: 3.237925182206997\n",
      "---------------------\n",
      "Iteration Number: 6367\n",
      "Loss: 12.266136385542872\n",
      "l2 norm of gradients: 0.07323008886137063\n",
      "l2 norm of weights: 3.2379785212466627\n",
      "---------------------\n",
      "Iteration Number: 6368\n",
      "Loss: 12.265464665055681\n",
      "l2 norm of gradients: 0.07321170264832719\n",
      "l2 norm of weights: 3.2380318828611294\n",
      "---------------------\n",
      "Iteration Number: 6369\n",
      "Loss: 12.264793336840594\n",
      "l2 norm of gradients: 0.07319332969987066\n",
      "l2 norm of weights: 3.238085267022109\n",
      "---------------------\n",
      "Iteration Number: 6370\n",
      "Loss: 12.264122400529903\n",
      "l2 norm of gradients: 0.07317497000340738\n",
      "l2 norm of weights: 3.238138673701341\n",
      "---------------------\n",
      "Iteration Number: 6371\n",
      "Loss: 12.263451855756323\n",
      "l2 norm of gradients: 0.0731566235463508\n",
      "l2 norm of weights: 3.238192102870592\n",
      "---------------------\n",
      "Iteration Number: 6372\n",
      "Loss: 12.262781702152992\n",
      "l2 norm of gradients: 0.07313829031612142\n",
      "l2 norm of weights: 3.2382455545016553\n",
      "---------------------\n",
      "Iteration Number: 6373\n",
      "Loss: 12.262111939353524\n",
      "l2 norm of gradients: 0.07311997030014687\n",
      "l2 norm of weights: 3.2382990285663515\n",
      "---------------------\n",
      "Iteration Number: 6374\n",
      "Loss: 12.261442566991967\n",
      "l2 norm of gradients: 0.07310166348586186\n",
      "l2 norm of weights: 3.238352525036529\n",
      "---------------------\n",
      "Iteration Number: 6375\n",
      "Loss: 12.260773584702827\n",
      "l2 norm of gradients: 0.07308336986070835\n",
      "l2 norm of weights: 3.2384060438840616\n",
      "---------------------\n",
      "Iteration Number: 6376\n",
      "Loss: 12.260104992121047\n",
      "l2 norm of gradients: 0.07306508941213535\n",
      "l2 norm of weights: 3.238459585080852\n",
      "---------------------\n",
      "Iteration Number: 6377\n",
      "Loss: 12.259436788882006\n",
      "l2 norm of gradients: 0.0730468221275991\n",
      "l2 norm of weights: 3.2385131485988294\n",
      "---------------------\n",
      "Iteration Number: 6378\n",
      "Loss: 12.258768974621528\n",
      "l2 norm of gradients: 0.07302856799456305\n",
      "l2 norm of weights: 3.238566734409949\n",
      "---------------------\n",
      "Iteration Number: 6379\n",
      "Loss: 12.258101548975894\n",
      "l2 norm of gradients: 0.07301032700049784\n",
      "l2 norm of weights: 3.238620342486194\n",
      "---------------------\n",
      "Iteration Number: 6380\n",
      "Loss: 12.257434511581804\n",
      "l2 norm of gradients: 0.07299209913288146\n",
      "l2 norm of weights: 3.238673972799575\n",
      "---------------------\n",
      "Iteration Number: 6381\n",
      "Loss: 12.256767862076426\n",
      "l2 norm of gradients: 0.07297388437919897\n",
      "l2 norm of weights: 3.238727625322128\n",
      "---------------------\n",
      "Iteration Number: 6382\n",
      "Loss: 12.256101600097335\n",
      "l2 norm of gradients: 0.0729556827269429\n",
      "l2 norm of weights: 3.2387813000259182\n",
      "---------------------\n",
      "Iteration Number: 6383\n",
      "Loss: 12.255435725282588\n",
      "l2 norm of gradients: 0.07293749416361296\n",
      "l2 norm of weights: 3.238834996883036\n",
      "---------------------\n",
      "Iteration Number: 6384\n",
      "Loss: 12.25477023727066\n",
      "l2 norm of gradients: 0.07291931867671622\n",
      "l2 norm of weights: 3.238888715865598\n",
      "---------------------\n",
      "Iteration Number: 6385\n",
      "Loss: 12.254105135700458\n",
      "l2 norm of gradients: 0.0729011562537671\n",
      "l2 norm of weights: 3.2389424569457503\n",
      "---------------------\n",
      "Iteration Number: 6386\n",
      "Loss: 12.25344042021135\n",
      "l2 norm of gradients: 0.07288300688228738\n",
      "l2 norm of weights: 3.2389962200956637\n",
      "---------------------\n",
      "Iteration Number: 6387\n",
      "Loss: 12.252776090443124\n",
      "l2 norm of gradients: 0.07286487054980616\n",
      "l2 norm of weights: 3.2390500052875364\n",
      "---------------------\n",
      "Iteration Number: 6388\n",
      "Loss: 12.252112146035998\n",
      "l2 norm of gradients: 0.07284674724385995\n",
      "l2 norm of weights: 3.239103812493594\n",
      "---------------------\n",
      "Iteration Number: 6389\n",
      "Loss: 12.25144858663067\n",
      "l2 norm of gradients: 0.07282863695199274\n",
      "l2 norm of weights: 3.239157641686088\n",
      "---------------------\n",
      "Iteration Number: 6390\n",
      "Loss: 12.250785411868229\n",
      "l2 norm of gradients: 0.07281053966175585\n",
      "l2 norm of weights: 3.239211492837297\n",
      "---------------------\n",
      "Iteration Number: 6391\n",
      "Loss: 12.25012262139024\n",
      "l2 norm of gradients: 0.07279245536070808\n",
      "l2 norm of weights: 3.239265365919527\n",
      "---------------------\n",
      "Iteration Number: 6392\n",
      "Loss: 12.249460214838647\n",
      "l2 norm of gradients: 0.07277438403641577\n",
      "l2 norm of weights: 3.23931926090511\n",
      "---------------------\n",
      "Iteration Number: 6393\n",
      "Loss: 12.248798191855897\n",
      "l2 norm of gradients: 0.07275632567645264\n",
      "l2 norm of weights: 3.239373177766405\n",
      "---------------------\n",
      "Iteration Number: 6394\n",
      "Loss: 12.248136552084832\n",
      "l2 norm of gradients: 0.07273828026839994\n",
      "l2 norm of weights: 3.2394271164757975\n",
      "---------------------\n",
      "Iteration Number: 6395\n",
      "Loss: 12.24747529516873\n",
      "l2 norm of gradients: 0.07272024779984643\n",
      "l2 norm of weights: 3.2394810770057005\n",
      "---------------------\n",
      "Iteration Number: 6396\n",
      "Loss: 12.246814420751331\n",
      "l2 norm of gradients: 0.07270222825838844\n",
      "l2 norm of weights: 3.2395350593285515\n",
      "---------------------\n",
      "Iteration Number: 6397\n",
      "Loss: 12.246153928476765\n",
      "l2 norm of gradients: 0.07268422163162987\n",
      "l2 norm of weights: 3.239589063416817\n",
      "---------------------\n",
      "Iteration Number: 6398\n",
      "Loss: 12.245493817989638\n",
      "l2 norm of gradients: 0.0726662279071821\n",
      "l2 norm of weights: 3.2396430892429895\n",
      "---------------------\n",
      "Iteration Number: 6399\n",
      "Loss: 12.244834088934955\n",
      "l2 norm of gradients: 0.07264824707266415\n",
      "l2 norm of weights: 3.2396971367795873\n",
      "---------------------\n",
      "Iteration Number: 6400\n",
      "Loss: 12.244174740958165\n",
      "l2 norm of gradients: 0.0726302791157027\n",
      "l2 norm of weights: 3.239751205999156\n",
      "---------------------\n",
      "Iteration Number: 6401\n",
      "Loss: 12.243515773705166\n",
      "l2 norm of gradients: 0.07261232402393193\n",
      "l2 norm of weights: 3.239805296874267\n",
      "---------------------\n",
      "Iteration Number: 6402\n",
      "Loss: 12.242857186822265\n",
      "l2 norm of gradients: 0.07259438178499376\n",
      "l2 norm of weights: 3.2398594093775195\n",
      "---------------------\n",
      "Iteration Number: 6403\n",
      "Loss: 12.242198979956202\n",
      "l2 norm of gradients: 0.07257645238653769\n",
      "l2 norm of weights: 3.2399135434815376\n",
      "---------------------\n",
      "Iteration Number: 6404\n",
      "Loss: 12.241541152754152\n",
      "l2 norm of gradients: 0.07255853581622097\n",
      "l2 norm of weights: 3.2399676991589734\n",
      "---------------------\n",
      "Iteration Number: 6405\n",
      "Loss: 12.240883704863723\n",
      "l2 norm of gradients: 0.0725406320617085\n",
      "l2 norm of weights: 3.240021876382505\n",
      "---------------------\n",
      "Iteration Number: 6406\n",
      "Loss: 12.24022663593295\n",
      "l2 norm of gradients: 0.07252274111067282\n",
      "l2 norm of weights: 3.2400760751248354\n",
      "---------------------\n",
      "Iteration Number: 6407\n",
      "Loss: 12.239569945610286\n",
      "l2 norm of gradients: 0.07250486295079432\n",
      "l2 norm of weights: 3.240130295358697\n",
      "---------------------\n",
      "Iteration Number: 6408\n",
      "Loss: 12.238913633544621\n",
      "l2 norm of gradients: 0.072486997569761\n",
      "l2 norm of weights: 3.240184537056846\n",
      "---------------------\n",
      "Iteration Number: 6409\n",
      "Loss: 12.238257699385285\n",
      "l2 norm of gradients: 0.07246914495526868\n",
      "l2 norm of weights: 3.2402388001920657\n",
      "---------------------\n",
      "Iteration Number: 6410\n",
      "Loss: 12.237602142782004\n",
      "l2 norm of gradients: 0.07245130509502094\n",
      "l2 norm of weights: 3.240293084737167\n",
      "---------------------\n",
      "Iteration Number: 6411\n",
      "Loss: 12.23694696338496\n",
      "l2 norm of gradients: 0.07243347797672915\n",
      "l2 norm of weights: 3.240347390664985\n",
      "---------------------\n",
      "Iteration Number: 6412\n",
      "Loss: 12.236292160844744\n",
      "l2 norm of gradients: 0.07241566358811248\n",
      "l2 norm of weights: 3.2404017179483837\n",
      "---------------------\n",
      "Iteration Number: 6413\n",
      "Loss: 12.235637734812379\n",
      "l2 norm of gradients: 0.07239786191689791\n",
      "l2 norm of weights: 3.2404560665602506\n",
      "---------------------\n",
      "Iteration Number: 6414\n",
      "Loss: 12.234983684939316\n",
      "l2 norm of gradients: 0.07238007295082019\n",
      "l2 norm of weights: 3.2405104364735013\n",
      "---------------------\n",
      "Iteration Number: 6415\n",
      "Loss: 12.234330010877416\n",
      "l2 norm of gradients: 0.07236229667762203\n",
      "l2 norm of weights: 3.240564827661077\n",
      "---------------------\n",
      "Iteration Number: 6416\n",
      "Loss: 12.233676712278985\n",
      "l2 norm of gradients: 0.07234453308505395\n",
      "l2 norm of weights: 3.2406192400959464\n",
      "---------------------\n",
      "Iteration Number: 6417\n",
      "Loss: 12.233023788796734\n",
      "l2 norm of gradients: 0.07232678216087432\n",
      "l2 norm of weights: 3.2406736737511017\n",
      "---------------------\n",
      "Iteration Number: 6418\n",
      "Loss: 12.232371240083804\n",
      "l2 norm of gradients: 0.07230904389284942\n",
      "l2 norm of weights: 3.240728128599564\n",
      "---------------------\n",
      "Iteration Number: 6419\n",
      "Loss: 12.231719065793778\n",
      "l2 norm of gradients: 0.07229131826875346\n",
      "l2 norm of weights: 3.240782604614379\n",
      "---------------------\n",
      "Iteration Number: 6420\n",
      "Loss: 12.231067265580608\n",
      "l2 norm of gradients: 0.07227360527636859\n",
      "l2 norm of weights: 3.2408371017686193\n",
      "---------------------\n",
      "Iteration Number: 6421\n",
      "Loss: 12.230415839098734\n",
      "l2 norm of gradients: 0.0722559049034848\n",
      "l2 norm of weights: 3.2408916200353834\n",
      "---------------------\n",
      "Iteration Number: 6422\n",
      "Loss: 12.229764786002955\n",
      "l2 norm of gradients: 0.07223821713790014\n",
      "l2 norm of weights: 3.240946159387795\n",
      "---------------------\n",
      "Iteration Number: 6423\n",
      "Loss: 12.229114105948545\n",
      "l2 norm of gradients: 0.0722205419674206\n",
      "l2 norm of weights: 3.2410007197990063\n",
      "---------------------\n",
      "Iteration Number: 6424\n",
      "Loss: 12.228463798591156\n",
      "l2 norm of gradients: 0.0722028793798601\n",
      "l2 norm of weights: 3.2410553012421928\n",
      "---------------------\n",
      "Iteration Number: 6425\n",
      "Loss: 12.22781386358687\n",
      "l2 norm of gradients: 0.07218522936304059\n",
      "l2 norm of weights: 3.2411099036905577\n",
      "---------------------\n",
      "Iteration Number: 6426\n",
      "Loss: 12.227164300592223\n",
      "l2 norm of gradients: 0.07216759190479206\n",
      "l2 norm of weights: 3.2411645271173297\n",
      "---------------------\n",
      "Iteration Number: 6427\n",
      "Loss: 12.226515109264115\n",
      "l2 norm of gradients: 0.07214996699295252\n",
      "l2 norm of weights: 3.241219171495764\n",
      "---------------------\n",
      "Iteration Number: 6428\n",
      "Loss: 12.225866289259894\n",
      "l2 norm of gradients: 0.07213235461536796\n",
      "l2 norm of weights: 3.2412738367991407\n",
      "---------------------\n",
      "Iteration Number: 6429\n",
      "Loss: 12.225217840237331\n",
      "l2 norm of gradients: 0.07211475475989246\n",
      "l2 norm of weights: 3.2413285230007665\n",
      "---------------------\n",
      "Iteration Number: 6430\n",
      "Loss: 12.224569761854587\n",
      "l2 norm of gradients: 0.0720971674143882\n",
      "l2 norm of weights: 3.2413832300739744\n",
      "---------------------\n",
      "Iteration Number: 6431\n",
      "Loss: 12.223922053770268\n",
      "l2 norm of gradients: 0.07207959256672536\n",
      "l2 norm of weights: 3.2414379579921233\n",
      "---------------------\n",
      "Iteration Number: 6432\n",
      "Loss: 12.223274715643376\n",
      "l2 norm of gradients: 0.07206203020478229\n",
      "l2 norm of weights: 3.2414927067285966\n",
      "---------------------\n",
      "Iteration Number: 6433\n",
      "Loss: 12.22262774713335\n",
      "l2 norm of gradients: 0.07204448031644543\n",
      "l2 norm of weights: 3.2415474762568057\n",
      "---------------------\n",
      "Iteration Number: 6434\n",
      "Loss: 12.221981147900008\n",
      "l2 norm of gradients: 0.07202694288960931\n",
      "l2 norm of weights: 3.2416022665501862\n",
      "---------------------\n",
      "Iteration Number: 6435\n",
      "Loss: 12.221334917603622\n",
      "l2 norm of gradients: 0.07200941791217669\n",
      "l2 norm of weights: 3.2416570775822\n",
      "---------------------\n",
      "Iteration Number: 6436\n",
      "Loss: 12.220689055904847\n",
      "l2 norm of gradients: 0.07199190537205831\n",
      "l2 norm of weights: 3.241711909326335\n",
      "---------------------\n",
      "Iteration Number: 6437\n",
      "Loss: 12.220043562464788\n",
      "l2 norm of gradients: 0.0719744052571732\n",
      "l2 norm of weights: 3.2417667617561055\n",
      "---------------------\n",
      "Iteration Number: 6438\n",
      "Loss: 12.219398436944918\n",
      "l2 norm of gradients: 0.07195691755544859\n",
      "l2 norm of weights: 3.24182163484505\n",
      "---------------------\n",
      "Iteration Number: 6439\n",
      "Loss: 12.21875367900714\n",
      "l2 norm of gradients: 0.07193944225481974\n",
      "l2 norm of weights: 3.2418765285667344\n",
      "---------------------\n",
      "Iteration Number: 6440\n",
      "Loss: 12.218109288313789\n",
      "l2 norm of gradients: 0.07192197934323033\n",
      "l2 norm of weights: 3.2419314428947485\n",
      "---------------------\n",
      "Iteration Number: 6441\n",
      "Loss: 12.217465264527597\n",
      "l2 norm of gradients: 0.07190452880863209\n",
      "l2 norm of weights: 3.2419863778027094\n",
      "---------------------\n",
      "Iteration Number: 6442\n",
      "Loss: 12.21682160731169\n",
      "l2 norm of gradients: 0.07188709063898503\n",
      "l2 norm of weights: 3.2420413332642597\n",
      "---------------------\n",
      "Iteration Number: 6443\n",
      "Loss: 12.216178316329628\n",
      "l2 norm of gradients: 0.07186966482225737\n",
      "l2 norm of weights: 3.2420963092530664\n",
      "---------------------\n",
      "Iteration Number: 6444\n",
      "Loss: 12.21553539124537\n",
      "l2 norm of gradients: 0.07185225134642569\n",
      "l2 norm of weights: 3.242151305742824\n",
      "---------------------\n",
      "Iteration Number: 6445\n",
      "Loss: 12.214892831723287\n",
      "l2 norm of gradients: 0.07183485019947468\n",
      "l2 norm of weights: 3.24220632270725\n",
      "---------------------\n",
      "Iteration Number: 6446\n",
      "Loss: 12.214250637428165\n",
      "l2 norm of gradients: 0.07181746136939743\n",
      "l2 norm of weights: 3.2422613601200916\n",
      "---------------------\n",
      "Iteration Number: 6447\n",
      "Loss: 12.213608808025192\n",
      "l2 norm of gradients: 0.07180008484419523\n",
      "l2 norm of weights: 3.242316417955117\n",
      "---------------------\n",
      "Iteration Number: 6448\n",
      "Loss: 12.212967343179956\n",
      "l2 norm of gradients: 0.07178272061187778\n",
      "l2 norm of weights: 3.2423714961861223\n",
      "---------------------\n",
      "Iteration Number: 6449\n",
      "Loss: 12.212326242558458\n",
      "l2 norm of gradients: 0.07176536866046296\n",
      "l2 norm of weights: 3.24242659478693\n",
      "---------------------\n",
      "Iteration Number: 6450\n",
      "Loss: 12.21168550582712\n",
      "l2 norm of gradients: 0.0717480289779771\n",
      "l2 norm of weights: 3.2424817137313857\n",
      "---------------------\n",
      "Iteration Number: 6451\n",
      "Loss: 12.211045132652753\n",
      "l2 norm of gradients: 0.0717307015524548\n",
      "l2 norm of weights: 3.2425368529933625\n",
      "---------------------\n",
      "Iteration Number: 6452\n",
      "Loss: 12.210405122702591\n",
      "l2 norm of gradients: 0.07171338637193904\n",
      "l2 norm of weights: 3.2425920125467584\n",
      "---------------------\n",
      "Iteration Number: 6453\n",
      "Loss: 12.209765475644243\n",
      "l2 norm of gradients: 0.07169608342448115\n",
      "l2 norm of weights: 3.242647192365496\n",
      "---------------------\n",
      "Iteration Number: 6454\n",
      "Loss: 12.209126191145764\n",
      "l2 norm of gradients: 0.07167879269814081\n",
      "l2 norm of weights: 3.242702392423524\n",
      "---------------------\n",
      "Iteration Number: 6455\n",
      "Loss: 12.208487268875569\n",
      "l2 norm of gradients: 0.07166151418098614\n",
      "l2 norm of weights: 3.242757612694817\n",
      "---------------------\n",
      "Iteration Number: 6456\n",
      "Loss: 12.20784870850251\n",
      "l2 norm of gradients: 0.07164424786109364\n",
      "l2 norm of weights: 3.2428128531533744\n",
      "---------------------\n",
      "Iteration Number: 6457\n",
      "Loss: 12.207210509695841\n",
      "l2 norm of gradients: 0.07162699372654818\n",
      "l2 norm of weights: 3.2428681137732207\n",
      "---------------------\n",
      "Iteration Number: 6458\n",
      "Loss: 12.206572672125198\n",
      "l2 norm of gradients: 0.07160975176544312\n",
      "l2 norm of weights: 3.242923394528406\n",
      "---------------------\n",
      "Iteration Number: 6459\n",
      "Loss: 12.205935195460643\n",
      "l2 norm of gradients: 0.07159252196588016\n",
      "l2 norm of weights: 3.242978695393006\n",
      "---------------------\n",
      "Iteration Number: 6460\n",
      "Loss: 12.20529807937262\n",
      "l2 norm of gradients: 0.07157530431596952\n",
      "l2 norm of weights: 3.2430340163411224\n",
      "---------------------\n",
      "Iteration Number: 6461\n",
      "Loss: 12.204661323531981\n",
      "l2 norm of gradients: 0.07155809880382984\n",
      "l2 norm of weights: 3.2430893573468795\n",
      "---------------------\n",
      "Iteration Number: 6462\n",
      "Loss: 12.204024927609995\n",
      "l2 norm of gradients: 0.07154090541758824\n",
      "l2 norm of weights: 3.2431447183844293\n",
      "---------------------\n",
      "Iteration Number: 6463\n",
      "Loss: 12.2033888912783\n",
      "l2 norm of gradients: 0.07152372414538032\n",
      "l2 norm of weights: 3.2432000994279493\n",
      "---------------------\n",
      "Iteration Number: 6464\n",
      "Loss: 12.202753214208963\n",
      "l2 norm of gradients: 0.07150655497535013\n",
      "l2 norm of weights: 3.2432555004516392\n",
      "---------------------\n",
      "Iteration Number: 6465\n",
      "Loss: 12.202117896074443\n",
      "l2 norm of gradients: 0.07148939789565027\n",
      "l2 norm of weights: 3.2433109214297278\n",
      "---------------------\n",
      "Iteration Number: 6466\n",
      "Loss: 12.201482936547581\n",
      "l2 norm of gradients: 0.07147225289444181\n",
      "l2 norm of weights: 3.243366362336466\n",
      "---------------------\n",
      "Iteration Number: 6467\n",
      "Loss: 12.200848335301641\n",
      "l2 norm of gradients: 0.07145511995989438\n",
      "l2 norm of weights: 3.2434218231461323\n",
      "---------------------\n",
      "Iteration Number: 6468\n",
      "Loss: 12.200214092010267\n",
      "l2 norm of gradients: 0.07143799908018607\n",
      "l2 norm of weights: 3.2434773038330276\n",
      "---------------------\n",
      "Iteration Number: 6469\n",
      "Loss: 12.199580206347505\n",
      "l2 norm of gradients: 0.07142089024350357\n",
      "l2 norm of weights: 3.2435328043714797\n",
      "---------------------\n",
      "Iteration Number: 6470\n",
      "Loss: 12.198946677987825\n",
      "l2 norm of gradients: 0.0714037934380421\n",
      "l2 norm of weights: 3.243588324735842\n",
      "---------------------\n",
      "Iteration Number: 6471\n",
      "Loss: 12.198313506606038\n",
      "l2 norm of gradients: 0.07138670865200547\n",
      "l2 norm of weights: 3.243643864900491\n",
      "---------------------\n",
      "Iteration Number: 6472\n",
      "Loss: 12.197680691877403\n",
      "l2 norm of gradients: 0.07136963587360601\n",
      "l2 norm of weights: 3.24369942483983\n",
      "---------------------\n",
      "Iteration Number: 6473\n",
      "Loss: 12.197048233477549\n",
      "l2 norm of gradients: 0.07135257509106466\n",
      "l2 norm of weights: 3.243755004528286\n",
      "---------------------\n",
      "Iteration Number: 6474\n",
      "Loss: 12.196416131082504\n",
      "l2 norm of gradients: 0.07133552629261093\n",
      "l2 norm of weights: 3.243810603940312\n",
      "---------------------\n",
      "Iteration Number: 6475\n",
      "Loss: 12.195784384368693\n",
      "l2 norm of gradients: 0.07131848946648296\n",
      "l2 norm of weights: 3.2438662230503854\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 6476\n",
      "Loss: 12.195152993012934\n",
      "l2 norm of gradients: 0.0713014646009275\n",
      "l2 norm of weights: 3.2439218618330092\n",
      "---------------------\n",
      "Iteration Number: 6477\n",
      "Loss: 12.194521956692432\n",
      "l2 norm of gradients: 0.07128445168419989\n",
      "l2 norm of weights: 3.2439775202627104\n",
      "---------------------\n",
      "Iteration Number: 6478\n",
      "Loss: 12.193891275084802\n",
      "l2 norm of gradients: 0.07126745070456415\n",
      "l2 norm of weights: 3.2440331983140416\n",
      "---------------------\n",
      "Iteration Number: 6479\n",
      "Loss: 12.193260947868025\n",
      "l2 norm of gradients: 0.0712504616502929\n",
      "l2 norm of weights: 3.2440888959615797\n",
      "---------------------\n",
      "Iteration Number: 6480\n",
      "Loss: 12.192630974720519\n",
      "l2 norm of gradients: 0.0712334845096674\n",
      "l2 norm of weights: 3.244144613179927\n",
      "---------------------\n",
      "Iteration Number: 6481\n",
      "Loss: 12.192001355321047\n",
      "l2 norm of gradients: 0.07121651927097761\n",
      "l2 norm of weights: 3.2442003499437106\n",
      "---------------------\n",
      "Iteration Number: 6482\n",
      "Loss: 12.191372089348784\n",
      "l2 norm of gradients: 0.0711995659225221\n",
      "l2 norm of weights: 3.244256106227582\n",
      "---------------------\n",
      "Iteration Number: 6483\n",
      "Loss: 12.190743176483291\n",
      "l2 norm of gradients: 0.07118262445260823\n",
      "l2 norm of weights: 3.2443118820062176\n",
      "---------------------\n",
      "Iteration Number: 6484\n",
      "Loss: 12.190114616404525\n",
      "l2 norm of gradients: 0.07116569484955186\n",
      "l2 norm of weights: 3.244367677254319\n",
      "---------------------\n",
      "Iteration Number: 6485\n",
      "Loss: 12.189486408792845\n",
      "l2 norm of gradients: 0.07114877710167775\n",
      "l2 norm of weights: 3.2444234919466126\n",
      "---------------------\n",
      "Iteration Number: 6486\n",
      "Loss: 12.188858553328954\n",
      "l2 norm of gradients: 0.07113187119731922\n",
      "l2 norm of weights: 3.244479326057849\n",
      "---------------------\n",
      "Iteration Number: 6487\n",
      "Loss: 12.188231049694002\n",
      "l2 norm of gradients: 0.07111497712481839\n",
      "l2 norm of weights: 3.2445351795628032\n",
      "---------------------\n",
      "Iteration Number: 6488\n",
      "Loss: 12.187603897569481\n",
      "l2 norm of gradients: 0.07109809487252605\n",
      "l2 norm of weights: 3.244591052436276\n",
      "---------------------\n",
      "Iteration Number: 6489\n",
      "Loss: 12.186977096637293\n",
      "l2 norm of gradients: 0.07108122442880177\n",
      "l2 norm of weights: 3.2446469446530917\n",
      "---------------------\n",
      "Iteration Number: 6490\n",
      "Loss: 12.186350646579736\n",
      "l2 norm of gradients: 0.07106436578201378\n",
      "l2 norm of weights: 3.244702856188101\n",
      "---------------------\n",
      "Iteration Number: 6491\n",
      "Loss: 12.185724547079458\n",
      "l2 norm of gradients: 0.07104751892053915\n",
      "l2 norm of weights: 3.2447587870161767\n",
      "---------------------\n",
      "Iteration Number: 6492\n",
      "Loss: 12.18509879781954\n",
      "l2 norm of gradients: 0.07103068383276366\n",
      "l2 norm of weights: 3.2448147371122182\n",
      "---------------------\n",
      "Iteration Number: 6493\n",
      "Loss: 12.18447339848342\n",
      "l2 norm of gradients: 0.07101386050708185\n",
      "l2 norm of weights: 3.244870706451149\n",
      "---------------------\n",
      "Iteration Number: 6494\n",
      "Loss: 12.183848348754932\n",
      "l2 norm of gradients: 0.07099704893189704\n",
      "l2 norm of weights: 3.2449266950079165\n",
      "---------------------\n",
      "Iteration Number: 6495\n",
      "Loss: 12.183223648318286\n",
      "l2 norm of gradients: 0.07098024909562138\n",
      "l2 norm of weights: 3.244982702757494\n",
      "---------------------\n",
      "Iteration Number: 6496\n",
      "Loss: 12.182599296858083\n",
      "l2 norm of gradients: 0.07096346098667577\n",
      "l2 norm of weights: 3.245038729674877\n",
      "---------------------\n",
      "Iteration Number: 6497\n",
      "Loss: 12.181975294059303\n",
      "l2 norm of gradients: 0.07094668459348986\n",
      "l2 norm of weights: 3.2450947757350876\n",
      "---------------------\n",
      "Iteration Number: 6498\n",
      "Loss: 12.18135163960733\n",
      "l2 norm of gradients: 0.07092991990450219\n",
      "l2 norm of weights: 3.2451508409131726\n",
      "---------------------\n",
      "Iteration Number: 6499\n",
      "Loss: 12.18072833318789\n",
      "l2 norm of gradients: 0.07091316690816012\n",
      "l2 norm of weights: 3.2452069251842013\n",
      "---------------------\n",
      "Iteration Number: 6500\n",
      "Loss: 12.180105374487132\n",
      "l2 norm of gradients: 0.07089642559291973\n",
      "l2 norm of weights: 3.245263028523269\n",
      "---------------------\n",
      "Iteration Number: 6501\n",
      "Loss: 12.179482763191565\n",
      "l2 norm of gradients: 0.07087969594724607\n",
      "l2 norm of weights: 3.2453191509054946\n",
      "---------------------\n",
      "Iteration Number: 6502\n",
      "Loss: 12.1788604989881\n",
      "l2 norm of gradients: 0.0708629779596129\n",
      "l2 norm of weights: 3.2453752923060217\n",
      "---------------------\n",
      "Iteration Number: 6503\n",
      "Loss: 12.17823858156399\n",
      "l2 norm of gradients: 0.07084627161850292\n",
      "l2 norm of weights: 3.2454314527000183\n",
      "---------------------\n",
      "Iteration Number: 6504\n",
      "Loss: 12.177617010606921\n",
      "l2 norm of gradients: 0.07082957691240763\n",
      "l2 norm of weights: 3.2454876320626767\n",
      "---------------------\n",
      "Iteration Number: 6505\n",
      "Loss: 12.17699578580492\n",
      "l2 norm of gradients: 0.07081289382982743\n",
      "l2 norm of weights: 3.2455438303692135\n",
      "---------------------\n",
      "Iteration Number: 6506\n",
      "Loss: 12.176374906846393\n",
      "l2 norm of gradients: 0.07079622235927156\n",
      "l2 norm of weights: 3.24560004759487\n",
      "---------------------\n",
      "Iteration Number: 6507\n",
      "Loss: 12.175754373420153\n",
      "l2 norm of gradients: 0.07077956248925818\n",
      "l2 norm of weights: 3.2456562837149106\n",
      "---------------------\n",
      "Iteration Number: 6508\n",
      "Loss: 12.175134185215374\n",
      "l2 norm of gradients: 0.07076291420831424\n",
      "l2 norm of weights: 3.245712538704625\n",
      "---------------------\n",
      "Iteration Number: 6509\n",
      "Loss: 12.174514341921617\n",
      "l2 norm of gradients: 0.07074627750497571\n",
      "l2 norm of weights: 3.2457688125393274\n",
      "---------------------\n",
      "Iteration Number: 6510\n",
      "Loss: 12.173894843228803\n",
      "l2 norm of gradients: 0.07072965236778736\n",
      "l2 norm of weights: 3.245825105194355\n",
      "---------------------\n",
      "Iteration Number: 6511\n",
      "Loss: 12.173275688827243\n",
      "l2 norm of gradients: 0.0707130387853029\n",
      "l2 norm of weights: 3.2458814166450702\n",
      "---------------------\n",
      "Iteration Number: 6512\n",
      "Loss: 12.172656878407636\n",
      "l2 norm of gradients: 0.07069643674608497\n",
      "l2 norm of weights: 3.2459377468668595\n",
      "---------------------\n",
      "Iteration Number: 6513\n",
      "Loss: 12.172038411661042\n",
      "l2 norm of gradients: 0.07067984623870513\n",
      "l2 norm of weights: 3.2459940958351328\n",
      "---------------------\n",
      "Iteration Number: 6514\n",
      "Loss: 12.171420288278892\n",
      "l2 norm of gradients: 0.07066326725174381\n",
      "l2 norm of weights: 3.2460504635253247\n",
      "---------------------\n",
      "Iteration Number: 6515\n",
      "Loss: 12.170802507953004\n",
      "l2 norm of gradients: 0.07064669977379044\n",
      "l2 norm of weights: 3.246106849912894\n",
      "---------------------\n",
      "Iteration Number: 6516\n",
      "Loss: 12.170185070375574\n",
      "l2 norm of gradients: 0.07063014379344335\n",
      "l2 norm of weights: 3.2461632549733235\n",
      "---------------------\n",
      "Iteration Number: 6517\n",
      "Loss: 12.169567975239165\n",
      "l2 norm of gradients: 0.07061359929930984\n",
      "l2 norm of weights: 3.2462196786821194\n",
      "---------------------\n",
      "Iteration Number: 6518\n",
      "Loss: 12.168951222236716\n",
      "l2 norm of gradients: 0.0705970662800062\n",
      "l2 norm of weights: 3.2462761210148128\n",
      "---------------------\n",
      "Iteration Number: 6519\n",
      "Loss: 12.168334811061529\n",
      "l2 norm of gradients: 0.07058054472415755\n",
      "l2 norm of weights: 3.2463325819469593\n",
      "---------------------\n",
      "Iteration Number: 6520\n",
      "Loss: 12.167718741407306\n",
      "l2 norm of gradients: 0.07056403462039817\n",
      "l2 norm of weights: 3.2463890614541366\n",
      "---------------------\n",
      "Iteration Number: 6521\n",
      "Loss: 12.16710301296809\n",
      "l2 norm of gradients: 0.0705475359573711\n",
      "l2 norm of weights: 3.2464455595119475\n",
      "---------------------\n",
      "Iteration Number: 6522\n",
      "Loss: 12.166487625438322\n",
      "l2 norm of gradients: 0.07053104872372859\n",
      "l2 norm of weights: 3.2465020760960197\n",
      "---------------------\n",
      "Iteration Number: 6523\n",
      "Loss: 12.165872578512788\n",
      "l2 norm of gradients: 0.07051457290813164\n",
      "l2 norm of weights: 3.2465586111820035\n",
      "---------------------\n",
      "Iteration Number: 6524\n",
      "Loss: 12.165257871886688\n",
      "l2 norm of gradients: 0.07049810849925042\n",
      "l2 norm of weights: 3.246615164745573\n",
      "---------------------\n",
      "Iteration Number: 6525\n",
      "Loss: 12.164643505255544\n",
      "l2 norm of gradients: 0.07048165548576404\n",
      "l2 norm of weights: 3.2466717367624267\n",
      "---------------------\n",
      "Iteration Number: 6526\n",
      "Loss: 12.16402947831528\n",
      "l2 norm of gradients: 0.07046521385636062\n",
      "l2 norm of weights: 3.2467283272082876\n",
      "---------------------\n",
      "Iteration Number: 6527\n",
      "Loss: 12.16341579076217\n",
      "l2 norm of gradients: 0.07044878359973725\n",
      "l2 norm of weights: 3.2467849360589005\n",
      "---------------------\n",
      "Iteration Number: 6528\n",
      "Loss: 12.162802442292888\n",
      "l2 norm of gradients: 0.07043236470460007\n",
      "l2 norm of weights: 3.2468415632900376\n",
      "---------------------\n",
      "Iteration Number: 6529\n",
      "Loss: 12.162189432604425\n",
      "l2 norm of gradients: 0.07041595715966428\n",
      "l2 norm of weights: 3.2468982088774907\n",
      "---------------------\n",
      "Iteration Number: 6530\n",
      "Loss: 12.161576761394196\n",
      "l2 norm of gradients: 0.07039956095365404\n",
      "l2 norm of weights: 3.2469548727970787\n",
      "---------------------\n",
      "Iteration Number: 6531\n",
      "Loss: 12.160964428359954\n",
      "l2 norm of gradients: 0.07038317607530258\n",
      "l2 norm of weights: 3.247011555024642\n",
      "---------------------\n",
      "Iteration Number: 6532\n",
      "Loss: 12.160352433199833\n",
      "l2 norm of gradients: 0.07036680251335219\n",
      "l2 norm of weights: 3.2470682555360457\n",
      "---------------------\n",
      "Iteration Number: 6533\n",
      "Loss: 12.159740775612306\n",
      "l2 norm of gradients: 0.07035044025655417\n",
      "l2 norm of weights: 3.2471249743071793\n",
      "---------------------\n",
      "Iteration Number: 6534\n",
      "Loss: 12.15912945529626\n",
      "l2 norm of gradients: 0.07033408929366886\n",
      "l2 norm of weights: 3.2471817113139547\n",
      "---------------------\n",
      "Iteration Number: 6535\n",
      "Loss: 12.158518471950886\n",
      "l2 norm of gradients: 0.07031774961346568\n",
      "l2 norm of weights: 3.2472384665323086\n",
      "---------------------\n",
      "Iteration Number: 6536\n",
      "Loss: 12.157907825275807\n",
      "l2 norm of gradients: 0.07030142120472316\n",
      "l2 norm of weights: 3.2472952399382\n",
      "---------------------\n",
      "Iteration Number: 6537\n",
      "Loss: 12.157297514970965\n",
      "l2 norm of gradients: 0.07028510405622879\n",
      "l2 norm of weights: 3.2473520315076128\n",
      "---------------------\n",
      "Iteration Number: 6538\n",
      "Loss: 12.156687540736685\n",
      "l2 norm of gradients: 0.07026879815677924\n",
      "l2 norm of weights: 3.2474088412165543\n",
      "---------------------\n",
      "Iteration Number: 6539\n",
      "Loss: 12.156077902273655\n",
      "l2 norm of gradients: 0.07025250349518017\n",
      "l2 norm of weights: 3.2474656690410546\n",
      "---------------------\n",
      "Iteration Number: 6540\n",
      "Loss: 12.155468599282925\n",
      "l2 norm of gradients: 0.07023622006024638\n",
      "l2 norm of weights: 3.2475225149571676\n",
      "---------------------\n",
      "Iteration Number: 6541\n",
      "Loss: 12.154859631465895\n",
      "l2 norm of gradients: 0.07021994784080175\n",
      "l2 norm of weights: 3.247579378940972\n",
      "---------------------\n",
      "Iteration Number: 6542\n",
      "Loss: 12.154250998524356\n",
      "l2 norm of gradients: 0.07020368682567925\n",
      "l2 norm of weights: 3.247636260968568\n",
      "---------------------\n",
      "Iteration Number: 6543\n",
      "Loss: 12.153642700160452\n",
      "l2 norm of gradients: 0.07018743700372093\n",
      "l2 norm of weights: 3.2476931610160817\n",
      "---------------------\n",
      "Iteration Number: 6544\n",
      "Loss: 12.153034736076663\n",
      "l2 norm of gradients: 0.07017119836377794\n",
      "l2 norm of weights: 3.2477500790596596\n",
      "---------------------\n",
      "Iteration Number: 6545\n",
      "Loss: 12.152427105975868\n",
      "l2 norm of gradients: 0.07015497089471058\n",
      "l2 norm of weights: 3.247807015075475\n",
      "---------------------\n",
      "Iteration Number: 6546\n",
      "Loss: 12.151819809561275\n",
      "l2 norm of gradients: 0.07013875458538822\n",
      "l2 norm of weights: 3.247863969039722\n",
      "---------------------\n",
      "Iteration Number: 6547\n",
      "Loss: 12.151212846536467\n",
      "l2 norm of gradients: 0.07012254942468937\n",
      "l2 norm of weights: 3.2479209409286187\n",
      "---------------------\n",
      "Iteration Number: 6548\n",
      "Loss: 12.150606216605405\n",
      "l2 norm of gradients: 0.07010635540150167\n",
      "l2 norm of weights: 3.2479779307184082\n",
      "---------------------\n",
      "Iteration Number: 6549\n",
      "Loss: 12.149999919472387\n",
      "l2 norm of gradients: 0.07009017250472188\n",
      "l2 norm of weights: 3.248034938385355\n",
      "---------------------\n",
      "Iteration Number: 6550\n",
      "Loss: 12.149393954842056\n",
      "l2 norm of gradients: 0.07007400072325584\n",
      "l2 norm of weights: 3.2480919639057477\n",
      "---------------------\n",
      "Iteration Number: 6551\n",
      "Loss: 12.14878832241947\n",
      "l2 norm of gradients: 0.07005784004601859\n",
      "l2 norm of weights: 3.2481490072558987\n",
      "---------------------\n",
      "Iteration Number: 6552\n",
      "Loss: 12.148183021909958\n",
      "l2 norm of gradients: 0.07004169046193431\n",
      "l2 norm of weights: 3.2482060684121428\n",
      "---------------------\n",
      "Iteration Number: 6553\n",
      "Loss: 12.147578053019302\n",
      "l2 norm of gradients: 0.07002555195993632\n",
      "l2 norm of weights: 3.2482631473508388\n",
      "---------------------\n",
      "Iteration Number: 6554\n",
      "Loss: 12.14697341545358\n",
      "l2 norm of gradients: 0.07000942452896702\n",
      "l2 norm of weights: 3.2483202440483687\n",
      "---------------------\n",
      "Iteration Number: 6555\n",
      "Loss: 12.146369108919236\n",
      "l2 norm of gradients: 0.06999330815797805\n",
      "l2 norm of weights: 3.248377358481137\n",
      "---------------------\n",
      "Iteration Number: 6556\n",
      "Loss: 12.145765133123097\n",
      "l2 norm of gradients: 0.06997720283593017\n",
      "l2 norm of weights: 3.248434490625572\n",
      "---------------------\n",
      "Iteration Number: 6557\n",
      "Loss: 12.145161487772299\n",
      "l2 norm of gradients: 0.06996110855179329\n",
      "l2 norm of weights: 3.248491640458125\n",
      "---------------------\n",
      "Iteration Number: 6558\n",
      "Loss: 12.144558172574381\n",
      "l2 norm of gradients: 0.06994502529454656\n",
      "l2 norm of weights: 3.2485488079552716\n",
      "---------------------\n",
      "Iteration Number: 6559\n",
      "Loss: 12.143955187237223\n",
      "l2 norm of gradients: 0.06992895305317814\n",
      "l2 norm of weights: 3.2486059930935083\n",
      "---------------------\n",
      "Iteration Number: 6560\n",
      "Loss: 12.143352531469029\n",
      "l2 norm of gradients: 0.06991289181668556\n",
      "l2 norm of weights: 3.2486631958493573\n",
      "---------------------\n",
      "Iteration Number: 6561\n",
      "Loss: 12.142750204978409\n",
      "l2 norm of gradients: 0.06989684157407534\n",
      "l2 norm of weights: 3.2487204161993612\n",
      "---------------------\n",
      "Iteration Number: 6562\n",
      "Loss: 12.142148207474294\n",
      "l2 norm of gradients: 0.06988080231436339\n",
      "l2 norm of weights: 3.2487776541200875\n",
      "---------------------\n",
      "Iteration Number: 6563\n",
      "Loss: 12.14154653866596\n",
      "l2 norm of gradients: 0.0698647740265746\n",
      "l2 norm of weights: 3.248834909588127\n",
      "---------------------\n",
      "Iteration Number: 6564\n",
      "Loss: 12.140945198263069\n",
      "l2 norm of gradients: 0.06984875669974315\n",
      "l2 norm of weights: 3.2488921825800925\n",
      "---------------------\n",
      "Iteration Number: 6565\n",
      "Loss: 12.140344185975597\n",
      "l2 norm of gradients: 0.06983275032291243\n",
      "l2 norm of weights: 3.2489494730726203\n",
      "---------------------\n",
      "Iteration Number: 6566\n",
      "Loss: 12.13974350151391\n",
      "l2 norm of gradients: 0.06981675488513495\n",
      "l2 norm of weights: 3.249006781042369\n",
      "---------------------\n",
      "Iteration Number: 6567\n",
      "Loss: 12.1391431445887\n",
      "l2 norm of gradients: 0.06980077037547247\n",
      "l2 norm of weights: 3.2490641064660224\n",
      "---------------------\n",
      "Iteration Number: 6568\n",
      "Loss: 12.138543114911018\n",
      "l2 norm of gradients: 0.06978479678299597\n",
      "l2 norm of weights: 3.249121449320284\n",
      "---------------------\n",
      "Iteration Number: 6569\n",
      "Loss: 12.137943412192264\n",
      "l2 norm of gradients: 0.0697688340967856\n",
      "l2 norm of weights: 3.249178809581882\n",
      "---------------------\n",
      "Iteration Number: 6570\n",
      "Loss: 12.137344036144189\n",
      "l2 norm of gradients: 0.0697528823059307\n",
      "l2 norm of weights: 3.2492361872275684\n",
      "---------------------\n",
      "Iteration Number: 6571\n",
      "Loss: 12.136744986478881\n",
      "l2 norm of gradients: 0.06973694139952992\n",
      "l2 norm of weights: 3.2492935822341167\n",
      "---------------------\n",
      "Iteration Number: 6572\n",
      "Loss: 12.136146262908806\n",
      "l2 norm of gradients: 0.06972101136669097\n",
      "l2 norm of weights: 3.249350994578324\n",
      "---------------------\n",
      "Iteration Number: 6573\n",
      "Loss: 12.135547865146766\n",
      "l2 norm of gradients: 0.06970509219653094\n",
      "l2 norm of weights: 3.249408424237009\n",
      "---------------------\n",
      "Iteration Number: 6574\n",
      "Loss: 12.134949792905891\n",
      "l2 norm of gradients: 0.06968918387817605\n",
      "l2 norm of weights: 3.2494658711870152\n",
      "---------------------\n",
      "Iteration Number: 6575\n",
      "Loss: 12.134352045899687\n",
      "l2 norm of gradients: 0.06967328640076173\n",
      "l2 norm of weights: 3.2495233354052067\n",
      "---------------------\n",
      "Iteration Number: 6576\n",
      "Loss: 12.13375462384199\n",
      "l2 norm of gradients: 0.06965739975343274\n",
      "l2 norm of weights: 3.249580816868473\n",
      "---------------------\n",
      "Iteration Number: 6577\n",
      "Loss: 12.133157526446995\n",
      "l2 norm of gradients: 0.06964152392534298\n",
      "l2 norm of weights: 3.2496383155537236\n",
      "---------------------\n",
      "Iteration Number: 6578\n",
      "Loss: 12.132560753429235\n",
      "l2 norm of gradients: 0.06962565890565556\n",
      "l2 norm of weights: 3.2496958314378936\n",
      "---------------------\n",
      "Iteration Number: 6579\n",
      "Loss: 12.131964304503594\n",
      "l2 norm of gradients: 0.06960980468354294\n",
      "l2 norm of weights: 3.2497533644979377\n",
      "---------------------\n",
      "Iteration Number: 6580\n",
      "Loss: 12.131368179385294\n",
      "l2 norm of gradients: 0.0695939612481867\n",
      "l2 norm of weights: 3.2498109147108356\n",
      "---------------------\n",
      "Iteration Number: 6581\n",
      "Loss: 12.130772377789913\n",
      "l2 norm of gradients: 0.06957812858877777\n",
      "l2 norm of weights: 3.249868482053589\n",
      "---------------------\n",
      "Iteration Number: 6582\n",
      "Loss: 12.130176899433362\n",
      "l2 norm of gradients: 0.06956230669451623\n",
      "l2 norm of weights: 3.2499260665032224\n",
      "---------------------\n",
      "Iteration Number: 6583\n",
      "Loss: 12.129581744031919\n",
      "l2 norm of gradients: 0.06954649555461147\n",
      "l2 norm of weights: 3.249983668036782\n",
      "---------------------\n",
      "Iteration Number: 6584\n",
      "Loss: 12.128986911302169\n",
      "l2 norm of gradients: 0.06953069515828214\n",
      "l2 norm of weights: 3.250041286631338\n",
      "---------------------\n",
      "Iteration Number: 6585\n",
      "Loss: 12.128392400961072\n",
      "l2 norm of gradients: 0.06951490549475606\n",
      "l2 norm of weights: 3.250098922263983\n",
      "---------------------\n",
      "Iteration Number: 6586\n",
      "Loss: 12.127798212725912\n",
      "l2 norm of gradients: 0.06949912655327038\n",
      "l2 norm of weights: 3.250156574911831\n",
      "---------------------\n",
      "Iteration Number: 6587\n",
      "Loss: 12.127204346314338\n",
      "l2 norm of gradients: 0.06948335832307151\n",
      "l2 norm of weights: 3.2502142445520192\n",
      "---------------------\n",
      "Iteration Number: 6588\n",
      "Loss: 12.126610801444318\n",
      "l2 norm of gradients: 0.06946760079341507\n",
      "l2 norm of weights: 3.2502719311617083\n",
      "---------------------\n",
      "Iteration Number: 6589\n",
      "Loss: 12.126017577834162\n",
      "l2 norm of gradients: 0.06945185395356597\n",
      "l2 norm of weights: 3.2503296347180797\n",
      "---------------------\n",
      "Iteration Number: 6590\n",
      "Loss: 12.125424675202536\n",
      "l2 norm of gradients: 0.06943611779279843\n",
      "l2 norm of weights: 3.2503873551983387\n",
      "---------------------\n",
      "Iteration Number: 6591\n",
      "Loss: 12.124832093268461\n",
      "l2 norm of gradients: 0.06942039230039583\n",
      "l2 norm of weights: 3.250445092579713\n",
      "---------------------\n",
      "Iteration Number: 6592\n",
      "Loss: 12.12423983175125\n",
      "l2 norm of gradients: 0.06940467746565093\n",
      "l2 norm of weights: 3.2505028468394515\n",
      "---------------------\n",
      "Iteration Number: 6593\n",
      "Loss: 12.123647890370583\n",
      "l2 norm of gradients: 0.06938897327786565\n",
      "l2 norm of weights: 3.2505606179548265\n",
      "---------------------\n",
      "Iteration Number: 6594\n",
      "Loss: 12.123056268846497\n",
      "l2 norm of gradients: 0.06937327972635132\n",
      "l2 norm of weights: 3.250618405903133\n",
      "---------------------\n",
      "Iteration Number: 6595\n",
      "Loss: 12.122464966899349\n",
      "l2 norm of gradients: 0.06935759680042843\n",
      "l2 norm of weights: 3.2506762106616875\n",
      "---------------------\n",
      "Iteration Number: 6596\n",
      "Loss: 12.121873984249838\n",
      "l2 norm of gradients: 0.0693419244894268\n",
      "l2 norm of weights: 3.2507340322078297\n",
      "---------------------\n",
      "Iteration Number: 6597\n",
      "Loss: 12.121283320618977\n",
      "l2 norm of gradients: 0.06932626278268547\n",
      "l2 norm of weights: 3.2507918705189214\n",
      "---------------------\n",
      "Iteration Number: 6598\n",
      "Loss: 12.120692975728165\n",
      "l2 norm of gradients: 0.06931061166955285\n",
      "l2 norm of weights: 3.250849725572346\n",
      "---------------------\n",
      "Iteration Number: 6599\n",
      "Loss: 12.1201029492991\n",
      "l2 norm of gradients: 0.0692949711393866\n",
      "l2 norm of weights: 3.25090759734551\n",
      "---------------------\n",
      "Iteration Number: 6600\n",
      "Loss: 12.119513241053848\n",
      "l2 norm of gradients: 0.06927934118155363\n",
      "l2 norm of weights: 3.2509654858158417\n",
      "---------------------\n",
      "Iteration Number: 6601\n",
      "Loss: 12.118923850714767\n",
      "l2 norm of gradients: 0.0692637217854302\n",
      "l2 norm of weights: 3.251023390960792\n",
      "---------------------\n",
      "Iteration Number: 6602\n",
      "Loss: 12.118334778004591\n",
      "l2 norm of gradients: 0.06924811294040173\n",
      "l2 norm of weights: 3.2510813127578344\n",
      "---------------------\n",
      "Iteration Number: 6603\n",
      "Loss: 12.117746022646372\n",
      "l2 norm of gradients: 0.0692325146358631\n",
      "l2 norm of weights: 3.2511392511844632\n",
      "---------------------\n",
      "Iteration Number: 6604\n",
      "Loss: 12.117157584363513\n",
      "l2 norm of gradients: 0.06921692686121839\n",
      "l2 norm of weights: 3.2511972062181966\n",
      "---------------------\n",
      "Iteration Number: 6605\n",
      "Loss: 12.116569462879717\n",
      "l2 norm of gradients: 0.06920134960588095\n",
      "l2 norm of weights: 3.251255177836574\n",
      "---------------------\n",
      "Iteration Number: 6606\n",
      "Loss: 12.115981657919065\n",
      "l2 norm of gradients: 0.06918578285927346\n",
      "l2 norm of weights: 3.251313166017157\n",
      "---------------------\n",
      "Iteration Number: 6607\n",
      "Loss: 12.115394169205937\n",
      "l2 norm of gradients: 0.06917022661082789\n",
      "l2 norm of weights: 3.2513711707375292\n",
      "---------------------\n",
      "Iteration Number: 6608\n",
      "Loss: 12.114806996465065\n",
      "l2 norm of gradients: 0.0691546808499855\n",
      "l2 norm of weights: 3.2514291919752965\n",
      "---------------------\n",
      "Iteration Number: 6609\n",
      "Loss: 12.114220139421516\n",
      "l2 norm of gradients: 0.06913914556619691\n",
      "l2 norm of weights: 3.2514872297080877\n",
      "---------------------\n",
      "Iteration Number: 6610\n",
      "Loss: 12.113633597800671\n",
      "l2 norm of gradients: 0.06912362074892191\n",
      "l2 norm of weights: 3.2515452839135524\n",
      "---------------------\n",
      "Iteration Number: 6611\n",
      "Loss: 12.113047371328252\n",
      "l2 norm of gradients: 0.0691081063876297\n",
      "l2 norm of weights: 3.251603354569362\n",
      "---------------------\n",
      "Iteration Number: 6612\n",
      "Loss: 12.112461459730321\n",
      "l2 norm of gradients: 0.06909260247179873\n",
      "l2 norm of weights: 3.251661441653212\n",
      "---------------------\n",
      "Iteration Number: 6613\n",
      "Loss: 12.11187586273327\n",
      "l2 norm of gradients: 0.06907710899091679\n",
      "l2 norm of weights: 3.251719545142818\n",
      "---------------------\n",
      "Iteration Number: 6614\n",
      "Loss: 12.111290580063816\n",
      "l2 norm of gradients: 0.06906162593448094\n",
      "l2 norm of weights: 3.2517776650159176\n",
      "---------------------\n",
      "Iteration Number: 6615\n",
      "Loss: 12.11070561144899\n",
      "l2 norm of gradients: 0.06904615329199758\n",
      "l2 norm of weights: 3.2518358012502717\n",
      "---------------------\n",
      "Iteration Number: 6616\n",
      "Loss: 12.110120956616193\n",
      "l2 norm of gradients: 0.06903069105298233\n",
      "l2 norm of weights: 3.2518939538236618\n",
      "---------------------\n",
      "Iteration Number: 6617\n",
      "Loss: 12.109536615293122\n",
      "l2 norm of gradients: 0.06901523920696023\n",
      "l2 norm of weights: 3.2519521227138917\n",
      "---------------------\n",
      "Iteration Number: 6618\n",
      "Loss: 12.108952587207813\n",
      "l2 norm of gradients: 0.0689997977434656\n",
      "l2 norm of weights: 3.252010307898787\n",
      "---------------------\n",
      "Iteration Number: 6619\n",
      "Loss: 12.108368872088638\n",
      "l2 norm of gradients: 0.06898436665204197\n",
      "l2 norm of weights: 3.2520685093561967\n",
      "---------------------\n",
      "Iteration Number: 6620\n",
      "Loss: 12.107785469664277\n",
      "l2 norm of gradients: 0.06896894592224231\n",
      "l2 norm of weights: 3.2521267270639895\n",
      "---------------------\n",
      "Iteration Number: 6621\n",
      "Loss: 12.107202379663779\n",
      "l2 norm of gradients: 0.06895353554362883\n",
      "l2 norm of weights: 3.2521849610000557\n",
      "---------------------\n",
      "Iteration Number: 6622\n",
      "Loss: 12.106619601816453\n",
      "l2 norm of gradients: 0.06893813550577305\n",
      "l2 norm of weights: 3.2522432111423103\n",
      "---------------------\n",
      "Iteration Number: 6623\n",
      "Loss: 12.106037135852004\n",
      "l2 norm of gradients: 0.06892274579825586\n",
      "l2 norm of weights: 3.2523014774686874\n",
      "---------------------\n",
      "Iteration Number: 6624\n",
      "Loss: 12.105454981500436\n",
      "l2 norm of gradients: 0.06890736641066736\n",
      "l2 norm of weights: 3.2523597599571437\n",
      "---------------------\n",
      "Iteration Number: 6625\n",
      "Loss: 12.104873138492051\n",
      "l2 norm of gradients: 0.06889199733260706\n",
      "l2 norm of weights: 3.2524180585856572\n",
      "---------------------\n",
      "Iteration Number: 6626\n",
      "Loss: 12.104291606557526\n",
      "l2 norm of gradients: 0.06887663855368371\n",
      "l2 norm of weights: 3.2524763733322293\n",
      "---------------------\n",
      "Iteration Number: 6627\n",
      "Loss: 12.103710385427833\n",
      "l2 norm of gradients: 0.06886129006351546\n",
      "l2 norm of weights: 3.2525347041748804\n",
      "---------------------\n",
      "Iteration Number: 6628\n",
      "Loss: 12.103129474834285\n",
      "l2 norm of gradients: 0.06884595185172969\n",
      "l2 norm of weights: 3.2525930510916554\n",
      "---------------------\n",
      "Iteration Number: 6629\n",
      "Loss: 12.1025488745085\n",
      "l2 norm of gradients: 0.06883062390796313\n",
      "l2 norm of weights: 3.2526514140606193\n",
      "---------------------\n",
      "Iteration Number: 6630\n",
      "Loss: 12.101968584182421\n",
      "l2 norm of gradients: 0.06881530622186177\n",
      "l2 norm of weights: 3.2527097930598576\n",
      "---------------------\n",
      "Iteration Number: 6631\n",
      "Loss: 12.101388603588351\n",
      "l2 norm of gradients: 0.06879999878308103\n",
      "l2 norm of weights: 3.252768188067481\n",
      "---------------------\n",
      "Iteration Number: 6632\n",
      "Loss: 12.100808932458857\n",
      "l2 norm of gradients: 0.06878470158128556\n",
      "l2 norm of weights: 3.252826599061617\n",
      "---------------------\n",
      "Iteration Number: 6633\n",
      "Loss: 12.100229570526885\n",
      "l2 norm of gradients: 0.06876941460614938\n",
      "l2 norm of weights: 3.25288502602042\n",
      "---------------------\n",
      "Iteration Number: 6634\n",
      "Loss: 12.099650517525667\n",
      "l2 norm of gradients: 0.06875413784735573\n",
      "l2 norm of weights: 3.2529434689220613\n",
      "---------------------\n",
      "Iteration Number: 6635\n",
      "Loss: 12.09907177318878\n",
      "l2 norm of gradients: 0.06873887129459726\n",
      "l2 norm of weights: 3.253001927744736\n",
      "---------------------\n",
      "Iteration Number: 6636\n",
      "Loss: 12.09849333725009\n",
      "l2 norm of gradients: 0.06872361493757587\n",
      "l2 norm of weights: 3.2530604024666605\n",
      "---------------------\n",
      "Iteration Number: 6637\n",
      "Loss: 12.09791520944383\n",
      "l2 norm of gradients: 0.06870836876600286\n",
      "l2 norm of weights: 3.2531188930660733\n",
      "---------------------\n",
      "Iteration Number: 6638\n",
      "Loss: 12.09733738950452\n",
      "l2 norm of gradients: 0.06869313276959876\n",
      "l2 norm of weights: 3.2531773995212325\n",
      "---------------------\n",
      "Iteration Number: 6639\n",
      "Loss: 12.096759877166999\n",
      "l2 norm of gradients: 0.06867790693809343\n",
      "l2 norm of weights: 3.2532359218104188\n",
      "---------------------\n",
      "Iteration Number: 6640\n",
      "Loss: 12.096182672166444\n",
      "l2 norm of gradients: 0.06866269126122614\n",
      "l2 norm of weights: 3.2532944599119347\n",
      "---------------------\n",
      "Iteration Number: 6641\n",
      "Loss: 12.095605774238342\n",
      "l2 norm of gradients: 0.06864748572874531\n",
      "l2 norm of weights: 3.2533530138041042\n",
      "---------------------\n",
      "Iteration Number: 6642\n",
      "Loss: 12.095029183118506\n",
      "l2 norm of gradients: 0.06863229033040885\n",
      "l2 norm of weights: 3.253411583465271\n",
      "---------------------\n",
      "Iteration Number: 6643\n",
      "Loss: 12.094452898543038\n",
      "l2 norm of gradients: 0.06861710505598384\n",
      "l2 norm of weights: 3.2534701688738026\n",
      "---------------------\n",
      "Iteration Number: 6644\n",
      "Loss: 12.093876920248416\n",
      "l2 norm of gradients: 0.06860192989524677\n",
      "l2 norm of weights: 3.2535287700080855\n",
      "---------------------\n",
      "Iteration Number: 6645\n",
      "Loss: 12.093301247971382\n",
      "l2 norm of gradients: 0.0685867648379834\n",
      "l2 norm of weights: 3.253587386846529\n",
      "---------------------\n",
      "Iteration Number: 6646\n",
      "Loss: 12.092725881449011\n",
      "l2 norm of gradients: 0.06857160987398883\n",
      "l2 norm of weights: 3.253646019367564\n",
      "---------------------\n",
      "Iteration Number: 6647\n",
      "Loss: 12.09215082041871\n",
      "l2 norm of gradients: 0.06855646499306743\n",
      "l2 norm of weights: 3.253704667549641\n",
      "---------------------\n",
      "Iteration Number: 6648\n",
      "Loss: 12.091576064618184\n",
      "l2 norm of gradients: 0.06854133018503296\n",
      "l2 norm of weights: 3.2537633313712333\n",
      "---------------------\n",
      "Iteration Number: 6649\n",
      "Loss: 12.091001613785455\n",
      "l2 norm of gradients: 0.06852620543970839\n",
      "l2 norm of weights: 3.2538220108108344\n",
      "---------------------\n",
      "Iteration Number: 6650\n",
      "Loss: 12.090427467658872\n",
      "l2 norm of gradients: 0.06851109074692613\n",
      "l2 norm of weights: 3.2538807058469605\n",
      "---------------------\n",
      "Iteration Number: 6651\n",
      "Loss: 12.089853625977092\n",
      "l2 norm of gradients: 0.0684959860965278\n",
      "l2 norm of weights: 3.2539394164581465\n",
      "---------------------\n",
      "Iteration Number: 6652\n",
      "Loss: 12.089280088479098\n",
      "l2 norm of gradients: 0.06848089147836434\n",
      "l2 norm of weights: 3.253998142622952\n",
      "---------------------\n",
      "Iteration Number: 6653\n",
      "Loss: 12.088706854904165\n",
      "l2 norm of gradients: 0.06846580688229603\n",
      "l2 norm of weights: 3.254056884319954\n",
      "---------------------\n",
      "Iteration Number: 6654\n",
      "Loss: 12.088133924991899\n",
      "l2 norm of gradients: 0.0684507322981925\n",
      "l2 norm of weights: 3.2541156415277532\n",
      "---------------------\n",
      "Iteration Number: 6655\n",
      "Loss: 12.08756129848222\n",
      "l2 norm of gradients: 0.06843566771593257\n",
      "l2 norm of weights: 3.2541744142249707\n",
      "---------------------\n",
      "Iteration Number: 6656\n",
      "Loss: 12.086988975115352\n",
      "l2 norm of gradients: 0.06842061312540454\n",
      "l2 norm of weights: 3.2542332023902487\n",
      "---------------------\n",
      "Iteration Number: 6657\n",
      "Loss: 12.086416954631817\n",
      "l2 norm of gradients: 0.06840556851650585\n",
      "l2 norm of weights: 3.2542920060022498\n",
      "---------------------\n",
      "Iteration Number: 6658\n",
      "Loss: 12.085845236772505\n",
      "l2 norm of gradients: 0.06839053387914334\n",
      "l2 norm of weights: 3.2543508250396584\n",
      "---------------------\n",
      "Iteration Number: 6659\n",
      "Loss: 12.08527382127856\n",
      "l2 norm of gradients: 0.06837550920323317\n",
      "l2 norm of weights: 3.2544096594811793\n",
      "---------------------\n",
      "Iteration Number: 6660\n",
      "Loss: 12.084702707891445\n",
      "l2 norm of gradients: 0.06836049447870071\n",
      "l2 norm of weights: 3.25446850930554\n",
      "---------------------\n",
      "Iteration Number: 6661\n",
      "Loss: 12.084131896352977\n",
      "l2 norm of gradients: 0.06834548969548075\n",
      "l2 norm of weights: 3.254527374491487\n",
      "---------------------\n",
      "Iteration Number: 6662\n",
      "Loss: 12.083561386405231\n",
      "l2 norm of gradients: 0.06833049484351732\n",
      "l2 norm of weights: 3.254586255017789\n",
      "---------------------\n",
      "Iteration Number: 6663\n",
      "Loss: 12.082991177790632\n",
      "l2 norm of gradients: 0.06831550991276371\n",
      "l2 norm of weights: 3.2546451508632344\n",
      "---------------------\n",
      "Iteration Number: 6664\n",
      "Loss: 12.082421270251897\n",
      "l2 norm of gradients: 0.06830053489318265\n",
      "l2 norm of weights: 3.2547040620066334\n",
      "---------------------\n",
      "Iteration Number: 6665\n",
      "Loss: 12.081851663532035\n",
      "l2 norm of gradients: 0.06828556977474606\n",
      "l2 norm of weights: 3.254762988426818\n",
      "---------------------\n",
      "Iteration Number: 6666\n",
      "Loss: 12.081282357374397\n",
      "l2 norm of gradients: 0.06827061454743516\n",
      "l2 norm of weights: 3.254821930102639\n",
      "---------------------\n",
      "Iteration Number: 6667\n",
      "Loss: 12.080713351522625\n",
      "l2 norm of gradients: 0.06825566920124054\n",
      "l2 norm of weights: 3.2548808870129697\n",
      "---------------------\n",
      "Iteration Number: 6668\n",
      "Loss: 12.080144645720685\n",
      "l2 norm of gradients: 0.068240733726162\n",
      "l2 norm of weights: 3.254939859136704\n",
      "---------------------\n",
      "Iteration Number: 6669\n",
      "Loss: 12.079576239712814\n",
      "l2 norm of gradients: 0.06822580811220869\n",
      "l2 norm of weights: 3.2549988464527555\n",
      "---------------------\n",
      "Iteration Number: 6670\n",
      "Loss: 12.079008133243594\n",
      "l2 norm of gradients: 0.06821089234939907\n",
      "l2 norm of weights: 3.2550578489400603\n",
      "---------------------\n",
      "Iteration Number: 6671\n",
      "Loss: 12.078440326057908\n",
      "l2 norm of gradients: 0.06819598642776084\n",
      "l2 norm of weights: 3.255116866577574\n",
      "---------------------\n",
      "Iteration Number: 6672\n",
      "Loss: 12.07787281790093\n",
      "l2 norm of gradients: 0.06818109033733104\n",
      "l2 norm of weights: 3.2551758993442728\n",
      "---------------------\n",
      "Iteration Number: 6673\n",
      "Loss: 12.077305608518142\n",
      "l2 norm of gradients: 0.06816620406815597\n",
      "l2 norm of weights: 3.2552349472191557\n",
      "---------------------\n",
      "Iteration Number: 6674\n",
      "Loss: 12.076738697655351\n",
      "l2 norm of gradients: 0.06815132761029125\n",
      "l2 norm of weights: 3.25529401018124\n",
      "---------------------\n",
      "Iteration Number: 6675\n",
      "Loss: 12.076172085058658\n",
      "l2 norm of gradients: 0.06813646095380176\n",
      "l2 norm of weights: 3.255353088209565\n",
      "---------------------\n",
      "Iteration Number: 6676\n",
      "Loss: 12.075605770474455\n",
      "l2 norm of gradients: 0.06812160408876165\n",
      "l2 norm of weights: 3.25541218128319\n",
      "---------------------\n",
      "Iteration Number: 6677\n",
      "Loss: 12.075039753649458\n",
      "l2 norm of gradients: 0.06810675700525443\n",
      "l2 norm of weights: 3.255471289381195\n",
      "---------------------\n",
      "Iteration Number: 6678\n",
      "Loss: 12.074474034330681\n",
      "l2 norm of gradients: 0.06809191969337282\n",
      "l2 norm of weights: 3.255530412482682\n",
      "---------------------\n",
      "Iteration Number: 6679\n",
      "Loss: 12.073908612265438\n",
      "l2 norm of gradients: 0.06807709214321885\n",
      "l2 norm of weights: 3.2555895505667722\n",
      "---------------------\n",
      "Iteration Number: 6680\n",
      "Loss: 12.073343487201354\n",
      "l2 norm of gradients: 0.06806227434490386\n",
      "l2 norm of weights: 3.255648703612607\n",
      "---------------------\n",
      "Iteration Number: 6681\n",
      "Loss: 12.072778658886344\n",
      "l2 norm of gradients: 0.0680474662885484\n",
      "l2 norm of weights: 3.2557078715993506\n",
      "---------------------\n",
      "Iteration Number: 6682\n",
      "Loss: 12.072214127068658\n",
      "l2 norm of gradients: 0.06803266796428234\n",
      "l2 norm of weights: 3.255767054506185\n",
      "---------------------\n",
      "Iteration Number: 6683\n",
      "Loss: 12.07164989149679\n",
      "l2 norm of gradients: 0.06801787936224486\n",
      "l2 norm of weights: 3.2558262523123136\n",
      "---------------------\n",
      "Iteration Number: 6684\n",
      "Loss: 12.071085951919592\n",
      "l2 norm of gradients: 0.06800310047258433\n",
      "l2 norm of weights: 3.2558854649969624\n",
      "---------------------\n",
      "Iteration Number: 6685\n",
      "Loss: 12.070522308086181\n",
      "l2 norm of gradients: 0.06798833128545848\n",
      "l2 norm of weights: 3.2559446925393756\n",
      "---------------------\n",
      "Iteration Number: 6686\n",
      "Loss: 12.06995895974601\n",
      "l2 norm of gradients: 0.06797357179103426\n",
      "l2 norm of weights: 3.256003934918818\n",
      "---------------------\n",
      "Iteration Number: 6687\n",
      "Loss: 12.0693959066488\n",
      "l2 norm of gradients: 0.06795882197948788\n",
      "l2 norm of weights: 3.256063192114576\n",
      "---------------------\n",
      "Iteration Number: 6688\n",
      "Loss: 12.068833148544577\n",
      "l2 norm of gradients: 0.06794408184100485\n",
      "l2 norm of weights: 3.256122464105956\n",
      "---------------------\n",
      "Iteration Number: 6689\n",
      "Loss: 12.068270685183686\n",
      "l2 norm of gradients: 0.06792935136577992\n",
      "l2 norm of weights: 3.2561817508722837\n",
      "---------------------\n",
      "Iteration Number: 6690\n",
      "Loss: 12.067708516316756\n",
      "l2 norm of gradients: 0.06791463054401715\n",
      "l2 norm of weights: 3.2562410523929066\n",
      "---------------------\n",
      "Iteration Number: 6691\n",
      "Loss: 12.067146641694729\n",
      "l2 norm of gradients: 0.0678999193659298\n",
      "l2 norm of weights: 3.2563003686471923\n",
      "---------------------\n",
      "Iteration Number: 6692\n",
      "Loss: 12.066585061068805\n",
      "l2 norm of gradients: 0.0678852178217404\n",
      "l2 norm of weights: 3.2563596996145288\n",
      "---------------------\n",
      "Iteration Number: 6693\n",
      "Loss: 12.066023774190542\n",
      "l2 norm of gradients: 0.06787052590168077\n",
      "l2 norm of weights: 3.2564190452743236\n",
      "---------------------\n",
      "Iteration Number: 6694\n",
      "Loss: 12.065462780811739\n",
      "l2 norm of gradients: 0.06785584359599196\n",
      "l2 norm of weights: 3.256478405606005\n",
      "---------------------\n",
      "Iteration Number: 6695\n",
      "Loss: 12.06490208068455\n",
      "l2 norm of gradients: 0.06784117089492427\n",
      "l2 norm of weights: 3.256537780589022\n",
      "---------------------\n",
      "Iteration Number: 6696\n",
      "Loss: 12.064341673561389\n",
      "l2 norm of gradients: 0.0678265077887373\n",
      "l2 norm of weights: 3.2565971702028444\n",
      "---------------------\n",
      "Iteration Number: 6697\n",
      "Loss: 12.063781559194943\n",
      "l2 norm of gradients: 0.06781185426769983\n",
      "l2 norm of weights: 3.25665657442696\n",
      "---------------------\n",
      "Iteration Number: 6698\n",
      "Loss: 12.063221737338262\n",
      "l2 norm of gradients: 0.06779721032208991\n",
      "l2 norm of weights: 3.256715993240879\n",
      "---------------------\n",
      "Iteration Number: 6699\n",
      "Loss: 12.062662207744644\n",
      "l2 norm of gradients: 0.06778257594219488\n",
      "l2 norm of weights: 3.2567754266241313\n",
      "---------------------\n",
      "Iteration Number: 6700\n",
      "Loss: 12.062102970167674\n",
      "l2 norm of gradients: 0.06776795111831123\n",
      "l2 norm of weights: 3.256834874556266\n",
      "---------------------\n",
      "Iteration Number: 6701\n",
      "Loss: 12.06154402436128\n",
      "l2 norm of gradients: 0.06775333584074478\n",
      "l2 norm of weights: 3.256894337016854\n",
      "---------------------\n",
      "Iteration Number: 6702\n",
      "Loss: 12.06098537007965\n",
      "l2 norm of gradients: 0.06773873009981056\n",
      "l2 norm of weights: 3.2569538139854846\n",
      "---------------------\n",
      "Iteration Number: 6703\n",
      "Loss: 12.060427007077251\n",
      "l2 norm of gradients: 0.06772413388583275\n",
      "l2 norm of weights: 3.2570133054417685\n",
      "---------------------\n",
      "Iteration Number: 6704\n",
      "Loss: 12.059868935108888\n",
      "l2 norm of gradients: 0.06770954718914496\n",
      "l2 norm of weights: 3.257072811365337\n",
      "---------------------\n",
      "Iteration Number: 6705\n",
      "Loss: 12.059311153929634\n",
      "l2 norm of gradients: 0.06769497000008984\n",
      "l2 norm of weights: 3.257132331735839\n",
      "---------------------\n",
      "Iteration Number: 6706\n",
      "Loss: 12.058753663294858\n",
      "l2 norm of gradients: 0.06768040230901932\n",
      "l2 norm of weights: 3.2571918665329465\n",
      "---------------------\n",
      "Iteration Number: 6707\n",
      "Loss: 12.058196462960217\n",
      "l2 norm of gradients: 0.06766584410629463\n",
      "l2 norm of weights: 3.2572514157363486\n",
      "---------------------\n",
      "Iteration Number: 6708\n",
      "Loss: 12.057639552681662\n",
      "l2 norm of gradients: 0.06765129538228613\n",
      "l2 norm of weights: 3.2573109793257573\n",
      "---------------------\n",
      "Iteration Number: 6709\n",
      "Loss: 12.057082932215442\n",
      "l2 norm of gradients: 0.06763675612737347\n",
      "l2 norm of weights: 3.2573705572809035\n",
      "---------------------\n",
      "Iteration Number: 6710\n",
      "Loss: 12.056526601318117\n",
      "l2 norm of gradients: 0.06762222633194545\n",
      "l2 norm of weights: 3.2574301495815368\n",
      "---------------------\n",
      "Iteration Number: 6711\n",
      "Loss: 12.05597055974648\n",
      "l2 norm of gradients: 0.06760770598640017\n",
      "l2 norm of weights: 3.257489756207428\n",
      "---------------------\n",
      "Iteration Number: 6712\n",
      "Loss: 12.05541480725766\n",
      "l2 norm of gradients: 0.06759319508114482\n",
      "l2 norm of weights: 3.2575493771383686\n",
      "---------------------\n",
      "Iteration Number: 6713\n",
      "Loss: 12.054859343609095\n",
      "l2 norm of gradients: 0.06757869360659596\n",
      "l2 norm of weights: 3.257609012354168\n",
      "---------------------\n",
      "Iteration Number: 6714\n",
      "Loss: 12.054304168558449\n",
      "l2 norm of gradients: 0.06756420155317924\n",
      "l2 norm of weights: 3.257668661834657\n",
      "---------------------\n",
      "Iteration Number: 6715\n",
      "Loss: 12.053749281863734\n",
      "l2 norm of gradients: 0.06754971891132952\n",
      "l2 norm of weights: 3.257728325559686\n",
      "---------------------\n",
      "Iteration Number: 6716\n",
      "Loss: 12.05319468328322\n",
      "l2 norm of gradients: 0.06753524567149097\n",
      "l2 norm of weights: 3.257788003509125\n",
      "---------------------\n",
      "Iteration Number: 6717\n",
      "Loss: 12.052640372575487\n",
      "l2 norm of gradients: 0.06752078182411682\n",
      "l2 norm of weights: 3.2578476956628646\n",
      "---------------------\n",
      "Iteration Number: 6718\n",
      "Loss: 12.052086349499378\n",
      "l2 norm of gradients: 0.06750632735966955\n",
      "l2 norm of weights: 3.2579074020008143\n",
      "---------------------\n",
      "Iteration Number: 6719\n",
      "Loss: 12.051532613814041\n",
      "l2 norm of gradients: 0.0674918822686209\n",
      "l2 norm of weights: 3.2579671225029037\n",
      "---------------------\n",
      "Iteration Number: 6720\n",
      "Loss: 12.050979165278909\n",
      "l2 norm of gradients: 0.0674774465414517\n",
      "l2 norm of weights: 3.2580268571490825\n",
      "---------------------\n",
      "Iteration Number: 6721\n",
      "Loss: 12.050426003653707\n",
      "l2 norm of gradients: 0.06746302016865204\n",
      "l2 norm of weights: 3.258086605919319\n",
      "---------------------\n",
      "Iteration Number: 6722\n",
      "Loss: 12.049873128698428\n",
      "l2 norm of gradients: 0.06744860314072115\n",
      "l2 norm of weights: 3.2581463687936036\n",
      "---------------------\n",
      "Iteration Number: 6723\n",
      "Loss: 12.049320540173378\n",
      "l2 norm of gradients: 0.06743419544816748\n",
      "l2 norm of weights: 3.258206145751944\n",
      "---------------------\n",
      "Iteration Number: 6724\n",
      "Loss: 12.048768237839138\n",
      "l2 norm of gradients: 0.06741979708150862\n",
      "l2 norm of weights: 3.2582659367743694\n",
      "---------------------\n",
      "Iteration Number: 6725\n",
      "Loss: 12.048216221456562\n",
      "l2 norm of gradients: 0.0674054080312714\n",
      "l2 norm of weights: 3.258325741840927\n",
      "---------------------\n",
      "Iteration Number: 6726\n",
      "Loss: 12.047664490786815\n",
      "l2 norm of gradients: 0.06739102828799173\n",
      "l2 norm of weights: 3.258385560931685\n",
      "---------------------\n",
      "Iteration Number: 6727\n",
      "Loss: 12.047113045591313\n",
      "l2 norm of gradients: 0.06737665784221482\n",
      "l2 norm of weights: 3.2584453940267313\n",
      "---------------------\n",
      "Iteration Number: 6728\n",
      "Loss: 12.046561885631796\n",
      "l2 norm of gradients: 0.06736229668449491\n",
      "l2 norm of weights: 3.2585052411061723\n",
      "---------------------\n",
      "Iteration Number: 6729\n",
      "Loss: 12.046011010670266\n",
      "l2 norm of gradients: 0.0673479448053955\n",
      "l2 norm of weights: 3.258565102150135\n",
      "---------------------\n",
      "Iteration Number: 6730\n",
      "Loss: 12.045460420468999\n",
      "l2 norm of gradients: 0.06733360219548917\n",
      "l2 norm of weights: 3.2586249771387656\n",
      "---------------------\n",
      "Iteration Number: 6731\n",
      "Loss: 12.044910114790577\n",
      "l2 norm of gradients: 0.06731926884535774\n",
      "l2 norm of weights: 3.25868486605223\n",
      "---------------------\n",
      "Iteration Number: 6732\n",
      "Loss: 12.044360093397845\n",
      "l2 norm of gradients: 0.06730494474559219\n",
      "l2 norm of weights: 3.258744768870713\n",
      "---------------------\n",
      "Iteration Number: 6733\n",
      "Loss: 12.043810356053967\n",
      "l2 norm of gradients: 0.0672906298867926\n",
      "l2 norm of weights: 3.25880468557442\n",
      "---------------------\n",
      "Iteration Number: 6734\n",
      "Loss: 12.043260902522338\n",
      "l2 norm of gradients: 0.06727632425956818\n",
      "l2 norm of weights: 3.2588646161435757\n",
      "---------------------\n",
      "Iteration Number: 6735\n",
      "Loss: 12.042711732566664\n",
      "l2 norm of gradients: 0.06726202785453736\n",
      "l2 norm of weights: 3.2589245605584236\n",
      "---------------------\n",
      "Iteration Number: 6736\n",
      "Loss: 12.042162845950942\n",
      "l2 norm of gradients: 0.06724774066232767\n",
      "l2 norm of weights: 3.258984518799227\n",
      "---------------------\n",
      "Iteration Number: 6737\n",
      "Loss: 12.041614242439422\n",
      "l2 norm of gradients: 0.06723346267357576\n",
      "l2 norm of weights: 3.2590444908462692\n",
      "---------------------\n",
      "Iteration Number: 6738\n",
      "Loss: 12.041065921796667\n",
      "l2 norm of gradients: 0.06721919387892748\n",
      "l2 norm of weights: 3.2591044766798523\n",
      "---------------------\n",
      "Iteration Number: 6739\n",
      "Loss: 12.040517883787494\n",
      "l2 norm of gradients: 0.06720493426903773\n",
      "l2 norm of weights: 3.2591644762802976\n",
      "---------------------\n",
      "Iteration Number: 6740\n",
      "Loss: 12.039970128177007\n",
      "l2 norm of gradients: 0.06719068383457064\n",
      "l2 norm of weights: 3.259224489627946\n",
      "---------------------\n",
      "Iteration Number: 6741\n",
      "Loss: 12.0394226547306\n",
      "l2 norm of gradients: 0.06717644256619938\n",
      "l2 norm of weights: 3.259284516703159\n",
      "---------------------\n",
      "Iteration Number: 6742\n",
      "Loss: 12.038875463213948\n",
      "l2 norm of gradients: 0.06716221045460628\n",
      "l2 norm of weights: 3.259344557486315\n",
      "---------------------\n",
      "Iteration Number: 6743\n",
      "Loss: 12.038328553392981\n",
      "l2 norm of gradients: 0.06714798749048276\n",
      "l2 norm of weights: 3.2594046119578146\n",
      "---------------------\n",
      "Iteration Number: 6744\n",
      "Loss: 12.037781925033926\n",
      "l2 norm of gradients: 0.06713377366452944\n",
      "l2 norm of weights: 3.2594646800980756\n",
      "---------------------\n",
      "Iteration Number: 6745\n",
      "Loss: 12.037235577903282\n",
      "l2 norm of gradients: 0.06711956896745588\n",
      "l2 norm of weights: 3.259524761887535\n",
      "---------------------\n",
      "Iteration Number: 6746\n",
      "Loss: 12.036689511767852\n",
      "l2 norm of gradients: 0.06710537338998099\n",
      "l2 norm of weights: 3.2595848573066517\n",
      "---------------------\n",
      "Iteration Number: 6747\n",
      "Loss: 12.036143726394661\n",
      "l2 norm of gradients: 0.06709118692283261\n",
      "l2 norm of weights: 3.2596449663358995\n",
      "---------------------\n",
      "Iteration Number: 6748\n",
      "Loss: 12.03559822155107\n",
      "l2 norm of gradients: 0.0670770095567477\n",
      "l2 norm of weights: 3.259705088955776\n",
      "---------------------\n",
      "Iteration Number: 6749\n",
      "Loss: 12.035052997004685\n",
      "l2 norm of gradients: 0.06706284128247238\n",
      "l2 norm of weights: 3.259765225146795\n",
      "---------------------\n",
      "Iteration Number: 6750\n",
      "Loss: 12.034508052523389\n",
      "l2 norm of gradients: 0.06704868209076181\n",
      "l2 norm of weights: 3.25982537488949\n",
      "---------------------\n",
      "Iteration Number: 6751\n",
      "Loss: 12.033963387875348\n",
      "l2 norm of gradients: 0.06703453197238028\n",
      "l2 norm of weights: 3.259885538164415\n",
      "---------------------\n",
      "Iteration Number: 6752\n",
      "Loss: 12.033419002829014\n",
      "l2 norm of gradients: 0.06702039091810116\n",
      "l2 norm of weights: 3.2599457149521416\n",
      "---------------------\n",
      "Iteration Number: 6753\n",
      "Loss: 12.032874897153082\n",
      "l2 norm of gradients: 0.0670062589187069\n",
      "l2 norm of weights: 3.2600059052332617\n",
      "---------------------\n",
      "Iteration Number: 6754\n",
      "Loss: 12.032331070616559\n",
      "l2 norm of gradients: 0.066992135964989\n",
      "l2 norm of weights: 3.2600661089883856\n",
      "---------------------\n",
      "Iteration Number: 6755\n",
      "Loss: 12.03178752298871\n",
      "l2 norm of gradients: 0.0669780220477481\n",
      "l2 norm of weights: 3.260126326198143\n",
      "---------------------\n",
      "Iteration Number: 6756\n",
      "Loss: 12.031244254039056\n",
      "l2 norm of gradients: 0.06696391715779387\n",
      "l2 norm of weights: 3.260186556843182\n",
      "---------------------\n",
      "Iteration Number: 6757\n",
      "Loss: 12.030701263537434\n",
      "l2 norm of gradients: 0.06694982128594504\n",
      "l2 norm of weights: 3.260246800904171\n",
      "---------------------\n",
      "Iteration Number: 6758\n",
      "Loss: 12.030158551253919\n",
      "l2 norm of gradients: 0.06693573442302944\n",
      "l2 norm of weights: 3.2603070583617964\n",
      "---------------------\n",
      "Iteration Number: 6759\n",
      "Loss: 12.029616116958879\n",
      "l2 norm of gradients: 0.06692165655988395\n",
      "l2 norm of weights: 3.260367329196764\n",
      "---------------------\n",
      "Iteration Number: 6760\n",
      "Loss: 12.029073960422942\n",
      "l2 norm of gradients: 0.06690758768735446\n",
      "l2 norm of weights: 3.2604276133897994\n",
      "---------------------\n",
      "Iteration Number: 6761\n",
      "Loss: 12.028532081417014\n",
      "l2 norm of gradients: 0.06689352779629601\n",
      "l2 norm of weights: 3.2604879109216456\n",
      "---------------------\n",
      "Iteration Number: 6762\n",
      "Loss: 12.02799047971226\n",
      "l2 norm of gradients: 0.06687947687757262\n",
      "l2 norm of weights: 3.260548221773065\n",
      "---------------------\n",
      "Iteration Number: 6763\n",
      "Loss: 12.02744915508016\n",
      "l2 norm of gradients: 0.06686543492205739\n",
      "l2 norm of weights: 3.26060854592484\n",
      "---------------------\n",
      "Iteration Number: 6764\n",
      "Loss: 12.026908107292398\n",
      "l2 norm of gradients: 0.06685140192063241\n",
      "l2 norm of weights: 3.2606688833577713\n",
      "---------------------\n",
      "Iteration Number: 6765\n",
      "Loss: 12.02636733612099\n",
      "l2 norm of gradients: 0.06683737786418892\n",
      "l2 norm of weights: 3.260729234052678\n",
      "---------------------\n",
      "Iteration Number: 6766\n",
      "Loss: 12.025826841338201\n",
      "l2 norm of gradients: 0.06682336274362702\n",
      "l2 norm of weights: 3.2607895979903985\n",
      "---------------------\n",
      "Iteration Number: 6767\n",
      "Loss: 12.02528662271655\n",
      "l2 norm of gradients: 0.06680935654985602\n",
      "l2 norm of weights: 3.2608499751517908\n",
      "---------------------\n",
      "Iteration Number: 6768\n",
      "Loss: 12.024746680028832\n",
      "l2 norm of gradients: 0.06679535927379412\n",
      "l2 norm of weights: 3.2609103655177303\n",
      "---------------------\n",
      "Iteration Number: 6769\n",
      "Loss: 12.024207013048146\n",
      "l2 norm of gradients: 0.0667813709063687\n",
      "l2 norm of weights: 3.260970769069112\n",
      "---------------------\n",
      "Iteration Number: 6770\n",
      "Loss: 12.023667621547816\n",
      "l2 norm of gradients: 0.06676739143851597\n",
      "l2 norm of weights: 3.2610311857868495\n",
      "---------------------\n",
      "Iteration Number: 6771\n",
      "Loss: 12.02312850530145\n",
      "l2 norm of gradients: 0.06675342086118126\n",
      "l2 norm of weights: 3.261091615651876\n",
      "---------------------\n",
      "Iteration Number: 6772\n",
      "Loss: 12.022589664082922\n",
      "l2 norm of gradients: 0.06673945916531895\n",
      "l2 norm of weights: 3.261152058645142\n",
      "---------------------\n",
      "Iteration Number: 6773\n",
      "Loss: 12.022051097666406\n",
      "l2 norm of gradients: 0.06672550634189234\n",
      "l2 norm of weights: 3.2612125147476188\n",
      "---------------------\n",
      "Iteration Number: 6774\n",
      "Loss: 12.021512805826285\n",
      "l2 norm of gradients: 0.06671156238187374\n",
      "l2 norm of weights: 3.2612729839402936\n",
      "---------------------\n",
      "Iteration Number: 6775\n",
      "Loss: 12.02097478833725\n",
      "l2 norm of gradients: 0.06669762727624455\n",
      "l2 norm of weights: 3.2613334662041753\n",
      "---------------------\n",
      "Iteration Number: 6776\n",
      "Loss: 12.020437044974262\n",
      "l2 norm of gradients: 0.06668370101599504\n",
      "l2 norm of weights: 3.261393961520289\n",
      "---------------------\n",
      "Iteration Number: 6777\n",
      "Loss: 12.01989957551252\n",
      "l2 norm of gradients: 0.06666978359212457\n",
      "l2 norm of weights: 3.26145446986968\n",
      "---------------------\n",
      "Iteration Number: 6778\n",
      "Loss: 12.01936237972751\n",
      "l2 norm of gradients: 0.06665587499564143\n",
      "l2 norm of weights: 3.2615149912334127\n",
      "---------------------\n",
      "Iteration Number: 6779\n",
      "Loss: 12.018825457394984\n",
      "l2 norm of gradients: 0.06664197521756295\n",
      "l2 norm of weights: 3.261575525592568\n",
      "---------------------\n",
      "Iteration Number: 6780\n",
      "Loss: 12.018288808290961\n",
      "l2 norm of gradients: 0.06662808424891532\n",
      "l2 norm of weights: 3.2616360729282476\n",
      "---------------------\n",
      "Iteration Number: 6781\n",
      "Loss: 12.017752432191706\n",
      "l2 norm of gradients: 0.06661420208073386\n",
      "l2 norm of weights: 3.26169663322157\n",
      "---------------------\n",
      "Iteration Number: 6782\n",
      "Loss: 12.017216328873769\n",
      "l2 norm of gradients: 0.06660032870406271\n",
      "l2 norm of weights: 3.2617572064536735\n",
      "---------------------\n",
      "Iteration Number: 6783\n",
      "Loss: 12.016680498113955\n",
      "l2 norm of gradients: 0.06658646410995513\n",
      "l2 norm of weights: 3.2618177926057155\n",
      "---------------------\n",
      "Iteration Number: 6784\n",
      "Loss: 12.016144939689338\n",
      "l2 norm of gradients: 0.06657260828947319\n",
      "l2 norm of weights: 3.2618783916588696\n",
      "---------------------\n",
      "Iteration Number: 6785\n",
      "Loss: 12.015609653377256\n",
      "l2 norm of gradients: 0.066558761233688\n",
      "l2 norm of weights: 3.2619390035943305\n",
      "---------------------\n",
      "Iteration Number: 6786\n",
      "Loss: 12.015074638955301\n",
      "l2 norm of gradients: 0.0665449229336796\n",
      "l2 norm of weights: 3.26199962839331\n",
      "---------------------\n",
      "Iteration Number: 6787\n",
      "Loss: 12.014539896201347\n",
      "l2 norm of gradients: 0.06653109338053702\n",
      "l2 norm of weights: 3.2620602660370386\n",
      "---------------------\n",
      "Iteration Number: 6788\n",
      "Loss: 12.014005424893512\n",
      "l2 norm of gradients: 0.06651727256535814\n",
      "l2 norm of weights: 3.2621209165067646\n",
      "---------------------\n",
      "Iteration Number: 6789\n",
      "Loss: 12.013471224810182\n",
      "l2 norm of gradients: 0.06650346047924988\n",
      "l2 norm of weights: 3.262181579783757\n",
      "---------------------\n",
      "Iteration Number: 6790\n",
      "Loss: 12.01293729573002\n",
      "l2 norm of gradients: 0.06648965711332802\n",
      "l2 norm of weights: 3.2622422558493005\n",
      "---------------------\n",
      "Iteration Number: 6791\n",
      "Loss: 12.012403637431909\n",
      "l2 norm of gradients: 0.06647586245871735\n",
      "l2 norm of weights: 3.2623029446846994\n",
      "---------------------\n",
      "Iteration Number: 6792\n",
      "Loss: 12.011870249695056\n",
      "l2 norm of gradients: 0.0664620765065515\n",
      "l2 norm of weights: 3.2623636462712775\n",
      "---------------------\n",
      "Iteration Number: 6793\n",
      "Loss: 12.011337132298866\n",
      "l2 norm of gradients: 0.066448299247973\n",
      "l2 norm of weights: 3.2624243605903747\n",
      "---------------------\n",
      "Iteration Number: 6794\n",
      "Loss: 12.010804285023063\n",
      "l2 norm of gradients: 0.0664345306741335\n",
      "l2 norm of weights: 3.2624850876233507\n",
      "---------------------\n",
      "Iteration Number: 6795\n",
      "Loss: 12.010271707647574\n",
      "l2 norm of gradients: 0.06642077077619332\n",
      "l2 norm of weights: 3.262545827351584\n",
      "---------------------\n",
      "Iteration Number: 6796\n",
      "Loss: 12.00973939995262\n",
      "l2 norm of gradients: 0.06640701954532179\n",
      "l2 norm of weights: 3.26260657975647\n",
      "---------------------\n",
      "Iteration Number: 6797\n",
      "Loss: 12.009207361718694\n",
      "l2 norm of gradients: 0.06639327697269716\n",
      "l2 norm of weights: 3.262667344819423\n",
      "---------------------\n",
      "Iteration Number: 6798\n",
      "Loss: 12.008675592726508\n",
      "l2 norm of gradients: 0.06637954304950654\n",
      "l2 norm of weights: 3.262728122521876\n",
      "---------------------\n",
      "Iteration Number: 6799\n",
      "Loss: 12.008144092757071\n",
      "l2 norm of gradients: 0.06636581776694596\n",
      "l2 norm of weights: 3.2627889128452794\n",
      "---------------------\n",
      "Iteration Number: 6800\n",
      "Loss: 12.007612861591621\n",
      "l2 norm of gradients: 0.06635210111622038\n",
      "l2 norm of weights: 3.262849715771103\n",
      "---------------------\n",
      "Iteration Number: 6801\n",
      "Loss: 12.007081899011666\n",
      "l2 norm of gradients: 0.06633839308854349\n",
      "l2 norm of weights: 3.2629105312808333\n",
      "---------------------\n",
      "Iteration Number: 6802\n",
      "Loss: 12.00655120479899\n",
      "l2 norm of gradients: 0.06632469367513805\n",
      "l2 norm of weights: 3.262971359355977\n",
      "---------------------\n",
      "Iteration Number: 6803\n",
      "Loss: 12.006020778735603\n",
      "l2 norm of gradients: 0.06631100286723562\n",
      "l2 norm of weights: 3.2630321999780563\n",
      "---------------------\n",
      "Iteration Number: 6804\n",
      "Loss: 12.005490620603798\n",
      "l2 norm of gradients: 0.06629732065607659\n",
      "l2 norm of weights: 3.2630930531286144\n",
      "---------------------\n",
      "Iteration Number: 6805\n",
      "Loss: 12.004960730186113\n",
      "l2 norm of gradients: 0.06628364703291029\n",
      "l2 norm of weights: 3.263153918789211\n",
      "---------------------\n",
      "Iteration Number: 6806\n",
      "Loss: 12.004431107265326\n",
      "l2 norm of gradients: 0.06626998198899482\n",
      "l2 norm of weights: 3.2632147969414236\n",
      "---------------------\n",
      "Iteration Number: 6807\n",
      "Loss: 12.003901751624506\n",
      "l2 norm of gradients: 0.06625632551559725\n",
      "l2 norm of weights: 3.2632756875668494\n",
      "---------------------\n",
      "Iteration Number: 6808\n",
      "Loss: 12.003372663046964\n",
      "l2 norm of gradients: 0.06624267760399341\n",
      "l2 norm of weights: 3.2633365906471026\n",
      "---------------------\n",
      "Iteration Number: 6809\n",
      "Loss: 12.002843841316247\n",
      "l2 norm of gradients: 0.06622903824546804\n",
      "l2 norm of weights: 3.263397506163815\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 6810\n",
      "Loss: 12.00231528621618\n",
      "l2 norm of gradients: 0.06621540743131464\n",
      "l2 norm of weights: 3.2634584340986375\n",
      "---------------------\n",
      "Iteration Number: 6811\n",
      "Loss: 12.001786997530845\n",
      "l2 norm of gradients: 0.06620178515283565\n",
      "l2 norm of weights: 3.263519374433239\n",
      "---------------------\n",
      "Iteration Number: 6812\n",
      "Loss: 12.001258975044562\n",
      "l2 norm of gradients: 0.06618817140134231\n",
      "l2 norm of weights: 3.2635803271493056\n",
      "---------------------\n",
      "Iteration Number: 6813\n",
      "Loss: 12.000731218541903\n",
      "l2 norm of gradients: 0.06617456616815462\n",
      "l2 norm of weights: 3.2636412922285416\n",
      "---------------------\n",
      "Iteration Number: 6814\n",
      "Loss: 12.000203727807714\n",
      "l2 norm of gradients: 0.06616096944460152\n",
      "l2 norm of weights: 3.26370226965267\n",
      "---------------------\n",
      "Iteration Number: 6815\n",
      "Loss: 11.999676502627095\n",
      "l2 norm of gradients: 0.06614738122202068\n",
      "l2 norm of weights: 3.2637632594034307\n",
      "---------------------\n",
      "Iteration Number: 6816\n",
      "Loss: 11.999149542785359\n",
      "l2 norm of gradients: 0.06613380149175864\n",
      "l2 norm of weights: 3.2638242614625823\n",
      "---------------------\n",
      "Iteration Number: 6817\n",
      "Loss: 11.998622848068129\n",
      "l2 norm of gradients: 0.06612023024517068\n",
      "l2 norm of weights: 3.2638852758119015\n",
      "---------------------\n",
      "Iteration Number: 6818\n",
      "Loss: 11.998096418261236\n",
      "l2 norm of gradients: 0.06610666747362094\n",
      "l2 norm of weights: 3.2639463024331823\n",
      "---------------------\n",
      "Iteration Number: 6819\n",
      "Loss: 11.997570253150782\n",
      "l2 norm of gradients: 0.06609311316848239\n",
      "l2 norm of weights: 3.2640073413082367\n",
      "---------------------\n",
      "Iteration Number: 6820\n",
      "Loss: 11.997044352523107\n",
      "l2 norm of gradients: 0.06607956732113672\n",
      "l2 norm of weights: 3.2640683924188947\n",
      "---------------------\n",
      "Iteration Number: 6821\n",
      "Loss: 11.996518716164838\n",
      "l2 norm of gradients: 0.06606602992297443\n",
      "l2 norm of weights: 3.2641294557470037\n",
      "---------------------\n",
      "Iteration Number: 6822\n",
      "Loss: 11.995993343862816\n",
      "l2 norm of gradients: 0.06605250096539486\n",
      "l2 norm of weights: 3.26419053127443\n",
      "---------------------\n",
      "Iteration Number: 6823\n",
      "Loss: 11.995468235404138\n",
      "l2 norm of gradients: 0.06603898043980605\n",
      "l2 norm of weights: 3.264251618983057\n",
      "---------------------\n",
      "Iteration Number: 6824\n",
      "Loss: 11.994943390576159\n",
      "l2 norm of gradients: 0.06602546833762483\n",
      "l2 norm of weights: 3.2643127188547854\n",
      "---------------------\n",
      "Iteration Number: 6825\n",
      "Loss: 11.994418809166497\n",
      "l2 norm of gradients: 0.06601196465027692\n",
      "l2 norm of weights: 3.2643738308715347\n",
      "---------------------\n",
      "Iteration Number: 6826\n",
      "Loss: 11.993894490962992\n",
      "l2 norm of gradients: 0.06599846936919665\n",
      "l2 norm of weights: 3.264434955015242\n",
      "---------------------\n",
      "Iteration Number: 6827\n",
      "Loss: 11.993370435753752\n",
      "l2 norm of gradients: 0.06598498248582715\n",
      "l2 norm of weights: 3.264496091267861\n",
      "---------------------\n",
      "Iteration Number: 6828\n",
      "Loss: 11.992846643327132\n",
      "l2 norm of gradients: 0.06597150399162034\n",
      "l2 norm of weights: 3.264557239611364\n",
      "---------------------\n",
      "Iteration Number: 6829\n",
      "Loss: 11.99232311347174\n",
      "l2 norm of gradients: 0.0659580338780369\n",
      "l2 norm of weights: 3.2646184000277416\n",
      "---------------------\n",
      "Iteration Number: 6830\n",
      "Loss: 11.99179984597641\n",
      "l2 norm of gradients: 0.06594457213654623\n",
      "l2 norm of weights: 3.2646795724990008\n",
      "---------------------\n",
      "Iteration Number: 6831\n",
      "Loss: 11.991276840630247\n",
      "l2 norm of gradients: 0.06593111875862642\n",
      "l2 norm of weights: 3.2647407570071674\n",
      "---------------------\n",
      "Iteration Number: 6832\n",
      "Loss: 11.990754097222593\n",
      "l2 norm of gradients: 0.06591767373576439\n",
      "l2 norm of weights: 3.264801953534284\n",
      "---------------------\n",
      "Iteration Number: 6833\n",
      "Loss: 11.99023161554305\n",
      "l2 norm of gradients: 0.06590423705945575\n",
      "l2 norm of weights: 3.264863162062411\n",
      "---------------------\n",
      "Iteration Number: 6834\n",
      "Loss: 11.98970939538145\n",
      "l2 norm of gradients: 0.06589080872120477\n",
      "l2 norm of weights: 3.264924382573627\n",
      "---------------------\n",
      "Iteration Number: 6835\n",
      "Loss: 11.989187436527873\n",
      "l2 norm of gradients: 0.06587738871252459\n",
      "l2 norm of weights: 3.2649856150500276\n",
      "---------------------\n",
      "Iteration Number: 6836\n",
      "Loss: 11.98866573877267\n",
      "l2 norm of gradients: 0.0658639770249369\n",
      "l2 norm of weights: 3.2650468594737254\n",
      "---------------------\n",
      "Iteration Number: 6837\n",
      "Loss: 11.988144301906413\n",
      "l2 norm of gradients: 0.06585057364997224\n",
      "l2 norm of weights: 3.2651081158268527\n",
      "---------------------\n",
      "Iteration Number: 6838\n",
      "Loss: 11.987623125719914\n",
      "l2 norm of gradients: 0.06583717857916972\n",
      "l2 norm of weights: 3.265169384091556\n",
      "---------------------\n",
      "Iteration Number: 6839\n",
      "Loss: 11.987102210004261\n",
      "l2 norm of gradients: 0.06582379180407727\n",
      "l2 norm of weights: 3.265230664250003\n",
      "---------------------\n",
      "Iteration Number: 6840\n",
      "Loss: 11.986581554550762\n",
      "l2 norm of gradients: 0.06581041331625147\n",
      "l2 norm of weights: 3.2652919562843765\n",
      "---------------------\n",
      "Iteration Number: 6841\n",
      "Loss: 11.98606115915097\n",
      "l2 norm of gradients: 0.06579704310725751\n",
      "l2 norm of weights: 3.265353260176877\n",
      "---------------------\n",
      "Iteration Number: 6842\n",
      "Loss: 11.985541023596692\n",
      "l2 norm of gradients: 0.06578368116866942\n",
      "l2 norm of weights: 3.2654145759097233\n",
      "---------------------\n",
      "Iteration Number: 6843\n",
      "Loss: 11.985021147679982\n",
      "l2 norm of gradients: 0.06577032749206974\n",
      "l2 norm of weights: 3.2654759034651515\n",
      "---------------------\n",
      "Iteration Number: 6844\n",
      "Loss: 11.984501531193132\n",
      "l2 norm of gradients: 0.06575698206904984\n",
      "l2 norm of weights: 3.2655372428254137\n",
      "---------------------\n",
      "Iteration Number: 6845\n",
      "Loss: 11.98398217392868\n",
      "l2 norm of gradients: 0.06574364489120965\n",
      "l2 norm of weights: 3.2655985939727823\n",
      "---------------------\n",
      "Iteration Number: 6846\n",
      "Loss: 11.983463075679394\n",
      "l2 norm of gradients: 0.06573031595015781\n",
      "l2 norm of weights: 3.265659956889544\n",
      "---------------------\n",
      "Iteration Number: 6847\n",
      "Loss: 11.982944236238302\n",
      "l2 norm of gradients: 0.0657169952375116\n",
      "l2 norm of weights: 3.2657213315580047\n",
      "---------------------\n",
      "Iteration Number: 6848\n",
      "Loss: 11.98242565539867\n",
      "l2 norm of gradients: 0.06570368274489694\n",
      "l2 norm of weights: 3.265782717960487\n",
      "---------------------\n",
      "Iteration Number: 6849\n",
      "Loss: 11.981907332953998\n",
      "l2 norm of gradients: 0.06569037846394844\n",
      "l2 norm of weights: 3.265844116079331\n",
      "---------------------\n",
      "Iteration Number: 6850\n",
      "Loss: 11.98138926869803\n",
      "l2 norm of gradients: 0.0656770823863093\n",
      "l2 norm of weights: 3.265905525896894\n",
      "---------------------\n",
      "Iteration Number: 6851\n",
      "Loss: 11.980871462424767\n",
      "l2 norm of gradients: 0.06566379450363141\n",
      "l2 norm of weights: 3.2659669473955515\n",
      "---------------------\n",
      "Iteration Number: 6852\n",
      "Loss: 11.980353913928433\n",
      "l2 norm of gradients: 0.06565051480757526\n",
      "l2 norm of weights: 3.2660283805576946\n",
      "---------------------\n",
      "Iteration Number: 6853\n",
      "Loss: 11.979836623003493\n",
      "l2 norm of gradients: 0.06563724328980998\n",
      "l2 norm of weights: 3.2660898253657336\n",
      "---------------------\n",
      "Iteration Number: 6854\n",
      "Loss: 11.979319589444676\n",
      "l2 norm of gradients: 0.06562397994201327\n",
      "l2 norm of weights: 3.266151281802094\n",
      "---------------------\n",
      "Iteration Number: 6855\n",
      "Loss: 11.978802813046915\n",
      "l2 norm of gradients: 0.06561072475587149\n",
      "l2 norm of weights: 3.2662127498492195\n",
      "---------------------\n",
      "Iteration Number: 6856\n",
      "Loss: 11.978286293605422\n",
      "l2 norm of gradients: 0.06559747772307964\n",
      "l2 norm of weights: 3.266274229489572\n",
      "---------------------\n",
      "Iteration Number: 6857\n",
      "Loss: 11.977770030915607\n",
      "l2 norm of gradients: 0.06558423883534126\n",
      "l2 norm of weights: 3.2663357207056287\n",
      "---------------------\n",
      "Iteration Number: 6858\n",
      "Loss: 11.977254024773146\n",
      "l2 norm of gradients: 0.06557100808436851\n",
      "l2 norm of weights: 3.266397223479886\n",
      "---------------------\n",
      "Iteration Number: 6859\n",
      "Loss: 11.976738274973956\n",
      "l2 norm of gradients: 0.06555778546188218\n",
      "l2 norm of weights: 3.266458737794856\n",
      "---------------------\n",
      "Iteration Number: 6860\n",
      "Loss: 11.976222781314181\n",
      "l2 norm of gradients: 0.06554457095961161\n",
      "l2 norm of weights: 3.2665202636330677\n",
      "---------------------\n",
      "Iteration Number: 6861\n",
      "Loss: 11.975707543590193\n",
      "l2 norm of gradients: 0.06553136456929469\n",
      "l2 norm of weights: 3.2665818009770686\n",
      "---------------------\n",
      "Iteration Number: 6862\n",
      "Loss: 11.975192561598647\n",
      "l2 norm of gradients: 0.065518166282678\n",
      "l2 norm of weights: 3.266643349809422\n",
      "---------------------\n",
      "Iteration Number: 6863\n",
      "Loss: 11.974677835136372\n",
      "l2 norm of gradients: 0.06550497609151656\n",
      "l2 norm of weights: 3.26670491011271\n",
      "---------------------\n",
      "Iteration Number: 6864\n",
      "Loss: 11.97416336400049\n",
      "l2 norm of gradients: 0.06549179398757404\n",
      "l2 norm of weights: 3.2667664818695292\n",
      "---------------------\n",
      "Iteration Number: 6865\n",
      "Loss: 11.973649147988322\n",
      "l2 norm of gradients: 0.06547861996262265\n",
      "l2 norm of weights: 3.2668280650624957\n",
      "---------------------\n",
      "Iteration Number: 6866\n",
      "Loss: 11.973135186897453\n",
      "l2 norm of gradients: 0.06546545400844316\n",
      "l2 norm of weights: 3.2668896596742405\n",
      "---------------------\n",
      "Iteration Number: 6867\n",
      "Loss: 11.972621480525682\n",
      "l2 norm of gradients: 0.06545229611682486\n",
      "l2 norm of weights: 3.266951265687414\n",
      "---------------------\n",
      "Iteration Number: 6868\n",
      "Loss: 11.972108028671055\n",
      "l2 norm of gradients: 0.06543914627956565\n",
      "l2 norm of weights: 3.2670128830846816\n",
      "---------------------\n",
      "Iteration Number: 6869\n",
      "Loss: 11.971594831131842\n",
      "l2 norm of gradients: 0.06542600448847183\n",
      "l2 norm of weights: 3.2670745118487265\n",
      "---------------------\n",
      "Iteration Number: 6870\n",
      "Loss: 11.971081887706593\n",
      "l2 norm of gradients: 0.06541287073535844\n",
      "l2 norm of weights: 3.267136151962249\n",
      "---------------------\n",
      "Iteration Number: 6871\n",
      "Loss: 11.970569198194024\n",
      "l2 norm of gradients: 0.06539974501204887\n",
      "l2 norm of weights: 3.267197803407966\n",
      "---------------------\n",
      "Iteration Number: 6872\n",
      "Loss: 11.97005676239313\n",
      "l2 norm of gradients: 0.06538662731037512\n",
      "l2 norm of weights: 3.267259466168611\n",
      "---------------------\n",
      "Iteration Number: 6873\n",
      "Loss: 11.969544580103134\n",
      "l2 norm of gradients: 0.06537351762217769\n",
      "l2 norm of weights: 3.2673211402269358\n",
      "---------------------\n",
      "Iteration Number: 6874\n",
      "Loss: 11.969032651123486\n",
      "l2 norm of gradients: 0.06536041593930557\n",
      "l2 norm of weights: 3.2673828255657074\n",
      "---------------------\n",
      "Iteration Number: 6875\n",
      "Loss: 11.968520975253877\n",
      "l2 norm of gradients: 0.06534732225361628\n",
      "l2 norm of weights: 3.2674445221677106\n",
      "---------------------\n",
      "Iteration Number: 6876\n",
      "Loss: 11.96800955229424\n",
      "l2 norm of gradients: 0.06533423655697583\n",
      "l2 norm of weights: 3.267506230015747\n",
      "---------------------\n",
      "Iteration Number: 6877\n",
      "Loss: 11.96749838204471\n",
      "l2 norm of gradients: 0.06532115884125876\n",
      "l2 norm of weights: 3.2675679490926353\n",
      "---------------------\n",
      "Iteration Number: 6878\n",
      "Loss: 11.966987464305685\n",
      "l2 norm of gradients: 0.065308089098348\n",
      "l2 norm of weights: 3.26762967938121\n",
      "---------------------\n",
      "Iteration Number: 6879\n",
      "Loss: 11.966476798877773\n",
      "l2 norm of gradients: 0.06529502732013508\n",
      "l2 norm of weights: 3.267691420864323\n",
      "---------------------\n",
      "Iteration Number: 6880\n",
      "Loss: 11.965966385561842\n",
      "l2 norm of gradients: 0.06528197349851994\n",
      "l2 norm of weights: 3.2677531735248437\n",
      "---------------------\n",
      "Iteration Number: 6881\n",
      "Loss: 11.965456224158967\n",
      "l2 norm of gradients: 0.065268927625411\n",
      "l2 norm of weights: 3.267814937345657\n",
      "---------------------\n",
      "Iteration Number: 6882\n",
      "Loss: 11.964946314470467\n",
      "l2 norm of gradients: 0.06525588969272514\n",
      "l2 norm of weights: 3.2678767123096657\n",
      "---------------------\n",
      "Iteration Number: 6883\n",
      "Loss: 11.964436656297885\n",
      "l2 norm of gradients: 0.06524285969238777\n",
      "l2 norm of weights: 3.2679384983997886\n",
      "---------------------\n",
      "Iteration Number: 6884\n",
      "Loss: 11.963927249442989\n",
      "l2 norm of gradients: 0.06522983761633265\n",
      "l2 norm of weights: 3.268000295598961\n",
      "---------------------\n",
      "Iteration Number: 6885\n",
      "Loss: 11.963418093707809\n",
      "l2 norm of gradients: 0.06521682345650204\n",
      "l2 norm of weights: 3.268062103890136\n",
      "---------------------\n",
      "Iteration Number: 6886\n",
      "Loss: 11.962909188894567\n",
      "l2 norm of gradients: 0.06520381720484666\n",
      "l2 norm of weights: 3.268123923256282\n",
      "---------------------\n",
      "Iteration Number: 6887\n",
      "Loss: 11.962400534805752\n",
      "l2 norm of gradients: 0.06519081885332566\n",
      "l2 norm of weights: 3.2681857536803864\n",
      "---------------------\n",
      "Iteration Number: 6888\n",
      "Loss: 11.96189213124404\n",
      "l2 norm of gradients: 0.06517782839390657\n",
      "l2 norm of weights: 3.26824759514545\n",
      "---------------------\n",
      "Iteration Number: 6889\n",
      "Loss: 11.961383978012364\n",
      "l2 norm of gradients: 0.06516484581856542\n",
      "l2 norm of weights: 3.2683094476344916\n",
      "---------------------\n",
      "Iteration Number: 6890\n",
      "Loss: 11.960876074913882\n",
      "l2 norm of gradients: 0.06515187111928666\n",
      "l2 norm of weights: 3.2683713111305477\n",
      "---------------------\n",
      "Iteration Number: 6891\n",
      "Loss: 11.96036842175199\n",
      "l2 norm of gradients: 0.06513890428806304\n",
      "l2 norm of weights: 3.2684331856166713\n",
      "---------------------\n",
      "Iteration Number: 6892\n",
      "Loss: 11.959861018330283\n",
      "l2 norm of gradients: 0.06512594531689589\n",
      "l2 norm of weights: 3.2684950710759297\n",
      "---------------------\n",
      "Iteration Number: 6893\n",
      "Loss: 11.959353864452618\n",
      "l2 norm of gradients: 0.0651129941977948\n",
      "l2 norm of weights: 3.268556967491409\n",
      "---------------------\n",
      "Iteration Number: 6894\n",
      "Loss: 11.958846959923063\n",
      "l2 norm of gradients: 0.06510005092277785\n",
      "l2 norm of weights: 3.2686188748462115\n",
      "---------------------\n",
      "Iteration Number: 6895\n",
      "Loss: 11.958340304545917\n",
      "l2 norm of gradients: 0.06508711548387149\n",
      "l2 norm of weights: 3.268680793123455\n",
      "---------------------\n",
      "Iteration Number: 6896\n",
      "Loss: 11.957833898125694\n",
      "l2 norm of gradients: 0.06507418787311048\n",
      "l2 norm of weights: 3.2687427223062753\n",
      "---------------------\n",
      "Iteration Number: 6897\n",
      "Loss: 11.957327740467168\n",
      "l2 norm of gradients: 0.06506126808253808\n",
      "l2 norm of weights: 3.268804662377823\n",
      "---------------------\n",
      "Iteration Number: 6898\n",
      "Loss: 11.95682183137529\n",
      "l2 norm of gradients: 0.06504835610420584\n",
      "l2 norm of weights: 3.268866613321267\n",
      "---------------------\n",
      "Iteration Number: 6899\n",
      "Loss: 11.95631617065529\n",
      "l2 norm of gradients: 0.06503545193017375\n",
      "l2 norm of weights: 3.2689285751197916\n",
      "---------------------\n",
      "Iteration Number: 6900\n",
      "Loss: 11.955810758112571\n",
      "l2 norm of gradients: 0.06502255555251005\n",
      "l2 norm of weights: 3.2689905477565966\n",
      "---------------------\n",
      "Iteration Number: 6901\n",
      "Loss: 11.955305593552822\n",
      "l2 norm of gradients: 0.0650096669632915\n",
      "l2 norm of weights: 3.2690525312149004\n",
      "---------------------\n",
      "Iteration Number: 6902\n",
      "Loss: 11.954800676781915\n",
      "l2 norm of gradients: 0.06499678615460303\n",
      "l2 norm of weights: 3.2691145254779364\n",
      "---------------------\n",
      "Iteration Number: 6903\n",
      "Loss: 11.95429600760594\n",
      "l2 norm of gradients: 0.06498391311853806\n",
      "l2 norm of weights: 3.269176530528955\n",
      "---------------------\n",
      "Iteration Number: 6904\n",
      "Loss: 11.953791585831247\n",
      "l2 norm of gradients: 0.06497104784719826\n",
      "l2 norm of weights: 3.2692385463512217\n",
      "---------------------\n",
      "Iteration Number: 6905\n",
      "Loss: 11.95328741126439\n",
      "l2 norm of gradients: 0.06495819033269373\n",
      "l2 norm of weights: 3.2693005729280205\n",
      "---------------------\n",
      "Iteration Number: 6906\n",
      "Loss: 11.952783483712151\n",
      "l2 norm of gradients: 0.0649453405671428\n",
      "l2 norm of weights: 3.2693626102426507\n",
      "---------------------\n",
      "Iteration Number: 6907\n",
      "Loss: 11.952279802981518\n",
      "l2 norm of gradients: 0.06493249854267212\n",
      "l2 norm of weights: 3.2694246582784263\n",
      "---------------------\n",
      "Iteration Number: 6908\n",
      "Loss: 11.951776368879743\n",
      "l2 norm of gradients: 0.06491966425141676\n",
      "l2 norm of weights: 3.2694867170186805\n",
      "---------------------\n",
      "Iteration Number: 6909\n",
      "Loss: 11.951273181214258\n",
      "l2 norm of gradients: 0.06490683768552001\n",
      "l2 norm of weights: 3.2695487864467605\n",
      "---------------------\n",
      "Iteration Number: 6910\n",
      "Loss: 11.950770239792751\n",
      "l2 norm of gradients: 0.06489401883713349\n",
      "l2 norm of weights: 3.2696108665460315\n",
      "---------------------\n",
      "Iteration Number: 6911\n",
      "Loss: 11.950267544423118\n",
      "l2 norm of gradients: 0.06488120769841715\n",
      "l2 norm of weights: 3.269672957299874\n",
      "---------------------\n",
      "Iteration Number: 6912\n",
      "Loss: 11.949765094913474\n",
      "l2 norm of gradients: 0.06486840426153918\n",
      "l2 norm of weights: 3.2697350586916842\n",
      "---------------------\n",
      "Iteration Number: 6913\n",
      "Loss: 11.949262891072166\n",
      "l2 norm of gradients: 0.06485560851867607\n",
      "l2 norm of weights: 3.2697971707048756\n",
      "---------------------\n",
      "Iteration Number: 6914\n",
      "Loss: 11.948760932707744\n",
      "l2 norm of gradients: 0.06484282046201263\n",
      "l2 norm of weights: 3.269859293322878\n",
      "---------------------\n",
      "Iteration Number: 6915\n",
      "Loss: 11.94825921962901\n",
      "l2 norm of gradients: 0.06483004008374188\n",
      "l2 norm of weights: 3.269921426529135\n",
      "---------------------\n",
      "Iteration Number: 6916\n",
      "Loss: 11.947757751644973\n",
      "l2 norm of gradients: 0.06481726737606518\n",
      "l2 norm of weights: 3.2699835703071107\n",
      "---------------------\n",
      "Iteration Number: 6917\n",
      "Loss: 11.94725652856484\n",
      "l2 norm of gradients: 0.06480450233119212\n",
      "l2 norm of weights: 3.270045724640282\n",
      "---------------------\n",
      "Iteration Number: 6918\n",
      "Loss: 11.946755550198072\n",
      "l2 norm of gradients: 0.06479174494134053\n",
      "l2 norm of weights: 3.2701078895121425\n",
      "---------------------\n",
      "Iteration Number: 6919\n",
      "Loss: 11.94625481635434\n",
      "l2 norm of gradients: 0.06477899519873653\n",
      "l2 norm of weights: 3.2701700649062024\n",
      "---------------------\n",
      "Iteration Number: 6920\n",
      "Loss: 11.94575432684353\n",
      "l2 norm of gradients: 0.06476625309561444\n",
      "l2 norm of weights: 3.270232250805988\n",
      "---------------------\n",
      "Iteration Number: 6921\n",
      "Loss: 11.945254081475735\n",
      "l2 norm of gradients: 0.06475351862421688\n",
      "l2 norm of weights: 3.2702944471950417\n",
      "---------------------\n",
      "Iteration Number: 6922\n",
      "Loss: 11.944754080061301\n",
      "l2 norm of gradients: 0.06474079177679463\n",
      "l2 norm of weights: 3.2703566540569216\n",
      "---------------------\n",
      "Iteration Number: 6923\n",
      "Loss: 11.944254322410774\n",
      "l2 norm of gradients: 0.06472807254560678\n",
      "l2 norm of weights: 3.270418871375202\n",
      "---------------------\n",
      "Iteration Number: 6924\n",
      "Loss: 11.943754808334923\n",
      "l2 norm of gradients: 0.06471536092292053\n",
      "l2 norm of weights: 3.2704810991334736\n",
      "---------------------\n",
      "Iteration Number: 6925\n",
      "Loss: 11.943255537644712\n",
      "l2 norm of gradients: 0.06470265690101143\n",
      "l2 norm of weights: 3.2705433373153427\n",
      "---------------------\n",
      "Iteration Number: 6926\n",
      "Loss: 11.94275651015136\n",
      "l2 norm of gradients: 0.06468996047216313\n",
      "l2 norm of weights: 3.270605585904432\n",
      "---------------------\n",
      "Iteration Number: 6927\n",
      "Loss: 11.942257725666275\n",
      "l2 norm of gradients: 0.06467727162866756\n",
      "l2 norm of weights: 3.270667844884379\n",
      "---------------------\n",
      "Iteration Number: 6928\n",
      "Loss: 11.9417591840011\n",
      "l2 norm of gradients: 0.06466459036282478\n",
      "l2 norm of weights: 3.270730114238839\n",
      "---------------------\n",
      "Iteration Number: 6929\n",
      "Loss: 11.941260884967706\n",
      "l2 norm of gradients: 0.06465191666694313\n",
      "l2 norm of weights: 3.2707923939514827\n",
      "---------------------\n",
      "Iteration Number: 6930\n",
      "Loss: 11.940762828378137\n",
      "l2 norm of gradients: 0.06463925053333903\n",
      "l2 norm of weights: 3.2708546840059958\n",
      "---------------------\n",
      "Iteration Number: 6931\n",
      "Loss: 11.940265014044696\n",
      "l2 norm of gradients: 0.06462659195433716\n",
      "l2 norm of weights: 3.2709169843860804\n",
      "---------------------\n",
      "Iteration Number: 6932\n",
      "Loss: 11.939767441779894\n",
      "l2 norm of gradients: 0.06461394092227031\n",
      "l2 norm of weights: 3.2709792950754553\n",
      "---------------------\n",
      "Iteration Number: 6933\n",
      "Loss: 11.939270111396441\n",
      "l2 norm of gradients: 0.06460129742947954\n",
      "l2 norm of weights: 3.271041616057854\n",
      "---------------------\n",
      "Iteration Number: 6934\n",
      "Loss: 11.938773022707286\n",
      "l2 norm of gradients: 0.06458866146831395\n",
      "l2 norm of weights: 3.2711039473170263\n",
      "---------------------\n",
      "Iteration Number: 6935\n",
      "Loss: 11.938276175525582\n",
      "l2 norm of gradients: 0.06457603303113088\n",
      "l2 norm of weights: 3.271166288836738\n",
      "---------------------\n",
      "Iteration Number: 6936\n",
      "Loss: 11.937779569664679\n",
      "l2 norm of gradients: 0.06456341211029577\n",
      "l2 norm of weights: 3.271228640600771\n",
      "---------------------\n",
      "Iteration Number: 6937\n",
      "Loss: 11.937283204938185\n",
      "l2 norm of gradients: 0.06455079869818224\n",
      "l2 norm of weights: 3.2712910025929225\n",
      "---------------------\n",
      "Iteration Number: 6938\n",
      "Loss: 11.936787081159885\n",
      "l2 norm of gradients: 0.06453819278717202\n",
      "l2 norm of weights: 3.271353374797006\n",
      "---------------------\n",
      "Iteration Number: 6939\n",
      "Loss: 11.936291198143785\n",
      "l2 norm of gradients: 0.06452559436965498\n",
      "l2 norm of weights: 3.27141575719685\n",
      "---------------------\n",
      "Iteration Number: 6940\n",
      "Loss: 11.935795555704127\n",
      "l2 norm of gradients: 0.06451300343802915\n",
      "l2 norm of weights: 3.2714781497763\n",
      "---------------------\n",
      "Iteration Number: 6941\n",
      "Loss: 11.93530015365534\n",
      "l2 norm of gradients: 0.0645004199847006\n",
      "l2 norm of weights: 3.271540552519215\n",
      "---------------------\n",
      "Iteration Number: 6942\n",
      "Loss: 11.934804991812086\n",
      "l2 norm of gradients: 0.0644878440020836\n",
      "l2 norm of weights: 3.2716029654094734\n",
      "---------------------\n",
      "Iteration Number: 6943\n",
      "Loss: 11.934310069989229\n",
      "l2 norm of gradients: 0.06447527548260047\n",
      "l2 norm of weights: 3.2716653884309657\n",
      "---------------------\n",
      "Iteration Number: 6944\n",
      "Loss: 11.933815388001847\n",
      "l2 norm of gradients: 0.06446271441868166\n",
      "l2 norm of weights: 3.2717278215676\n",
      "---------------------\n",
      "Iteration Number: 6945\n",
      "Loss: 11.933320945665235\n",
      "l2 norm of gradients: 0.0644501608027657\n",
      "l2 norm of weights: 3.2717902648033\n",
      "---------------------\n",
      "Iteration Number: 6946\n",
      "Loss: 11.932826742794905\n",
      "l2 norm of gradients: 0.0644376146272992\n",
      "l2 norm of weights: 3.2718527181220045\n",
      "---------------------\n",
      "Iteration Number: 6947\n",
      "Loss: 11.932332779206575\n",
      "l2 norm of gradients: 0.06442507588473684\n",
      "l2 norm of weights: 3.271915181507668\n",
      "---------------------\n",
      "Iteration Number: 6948\n",
      "Loss: 11.931839054716164\n",
      "l2 norm of gradients: 0.06441254456754152\n",
      "l2 norm of weights: 3.271977654944261\n",
      "---------------------\n",
      "Iteration Number: 6949\n",
      "Loss: 11.931345569139815\n",
      "l2 norm of gradients: 0.06440002066818395\n",
      "l2 norm of weights: 3.27204013841577\n",
      "---------------------\n",
      "Iteration Number: 6950\n",
      "Loss: 11.93085232229388\n",
      "l2 norm of gradients: 0.06438750417914312\n",
      "l2 norm of weights: 3.2721026319061965\n",
      "---------------------\n",
      "Iteration Number: 6951\n",
      "Loss: 11.930359313994934\n",
      "l2 norm of gradients: 0.06437499509290599\n",
      "l2 norm of weights: 3.272165135399557\n",
      "---------------------\n",
      "Iteration Number: 6952\n",
      "Loss: 11.929866544059752\n",
      "l2 norm of gradients: 0.0643624934019676\n",
      "l2 norm of weights: 3.272227648879885\n",
      "---------------------\n",
      "Iteration Number: 6953\n",
      "Loss: 11.929374012305306\n",
      "l2 norm of gradients: 0.06434999909883102\n",
      "l2 norm of weights: 3.2722901723312283\n",
      "---------------------\n",
      "Iteration Number: 6954\n",
      "Loss: 11.928881718548794\n",
      "l2 norm of gradients: 0.06433751217600736\n",
      "l2 norm of weights: 3.272352705737651\n",
      "---------------------\n",
      "Iteration Number: 6955\n",
      "Loss: 11.928389662607614\n",
      "l2 norm of gradients: 0.06432503262601576\n",
      "l2 norm of weights: 3.272415249083233\n",
      "---------------------\n",
      "Iteration Number: 6956\n",
      "Loss: 11.927897844299393\n",
      "l2 norm of gradients: 0.06431256044138337\n",
      "l2 norm of weights: 3.2724778023520686\n",
      "---------------------\n",
      "Iteration Number: 6957\n",
      "Loss: 11.927406263441958\n",
      "l2 norm of gradients: 0.06430009561464545\n",
      "l2 norm of weights: 3.2725403655282683\n",
      "---------------------\n",
      "Iteration Number: 6958\n",
      "Loss: 11.92691491985332\n",
      "l2 norm of gradients: 0.06428763813834515\n",
      "l2 norm of weights: 3.2726029385959587\n",
      "---------------------\n",
      "Iteration Number: 6959\n",
      "Loss: 11.926423813351747\n",
      "l2 norm of gradients: 0.06427518800503373\n",
      "l2 norm of weights: 3.27266552153928\n",
      "---------------------\n",
      "Iteration Number: 6960\n",
      "Loss: 11.92593294375566\n",
      "l2 norm of gradients: 0.0642627452072704\n",
      "l2 norm of weights: 3.27272811434239\n",
      "---------------------\n",
      "Iteration Number: 6961\n",
      "Loss: 11.925442310883748\n",
      "l2 norm of gradients: 0.06425030973762239\n",
      "l2 norm of weights: 3.272790716989461\n",
      "---------------------\n",
      "Iteration Number: 6962\n",
      "Loss: 11.924951914554857\n",
      "l2 norm of gradients: 0.06423788158866488\n",
      "l2 norm of weights: 3.2728533294646796\n",
      "---------------------\n",
      "Iteration Number: 6963\n",
      "Loss: 11.92446175458806\n",
      "l2 norm of gradients: 0.06422546075298109\n",
      "l2 norm of weights: 3.27291595175225\n",
      "---------------------\n",
      "Iteration Number: 6964\n",
      "Loss: 11.923971830802643\n",
      "l2 norm of gradients: 0.06421304722316219\n",
      "l2 norm of weights: 3.27297858383639\n",
      "---------------------\n",
      "Iteration Number: 6965\n",
      "Loss: 11.923482143018093\n",
      "l2 norm of gradients: 0.06420064099180733\n",
      "l2 norm of weights: 3.2730412257013337\n",
      "---------------------\n",
      "Iteration Number: 6966\n",
      "Loss: 11.922992691054105\n",
      "l2 norm of gradients: 0.06418824205152361\n",
      "l2 norm of weights: 3.2731038773313306\n",
      "---------------------\n",
      "Iteration Number: 6967\n",
      "Loss: 11.922503474730581\n",
      "l2 norm of gradients: 0.0641758503949261\n",
      "l2 norm of weights: 3.2731665387106443\n",
      "---------------------\n",
      "Iteration Number: 6968\n",
      "Loss: 11.922014493867621\n",
      "l2 norm of gradients: 0.06416346601463783\n",
      "l2 norm of weights: 3.273229209823555\n",
      "---------------------\n",
      "Iteration Number: 6969\n",
      "Loss: 11.921525748285545\n",
      "l2 norm of gradients: 0.06415108890328979\n",
      "l2 norm of weights: 3.273291890654359\n",
      "---------------------\n",
      "Iteration Number: 6970\n",
      "Loss: 11.92103723780487\n",
      "l2 norm of gradients: 0.06413871905352084\n",
      "l2 norm of weights: 3.2733545811873648\n",
      "---------------------\n",
      "Iteration Number: 6971\n",
      "Loss: 11.92054896224633\n",
      "l2 norm of gradients: 0.06412635645797789\n",
      "l2 norm of weights: 3.2734172814069\n",
      "---------------------\n",
      "Iteration Number: 6972\n",
      "Loss: 11.920060921430832\n",
      "l2 norm of gradients: 0.06411400110931564\n",
      "l2 norm of weights: 3.273479991297304\n",
      "---------------------\n",
      "Iteration Number: 6973\n",
      "Loss: 11.919573115179528\n",
      "l2 norm of gradients: 0.06410165300019686\n",
      "l2 norm of weights: 3.2735427108429334\n",
      "---------------------\n",
      "Iteration Number: 6974\n",
      "Loss: 11.919085543313757\n",
      "l2 norm of gradients: 0.06408931212329212\n",
      "l2 norm of weights: 3.27360544002816\n",
      "---------------------\n",
      "Iteration Number: 6975\n",
      "Loss: 11.918598205655043\n",
      "l2 norm of gradients: 0.06407697847128\n",
      "l2 norm of weights: 3.273668178837371\n",
      "---------------------\n",
      "Iteration Number: 6976\n",
      "Loss: 11.91811110202516\n",
      "l2 norm of gradients: 0.06406465203684687\n",
      "l2 norm of weights: 3.2737309272549666\n",
      "---------------------\n",
      "Iteration Number: 6977\n",
      "Loss: 11.917624232246032\n",
      "l2 norm of gradients: 0.06405233281268706\n",
      "l2 norm of weights: 3.2737936852653653\n",
      "---------------------\n",
      "Iteration Number: 6978\n",
      "Loss: 11.917137596139838\n",
      "l2 norm of gradients: 0.06404002079150282\n",
      "l2 norm of weights: 3.273856452852998\n",
      "---------------------\n",
      "Iteration Number: 6979\n",
      "Loss: 11.916651193528912\n",
      "l2 norm of gradients: 0.06402771596600425\n",
      "l2 norm of weights: 3.273919230002314\n",
      "---------------------\n",
      "Iteration Number: 6980\n",
      "Loss: 11.91616502423584\n",
      "l2 norm of gradients: 0.06401541832890928\n",
      "l2 norm of weights: 3.273982016697773\n",
      "---------------------\n",
      "Iteration Number: 6981\n",
      "Loss: 11.915679088083357\n",
      "l2 norm of gradients: 0.06400312787294381\n",
      "l2 norm of weights: 3.2740448129238553\n",
      "---------------------\n",
      "Iteration Number: 6982\n",
      "Loss: 11.915193384894442\n",
      "l2 norm of gradients: 0.06399084459084152\n",
      "l2 norm of weights: 3.274107618665052\n",
      "---------------------\n",
      "Iteration Number: 6983\n",
      "Loss: 11.914707914492261\n",
      "l2 norm of gradients: 0.06397856847534403\n",
      "l2 norm of weights: 3.274170433905871\n",
      "---------------------\n",
      "Iteration Number: 6984\n",
      "Loss: 11.914222676700179\n",
      "l2 norm of gradients: 0.06396629951920074\n",
      "l2 norm of weights: 3.274233258630835\n",
      "---------------------\n",
      "Iteration Number: 6985\n",
      "Loss: 11.91373767134177\n",
      "l2 norm of gradients: 0.06395403771516898\n",
      "l2 norm of weights: 3.2742960928244824\n",
      "---------------------\n",
      "Iteration Number: 6986\n",
      "Loss: 11.913252898240817\n",
      "l2 norm of gradients: 0.0639417830560138\n",
      "l2 norm of weights: 3.274358936471366\n",
      "---------------------\n",
      "Iteration Number: 6987\n",
      "Loss: 11.912768357221267\n",
      "l2 norm of gradients: 0.06392953553450823\n",
      "l2 norm of weights: 3.2744217895560532\n",
      "---------------------\n",
      "Iteration Number: 6988\n",
      "Loss: 11.912284048107315\n",
      "l2 norm of gradients: 0.06391729514343297\n",
      "l2 norm of weights: 3.2744846520631277\n",
      "---------------------\n",
      "Iteration Number: 6989\n",
      "Loss: 11.911799970723331\n",
      "l2 norm of gradients: 0.06390506187557672\n",
      "l2 norm of weights: 3.2745475239771875\n",
      "---------------------\n",
      "Iteration Number: 6990\n",
      "Loss: 11.911316124893881\n",
      "l2 norm of gradients: 0.06389283572373586\n",
      "l2 norm of weights: 3.274610405282844\n",
      "---------------------\n",
      "Iteration Number: 6991\n",
      "Loss: 11.91083251044374\n",
      "l2 norm of gradients: 0.06388061668071461\n",
      "l2 norm of weights: 3.274673295964727\n",
      "---------------------\n",
      "Iteration Number: 6992\n",
      "Loss: 11.910349127197883\n",
      "l2 norm of gradients: 0.06386840473932502\n",
      "l2 norm of weights: 3.274736196007478\n",
      "---------------------\n",
      "Iteration Number: 6993\n",
      "Loss: 11.90986597498149\n",
      "l2 norm of gradients: 0.06385619989238692\n",
      "l2 norm of weights: 3.274799105395756\n",
      "---------------------\n",
      "Iteration Number: 6994\n",
      "Loss: 11.909383053619925\n",
      "l2 norm of gradients: 0.06384400213272796\n",
      "l2 norm of weights: 3.274862024114232\n",
      "---------------------\n",
      "Iteration Number: 6995\n",
      "Loss: 11.90890036293876\n",
      "l2 norm of gradients: 0.0638318114531835\n",
      "l2 norm of weights: 3.2749249521475945\n",
      "---------------------\n",
      "Iteration Number: 6996\n",
      "Loss: 11.908417902763766\n",
      "l2 norm of gradients: 0.06381962784659678\n",
      "l2 norm of weights: 3.274987889480546\n",
      "---------------------\n",
      "Iteration Number: 6997\n",
      "Loss: 11.907935672920903\n",
      "l2 norm of gradients: 0.06380745130581872\n",
      "l2 norm of weights: 3.2750508360978032\n",
      "---------------------\n",
      "Iteration Number: 6998\n",
      "Loss: 11.907453673236354\n",
      "l2 norm of gradients: 0.06379528182370807\n",
      "l2 norm of weights: 3.275113791984099\n",
      "---------------------\n",
      "Iteration Number: 6999\n",
      "Loss: 11.906971903536466\n",
      "l2 norm of gradients: 0.0637831193931313\n",
      "l2 norm of weights: 3.2751767571241794\n",
      "---------------------\n",
      "Iteration Number: 7000\n",
      "Loss: 11.906490363647803\n",
      "l2 norm of gradients: 0.06377096400696268\n",
      "l2 norm of weights: 3.2752397315028077\n",
      "---------------------\n",
      "Iteration Number: 7001\n",
      "Loss: 11.906009053397128\n",
      "l2 norm of gradients: 0.06375881565808417\n",
      "l2 norm of weights: 3.2753027151047593\n",
      "---------------------\n",
      "Iteration Number: 7002\n",
      "Loss: 11.9055279726114\n",
      "l2 norm of gradients: 0.06374667433938552\n",
      "l2 norm of weights: 3.2753657079148257\n",
      "---------------------\n",
      "Iteration Number: 7003\n",
      "Loss: 11.90504712111775\n",
      "l2 norm of gradients: 0.06373454004376417\n",
      "l2 norm of weights: 3.2754287099178137\n",
      "---------------------\n",
      "Iteration Number: 7004\n",
      "Loss: 11.904566498743549\n",
      "l2 norm of gradients: 0.06372241276412528\n",
      "l2 norm of weights: 3.2754917210985433\n",
      "---------------------\n",
      "Iteration Number: 7005\n",
      "Loss: 11.904086105316328\n",
      "l2 norm of gradients: 0.06371029249338181\n",
      "l2 norm of weights: 3.275554741441851\n",
      "---------------------\n",
      "Iteration Number: 7006\n",
      "Loss: 11.903605940663844\n",
      "l2 norm of gradients: 0.06369817922445437\n",
      "l2 norm of weights: 3.275617770932587\n",
      "---------------------\n",
      "Iteration Number: 7007\n",
      "Loss: 11.903126004614004\n",
      "l2 norm of gradients: 0.06368607295027129\n",
      "l2 norm of weights: 3.2756808095556167\n",
      "---------------------\n",
      "Iteration Number: 7008\n",
      "Loss: 11.90264629699496\n",
      "l2 norm of gradients: 0.06367397366376863\n",
      "l2 norm of weights: 3.275743857295819\n",
      "---------------------\n",
      "Iteration Number: 7009\n",
      "Loss: 11.902166817635031\n",
      "l2 norm of gradients: 0.06366188135789011\n",
      "l2 norm of weights: 3.275806914138089\n",
      "---------------------\n",
      "Iteration Number: 7010\n",
      "Loss: 11.901687566362742\n",
      "l2 norm of gradients: 0.0636497960255871\n",
      "l2 norm of weights: 3.275869980067336\n",
      "---------------------\n",
      "Iteration Number: 7011\n",
      "Loss: 11.901208543006803\n",
      "l2 norm of gradients: 0.0636377176598188\n",
      "l2 norm of weights: 3.2759330550684838\n",
      "---------------------\n",
      "Iteration Number: 7012\n",
      "Loss: 11.900729747396129\n",
      "l2 norm of gradients: 0.06362564625355192\n",
      "l2 norm of weights: 3.2759961391264705\n",
      "---------------------\n",
      "Iteration Number: 7013\n",
      "Loss: 11.900251179359833\n",
      "l2 norm of gradients: 0.063613581799761\n",
      "l2 norm of weights: 3.2760592322262494\n",
      "---------------------\n",
      "Iteration Number: 7014\n",
      "Loss: 11.89977283872719\n",
      "l2 norm of gradients: 0.06360152429142808\n",
      "l2 norm of weights: 3.2761223343527885\n",
      "---------------------\n",
      "Iteration Number: 7015\n",
      "Loss: 11.899294725327701\n",
      "l2 norm of gradients: 0.06358947372154299\n",
      "l2 norm of weights: 3.2761854454910697\n",
      "---------------------\n",
      "Iteration Number: 7016\n",
      "Loss: 11.898816838991062\n",
      "l2 norm of gradients: 0.06357743008310311\n",
      "l2 norm of weights: 3.2762485656260893\n",
      "---------------------\n",
      "Iteration Number: 7017\n",
      "Loss: 11.89833917954714\n",
      "l2 norm of gradients: 0.06356539336911358\n",
      "l2 norm of weights: 3.27631169474286\n",
      "---------------------\n",
      "Iteration Number: 7018\n",
      "Loss: 11.897861746825992\n",
      "l2 norm of gradients: 0.06355336357258709\n",
      "l2 norm of weights: 3.276374832826407\n",
      "---------------------\n",
      "Iteration Number: 7019\n",
      "Loss: 11.897384540657905\n",
      "l2 norm of gradients: 0.06354134068654398\n",
      "l2 norm of weights: 3.276437979861771\n",
      "---------------------\n",
      "Iteration Number: 7020\n",
      "Loss: 11.896907560873327\n",
      "l2 norm of gradients: 0.06352932470401221\n",
      "l2 norm of weights: 3.2765011358340064\n",
      "---------------------\n",
      "Iteration Number: 7021\n",
      "Loss: 11.896430807302888\n",
      "l2 norm of gradients: 0.06351731561802744\n",
      "l2 norm of weights: 3.2765643007281837\n",
      "---------------------\n",
      "Iteration Number: 7022\n",
      "Loss: 11.89595427977745\n",
      "l2 norm of gradients: 0.06350531342163285\n",
      "l2 norm of weights: 3.2766274745293864\n",
      "---------------------\n",
      "Iteration Number: 7023\n",
      "Loss: 11.895477978128035\n",
      "l2 norm of gradients: 0.06349331810787925\n",
      "l2 norm of weights: 3.276690657222713\n",
      "---------------------\n",
      "Iteration Number: 7024\n",
      "Loss: 11.89500190218585\n",
      "l2 norm of gradients: 0.06348132966982505\n",
      "l2 norm of weights: 3.276753848793276\n",
      "---------------------\n",
      "Iteration Number: 7025\n",
      "Loss: 11.89452605178233\n",
      "l2 norm of gradients: 0.06346934810053632\n",
      "l2 norm of weights: 3.276817049226204\n",
      "---------------------\n",
      "Iteration Number: 7026\n",
      "Loss: 11.894050426749061\n",
      "l2 norm of gradients: 0.06345737339308659\n",
      "l2 norm of weights: 3.2768802585066377\n",
      "---------------------\n",
      "Iteration Number: 7027\n",
      "Loss: 11.893575026917842\n",
      "l2 norm of gradients: 0.06344540554055711\n",
      "l2 norm of weights: 3.2769434766197336\n",
      "---------------------\n",
      "Iteration Number: 7028\n",
      "Loss: 11.893099852120649\n",
      "l2 norm of gradients: 0.0634334445360366\n",
      "l2 norm of weights: 3.2770067035506627\n",
      "---------------------\n",
      "Iteration Number: 7029\n",
      "Loss: 11.892624902189672\n",
      "l2 norm of gradients: 0.0634214903726214\n",
      "l2 norm of weights: 3.277069939284609\n",
      "---------------------\n",
      "Iteration Number: 7030\n",
      "Loss: 11.892150176957259\n",
      "l2 norm of gradients: 0.06340954304341544\n",
      "l2 norm of weights: 3.2771331838067734\n",
      "---------------------\n",
      "Iteration Number: 7031\n",
      "Loss: 11.891675676255971\n",
      "l2 norm of gradients: 0.06339760254153012\n",
      "l2 norm of weights: 3.277196437102368\n",
      "---------------------\n",
      "Iteration Number: 7032\n",
      "Loss: 11.89120139991853\n",
      "l2 norm of gradients: 0.06338566886008447\n",
      "l2 norm of weights: 3.2772596991566223\n",
      "---------------------\n",
      "Iteration Number: 7033\n",
      "Loss: 11.890727347777892\n",
      "l2 norm of gradients: 0.06337374199220504\n",
      "l2 norm of weights: 3.2773229699547777\n",
      "---------------------\n",
      "Iteration Number: 7034\n",
      "Loss: 11.890253519667167\n",
      "l2 norm of gradients: 0.0633618219310259\n",
      "l2 norm of weights: 3.2773862494820913\n",
      "---------------------\n",
      "Iteration Number: 7035\n",
      "Loss: 11.889779915419659\n",
      "l2 norm of gradients: 0.06334990866968872\n",
      "l2 norm of weights: 3.2774495377238337\n",
      "---------------------\n",
      "Iteration Number: 7036\n",
      "Loss: 11.889306534868851\n",
      "l2 norm of gradients: 0.06333800220134254\n",
      "l2 norm of weights: 3.277512834665291\n",
      "---------------------\n",
      "Iteration Number: 7037\n",
      "Loss: 11.888833377848453\n",
      "l2 norm of gradients: 0.06332610251914413\n",
      "l2 norm of weights: 3.277576140291762\n",
      "---------------------\n",
      "Iteration Number: 7038\n",
      "Loss: 11.88836044419231\n",
      "l2 norm of gradients: 0.0633142096162576\n",
      "l2 norm of weights: 3.27763945458856\n",
      "---------------------\n",
      "Iteration Number: 7039\n",
      "Loss: 11.887887733734505\n",
      "l2 norm of gradients: 0.06330232348585459\n",
      "l2 norm of weights: 3.2777027775410144\n",
      "---------------------\n",
      "Iteration Number: 7040\n",
      "Loss: 11.88741524630926\n",
      "l2 norm of gradients: 0.06329044412111438\n",
      "l2 norm of weights: 3.2777661091344665\n",
      "---------------------\n",
      "Iteration Number: 7041\n",
      "Loss: 11.886942981751014\n",
      "l2 norm of gradients: 0.06327857151522359\n",
      "l2 norm of weights: 3.2778294493542726\n",
      "---------------------\n",
      "Iteration Number: 7042\n",
      "Loss: 11.88647093989439\n",
      "l2 norm of gradients: 0.06326670566137638\n",
      "l2 norm of weights: 3.2778927981858046\n",
      "---------------------\n",
      "Iteration Number: 7043\n",
      "Loss: 11.885999120574187\n",
      "l2 norm of gradients: 0.06325484655277437\n",
      "l2 norm of weights: 3.2779561556144454\n",
      "---------------------\n",
      "Iteration Number: 7044\n",
      "Loss: 11.885527523625399\n",
      "l2 norm of gradients: 0.06324299418262669\n",
      "l2 norm of weights: 3.2780195216255956\n",
      "---------------------\n",
      "Iteration Number: 7045\n",
      "Loss: 11.885056148883212\n",
      "l2 norm of gradients: 0.06323114854414993\n",
      "l2 norm of weights: 3.2780828962046673\n",
      "---------------------\n",
      "Iteration Number: 7046\n",
      "Loss: 11.884584996182962\n",
      "l2 norm of gradients: 0.06321930963056814\n",
      "l2 norm of weights: 3.278146279337088\n",
      "---------------------\n",
      "Iteration Number: 7047\n",
      "Loss: 11.884114065360226\n",
      "l2 norm of gradients: 0.06320747743511278\n",
      "l2 norm of weights: 3.2782096710082995\n",
      "---------------------\n",
      "Iteration Number: 7048\n",
      "Loss: 11.88364335625072\n",
      "l2 norm of gradients: 0.06319565195102282\n",
      "l2 norm of weights: 3.278273071203756\n",
      "---------------------\n",
      "Iteration Number: 7049\n",
      "Loss: 11.883172868690375\n",
      "l2 norm of gradients: 0.06318383317154468\n",
      "l2 norm of weights: 3.2783364799089294\n",
      "---------------------\n",
      "Iteration Number: 7050\n",
      "Loss: 11.88270260251527\n",
      "l2 norm of gradients: 0.06317202108993213\n",
      "l2 norm of weights: 3.278399897109301\n",
      "---------------------\n",
      "Iteration Number: 7051\n",
      "Loss: 11.882232557561707\n",
      "l2 norm of gradients: 0.06316021569944642\n",
      "l2 norm of weights: 3.27846332279037\n",
      "---------------------\n",
      "Iteration Number: 7052\n",
      "Loss: 11.881762733666156\n",
      "l2 norm of gradients: 0.06314841699335627\n",
      "l2 norm of weights: 3.278526756937647\n",
      "---------------------\n",
      "Iteration Number: 7053\n",
      "Loss: 11.88129313066527\n",
      "l2 norm of gradients: 0.06313662496493777\n",
      "l2 norm of weights: 3.2785901995366586\n",
      "---------------------\n",
      "Iteration Number: 7054\n",
      "Loss: 11.880823748395889\n",
      "l2 norm of gradients: 0.0631248396074744\n",
      "l2 norm of weights: 3.2786536505729442\n",
      "---------------------\n",
      "Iteration Number: 7055\n",
      "Loss: 11.880354586695024\n",
      "l2 norm of gradients: 0.06311306091425709\n",
      "l2 norm of weights: 3.2787171100320576\n",
      "---------------------\n",
      "Iteration Number: 7056\n",
      "Loss: 11.879885645399897\n",
      "l2 norm of gradients: 0.06310128887858414\n",
      "l2 norm of weights: 3.278780577899567\n",
      "---------------------\n",
      "Iteration Number: 7057\n",
      "Loss: 11.879416924347876\n",
      "l2 norm of gradients: 0.06308952349376123\n",
      "l2 norm of weights: 3.2788440541610537\n",
      "---------------------\n",
      "Iteration Number: 7058\n",
      "Loss: 11.878948423376546\n",
      "l2 norm of gradients: 0.06307776475310145\n",
      "l2 norm of weights: 3.2789075388021134\n",
      "---------------------\n",
      "Iteration Number: 7059\n",
      "Loss: 11.878480142323644\n",
      "l2 norm of gradients: 0.06306601264992528\n",
      "l2 norm of weights: 3.2789710318083563\n",
      "---------------------\n",
      "Iteration Number: 7060\n",
      "Loss: 11.87801208102712\n",
      "l2 norm of gradients: 0.06305426717756053\n",
      "l2 norm of weights: 3.2790345331654054\n",
      "---------------------\n",
      "Iteration Number: 7061\n",
      "Loss: 11.87754423932508\n",
      "l2 norm of gradients: 0.06304252832934241\n",
      "l2 norm of weights: 3.2790980428588985\n",
      "---------------------\n",
      "Iteration Number: 7062\n",
      "Loss: 11.877076617055831\n",
      "l2 norm of gradients: 0.06303079609861348\n",
      "l2 norm of weights: 3.2791615608744866\n",
      "---------------------\n",
      "Iteration Number: 7063\n",
      "Loss: 11.876609214057837\n",
      "l2 norm of gradients: 0.06301907047872365\n",
      "l2 norm of weights: 3.279225087197836\n",
      "---------------------\n",
      "Iteration Number: 7064\n",
      "Loss: 11.876142030169767\n",
      "l2 norm of gradients: 0.06300735146303017\n",
      "l2 norm of weights: 3.279288621814624\n",
      "---------------------\n",
      "Iteration Number: 7065\n",
      "Loss: 11.87567506523046\n",
      "l2 norm of gradients: 0.06299563904489765\n",
      "l2 norm of weights: 3.279352164710546\n",
      "---------------------\n",
      "Iteration Number: 7066\n",
      "Loss: 11.87520831907894\n",
      "l2 norm of gradients: 0.06298393321769799\n",
      "l2 norm of weights: 3.279415715871307\n",
      "---------------------\n",
      "Iteration Number: 7067\n",
      "Loss: 11.874741791554403\n",
      "l2 norm of gradients: 0.0629722339748105\n",
      "l2 norm of weights: 3.2794792752826285\n",
      "---------------------\n",
      "Iteration Number: 7068\n",
      "Loss: 11.874275482496248\n",
      "l2 norm of gradients: 0.06296054130962173\n",
      "l2 norm of weights: 3.279542842930245\n",
      "---------------------\n",
      "Iteration Number: 7069\n",
      "Loss: 11.873809391744016\n",
      "l2 norm of gradients: 0.06294885521552558\n",
      "l2 norm of weights: 3.2796064187999048\n",
      "---------------------\n",
      "Iteration Number: 7070\n",
      "Loss: 11.873343519137451\n",
      "l2 norm of gradients: 0.06293717568592327\n",
      "l2 norm of weights: 3.2796700028773693\n",
      "---------------------\n",
      "Iteration Number: 7071\n",
      "Loss: 11.872877864516495\n",
      "l2 norm of gradients: 0.0629255027142233\n",
      "l2 norm of weights: 3.2797335951484157\n",
      "---------------------\n",
      "Iteration Number: 7072\n",
      "Loss: 11.87241242772122\n",
      "l2 norm of gradients: 0.06291383629384145\n",
      "l2 norm of weights: 3.2797971955988325\n",
      "---------------------\n",
      "Iteration Number: 7073\n",
      "Loss: 11.871947208591926\n",
      "l2 norm of gradients: 0.0629021764182008\n",
      "l2 norm of weights: 3.2798608042144237\n",
      "---------------------\n",
      "Iteration Number: 7074\n",
      "Loss: 11.871482206969066\n",
      "l2 norm of gradients: 0.06289052308073177\n",
      "l2 norm of weights: 3.2799244209810063\n",
      "---------------------\n",
      "Iteration Number: 7075\n",
      "Loss: 11.871017422693262\n",
      "l2 norm of gradients: 0.06287887627487197\n",
      "l2 norm of weights: 3.2799880458844113\n",
      "---------------------\n",
      "Iteration Number: 7076\n",
      "Loss: 11.87055285560535\n",
      "l2 norm of gradients: 0.06286723599406636\n",
      "l2 norm of weights: 3.2800516789104828\n",
      "---------------------\n",
      "Iteration Number: 7077\n",
      "Loss: 11.870088505546308\n",
      "l2 norm of gradients: 0.06285560223176712\n",
      "l2 norm of weights: 3.280115320045079\n",
      "---------------------\n",
      "Iteration Number: 7078\n",
      "Loss: 11.869624372357316\n",
      "l2 norm of gradients: 0.06284397498143365\n",
      "l2 norm of weights: 3.280178969274073\n",
      "---------------------\n",
      "Iteration Number: 7079\n",
      "Loss: 11.869160455879708\n",
      "l2 norm of gradients: 0.06283235423653268\n",
      "l2 norm of weights: 3.2802426265833486\n",
      "---------------------\n",
      "Iteration Number: 7080\n",
      "Loss: 11.868696755955018\n",
      "l2 norm of gradients: 0.06282073999053814\n",
      "l2 norm of weights: 3.2803062919588064\n",
      "---------------------\n",
      "Iteration Number: 7081\n",
      "Loss: 11.868233272424956\n",
      "l2 norm of gradients: 0.06280913223693119\n",
      "l2 norm of weights: 3.280369965386358\n",
      "---------------------\n",
      "Iteration Number: 7082\n",
      "Loss: 11.86777000513138\n",
      "l2 norm of gradients: 0.06279753096920025\n",
      "l2 norm of weights: 3.2804336468519306\n",
      "---------------------\n",
      "Iteration Number: 7083\n",
      "Loss: 11.867306953916364\n",
      "l2 norm of gradients: 0.06278593618084093\n",
      "l2 norm of weights: 3.280497336341465\n",
      "---------------------\n",
      "Iteration Number: 7084\n",
      "Loss: 11.86684411862213\n",
      "l2 norm of gradients: 0.06277434786535614\n",
      "l2 norm of weights: 3.2805610338409132\n",
      "---------------------\n",
      "Iteration Number: 7085\n",
      "Loss: 11.866381499091084\n",
      "l2 norm of gradients: 0.06276276601625587\n",
      "l2 norm of weights: 3.280624739336244\n",
      "---------------------\n",
      "Iteration Number: 7086\n",
      "Loss: 11.865919095165816\n",
      "l2 norm of gradients: 0.06275119062705745\n",
      "l2 norm of weights: 3.2806884528134375\n",
      "---------------------\n",
      "Iteration Number: 7087\n",
      "Loss: 11.865456906689086\n",
      "l2 norm of gradients: 0.06273962169128532\n",
      "l2 norm of weights: 3.280752174258488\n",
      "---------------------\n",
      "Iteration Number: 7088\n",
      "Loss: 11.864994933503809\n",
      "l2 norm of gradients: 0.06272805920247115\n",
      "l2 norm of weights: 3.280815903657404\n",
      "---------------------\n",
      "Iteration Number: 7089\n",
      "Loss: 11.864533175453122\n",
      "l2 norm of gradients: 0.06271650315415381\n",
      "l2 norm of weights: 3.280879640996206\n",
      "---------------------\n",
      "Iteration Number: 7090\n",
      "Loss: 11.864071632380295\n",
      "l2 norm of gradients: 0.06270495353987932\n",
      "l2 norm of weights: 3.2809433862609296\n",
      "---------------------\n",
      "Iteration Number: 7091\n",
      "Loss: 11.863610304128791\n",
      "l2 norm of gradients: 0.06269341035320086\n",
      "l2 norm of weights: 3.281007139437623\n",
      "---------------------\n",
      "Iteration Number: 7092\n",
      "Loss: 11.863149190542229\n",
      "l2 norm of gradients: 0.06268187358767884\n",
      "l2 norm of weights: 3.281070900512348\n",
      "---------------------\n",
      "Iteration Number: 7093\n",
      "Loss: 11.862688291464442\n",
      "l2 norm of gradients: 0.0626703432368808\n",
      "l2 norm of weights: 3.28113466947118\n",
      "---------------------\n",
      "Iteration Number: 7094\n",
      "Loss: 11.862227606739387\n",
      "l2 norm of gradients: 0.06265881929438144\n",
      "l2 norm of weights: 3.2811984463002077\n",
      "---------------------\n",
      "Iteration Number: 7095\n",
      "Loss: 11.861767136211228\n",
      "l2 norm of gradients: 0.06264730175376254\n",
      "l2 norm of weights: 3.281262230985534\n",
      "---------------------\n",
      "Iteration Number: 7096\n",
      "Loss: 11.861306879724298\n",
      "l2 norm of gradients: 0.06263579060861316\n",
      "l2 norm of weights: 3.2813260235132744\n",
      "---------------------\n",
      "Iteration Number: 7097\n",
      "Loss: 11.860846837123095\n",
      "l2 norm of gradients: 0.06262428585252937\n",
      "l2 norm of weights: 3.281389823869557\n",
      "---------------------\n",
      "Iteration Number: 7098\n",
      "Loss: 11.860387008252296\n",
      "l2 norm of gradients: 0.06261278747911445\n",
      "l2 norm of weights: 3.2814536320405256\n",
      "---------------------\n",
      "Iteration Number: 7099\n",
      "Loss: 11.859927392956742\n",
      "l2 norm of gradients: 0.06260129548197872\n",
      "l2 norm of weights: 3.2815174480123357\n",
      "---------------------\n",
      "Iteration Number: 7100\n",
      "Loss: 11.859467991081452\n",
      "l2 norm of gradients: 0.06258980985473976\n",
      "l2 norm of weights: 3.281581271771156\n",
      "---------------------\n",
      "Iteration Number: 7101\n",
      "Loss: 11.859008802471632\n",
      "l2 norm of gradients: 0.0625783305910221\n",
      "l2 norm of weights: 3.2816451033031697\n",
      "---------------------\n",
      "Iteration Number: 7102\n",
      "Loss: 11.858549826972629\n",
      "l2 norm of gradients: 0.0625668576844575\n",
      "l2 norm of weights: 3.2817089425945727\n",
      "---------------------\n",
      "Iteration Number: 7103\n",
      "Loss: 11.858091064429994\n",
      "l2 norm of gradients: 0.06255539112868468\n",
      "l2 norm of weights: 3.281772789631574\n",
      "---------------------\n",
      "Iteration Number: 7104\n",
      "Loss: 11.857632514689424\n",
      "l2 norm of gradients: 0.06254393091734964\n",
      "l2 norm of weights: 3.2818366444003964\n",
      "---------------------\n",
      "Iteration Number: 7105\n",
      "Loss: 11.857174177596807\n",
      "l2 norm of gradients: 0.0625324770441053\n",
      "l2 norm of weights: 3.281900506887276\n",
      "---------------------\n",
      "Iteration Number: 7106\n",
      "Loss: 11.856716052998179\n",
      "l2 norm of gradients: 0.06252102950261175\n",
      "l2 norm of weights: 3.2819643770784617\n",
      "---------------------\n",
      "Iteration Number: 7107\n",
      "Loss: 11.85625814073977\n",
      "l2 norm of gradients: 0.0625095882865361\n",
      "l2 norm of weights: 3.282028254960216\n",
      "---------------------\n",
      "Iteration Number: 7108\n",
      "Loss: 11.85580044066798\n",
      "l2 norm of gradients: 0.06249815338955257\n",
      "l2 norm of weights: 3.282092140518815\n",
      "---------------------\n",
      "Iteration Number: 7109\n",
      "Loss: 11.855342952629357\n",
      "l2 norm of gradients: 0.0624867248053424\n",
      "l2 norm of weights: 3.282156033740547\n",
      "---------------------\n",
      "Iteration Number: 7110\n",
      "Loss: 11.85488567647064\n",
      "l2 norm of gradients: 0.0624753025275939\n",
      "l2 norm of weights: 3.2822199346117147\n",
      "---------------------\n",
      "Iteration Number: 7111\n",
      "Loss: 11.854428612038726\n",
      "l2 norm of gradients: 0.06246388655000244\n",
      "l2 norm of weights: 3.2822838431186336\n",
      "---------------------\n",
      "Iteration Number: 7112\n",
      "Loss: 11.853971759180693\n",
      "l2 norm of gradients: 0.062452476866270444\n",
      "l2 norm of weights: 3.282347759247632\n",
      "---------------------\n",
      "Iteration Number: 7113\n",
      "Loss: 11.85351511774378\n",
      "l2 norm of gradients: 0.062441073470107335\n",
      "l2 norm of weights: 3.2824116829850523\n",
      "---------------------\n",
      "Iteration Number: 7114\n",
      "Loss: 11.853058687575398\n",
      "l2 norm of gradients: 0.06242967635522956\n",
      "l2 norm of weights: 3.2824756143172493\n",
      "---------------------\n",
      "Iteration Number: 7115\n",
      "Loss: 11.852602468523138\n",
      "l2 norm of gradients: 0.06241828551536064\n",
      "l2 norm of weights: 3.282539553230591\n",
      "---------------------\n",
      "Iteration Number: 7116\n",
      "Loss: 11.852146460434728\n",
      "l2 norm of gradients: 0.062406900944231014\n",
      "l2 norm of weights: 3.282603499711459\n",
      "---------------------\n",
      "Iteration Number: 7117\n",
      "Loss: 11.85169066315809\n",
      "l2 norm of gradients: 0.062395522635578266\n",
      "l2 norm of weights: 3.2826674537462472\n",
      "---------------------\n",
      "Iteration Number: 7118\n",
      "Loss: 11.851235076541323\n",
      "l2 norm of gradients: 0.062384150583146884\n",
      "l2 norm of weights: 3.2827314153213645\n",
      "---------------------\n",
      "Iteration Number: 7119\n",
      "Loss: 11.850779700432671\n",
      "l2 norm of gradients: 0.06237278478068835\n",
      "l2 norm of weights: 3.2827953844232303\n",
      "---------------------\n",
      "Iteration Number: 7120\n",
      "Loss: 11.850324534680562\n",
      "l2 norm of gradients: 0.0623614252219612\n",
      "l2 norm of weights: 3.282859361038279\n",
      "---------------------\n",
      "Iteration Number: 7121\n",
      "Loss: 11.849869579133564\n",
      "l2 norm of gradients: 0.062350071900730906\n",
      "l2 norm of weights: 3.282923345152958\n",
      "---------------------\n",
      "Iteration Number: 7122\n",
      "Loss: 11.849414833640468\n",
      "l2 norm of gradients: 0.06233872481076993\n",
      "l2 norm of weights: 3.2829873367537266\n",
      "---------------------\n",
      "Iteration Number: 7123\n",
      "Loss: 11.848960298050175\n",
      "l2 norm of gradients: 0.062327383945857684\n",
      "l2 norm of weights: 3.2830513358270577\n",
      "---------------------\n",
      "Iteration Number: 7124\n",
      "Loss: 11.84850597221178\n",
      "l2 norm of gradients: 0.06231604929978062\n",
      "l2 norm of weights: 3.283115342359438\n",
      "---------------------\n",
      "Iteration Number: 7125\n",
      "Loss: 11.848051855974541\n",
      "l2 norm of gradients: 0.06230472086633202\n",
      "l2 norm of weights: 3.283179356337367\n",
      "---------------------\n",
      "Iteration Number: 7126\n",
      "Loss: 11.847597949187891\n",
      "l2 norm of gradients: 0.06229339863931227\n",
      "l2 norm of weights: 3.2832433777473553\n",
      "---------------------\n",
      "Iteration Number: 7127\n",
      "Loss: 11.847144251701412\n",
      "l2 norm of gradients: 0.06228208261252856\n",
      "l2 norm of weights: 3.28330740657593\n",
      "---------------------\n",
      "Iteration Number: 7128\n",
      "Loss: 11.846690763364858\n",
      "l2 norm of gradients: 0.062270772779795126\n",
      "l2 norm of weights: 3.2833714428096283\n",
      "---------------------\n",
      "Iteration Number: 7129\n",
      "Loss: 11.846237484028165\n",
      "l2 norm of gradients: 0.06225946913493309\n",
      "l2 norm of weights: 3.283435486435002\n",
      "---------------------\n",
      "Iteration Number: 7130\n",
      "Loss: 11.845784413541406\n",
      "l2 norm of gradients: 0.06224817167177046\n",
      "l2 norm of weights: 3.2834995374386136\n",
      "---------------------\n",
      "Iteration Number: 7131\n",
      "Loss: 11.84533155175485\n",
      "l2 norm of gradients: 0.06223688038414227\n",
      "l2 norm of weights: 3.283563595807042\n",
      "---------------------\n",
      "Iteration Number: 7132\n",
      "Loss: 11.844878898518905\n",
      "l2 norm of gradients: 0.06222559526589037\n",
      "l2 norm of weights: 3.283627661526876\n",
      "---------------------\n",
      "Iteration Number: 7133\n",
      "Loss: 11.844426453684159\n",
      "l2 norm of gradients: 0.062214316310863556\n",
      "l2 norm of weights: 3.28369173458472\n",
      "---------------------\n",
      "Iteration Number: 7134\n",
      "Loss: 11.843974217101358\n",
      "l2 norm of gradients: 0.06220304351291758\n",
      "l2 norm of weights: 3.283755814967188\n",
      "---------------------\n",
      "Iteration Number: 7135\n",
      "Loss: 11.843522188621424\n",
      "l2 norm of gradients: 0.06219177686591496\n",
      "l2 norm of weights: 3.2838199026609103\n",
      "---------------------\n",
      "Iteration Number: 7136\n",
      "Loss: 11.843070368095423\n",
      "l2 norm of gradients: 0.06218051636372524\n",
      "l2 norm of weights: 3.283883997652528\n",
      "---------------------\n",
      "Iteration Number: 7137\n",
      "Loss: 11.842618755374604\n",
      "l2 norm of gradients: 0.06216926200022475\n",
      "l2 norm of weights: 3.2839480999286956\n",
      "---------------------\n",
      "Iteration Number: 7138\n",
      "Loss: 11.842167350310383\n",
      "l2 norm of gradients: 0.062158013769296735\n",
      "l2 norm of weights: 3.284012209476081\n",
      "---------------------\n",
      "Iteration Number: 7139\n",
      "Loss: 11.841716152754309\n",
      "l2 norm of gradients: 0.06214677166483129\n",
      "l2 norm of weights: 3.284076326281364\n",
      "---------------------\n",
      "Iteration Number: 7140\n",
      "Loss: 11.841265162558127\n",
      "l2 norm of gradients: 0.06213553568072541\n",
      "l2 norm of weights: 3.2841404503312375\n",
      "---------------------\n",
      "Iteration Number: 7141\n",
      "Loss: 11.840814379573729\n",
      "l2 norm of gradients: 0.06212430581088293\n",
      "l2 norm of weights: 3.284204581612408\n",
      "---------------------\n",
      "Iteration Number: 7142\n",
      "Loss: 11.840363803653174\n",
      "l2 norm of gradients: 0.062113082049214514\n",
      "l2 norm of weights: 3.2842687201115934\n",
      "---------------------\n",
      "Iteration Number: 7143\n",
      "Loss: 11.839913434648693\n",
      "l2 norm of gradients: 0.062101864389637704\n",
      "l2 norm of weights: 3.284332865815526\n",
      "---------------------\n",
      "Iteration Number: 7144\n",
      "Loss: 11.839463272412655\n",
      "l2 norm of gradients: 0.06209065282607684\n",
      "l2 norm of weights: 3.2843970187109504\n",
      "---------------------\n",
      "Iteration Number: 7145\n",
      "Loss: 11.839013316797619\n",
      "l2 norm of gradients: 0.06207944735246316\n",
      "l2 norm of weights: 3.2844611787846225\n",
      "---------------------\n",
      "Iteration Number: 7146\n",
      "Loss: 11.838563567656296\n",
      "l2 norm of gradients: 0.06206824796273466\n",
      "l2 norm of weights: 3.284525346023314\n",
      "---------------------\n",
      "Iteration Number: 7147\n",
      "Loss: 11.838114024841543\n",
      "l2 norm of gradients: 0.06205705465083619\n",
      "l2 norm of weights: 3.284589520413806\n",
      "---------------------\n",
      "Iteration Number: 7148\n",
      "Loss: 11.837664688206406\n",
      "l2 norm of gradients: 0.06204586741071941\n",
      "l2 norm of weights: 3.284653701942894\n",
      "---------------------\n",
      "Iteration Number: 7149\n",
      "Loss: 11.837215557604067\n",
      "l2 norm of gradients: 0.06203468623634278\n",
      "l2 norm of weights: 3.284717890597386\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 7150\n",
      "Loss: 11.836766632887894\n",
      "l2 norm of gradients: 0.06202351112167154\n",
      "l2 norm of weights: 3.2847820863641037\n",
      "---------------------\n",
      "Iteration Number: 7151\n",
      "Loss: 11.8363179139114\n",
      "l2 norm of gradients: 0.06201234206067779\n",
      "l2 norm of weights: 3.2848462892298804\n",
      "---------------------\n",
      "Iteration Number: 7152\n",
      "Loss: 11.835869400528255\n",
      "l2 norm of gradients: 0.062001179047340337\n",
      "l2 norm of weights: 3.284910499181561\n",
      "---------------------\n",
      "Iteration Number: 7153\n",
      "Loss: 11.835421092592307\n",
      "l2 norm of gradients: 0.06199002207564485\n",
      "l2 norm of weights: 3.2849747162060052\n",
      "---------------------\n",
      "Iteration Number: 7154\n",
      "Loss: 11.834972989957546\n",
      "l2 norm of gradients: 0.06197887113958372\n",
      "l2 norm of weights: 3.2850389402900855\n",
      "---------------------\n",
      "Iteration Number: 7155\n",
      "Loss: 11.83452509247814\n",
      "l2 norm of gradients: 0.061967726233156065\n",
      "l2 norm of weights: 3.2851031714206846\n",
      "---------------------\n",
      "Iteration Number: 7156\n",
      "Loss: 11.834077400008395\n",
      "l2 norm of gradients: 0.06195658735036789\n",
      "l2 norm of weights: 3.2851674095846994\n",
      "---------------------\n",
      "Iteration Number: 7157\n",
      "Loss: 11.833629912402795\n",
      "l2 norm of gradients: 0.06194545448523184\n",
      "l2 norm of weights: 3.28523165476904\n",
      "---------------------\n",
      "Iteration Number: 7158\n",
      "Loss: 11.833182629515976\n",
      "l2 norm of gradients: 0.06193432763176737\n",
      "l2 norm of weights: 3.285295906960628\n",
      "---------------------\n",
      "Iteration Number: 7159\n",
      "Loss: 11.83273555120274\n",
      "l2 norm of gradients: 0.061923206784000706\n",
      "l2 norm of weights: 3.285360166146398\n",
      "---------------------\n",
      "Iteration Number: 7160\n",
      "Loss: 11.832288677318038\n",
      "l2 norm of gradients: 0.061912091935964704\n",
      "l2 norm of weights: 3.2854244323132966\n",
      "---------------------\n",
      "Iteration Number: 7161\n",
      "Loss: 11.831842007716988\n",
      "l2 norm of gradients: 0.061900983081699064\n",
      "l2 norm of weights: 3.2854887054482846\n",
      "---------------------\n",
      "Iteration Number: 7162\n",
      "Loss: 11.83139554225487\n",
      "l2 norm of gradients: 0.06188988021525016\n",
      "l2 norm of weights: 3.2855529855383336\n",
      "---------------------\n",
      "Iteration Number: 7163\n",
      "Loss: 11.830949280787095\n",
      "l2 norm of gradients: 0.06187878333067108\n",
      "l2 norm of weights: 3.2856172725704287\n",
      "---------------------\n",
      "Iteration Number: 7164\n",
      "Loss: 11.830503223169266\n",
      "l2 norm of gradients: 0.06186769242202162\n",
      "l2 norm of weights: 3.2856815665315677\n",
      "---------------------\n",
      "Iteration Number: 7165\n",
      "Loss: 11.830057369257137\n",
      "l2 norm of gradients: 0.06185660748336835\n",
      "l2 norm of weights: 3.28574586740876\n",
      "---------------------\n",
      "Iteration Number: 7166\n",
      "Loss: 11.82961171890661\n",
      "l2 norm of gradients: 0.06184552850878445\n",
      "l2 norm of weights: 3.285810175189028\n",
      "---------------------\n",
      "Iteration Number: 7167\n",
      "Loss: 11.829166271973747\n",
      "l2 norm of gradients: 0.06183445549234983\n",
      "l2 norm of weights: 3.2858744898594066\n",
      "---------------------\n",
      "Iteration Number: 7168\n",
      "Loss: 11.828721028314776\n",
      "l2 norm of gradients: 0.061823388428151105\n",
      "l2 norm of weights: 3.285938811406943\n",
      "---------------------\n",
      "Iteration Number: 7169\n",
      "Loss: 11.828275987786055\n",
      "l2 norm of gradients: 0.061812327310281565\n",
      "l2 norm of weights: 3.2860031398186975\n",
      "---------------------\n",
      "Iteration Number: 7170\n",
      "Loss: 11.827831150244142\n",
      "l2 norm of gradients: 0.06180127213284114\n",
      "l2 norm of weights: 3.2860674750817425\n",
      "---------------------\n",
      "Iteration Number: 7171\n",
      "Loss: 11.827386515545717\n",
      "l2 norm of gradients: 0.06179022288993648\n",
      "l2 norm of weights: 3.286131817183162\n",
      "---------------------\n",
      "Iteration Number: 7172\n",
      "Loss: 11.826942083547635\n",
      "l2 norm of gradients: 0.061779179575680854\n",
      "l2 norm of weights: 3.286196166110054\n",
      "---------------------\n",
      "Iteration Number: 7173\n",
      "Loss: 11.826497854106888\n",
      "l2 norm of gradients: 0.06176814218419422\n",
      "l2 norm of weights: 3.286260521849528\n",
      "---------------------\n",
      "Iteration Number: 7174\n",
      "Loss: 11.826053827080655\n",
      "l2 norm of gradients: 0.06175711070960315\n",
      "l2 norm of weights: 3.286324884388705\n",
      "---------------------\n",
      "Iteration Number: 7175\n",
      "Loss: 11.825610002326247\n",
      "l2 norm of gradients: 0.0617460851460409\n",
      "l2 norm of weights: 3.286389253714721\n",
      "---------------------\n",
      "Iteration Number: 7176\n",
      "Loss: 11.825166379701137\n",
      "l2 norm of gradients: 0.06173506548764739\n",
      "l2 norm of weights: 3.286453629814721\n",
      "---------------------\n",
      "Iteration Number: 7177\n",
      "Loss: 11.824722959062939\n",
      "l2 norm of gradients: 0.06172405172856906\n",
      "l2 norm of weights: 3.2865180126758657\n",
      "---------------------\n",
      "Iteration Number: 7178\n",
      "Loss: 11.82427974026945\n",
      "l2 norm of gradients: 0.06171304386295909\n",
      "l2 norm of weights: 3.2865824022853256\n",
      "---------------------\n",
      "Iteration Number: 7179\n",
      "Loss: 11.823836723178601\n",
      "l2 norm of gradients: 0.06170204188497718\n",
      "l2 norm of weights: 3.2866467986302847\n",
      "---------------------\n",
      "Iteration Number: 7180\n",
      "Loss: 11.823393907648505\n",
      "l2 norm of gradients: 0.06169104578878976\n",
      "l2 norm of weights: 3.28671120169794\n",
      "---------------------\n",
      "Iteration Number: 7181\n",
      "Loss: 11.822951293537384\n",
      "l2 norm of gradients: 0.06168005556856976\n",
      "l2 norm of weights: 3.286775611475499\n",
      "---------------------\n",
      "Iteration Number: 7182\n",
      "Loss: 11.82250888070366\n",
      "l2 norm of gradients: 0.061669071218496734\n",
      "l2 norm of weights: 3.2868400279501833\n",
      "---------------------\n",
      "Iteration Number: 7183\n",
      "Loss: 11.822066669005878\n",
      "l2 norm of gradients: 0.06165809273275689\n",
      "l2 norm of weights: 3.286904451109225\n",
      "---------------------\n",
      "Iteration Number: 7184\n",
      "Loss: 11.821624658302742\n",
      "l2 norm of gradients: 0.061647120105542984\n",
      "l2 norm of weights: 3.2869688809398707\n",
      "---------------------\n",
      "Iteration Number: 7185\n",
      "Loss: 11.821182848453137\n",
      "l2 norm of gradients: 0.06163615333105431\n",
      "l2 norm of weights: 3.287033317429377\n",
      "---------------------\n",
      "Iteration Number: 7186\n",
      "Loss: 11.82074123931607\n",
      "l2 norm of gradients: 0.061625192403496794\n",
      "l2 norm of weights: 3.2870977605650147\n",
      "---------------------\n",
      "Iteration Number: 7187\n",
      "Loss: 11.820299830750713\n",
      "l2 norm of gradients: 0.061614237317082945\n",
      "l2 norm of weights: 3.287162210334065\n",
      "---------------------\n",
      "Iteration Number: 7188\n",
      "Loss: 11.819858622616387\n",
      "l2 norm of gradients: 0.06160328806603177\n",
      "l2 norm of weights: 3.2872266667238232\n",
      "---------------------\n",
      "Iteration Number: 7189\n",
      "Loss: 11.819417614772581\n",
      "l2 norm of gradients: 0.0615923446445689\n",
      "l2 norm of weights: 3.287291129721596\n",
      "---------------------\n",
      "Iteration Number: 7190\n",
      "Loss: 11.818976807078903\n",
      "l2 norm of gradients: 0.06158140704692646\n",
      "l2 norm of weights: 3.2873555993147012\n",
      "---------------------\n",
      "Iteration Number: 7191\n",
      "Loss: 11.818536199395162\n",
      "l2 norm of gradients: 0.061570475267343154\n",
      "l2 norm of weights: 3.2874200754904708\n",
      "---------------------\n",
      "Iteration Number: 7192\n",
      "Loss: 11.818095791581287\n",
      "l2 norm of gradients: 0.061559549300064194\n",
      "l2 norm of weights: 3.287484558236247\n",
      "---------------------\n",
      "Iteration Number: 7193\n",
      "Loss: 11.817655583497356\n",
      "l2 norm of gradients: 0.06154862913934139\n",
      "l2 norm of weights: 3.287549047539386\n",
      "---------------------\n",
      "Iteration Number: 7194\n",
      "Loss: 11.817215575003605\n",
      "l2 norm of gradients: 0.061537714779432984\n",
      "l2 norm of weights: 3.2876135433872555\n",
      "---------------------\n",
      "Iteration Number: 7195\n",
      "Loss: 11.81677576596044\n",
      "l2 norm of gradients: 0.06152680621460383\n",
      "l2 norm of weights: 3.2876780457672345\n",
      "---------------------\n",
      "Iteration Number: 7196\n",
      "Loss: 11.816336156228402\n",
      "l2 norm of gradients: 0.061515903439125226\n",
      "l2 norm of weights: 3.287742554666715\n",
      "---------------------\n",
      "Iteration Number: 7197\n",
      "Loss: 11.815896745668178\n",
      "l2 norm of gradients: 0.06150500644727498\n",
      "l2 norm of weights: 3.2878070700731015\n",
      "---------------------\n",
      "Iteration Number: 7198\n",
      "Loss: 11.81545753414062\n",
      "l2 norm of gradients: 0.06149411523333747\n",
      "l2 norm of weights: 3.287871591973809\n",
      "---------------------\n",
      "Iteration Number: 7199\n",
      "Loss: 11.815018521506712\n",
      "l2 norm of gradients: 0.06148322979160352\n",
      "l2 norm of weights: 3.287936120356267\n",
      "---------------------\n",
      "Iteration Number: 7200\n",
      "Loss: 11.814579707627619\n",
      "l2 norm of gradients: 0.061472350116370436\n",
      "l2 norm of weights: 3.2880006552079144\n",
      "---------------------\n",
      "Iteration Number: 7201\n",
      "Loss: 11.814141092364629\n",
      "l2 norm of gradients: 0.061461476201942\n",
      "l2 norm of weights: 3.2880651965162047\n",
      "---------------------\n",
      "Iteration Number: 7202\n",
      "Loss: 11.813702675579195\n",
      "l2 norm of gradients: 0.06145060804262848\n",
      "l2 norm of weights: 3.2881297442686015\n",
      "---------------------\n",
      "Iteration Number: 7203\n",
      "Loss: 11.813264457132895\n",
      "l2 norm of gradients: 0.06143974563274667\n",
      "l2 norm of weights: 3.288194298452582\n",
      "---------------------\n",
      "Iteration Number: 7204\n",
      "Loss: 11.812826436887505\n",
      "l2 norm of gradients: 0.061428888966619716\n",
      "l2 norm of weights: 3.2882588590556336\n",
      "---------------------\n",
      "Iteration Number: 7205\n",
      "Loss: 11.812388614704913\n",
      "l2 norm of gradients: 0.06141803803857732\n",
      "l2 norm of weights: 3.2883234260652574\n",
      "---------------------\n",
      "Iteration Number: 7206\n",
      "Loss: 11.811950990447157\n",
      "l2 norm of gradients: 0.061407192842955614\n",
      "l2 norm of weights: 3.288387999468966\n",
      "---------------------\n",
      "Iteration Number: 7207\n",
      "Loss: 11.811513563976437\n",
      "l2 norm of gradients: 0.06139635337409711\n",
      "l2 norm of weights: 3.288452579254284\n",
      "---------------------\n",
      "Iteration Number: 7208\n",
      "Loss: 11.811076335155112\n",
      "l2 norm of gradients: 0.061385519626350846\n",
      "l2 norm of weights: 3.2885171654087477\n",
      "---------------------\n",
      "Iteration Number: 7209\n",
      "Loss: 11.810639303845676\n",
      "l2 norm of gradients: 0.06137469159407229\n",
      "l2 norm of weights: 3.2885817579199057\n",
      "---------------------\n",
      "Iteration Number: 7210\n",
      "Loss: 11.810202469910742\n",
      "l2 norm of gradients: 0.061363869271623225\n",
      "l2 norm of weights: 3.288646356775318\n",
      "---------------------\n",
      "Iteration Number: 7211\n",
      "Loss: 11.809765833213138\n",
      "l2 norm of gradients: 0.061353052653372\n",
      "l2 norm of weights: 3.288710961962558\n",
      "---------------------\n",
      "Iteration Number: 7212\n",
      "Loss: 11.809329393615782\n",
      "l2 norm of gradients: 0.06134224173369328\n",
      "l2 norm of weights: 3.2887755734692092\n",
      "---------------------\n",
      "Iteration Number: 7213\n",
      "Loss: 11.80889315098178\n",
      "l2 norm of gradients: 0.06133143650696819\n",
      "l2 norm of weights: 3.2888401912828686\n",
      "---------------------\n",
      "Iteration Number: 7214\n",
      "Loss: 11.808457105174353\n",
      "l2 norm of gradients: 0.061320636967584254\n",
      "l2 norm of weights: 3.288904815391144\n",
      "---------------------\n",
      "Iteration Number: 7215\n",
      "Loss: 11.808021256056888\n",
      "l2 norm of gradients: 0.06130984310993532\n",
      "l2 norm of weights: 3.2889694457816554\n",
      "---------------------\n",
      "Iteration Number: 7216\n",
      "Loss: 11.807585603492925\n",
      "l2 norm of gradients: 0.06129905492842175\n",
      "l2 norm of weights: 3.2890340824420345\n",
      "---------------------\n",
      "Iteration Number: 7217\n",
      "Loss: 11.807150147346134\n",
      "l2 norm of gradients: 0.0612882724174502\n",
      "l2 norm of weights: 3.2890987253599264\n",
      "---------------------\n",
      "Iteration Number: 7218\n",
      "Loss: 11.806714887480345\n",
      "l2 norm of gradients: 0.06127749557143372\n",
      "l2 norm of weights: 3.2891633745229862\n",
      "---------------------\n",
      "Iteration Number: 7219\n",
      "Loss: 11.806279823759525\n",
      "l2 norm of gradients: 0.06126672438479172\n",
      "l2 norm of weights: 3.289228029918881\n",
      "---------------------\n",
      "Iteration Number: 7220\n",
      "Loss: 11.805844956047816\n",
      "l2 norm of gradients: 0.061255958851950024\n",
      "l2 norm of weights: 3.2892926915352914\n",
      "---------------------\n",
      "Iteration Number: 7221\n",
      "Loss: 11.805410284209454\n",
      "l2 norm of gradients: 0.06124519896734079\n",
      "l2 norm of weights: 3.289357359359908\n",
      "---------------------\n",
      "Iteration Number: 7222\n",
      "Loss: 11.804975808108868\n",
      "l2 norm of gradients: 0.06123444472540255\n",
      "l2 norm of weights: 3.2894220333804345\n",
      "---------------------\n",
      "Iteration Number: 7223\n",
      "Loss: 11.804541527610608\n",
      "l2 norm of gradients: 0.06122369612058008\n",
      "l2 norm of weights: 3.289486713584585\n",
      "---------------------\n",
      "Iteration Number: 7224\n",
      "Loss: 11.804107442579395\n",
      "l2 norm of gradients: 0.061212953147324646\n",
      "l2 norm of weights: 3.289551399960087\n",
      "---------------------\n",
      "Iteration Number: 7225\n",
      "Loss: 11.803673552880065\n",
      "l2 norm of gradients: 0.06120221580009375\n",
      "l2 norm of weights: 3.289616092494679\n",
      "---------------------\n",
      "Iteration Number: 7226\n",
      "Loss: 11.803239858377614\n",
      "l2 norm of gradients: 0.06119148407335125\n",
      "l2 norm of weights: 3.289680791176111\n",
      "---------------------\n",
      "Iteration Number: 7227\n",
      "Loss: 11.802806358937197\n",
      "l2 norm of gradients: 0.06118075796156738\n",
      "l2 norm of weights: 3.289745495992146\n",
      "---------------------\n",
      "Iteration Number: 7228\n",
      "Loss: 11.80237305442408\n",
      "l2 norm of gradients: 0.06117003745921857\n",
      "l2 norm of weights: 3.2898102069305564\n",
      "---------------------\n",
      "Iteration Number: 7229\n",
      "Loss: 11.801939944703715\n",
      "l2 norm of gradients: 0.061159322560787675\n",
      "l2 norm of weights: 3.2898749239791294\n",
      "---------------------\n",
      "Iteration Number: 7230\n",
      "Loss: 11.801507029641664\n",
      "l2 norm of gradients: 0.06114861326076378\n",
      "l2 norm of weights: 3.289939647125661\n",
      "---------------------\n",
      "Iteration Number: 7231\n",
      "Loss: 11.801074309103651\n",
      "l2 norm of gradients: 0.06113790955364235\n",
      "l2 norm of weights: 3.290004376357961\n",
      "---------------------\n",
      "Iteration Number: 7232\n",
      "Loss: 11.800641782955555\n",
      "l2 norm of gradients: 0.06112721143392505\n",
      "l2 norm of weights: 3.29006911166385\n",
      "---------------------\n",
      "Iteration Number: 7233\n",
      "Loss: 11.800209451063361\n",
      "l2 norm of gradients: 0.06111651889611986\n",
      "l2 norm of weights: 3.2901338530311612\n",
      "---------------------\n",
      "Iteration Number: 7234\n",
      "Loss: 11.799777313293234\n",
      "l2 norm of gradients: 0.06110583193474106\n",
      "l2 norm of weights: 3.290198600447738\n",
      "---------------------\n",
      "Iteration Number: 7235\n",
      "Loss: 11.799345369511487\n",
      "l2 norm of gradients: 0.06109515054430922\n",
      "l2 norm of weights: 3.290263353901436\n",
      "---------------------\n",
      "Iteration Number: 7236\n",
      "Loss: 11.798913619584535\n",
      "l2 norm of gradients: 0.06108447471935119\n",
      "l2 norm of weights: 3.290328113380123\n",
      "---------------------\n",
      "Iteration Number: 7237\n",
      "Loss: 11.79848206337898\n",
      "l2 norm of gradients: 0.061073804454399964\n",
      "l2 norm of weights: 3.2903928788716788\n",
      "---------------------\n",
      "Iteration Number: 7238\n",
      "Loss: 11.798050700761532\n",
      "l2 norm of gradients: 0.06106313974399492\n",
      "l2 norm of weights: 3.2904576503639937\n",
      "---------------------\n",
      "Iteration Number: 7239\n",
      "Loss: 11.797619531599079\n",
      "l2 norm of gradients: 0.06105248058268163\n",
      "l2 norm of weights: 3.29052242784497\n",
      "---------------------\n",
      "Iteration Number: 7240\n",
      "Loss: 11.797188555758636\n",
      "l2 norm of gradients: 0.061041826965011946\n",
      "l2 norm of weights: 3.2905872113025216\n",
      "---------------------\n",
      "Iteration Number: 7241\n",
      "Loss: 11.796757773107341\n",
      "l2 norm of gradients: 0.06103117888554389\n",
      "l2 norm of weights: 3.2906520007245748\n",
      "---------------------\n",
      "Iteration Number: 7242\n",
      "Loss: 11.796327183512508\n",
      "l2 norm of gradients: 0.06102053633884178\n",
      "l2 norm of weights: 3.2907167960990664\n",
      "---------------------\n",
      "Iteration Number: 7243\n",
      "Loss: 11.795896786841567\n",
      "l2 norm of gradients: 0.06100989931947613\n",
      "l2 norm of weights: 3.2907815974139454\n",
      "---------------------\n",
      "Iteration Number: 7244\n",
      "Loss: 11.79546658296211\n",
      "l2 norm of gradients: 0.06099926782202369\n",
      "l2 norm of weights: 3.290846404657172\n",
      "---------------------\n",
      "Iteration Number: 7245\n",
      "Loss: 11.79503657174186\n",
      "l2 norm of gradients: 0.060988641841067434\n",
      "l2 norm of weights: 3.290911217816719\n",
      "---------------------\n",
      "Iteration Number: 7246\n",
      "Loss: 11.794606753048676\n",
      "l2 norm of gradients: 0.06097802137119648\n",
      "l2 norm of weights: 3.290976036880568\n",
      "---------------------\n",
      "Iteration Number: 7247\n",
      "Loss: 11.79417712675057\n",
      "l2 norm of gradients: 0.060967406407006194\n",
      "l2 norm of weights: 3.291040861836716\n",
      "---------------------\n",
      "Iteration Number: 7248\n",
      "Loss: 11.7937476927157\n",
      "l2 norm of gradients: 0.06095679694309819\n",
      "l2 norm of weights: 3.2911056926731694\n",
      "---------------------\n",
      "Iteration Number: 7249\n",
      "Loss: 11.793318450812347\n",
      "l2 norm of gradients: 0.060946192974080146\n",
      "l2 norm of weights: 3.291170529377945\n",
      "---------------------\n",
      "Iteration Number: 7250\n",
      "Loss: 11.792889400908942\n",
      "l2 norm of gradients: 0.06093559449456606\n",
      "l2 norm of weights: 3.2912353719390737\n",
      "---------------------\n",
      "Iteration Number: 7251\n",
      "Loss: 11.79246054287406\n",
      "l2 norm of gradients: 0.06092500149917598\n",
      "l2 norm of weights: 3.291300220344596\n",
      "---------------------\n",
      "Iteration Number: 7252\n",
      "Loss: 11.792031876576413\n",
      "l2 norm of gradients: 0.060914413982536206\n",
      "l2 norm of weights: 3.291365074582565\n",
      "---------------------\n",
      "Iteration Number: 7253\n",
      "Loss: 11.791603401884856\n",
      "l2 norm of gradients: 0.060903831939279195\n",
      "l2 norm of weights: 3.291429934641044\n",
      "---------------------\n",
      "Iteration Number: 7254\n",
      "Loss: 11.79117511866838\n",
      "l2 norm of gradients: 0.06089325536404355\n",
      "l2 norm of weights: 3.291494800508109\n",
      "---------------------\n",
      "Iteration Number: 7255\n",
      "Loss: 11.790747026796117\n",
      "l2 norm of gradients: 0.060882684251474015\n",
      "l2 norm of weights: 3.291559672171847\n",
      "---------------------\n",
      "Iteration Number: 7256\n",
      "Loss: 11.79031912613734\n",
      "l2 norm of gradients: 0.06087211859622149\n",
      "l2 norm of weights: 3.2916245496203556\n",
      "---------------------\n",
      "Iteration Number: 7257\n",
      "Loss: 11.789891416561463\n",
      "l2 norm of gradients: 0.06086155839294303\n",
      "l2 norm of weights: 3.291689432841746\n",
      "---------------------\n",
      "Iteration Number: 7258\n",
      "Loss: 11.789463897938045\n",
      "l2 norm of gradients: 0.060851003636301804\n",
      "l2 norm of weights: 3.291754321824139\n",
      "---------------------\n",
      "Iteration Number: 7259\n",
      "Loss: 11.789036570136757\n",
      "l2 norm of gradients: 0.060840454320967144\n",
      "l2 norm of weights: 3.2918192165556666\n",
      "---------------------\n",
      "Iteration Number: 7260\n",
      "Loss: 11.788609433027446\n",
      "l2 norm of gradients: 0.06082991044161445\n",
      "l2 norm of weights: 3.291884117024473\n",
      "---------------------\n",
      "Iteration Number: 7261\n",
      "Loss: 11.78818248648008\n",
      "l2 norm of gradients: 0.06081937199292531\n",
      "l2 norm of weights: 3.291949023218714\n",
      "---------------------\n",
      "Iteration Number: 7262\n",
      "Loss: 11.787755730364765\n",
      "l2 norm of gradients: 0.060808838969587345\n",
      "l2 norm of weights: 3.2920139351265565\n",
      "---------------------\n",
      "Iteration Number: 7263\n",
      "Loss: 11.78732916455175\n",
      "l2 norm of gradients: 0.06079831136629436\n",
      "l2 norm of weights: 3.2920788527361777\n",
      "---------------------\n",
      "Iteration Number: 7264\n",
      "Loss: 11.786902788911403\n",
      "l2 norm of gradients: 0.060787789177746195\n",
      "l2 norm of weights: 3.2921437760357684\n",
      "---------------------\n",
      "Iteration Number: 7265\n",
      "Loss: 11.786476603314268\n",
      "l2 norm of gradients: 0.060777272398648795\n",
      "l2 norm of weights: 3.2922087050135285\n",
      "---------------------\n",
      "Iteration Number: 7266\n",
      "Loss: 11.786050607630997\n",
      "l2 norm of gradients: 0.06076676102371423\n",
      "l2 norm of weights: 3.2922736396576706\n",
      "---------------------\n",
      "Iteration Number: 7267\n",
      "Loss: 11.785624801732395\n",
      "l2 norm of gradients: 0.060756255047660614\n",
      "l2 norm of weights: 3.292338579956418\n",
      "---------------------\n",
      "Iteration Number: 7268\n",
      "Loss: 11.78519918548938\n",
      "l2 norm of gradients: 0.06074575446521216\n",
      "l2 norm of weights: 3.2924035258980053\n",
      "---------------------\n",
      "Iteration Number: 7269\n",
      "Loss: 11.784773758773046\n",
      "l2 norm of gradients: 0.06073525927109911\n",
      "l2 norm of weights: 3.292468477470679\n",
      "---------------------\n",
      "Iteration Number: 7270\n",
      "Loss: 11.78434852145459\n",
      "l2 norm of gradients: 0.06072476946005782\n",
      "l2 norm of weights: 3.292533434662696\n",
      "---------------------\n",
      "Iteration Number: 7271\n",
      "Loss: 11.783923473405364\n",
      "l2 norm of gradients: 0.06071428502683066\n",
      "l2 norm of weights: 3.2925983974623256\n",
      "---------------------\n",
      "Iteration Number: 7272\n",
      "Loss: 11.783498614496848\n",
      "l2 norm of gradients: 0.06070380596616611\n",
      "l2 norm of weights: 3.292663365857847\n",
      "---------------------\n",
      "Iteration Number: 7273\n",
      "Loss: 11.783073944600677\n",
      "l2 norm of gradients: 0.06069333227281863\n",
      "l2 norm of weights: 3.2927283398375518\n",
      "---------------------\n",
      "Iteration Number: 7274\n",
      "Loss: 11.78264946358859\n",
      "l2 norm of gradients: 0.060682863941548724\n",
      "l2 norm of weights: 3.2927933193897414\n",
      "---------------------\n",
      "Iteration Number: 7275\n",
      "Loss: 11.782225171332493\n",
      "l2 norm of gradients: 0.060672400967122994\n",
      "l2 norm of weights: 3.292858304502731\n",
      "---------------------\n",
      "Iteration Number: 7276\n",
      "Loss: 11.781801067704414\n",
      "l2 norm of gradients: 0.060661943344314025\n",
      "l2 norm of weights: 3.2929232951648437\n",
      "---------------------\n",
      "Iteration Number: 7277\n",
      "Loss: 11.781377152576514\n",
      "l2 norm of gradients: 0.060651491067900375\n",
      "l2 norm of weights: 3.2929882913644164\n",
      "---------------------\n",
      "Iteration Number: 7278\n",
      "Loss: 11.7809534258211\n",
      "l2 norm of gradients: 0.060641044132666716\n",
      "l2 norm of weights: 3.2930532930897964\n",
      "---------------------\n",
      "Iteration Number: 7279\n",
      "Loss: 11.780529887310603\n",
      "l2 norm of gradients: 0.060630602533403674\n",
      "l2 norm of weights: 3.293118300329342\n",
      "---------------------\n",
      "Iteration Number: 7280\n",
      "Loss: 11.7801065369176\n",
      "l2 norm of gradients: 0.06062016626490787\n",
      "l2 norm of weights: 3.293183313071423\n",
      "---------------------\n",
      "Iteration Number: 7281\n",
      "Loss: 11.779683374514802\n",
      "l2 norm of gradients: 0.06060973532198195\n",
      "l2 norm of weights: 3.293248331304419\n",
      "---------------------\n",
      "Iteration Number: 7282\n",
      "Loss: 11.779260399975048\n",
      "l2 norm of gradients: 0.06059930969943453\n",
      "l2 norm of weights: 3.293313355016723\n",
      "---------------------\n",
      "Iteration Number: 7283\n",
      "Loss: 11.778837613171307\n",
      "l2 norm of gradients: 0.060588889392080236\n",
      "l2 norm of weights: 3.293378384196738\n",
      "---------------------\n",
      "Iteration Number: 7284\n",
      "Loss: 11.778415013976705\n",
      "l2 norm of gradients: 0.060578474394739644\n",
      "l2 norm of weights: 3.2934434188328776\n",
      "---------------------\n",
      "Iteration Number: 7285\n",
      "Loss: 11.77799260226448\n",
      "l2 norm of gradients: 0.060568064702239355\n",
      "l2 norm of weights: 3.2935084589135672\n",
      "---------------------\n",
      "Iteration Number: 7286\n",
      "Loss: 11.777570377908015\n",
      "l2 norm of gradients: 0.06055766030941184\n",
      "l2 norm of weights: 3.293573504427244\n",
      "---------------------\n",
      "Iteration Number: 7287\n",
      "Loss: 11.777148340780823\n",
      "l2 norm of gradients: 0.06054726121109566\n",
      "l2 norm of weights: 3.2936385553623535\n",
      "---------------------\n",
      "Iteration Number: 7288\n",
      "Loss: 11.776726490756552\n",
      "l2 norm of gradients: 0.060536867402135236\n",
      "l2 norm of weights: 3.2937036117073557\n",
      "---------------------\n",
      "Iteration Number: 7289\n",
      "Loss: 11.776304827708993\n",
      "l2 norm of gradients: 0.060526478877380975\n",
      "l2 norm of weights: 3.29376867345072\n",
      "---------------------\n",
      "Iteration Number: 7290\n",
      "Loss: 11.77588335151205\n",
      "l2 norm of gradients: 0.06051609563168924\n",
      "l2 norm of weights: 3.2938337405809275\n",
      "---------------------\n",
      "Iteration Number: 7291\n",
      "Loss: 11.775462062039777\n",
      "l2 norm of gradients: 0.06050571765992229\n",
      "l2 norm of weights: 3.2938988130864693\n",
      "---------------------\n",
      "Iteration Number: 7292\n",
      "Loss: 11.775040959166361\n",
      "l2 norm of gradients: 0.060495344956948346\n",
      "l2 norm of weights: 3.2939638909558484\n",
      "---------------------\n",
      "Iteration Number: 7293\n",
      "Loss: 11.774620042766118\n",
      "l2 norm of gradients: 0.06048497751764158\n",
      "l2 norm of weights: 3.294028974177578\n",
      "---------------------\n",
      "Iteration Number: 7294\n",
      "Loss: 11.774199312713485\n",
      "l2 norm of gradients: 0.06047461533688203\n",
      "l2 norm of weights: 3.294094062740184\n",
      "---------------------\n",
      "Iteration Number: 7295\n",
      "Loss: 11.77377876888306\n",
      "l2 norm of gradients: 0.0604642584095557\n",
      "l2 norm of weights: 3.294159156632202\n",
      "---------------------\n",
      "Iteration Number: 7296\n",
      "Loss: 11.77335841114955\n",
      "l2 norm of gradients: 0.060453906730554445\n",
      "l2 norm of weights: 3.294224255842178\n",
      "---------------------\n",
      "Iteration Number: 7297\n",
      "Loss: 11.772938239387795\n",
      "l2 norm of gradients: 0.06044356029477609\n",
      "l2 norm of weights: 3.2942893603586705\n",
      "---------------------\n",
      "Iteration Number: 7298\n",
      "Loss: 11.772518253472771\n",
      "l2 norm of gradients: 0.060433219097124305\n",
      "l2 norm of weights: 3.2943544701702483\n",
      "---------------------\n",
      "Iteration Number: 7299\n",
      "Loss: 11.7720984532796\n",
      "l2 norm of gradients: 0.06042288313250871\n",
      "l2 norm of weights: 3.294419585265491\n",
      "---------------------\n",
      "Iteration Number: 7300\n",
      "Loss: 11.771678838683522\n",
      "l2 norm of gradients: 0.060412552395844726\n",
      "l2 norm of weights: 3.29448470563299\n",
      "---------------------\n",
      "Iteration Number: 7301\n",
      "Loss: 11.771259409559896\n",
      "l2 norm of gradients: 0.06040222688205371\n",
      "l2 norm of weights: 3.2945498312613464\n",
      "---------------------\n",
      "Iteration Number: 7302\n",
      "Loss: 11.770840165784241\n",
      "l2 norm of gradients: 0.06039190658606288\n",
      "l2 norm of weights: 3.2946149621391725\n",
      "---------------------\n",
      "Iteration Number: 7303\n",
      "Loss: 11.770421107232192\n",
      "l2 norm of gradients: 0.06038159150280534\n",
      "l2 norm of weights: 3.294680098255092\n",
      "---------------------\n",
      "Iteration Number: 7304\n",
      "Loss: 11.77000223377952\n",
      "l2 norm of gradients: 0.06037128162722004\n",
      "l2 norm of weights: 3.2947452395977406\n",
      "---------------------\n",
      "Iteration Number: 7305\n",
      "Loss: 11.769583545302101\n",
      "l2 norm of gradients: 0.060360976954251785\n",
      "l2 norm of weights: 3.2948103861557616\n",
      "---------------------\n",
      "Iteration Number: 7306\n",
      "Loss: 11.769165041675985\n",
      "l2 norm of gradients: 0.06035067747885123\n",
      "l2 norm of weights: 3.2948755379178123\n",
      "---------------------\n",
      "Iteration Number: 7307\n",
      "Loss: 11.768746722777333\n",
      "l2 norm of gradients: 0.060340383195974866\n",
      "l2 norm of weights: 3.29494069487256\n",
      "---------------------\n",
      "Iteration Number: 7308\n",
      "Loss: 11.768328588482419\n",
      "l2 norm of gradients: 0.06033009410058502\n",
      "l2 norm of weights: 3.295005857008683\n",
      "---------------------\n",
      "Iteration Number: 7309\n",
      "Loss: 11.767910638667681\n",
      "l2 norm of gradients: 0.06031981018764994\n",
      "l2 norm of weights: 3.295071024314869\n",
      "---------------------\n",
      "Iteration Number: 7310\n",
      "Loss: 11.767492873209648\n",
      "l2 norm of gradients: 0.06030953145214355\n",
      "l2 norm of weights: 3.2951361967798185\n",
      "---------------------\n",
      "Iteration Number: 7311\n",
      "Loss: 11.767075291985016\n",
      "l2 norm of gradients: 0.0602992578890457\n",
      "l2 norm of weights: 3.295201374392242\n",
      "---------------------\n",
      "Iteration Number: 7312\n",
      "Loss: 11.7666578948706\n",
      "l2 norm of gradients: 0.060288989493342\n",
      "l2 norm of weights: 3.2952665571408617\n",
      "---------------------\n",
      "Iteration Number: 7313\n",
      "Loss: 11.766240681743307\n",
      "l2 norm of gradients: 0.06027872626002393\n",
      "l2 norm of weights: 3.2953317450144084\n",
      "---------------------\n",
      "Iteration Number: 7314\n",
      "Loss: 11.765823652480242\n",
      "l2 norm of gradients: 0.06026846818408872\n",
      "l2 norm of weights: 3.2953969380016255\n",
      "---------------------\n",
      "Iteration Number: 7315\n",
      "Loss: 11.765406806958579\n",
      "l2 norm of gradients: 0.06025821526053943\n",
      "l2 norm of weights: 3.2954621360912677\n",
      "---------------------\n",
      "Iteration Number: 7316\n",
      "Loss: 11.764990145055664\n",
      "l2 norm of gradients: 0.06024796748438486\n",
      "l2 norm of weights: 3.2955273392720987\n",
      "---------------------\n",
      "Iteration Number: 7317\n",
      "Loss: 11.76457366664893\n",
      "l2 norm of gradients: 0.060237724850639725\n",
      "l2 norm of weights: 3.295592547532894\n",
      "---------------------\n",
      "Iteration Number: 7318\n",
      "Loss: 11.764157371615985\n",
      "l2 norm of gradients: 0.06022748735432432\n",
      "l2 norm of weights: 3.2956577608624404\n",
      "---------------------\n",
      "Iteration Number: 7319\n",
      "Loss: 11.763741259834514\n",
      "l2 norm of gradients: 0.060217254990464875\n",
      "l2 norm of weights: 3.2957229792495344\n",
      "---------------------\n",
      "Iteration Number: 7320\n",
      "Loss: 11.763325331182378\n",
      "l2 norm of gradients: 0.06020702775409337\n",
      "l2 norm of weights: 3.295788202682984\n",
      "---------------------\n",
      "Iteration Number: 7321\n",
      "Loss: 11.762909585537537\n",
      "l2 norm of gradients: 0.060196805640247486\n",
      "l2 norm of weights: 3.295853431151607\n",
      "---------------------\n",
      "Iteration Number: 7322\n",
      "Loss: 11.762494022778096\n",
      "l2 norm of gradients: 0.06018658864397069\n",
      "l2 norm of weights: 3.2959186646442324\n",
      "---------------------\n",
      "Iteration Number: 7323\n",
      "Loss: 11.762078642782264\n",
      "l2 norm of gradients: 0.06017637676031221\n",
      "l2 norm of weights: 3.2959839031497014\n",
      "---------------------\n",
      "Iteration Number: 7324\n",
      "Loss: 11.761663445428404\n",
      "l2 norm of gradients: 0.06016616998432699\n",
      "l2 norm of weights: 3.296049146656863\n",
      "---------------------\n",
      "Iteration Number: 7325\n",
      "Loss: 11.761248430595\n",
      "l2 norm of gradients: 0.06015596831107579\n",
      "l2 norm of weights: 3.2961143951545795\n",
      "---------------------\n",
      "Iteration Number: 7326\n",
      "Loss: 11.760833598160641\n",
      "l2 norm of gradients: 0.060145771735625\n",
      "l2 norm of weights: 3.2961796486317225\n",
      "---------------------\n",
      "Iteration Number: 7327\n",
      "Loss: 11.760418948004089\n",
      "l2 norm of gradients: 0.06013558025304681\n",
      "l2 norm of weights: 3.2962449070771744\n",
      "---------------------\n",
      "Iteration Number: 7328\n",
      "Loss: 11.760004480004177\n",
      "l2 norm of gradients: 0.06012539385841908\n",
      "l2 norm of weights: 3.296310170479829\n",
      "---------------------\n",
      "Iteration Number: 7329\n",
      "Loss: 11.759590194039905\n",
      "l2 norm of gradients: 0.06011521254682546\n",
      "l2 norm of weights: 3.2963754388285897\n",
      "---------------------\n",
      "Iteration Number: 7330\n",
      "Loss: 11.759176089990394\n",
      "l2 norm of gradients: 0.06010503631335526\n",
      "l2 norm of weights: 3.2964407121123718\n",
      "---------------------\n",
      "Iteration Number: 7331\n",
      "Loss: 11.75876216773487\n",
      "l2 norm of gradients: 0.060094865153103465\n",
      "l2 norm of weights: 3.2965059903201\n",
      "---------------------\n",
      "Iteration Number: 7332\n",
      "Loss: 11.758348427152711\n",
      "l2 norm of gradients: 0.06008469906117085\n",
      "l2 norm of weights: 3.2965712734407098\n",
      "---------------------\n",
      "Iteration Number: 7333\n",
      "Loss: 11.757934868123398\n",
      "l2 norm of gradients: 0.06007453803266379\n",
      "l2 norm of weights: 3.2966365614631488\n",
      "---------------------\n",
      "Iteration Number: 7334\n",
      "Loss: 11.757521490526566\n",
      "l2 norm of gradients: 0.0600643820626944\n",
      "l2 norm of weights: 3.296701854376373\n",
      "---------------------\n",
      "Iteration Number: 7335\n",
      "Loss: 11.757108294241949\n",
      "l2 norm of gradients: 0.0600542311463805\n",
      "l2 norm of weights: 3.29676715216935\n",
      "---------------------\n",
      "Iteration Number: 7336\n",
      "Loss: 11.756695279149413\n",
      "l2 norm of gradients: 0.0600440852788455\n",
      "l2 norm of weights: 3.2968324548310592\n",
      "---------------------\n",
      "Iteration Number: 7337\n",
      "Loss: 11.756282445128951\n",
      "l2 norm of gradients: 0.06003394445521855\n",
      "l2 norm of weights: 3.296897762350489\n",
      "---------------------\n",
      "Iteration Number: 7338\n",
      "Loss: 11.755869792060707\n",
      "l2 norm of gradients: 0.060023808670634456\n",
      "l2 norm of weights: 3.296963074716638\n",
      "---------------------\n",
      "Iteration Number: 7339\n",
      "Loss: 11.755457319824908\n",
      "l2 norm of gradients: 0.0600136779202337\n",
      "l2 norm of weights: 3.297028391918517\n",
      "---------------------\n",
      "Iteration Number: 7340\n",
      "Loss: 11.755045028301927\n",
      "l2 norm of gradients: 0.06000355219916234\n",
      "l2 norm of weights: 3.2970937139451455\n",
      "---------------------\n",
      "Iteration Number: 7341\n",
      "Loss: 11.754632917372255\n",
      "l2 norm of gradients: 0.059993431502572164\n",
      "l2 norm of weights: 3.297159040785556\n",
      "---------------------\n",
      "Iteration Number: 7342\n",
      "Loss: 11.754220986916522\n",
      "l2 norm of gradients: 0.059983315825620574\n",
      "l2 norm of weights: 3.2972243724287886\n",
      "---------------------\n",
      "Iteration Number: 7343\n",
      "Loss: 11.753809236815465\n",
      "l2 norm of gradients: 0.05997320516347061\n",
      "l2 norm of weights: 3.2972897088638966\n",
      "---------------------\n",
      "Iteration Number: 7344\n",
      "Loss: 11.753397666949962\n",
      "l2 norm of gradients: 0.0599630995112909\n",
      "l2 norm of weights: 3.297355050079942\n",
      "---------------------\n",
      "Iteration Number: 7345\n",
      "Loss: 11.752986277200996\n",
      "l2 norm of gradients: 0.05995299886425578\n",
      "l2 norm of weights: 3.297420396065997\n",
      "---------------------\n",
      "Iteration Number: 7346\n",
      "Loss: 11.75257506744968\n",
      "l2 norm of gradients: 0.059942903217545154\n",
      "l2 norm of weights: 3.2974857468111476\n",
      "---------------------\n",
      "Iteration Number: 7347\n",
      "Loss: 11.752164037577268\n",
      "l2 norm of gradients: 0.059932812566344515\n",
      "l2 norm of weights: 3.2975511023044852\n",
      "---------------------\n",
      "Iteration Number: 7348\n",
      "Loss: 11.751753187465106\n",
      "l2 norm of gradients: 0.05992272690584505\n",
      "l2 norm of weights: 3.297616462535115\n",
      "---------------------\n",
      "Iteration Number: 7349\n",
      "Loss: 11.751342516994706\n",
      "l2 norm of gradients: 0.05991264623124343\n",
      "l2 norm of weights: 3.297681827492153\n",
      "---------------------\n",
      "Iteration Number: 7350\n",
      "Loss: 11.750932026047657\n",
      "l2 norm of gradients: 0.05990257053774202\n",
      "l2 norm of weights: 3.2977471971647234\n",
      "---------------------\n",
      "Iteration Number: 7351\n",
      "Loss: 11.7505217145057\n",
      "l2 norm of gradients: 0.059892499820548735\n",
      "l2 norm of weights: 3.297812571541963\n",
      "---------------------\n",
      "Iteration Number: 7352\n",
      "Loss: 11.750111582250689\n",
      "l2 norm of gradients: 0.05988243407487707\n",
      "l2 norm of weights: 3.297877950613017\n",
      "---------------------\n",
      "Iteration Number: 7353\n",
      "Loss: 11.749701629164612\n",
      "l2 norm of gradients: 0.059872373295946114\n",
      "l2 norm of weights: 3.2979433343670426\n",
      "---------------------\n",
      "Iteration Number: 7354\n",
      "Loss: 11.74929185512956\n",
      "l2 norm of gradients: 0.05986231747898055\n",
      "l2 norm of weights: 3.2980087227932064\n",
      "---------------------\n",
      "Iteration Number: 7355\n",
      "Loss: 11.748882260027765\n",
      "l2 norm of gradients: 0.05985226661921057\n",
      "l2 norm of weights: 3.298074115880687\n",
      "---------------------\n",
      "Iteration Number: 7356\n",
      "Loss: 11.748472843741563\n",
      "l2 norm of gradients: 0.059842220711871945\n",
      "l2 norm of weights: 3.2981395136186715\n",
      "---------------------\n",
      "Iteration Number: 7357\n",
      "Loss: 11.748063606153435\n",
      "l2 norm of gradients: 0.05983217975220606\n",
      "l2 norm of weights: 3.2982049159963576\n",
      "---------------------\n",
      "Iteration Number: 7358\n",
      "Loss: 11.74765454714596\n",
      "l2 norm of gradients: 0.059822143735459826\n",
      "l2 norm of weights: 3.2982703230029546\n",
      "---------------------\n",
      "Iteration Number: 7359\n",
      "Loss: 11.74724566660187\n",
      "l2 norm of gradients: 0.059812112656885616\n",
      "l2 norm of weights: 3.2983357346276816\n",
      "---------------------\n",
      "Iteration Number: 7360\n",
      "Loss: 11.746836964403979\n",
      "l2 norm of gradients: 0.059802086511741466\n",
      "l2 norm of weights: 3.298401150859767\n",
      "---------------------\n",
      "Iteration Number: 7361\n",
      "Loss: 11.746428440435242\n",
      "l2 norm of gradients: 0.059792065295290846\n",
      "l2 norm of weights: 3.2984665716884516\n",
      "---------------------\n",
      "Iteration Number: 7362\n",
      "Loss: 11.746020094578748\n",
      "l2 norm of gradients: 0.059782049002802845\n",
      "l2 norm of weights: 3.2985319971029843\n",
      "---------------------\n",
      "Iteration Number: 7363\n",
      "Loss: 11.745611926717686\n",
      "l2 norm of gradients: 0.05977203762955199\n",
      "l2 norm of weights: 3.2985974270926257\n",
      "---------------------\n",
      "Iteration Number: 7364\n",
      "Loss: 11.745203936735376\n",
      "l2 norm of gradients: 0.05976203117081837\n",
      "l2 norm of weights: 3.2986628616466467\n",
      "---------------------\n",
      "Iteration Number: 7365\n",
      "Loss: 11.744796124515256\n",
      "l2 norm of gradients: 0.059752029621887554\n",
      "l2 norm of weights: 3.2987283007543278\n",
      "---------------------\n",
      "Iteration Number: 7366\n",
      "Loss: 11.744388489940903\n",
      "l2 norm of gradients: 0.05974203297805069\n",
      "l2 norm of weights: 3.2987937444049606\n",
      "---------------------\n",
      "Iteration Number: 7367\n",
      "Loss: 11.74398103289597\n",
      "l2 norm of gradients: 0.059732041234604356\n",
      "l2 norm of weights: 3.298859192587846\n",
      "---------------------\n",
      "Iteration Number: 7368\n",
      "Loss: 11.743573753264274\n",
      "l2 norm of gradients: 0.05972205438685059\n",
      "l2 norm of weights: 3.298924645292295\n",
      "---------------------\n",
      "Iteration Number: 7369\n",
      "Loss: 11.743166650929734\n",
      "l2 norm of gradients: 0.05971207243009705\n",
      "l2 norm of weights: 3.2989901025076316\n",
      "---------------------\n",
      "Iteration Number: 7370\n",
      "Loss: 11.742759725776388\n",
      "l2 norm of gradients: 0.05970209535965677\n",
      "l2 norm of weights: 3.2990555642231865\n",
      "---------------------\n",
      "Iteration Number: 7371\n",
      "Loss: 11.7423529776884\n",
      "l2 norm of gradients: 0.05969212317084826\n",
      "l2 norm of weights: 3.2991210304283025\n",
      "---------------------\n",
      "Iteration Number: 7372\n",
      "Loss: 11.741946406550053\n",
      "l2 norm of gradients: 0.05968215585899555\n",
      "l2 norm of weights: 3.2991865011123327\n",
      "---------------------\n",
      "Iteration Number: 7373\n",
      "Loss: 11.741540012245737\n",
      "l2 norm of gradients: 0.05967219341942813\n",
      "l2 norm of weights: 3.299251976264639\n",
      "---------------------\n",
      "Iteration Number: 7374\n",
      "Loss: 11.741133794659987\n",
      "l2 norm of gradients: 0.059662235847480934\n",
      "l2 norm of weights: 3.299317455874595\n",
      "---------------------\n",
      "Iteration Number: 7375\n",
      "Loss: 11.740727753677419\n",
      "l2 norm of gradients: 0.059652283138494334\n",
      "l2 norm of weights: 3.299382939931584\n",
      "---------------------\n",
      "Iteration Number: 7376\n",
      "Loss: 11.740321889182816\n",
      "l2 norm of gradients: 0.05964233528781422\n",
      "l2 norm of weights: 3.2994484284250003\n",
      "---------------------\n",
      "Iteration Number: 7377\n",
      "Loss: 11.739916201061034\n",
      "l2 norm of gradients: 0.059632392290791814\n",
      "l2 norm of weights: 3.299513921344246\n",
      "---------------------\n",
      "Iteration Number: 7378\n",
      "Loss: 11.739510689197074\n",
      "l2 norm of gradients: 0.0596224541427839\n",
      "l2 norm of weights: 3.299579418678736\n",
      "---------------------\n",
      "Iteration Number: 7379\n",
      "Loss: 11.739105353476054\n",
      "l2 norm of gradients: 0.059612520839152604\n",
      "l2 norm of weights: 3.2996449204178937\n",
      "---------------------\n",
      "Iteration Number: 7380\n",
      "Loss: 11.738700193783204\n",
      "l2 norm of gradients: 0.05960259237526551\n",
      "l2 norm of weights: 3.2997104265511545\n",
      "---------------------\n",
      "Iteration Number: 7381\n",
      "Loss: 11.738295210003884\n",
      "l2 norm of gradients: 0.05959266874649563\n",
      "l2 norm of weights: 3.2997759370679614\n",
      "---------------------\n",
      "Iteration Number: 7382\n",
      "Loss: 11.737890402023535\n",
      "l2 norm of gradients: 0.05958274994822138\n",
      "l2 norm of weights: 3.299841451957769\n",
      "---------------------\n",
      "Iteration Number: 7383\n",
      "Loss: 11.737485769727769\n",
      "l2 norm of gradients: 0.05957283597582656\n",
      "l2 norm of weights: 3.2999069712100426\n",
      "---------------------\n",
      "Iteration Number: 7384\n",
      "Loss: 11.737081313002287\n",
      "l2 norm of gradients: 0.05956292682470048\n",
      "l2 norm of weights: 3.2999724948142557\n",
      "---------------------\n",
      "Iteration Number: 7385\n",
      "Loss: 11.736677031732896\n",
      "l2 norm of gradients: 0.05955302249023771\n",
      "l2 norm of weights: 3.300038022759894\n",
      "---------------------\n",
      "Iteration Number: 7386\n",
      "Loss: 11.736272925805551\n",
      "l2 norm of gradients: 0.05954312296783827\n",
      "l2 norm of weights: 3.300103555036453\n",
      "---------------------\n",
      "Iteration Number: 7387\n",
      "Loss: 11.735868995106296\n",
      "l2 norm of gradients: 0.05953322825290764\n",
      "l2 norm of weights: 3.300169091633436\n",
      "---------------------\n",
      "Iteration Number: 7388\n",
      "Loss: 11.73546523952131\n",
      "l2 norm of gradients: 0.05952333834085657\n",
      "l2 norm of weights: 3.300234632540359\n",
      "---------------------\n",
      "Iteration Number: 7389\n",
      "Loss: 11.735061658936887\n",
      "l2 norm of gradients: 0.059513453227101235\n",
      "l2 norm of weights: 3.3003001777467467\n",
      "---------------------\n",
      "Iteration Number: 7390\n",
      "Loss: 11.734658253239425\n",
      "l2 norm of gradients: 0.059503572907063186\n",
      "l2 norm of weights: 3.3003657272421347\n",
      "---------------------\n",
      "Iteration Number: 7391\n",
      "Loss: 11.734255022315466\n",
      "l2 norm of gradients: 0.05949369737616935\n",
      "l2 norm of weights: 3.300431281016068\n",
      "---------------------\n",
      "Iteration Number: 7392\n",
      "Loss: 11.733851966051633\n",
      "l2 norm of gradients: 0.05948382662985196\n",
      "l2 norm of weights: 3.300496839058101\n",
      "---------------------\n",
      "Iteration Number: 7393\n",
      "Loss: 11.73344908433468\n",
      "l2 norm of gradients: 0.0594739606635487\n",
      "l2 norm of weights: 3.300562401357801\n",
      "---------------------\n",
      "Iteration Number: 7394\n",
      "Loss: 11.733046377051501\n",
      "l2 norm of gradients: 0.059464099472702474\n",
      "l2 norm of weights: 3.3006279679047417\n",
      "---------------------\n",
      "Iteration Number: 7395\n",
      "Loss: 11.732643844089065\n",
      "l2 norm of gradients: 0.05945424305276166\n",
      "l2 norm of weights: 3.3006935386885083\n",
      "---------------------\n",
      "Iteration Number: 7396\n",
      "Loss: 11.732241485334479\n",
      "l2 norm of gradients: 0.059444391399179876\n",
      "l2 norm of weights: 3.300759113698697\n",
      "---------------------\n",
      "Iteration Number: 7397\n",
      "Loss: 11.731839300674975\n",
      "l2 norm of gradients: 0.059434544507416136\n",
      "l2 norm of weights: 3.300824692924912\n",
      "---------------------\n",
      "Iteration Number: 7398\n",
      "Loss: 11.731437289997894\n",
      "l2 norm of gradients: 0.05942470237293475\n",
      "l2 norm of weights: 3.3008902763567702\n",
      "---------------------\n",
      "Iteration Number: 7399\n",
      "Loss: 11.731035453190664\n",
      "l2 norm of gradients: 0.05941486499120532\n",
      "l2 norm of weights: 3.300955863983895\n",
      "---------------------\n",
      "Iteration Number: 7400\n",
      "Loss: 11.730633790140873\n",
      "l2 norm of gradients: 0.05940503235770282\n",
      "l2 norm of weights: 3.301021455795923\n",
      "---------------------\n",
      "Iteration Number: 7401\n",
      "Loss: 11.73023230073619\n",
      "l2 norm of gradients: 0.05939520446790755\n",
      "l2 norm of weights: 3.301087051782499\n",
      "---------------------\n",
      "Iteration Number: 7402\n",
      "Loss: 11.729830984864417\n",
      "l2 norm of gradients: 0.059385381317305024\n",
      "l2 norm of weights: 3.301152651933278\n",
      "---------------------\n",
      "Iteration Number: 7403\n",
      "Loss: 11.729429842413468\n",
      "l2 norm of gradients: 0.05937556290138616\n",
      "l2 norm of weights: 3.3012182562379246\n",
      "---------------------\n",
      "Iteration Number: 7404\n",
      "Loss: 11.729028873271364\n",
      "l2 norm of gradients: 0.059365749215647065\n",
      "l2 norm of weights: 3.301283864686114\n",
      "---------------------\n",
      "Iteration Number: 7405\n",
      "Loss: 11.72862807732626\n",
      "l2 norm of gradients: 0.0593559402555892\n",
      "l2 norm of weights: 3.3013494772675314\n",
      "---------------------\n",
      "Iteration Number: 7406\n",
      "Loss: 11.728227454466388\n",
      "l2 norm of gradients: 0.059346136016719314\n",
      "l2 norm of weights: 3.3014150939718716\n",
      "---------------------\n",
      "Iteration Number: 7407\n",
      "Loss: 11.72782700458014\n",
      "l2 norm of gradients: 0.05933633649454939\n",
      "l2 norm of weights: 3.3014807147888385\n",
      "---------------------\n",
      "Iteration Number: 7408\n",
      "Loss: 11.72742672755599\n",
      "l2 norm of gradients: 0.05932654168459676\n",
      "l2 norm of weights: 3.3015463397081484\n",
      "---------------------\n",
      "Iteration Number: 7409\n",
      "Loss: 11.727026623282535\n",
      "l2 norm of gradients: 0.05931675158238391\n",
      "l2 norm of weights: 3.3016119687195236\n",
      "---------------------\n",
      "Iteration Number: 7410\n",
      "Loss: 11.726626691648491\n",
      "l2 norm of gradients: 0.05930696618343868\n",
      "l2 norm of weights: 3.3016776018127003\n",
      "---------------------\n",
      "Iteration Number: 7411\n",
      "Loss: 11.726226932542678\n",
      "l2 norm of gradients: 0.0592971854832941\n",
      "l2 norm of weights: 3.3017432389774215\n",
      "---------------------\n",
      "Iteration Number: 7412\n",
      "Loss: 11.72582734585404\n",
      "l2 norm of gradients: 0.05928740947748853\n",
      "l2 norm of weights: 3.301808880203442\n",
      "---------------------\n",
      "Iteration Number: 7413\n",
      "Loss: 11.72542793147161\n",
      "l2 norm of gradients: 0.0592776381615655\n",
      "l2 norm of weights: 3.301874525480525\n",
      "---------------------\n",
      "Iteration Number: 7414\n",
      "Loss: 11.725028689284585\n",
      "l2 norm of gradients: 0.05926787153107378\n",
      "l2 norm of weights: 3.3019401747984447\n",
      "---------------------\n",
      "Iteration Number: 7415\n",
      "Loss: 11.72462961918221\n",
      "l2 norm of gradients: 0.059258109581567436\n",
      "l2 norm of weights: 3.3020058281469846\n",
      "---------------------\n",
      "Iteration Number: 7416\n",
      "Loss: 11.724230721053903\n",
      "l2 norm of gradients: 0.05924835230860571\n",
      "l2 norm of weights: 3.3020714855159374\n",
      "---------------------\n",
      "Iteration Number: 7417\n",
      "Loss: 11.723831994789158\n",
      "l2 norm of gradients: 0.05923859970775311\n",
      "l2 norm of weights: 3.3021371468951077\n",
      "---------------------\n",
      "Iteration Number: 7418\n",
      "Loss: 11.72343344027758\n",
      "l2 norm of gradients: 0.05922885177457929\n",
      "l2 norm of weights: 3.3022028122743072\n",
      "---------------------\n",
      "Iteration Number: 7419\n",
      "Loss: 11.723035057408906\n",
      "l2 norm of gradients: 0.05921910850465916\n",
      "l2 norm of weights: 3.302268481643359\n",
      "---------------------\n",
      "Iteration Number: 7420\n",
      "Loss: 11.722636846072984\n",
      "l2 norm of gradients: 0.05920936989357285\n",
      "l2 norm of weights: 3.302334154992096\n",
      "---------------------\n",
      "Iteration Number: 7421\n",
      "Loss: 11.722238806159753\n",
      "l2 norm of gradients: 0.05919963593690569\n",
      "l2 norm of weights: 3.30239983231036\n",
      "---------------------\n",
      "Iteration Number: 7422\n",
      "Loss: 11.72184093755928\n",
      "l2 norm of gradients: 0.059189906630248174\n",
      "l2 norm of weights: 3.302465513588003\n",
      "---------------------\n",
      "Iteration Number: 7423\n",
      "Loss: 11.721443240161744\n",
      "l2 norm of gradients: 0.05918018196919599\n",
      "l2 norm of weights: 3.3025311988148878\n",
      "---------------------\n",
      "Iteration Number: 7424\n",
      "Loss: 11.721045713857439\n",
      "l2 norm of gradients: 0.05917046194935002\n",
      "l2 norm of weights: 3.3025968879808847\n",
      "---------------------\n",
      "Iteration Number: 7425\n",
      "Loss: 11.720648358536765\n",
      "l2 norm of gradients: 0.05916074656631637\n",
      "l2 norm of weights: 3.302662581075875\n",
      "---------------------\n",
      "Iteration Number: 7426\n",
      "Loss: 11.720251174090217\n",
      "l2 norm of gradients: 0.059151035815706215\n",
      "l2 norm of weights: 3.3027282780897513\n",
      "---------------------\n",
      "Iteration Number: 7427\n",
      "Loss: 11.719854160408428\n",
      "l2 norm of gradients: 0.059141329693135986\n",
      "l2 norm of weights: 3.3027939790124132\n",
      "---------------------\n",
      "Iteration Number: 7428\n",
      "Loss: 11.71945731738213\n",
      "l2 norm of gradients: 0.059131628194227265\n",
      "l2 norm of weights: 3.302859683833771\n",
      "---------------------\n",
      "Iteration Number: 7429\n",
      "Loss: 11.719060644902171\n",
      "l2 norm of gradients: 0.059121931314606746\n",
      "l2 norm of weights: 3.302925392543745\n",
      "---------------------\n",
      "Iteration Number: 7430\n",
      "Loss: 11.718664142859499\n",
      "l2 norm of gradients: 0.05911223904990634\n",
      "l2 norm of weights: 3.3029911051322656\n",
      "---------------------\n",
      "Iteration Number: 7431\n",
      "Loss: 11.718267811145195\n",
      "l2 norm of gradients: 0.05910255139576307\n",
      "l2 norm of weights: 3.3030568215892715\n",
      "---------------------\n",
      "Iteration Number: 7432\n",
      "Loss: 11.717871649650407\n",
      "l2 norm of gradients: 0.05909286834781906\n",
      "l2 norm of weights: 3.3031225419047123\n",
      "---------------------\n",
      "Iteration Number: 7433\n",
      "Loss: 11.717475658266437\n",
      "l2 norm of gradients: 0.05908318990172162\n",
      "l2 norm of weights: 3.3031882660685463\n",
      "---------------------\n",
      "Iteration Number: 7434\n",
      "Loss: 11.717079836884691\n",
      "l2 norm of gradients: 0.059073516053123214\n",
      "l2 norm of weights: 3.3032539940707433\n",
      "---------------------\n",
      "Iteration Number: 7435\n",
      "Loss: 11.716684185396657\n",
      "l2 norm of gradients: 0.05906384679768136\n",
      "l2 norm of weights: 3.30331972590128\n",
      "---------------------\n",
      "Iteration Number: 7436\n",
      "Loss: 11.716288703693959\n",
      "l2 norm of gradients: 0.059054182131058744\n",
      "l2 norm of weights: 3.303385461550145\n",
      "---------------------\n",
      "Iteration Number: 7437\n",
      "Loss: 11.715893391668322\n",
      "l2 norm of gradients: 0.05904452204892318\n",
      "l2 norm of weights: 3.3034512010073356\n",
      "---------------------\n",
      "Iteration Number: 7438\n",
      "Loss: 11.715498249211594\n",
      "l2 norm of gradients: 0.0590348665469475\n",
      "l2 norm of weights: 3.3035169442628587\n",
      "---------------------\n",
      "Iteration Number: 7439\n",
      "Loss: 11.715103276215698\n",
      "l2 norm of gradients: 0.059025215620809764\n",
      "l2 norm of weights: 3.303582691306731\n",
      "---------------------\n",
      "Iteration Number: 7440\n",
      "Loss: 11.714708472572706\n",
      "l2 norm of gradients: 0.05901556926619305\n",
      "l2 norm of weights: 3.3036484421289787\n",
      "---------------------\n",
      "Iteration Number: 7441\n",
      "Loss: 11.714313838174768\n",
      "l2 norm of gradients: 0.059005927478785546\n",
      "l2 norm of weights: 3.3037141967196373\n",
      "---------------------\n",
      "Iteration Number: 7442\n",
      "Loss: 11.713919372914166\n",
      "l2 norm of gradients: 0.05899629025428053\n",
      "l2 norm of weights: 3.303779955068753\n",
      "---------------------\n",
      "Iteration Number: 7443\n",
      "Loss: 11.713525076683288\n",
      "l2 norm of gradients: 0.058986657588376346\n",
      "l2 norm of weights: 3.30384571716638\n",
      "---------------------\n",
      "Iteration Number: 7444\n",
      "Loss: 11.713130949374598\n",
      "l2 norm of gradients: 0.05897702947677644\n",
      "l2 norm of weights: 3.303911483002583\n",
      "---------------------\n",
      "Iteration Number: 7445\n",
      "Loss: 11.712736990880718\n",
      "l2 norm of gradients: 0.05896740591518932\n",
      "l2 norm of weights: 3.303977252567437\n",
      "---------------------\n",
      "Iteration Number: 7446\n",
      "Loss: 11.712343201094349\n",
      "l2 norm of gradients: 0.05895778689932858\n",
      "l2 norm of weights: 3.304043025851024\n",
      "---------------------\n",
      "Iteration Number: 7447\n",
      "Loss: 11.711949579908309\n",
      "l2 norm of gradients: 0.05894817242491283\n",
      "l2 norm of weights: 3.3041088028434387\n",
      "---------------------\n",
      "Iteration Number: 7448\n",
      "Loss: 11.711556127215513\n",
      "l2 norm of gradients: 0.058938562487665755\n",
      "l2 norm of weights: 3.3041745835347824\n",
      "---------------------\n",
      "Iteration Number: 7449\n",
      "Loss: 11.71116284290899\n",
      "l2 norm of gradients: 0.058928957083316094\n",
      "l2 norm of weights: 3.304240367915169\n",
      "---------------------\n",
      "Iteration Number: 7450\n",
      "Loss: 11.710769726881901\n",
      "l2 norm of gradients: 0.058919356207597647\n",
      "l2 norm of weights: 3.304306155974719\n",
      "---------------------\n",
      "Iteration Number: 7451\n",
      "Loss: 11.710376779027472\n",
      "l2 norm of gradients: 0.05890975985624922\n",
      "l2 norm of weights: 3.3043719477035642\n",
      "---------------------\n",
      "Iteration Number: 7452\n",
      "Loss: 11.709983999239059\n",
      "l2 norm of gradients: 0.05890016802501467\n",
      "l2 norm of weights: 3.304437743091845\n",
      "---------------------\n",
      "Iteration Number: 7453\n",
      "Loss: 11.709591387410137\n",
      "l2 norm of gradients: 0.05889058070964287\n",
      "l2 norm of weights: 3.3045035421297118\n",
      "---------------------\n",
      "Iteration Number: 7454\n",
      "Loss: 11.709198943434256\n",
      "l2 norm of gradients: 0.058880997905887716\n",
      "l2 norm of weights: 3.304569344807325\n",
      "---------------------\n",
      "Iteration Number: 7455\n",
      "Loss: 11.708806667205119\n",
      "l2 norm of gradients: 0.05887141960950818\n",
      "l2 norm of weights: 3.3046351511148515\n",
      "---------------------\n",
      "Iteration Number: 7456\n",
      "Loss: 11.708414558616486\n",
      "l2 norm of gradients: 0.05886184581626817\n",
      "l2 norm of weights: 3.3047009610424727\n",
      "---------------------\n",
      "Iteration Number: 7457\n",
      "Loss: 11.70802261756225\n",
      "l2 norm of gradients: 0.05885227652193661\n",
      "l2 norm of weights: 3.3047667745803753\n",
      "---------------------\n",
      "Iteration Number: 7458\n",
      "Loss: 11.70763084393642\n",
      "l2 norm of gradients: 0.05884271172228747\n",
      "l2 norm of weights: 3.304832591718757\n",
      "---------------------\n",
      "Iteration Number: 7459\n",
      "Loss: 11.707239237633093\n",
      "l2 norm of gradients: 0.0588331514130997\n",
      "l2 norm of weights: 3.304898412447825\n",
      "---------------------\n",
      "Iteration Number: 7460\n",
      "Loss: 11.706847798546482\n",
      "l2 norm of gradients: 0.05882359559015719\n",
      "l2 norm of weights: 3.3049642367577947\n",
      "---------------------\n",
      "Iteration Number: 7461\n",
      "Loss: 11.706456526570893\n",
      "l2 norm of gradients: 0.05881404424924891\n",
      "l2 norm of weights: 3.3050300646388937\n",
      "---------------------\n",
      "Iteration Number: 7462\n",
      "Loss: 11.706065421600746\n",
      "l2 norm of gradients: 0.0588044973861687\n",
      "l2 norm of weights: 3.305095896081356\n",
      "---------------------\n",
      "Iteration Number: 7463\n",
      "Loss: 11.705674483530602\n",
      "l2 norm of gradients: 0.058794954996715484\n",
      "l2 norm of weights: 3.3051617310754264\n",
      "---------------------\n",
      "Iteration Number: 7464\n",
      "Loss: 11.705283712255055\n",
      "l2 norm of gradients: 0.05878541707669307\n",
      "l2 norm of weights: 3.3052275696113584\n",
      "---------------------\n",
      "Iteration Number: 7465\n",
      "Loss: 11.704893107668862\n",
      "l2 norm of gradients: 0.058775883621910316\n",
      "l2 norm of weights: 3.305293411679416\n",
      "---------------------\n",
      "Iteration Number: 7466\n",
      "Loss: 11.704502669666873\n",
      "l2 norm of gradients: 0.05876635462818096\n",
      "l2 norm of weights: 3.3053592572698722\n",
      "---------------------\n",
      "Iteration Number: 7467\n",
      "Loss: 11.704112398144032\n",
      "l2 norm of gradients: 0.058756830091323695\n",
      "l2 norm of weights: 3.3054251063730087\n",
      "---------------------\n",
      "Iteration Number: 7468\n",
      "Loss: 11.703722292995401\n",
      "l2 norm of gradients: 0.05874731000716225\n",
      "l2 norm of weights: 3.3054909589791173\n",
      "---------------------\n",
      "Iteration Number: 7469\n",
      "Loss: 11.703332354116135\n",
      "l2 norm of gradients: 0.05873779437152519\n",
      "l2 norm of weights: 3.305556815078498\n",
      "---------------------\n",
      "Iteration Number: 7470\n",
      "Loss: 11.702942581401501\n",
      "l2 norm of gradients: 0.058728283180246095\n",
      "l2 norm of weights: 3.3056226746614623\n",
      "---------------------\n",
      "Iteration Number: 7471\n",
      "Loss: 11.702552974746883\n",
      "l2 norm of gradients: 0.05871877642916344\n",
      "l2 norm of weights: 3.3056885377183285\n",
      "---------------------\n",
      "Iteration Number: 7472\n",
      "Loss: 11.702163534047745\n",
      "l2 norm of gradients: 0.058709274114120635\n",
      "l2 norm of weights: 3.3057544042394262\n",
      "---------------------\n",
      "Iteration Number: 7473\n",
      "Loss: 11.70177425919966\n",
      "l2 norm of gradients: 0.05869977623096605\n",
      "l2 norm of weights: 3.3058202742150926\n",
      "---------------------\n",
      "Iteration Number: 7474\n",
      "Loss: 11.701385150098334\n",
      "l2 norm of gradients: 0.05869028277555289\n",
      "l2 norm of weights: 3.3058861476356762\n",
      "---------------------\n",
      "Iteration Number: 7475\n",
      "Loss: 11.700996206639534\n",
      "l2 norm of gradients: 0.05868079374373935\n",
      "l2 norm of weights: 3.3059520244915332\n",
      "---------------------\n",
      "Iteration Number: 7476\n",
      "Loss: 11.700607428719175\n",
      "l2 norm of gradients: 0.058671309131388516\n",
      "l2 norm of weights: 3.3060179047730296\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 7477\n",
      "Loss: 11.700218816233244\n",
      "l2 norm of gradients: 0.05866182893436829\n",
      "l2 norm of weights: 3.306083788470541\n",
      "---------------------\n",
      "Iteration Number: 7478\n",
      "Loss: 11.699830369077839\n",
      "l2 norm of gradients: 0.05865235314855163\n",
      "l2 norm of weights: 3.3061496755744515\n",
      "---------------------\n",
      "Iteration Number: 7479\n",
      "Loss: 11.699442087149169\n",
      "l2 norm of gradients: 0.05864288176981624\n",
      "l2 norm of weights: 3.306215566075155\n",
      "---------------------\n",
      "Iteration Number: 7480\n",
      "Loss: 11.699053970343556\n",
      "l2 norm of gradients: 0.05863341479404481\n",
      "l2 norm of weights: 3.3062814599630554\n",
      "---------------------\n",
      "Iteration Number: 7481\n",
      "Loss: 11.698666018557391\n",
      "l2 norm of gradients: 0.058623952217124846\n",
      "l2 norm of weights: 3.3063473572285638\n",
      "---------------------\n",
      "Iteration Number: 7482\n",
      "Loss: 11.698278231687198\n",
      "l2 norm of gradients: 0.05861449403494874\n",
      "l2 norm of weights: 3.306413257862103\n",
      "---------------------\n",
      "Iteration Number: 7483\n",
      "Loss: 11.6978906096296\n",
      "l2 norm of gradients: 0.05860504024341377\n",
      "l2 norm of weights: 3.306479161854103\n",
      "---------------------\n",
      "Iteration Number: 7484\n",
      "Loss: 11.697503152281323\n",
      "l2 norm of gradients: 0.05859559083842211\n",
      "l2 norm of weights: 3.306545069195004\n",
      "---------------------\n",
      "Iteration Number: 7485\n",
      "Loss: 11.697115859539176\n",
      "l2 norm of gradients: 0.058586145815880675\n",
      "l2 norm of weights: 3.306610979875255\n",
      "---------------------\n",
      "Iteration Number: 7486\n",
      "Loss: 11.696728731300114\n",
      "l2 norm of gradients: 0.058576705171701396\n",
      "l2 norm of weights: 3.306676893885315\n",
      "---------------------\n",
      "Iteration Number: 7487\n",
      "Loss: 11.696341767461142\n",
      "l2 norm of gradients: 0.05856726890180094\n",
      "l2 norm of weights: 3.306742811215652\n",
      "---------------------\n",
      "Iteration Number: 7488\n",
      "Loss: 11.695954967919398\n",
      "l2 norm of gradients: 0.05855783700210088\n",
      "l2 norm of weights: 3.306808731856741\n",
      "---------------------\n",
      "Iteration Number: 7489\n",
      "Loss: 11.695568332572131\n",
      "l2 norm of gradients: 0.05854840946852755\n",
      "l2 norm of weights: 3.30687465579907\n",
      "---------------------\n",
      "Iteration Number: 7490\n",
      "Loss: 11.69518186131667\n",
      "l2 norm of gradients: 0.058538986297012235\n",
      "l2 norm of weights: 3.306940583033133\n",
      "---------------------\n",
      "Iteration Number: 7491\n",
      "Loss: 11.694795554050454\n",
      "l2 norm of gradients: 0.05852956748349092\n",
      "l2 norm of weights: 3.3070065135494344\n",
      "---------------------\n",
      "Iteration Number: 7492\n",
      "Loss: 11.694409410671021\n",
      "l2 norm of gradients: 0.05852015302390451\n",
      "l2 norm of weights: 3.3070724473384883\n",
      "---------------------\n",
      "Iteration Number: 7493\n",
      "Loss: 11.694023431076026\n",
      "l2 norm of gradients: 0.05851074291419868\n",
      "l2 norm of weights: 3.307138384390817\n",
      "---------------------\n",
      "Iteration Number: 7494\n",
      "Loss: 11.693637615163215\n",
      "l2 norm of gradients: 0.05850133715032395\n",
      "l2 norm of weights: 3.3072043246969525\n",
      "---------------------\n",
      "Iteration Number: 7495\n",
      "Loss: 11.69325196283042\n",
      "l2 norm of gradients: 0.058491935728235606\n",
      "l2 norm of weights: 3.3072702682474353\n",
      "---------------------\n",
      "Iteration Number: 7496\n",
      "Loss: 11.69286647397561\n",
      "l2 norm of gradients: 0.05848253864389374\n",
      "l2 norm of weights: 3.3073362150328154\n",
      "---------------------\n",
      "Iteration Number: 7497\n",
      "Loss: 11.69248114849682\n",
      "l2 norm of gradients: 0.058473145893263315\n",
      "l2 norm of weights: 3.3074021650436523\n",
      "---------------------\n",
      "Iteration Number: 7498\n",
      "Loss: 11.69209598629221\n",
      "l2 norm of gradients: 0.05846375747231399\n",
      "l2 norm of weights: 3.307468118270514\n",
      "---------------------\n",
      "Iteration Number: 7499\n",
      "Loss: 11.691710987260022\n",
      "l2 norm of gradients: 0.0584543733770203\n",
      "l2 norm of weights: 3.3075340747039776\n",
      "---------------------\n",
      "Iteration Number: 7500\n",
      "Loss: 11.69132615129862\n",
      "l2 norm of gradients: 0.05844499360336144\n",
      "l2 norm of weights: 3.3076000343346306\n",
      "---------------------\n",
      "Iteration Number: 7501\n",
      "Loss: 11.690941478306463\n",
      "l2 norm of gradients: 0.058435618147321546\n",
      "l2 norm of weights: 3.3076659971530673\n",
      "---------------------\n",
      "Iteration Number: 7502\n",
      "Loss: 11.690556968182086\n",
      "l2 norm of gradients: 0.05842624700488936\n",
      "l2 norm of weights: 3.3077319631498927\n",
      "---------------------\n",
      "Iteration Number: 7503\n",
      "Loss: 11.69017262082416\n",
      "l2 norm of gradients: 0.05841688017205851\n",
      "l2 norm of weights: 3.3077979323157205\n",
      "---------------------\n",
      "Iteration Number: 7504\n",
      "Loss: 11.689788436131439\n",
      "l2 norm of gradients: 0.05840751764482732\n",
      "l2 norm of weights: 3.3078639046411733\n",
      "---------------------\n",
      "Iteration Number: 7505\n",
      "Loss: 11.689404414002771\n",
      "l2 norm of gradients: 0.05839815941919891\n",
      "l2 norm of weights: 3.307929880116883\n",
      "---------------------\n",
      "Iteration Number: 7506\n",
      "Loss: 11.689020554337114\n",
      "l2 norm of gradients: 0.05838880549118112\n",
      "l2 norm of weights: 3.30799585873349\n",
      "---------------------\n",
      "Iteration Number: 7507\n",
      "Loss: 11.688636857033542\n",
      "l2 norm of gradients: 0.058379455856786576\n",
      "l2 norm of weights: 3.3080618404816446\n",
      "---------------------\n",
      "Iteration Number: 7508\n",
      "Loss: 11.68825332199119\n",
      "l2 norm of gradients: 0.058370110512032586\n",
      "l2 norm of weights: 3.3081278253520052\n",
      "---------------------\n",
      "Iteration Number: 7509\n",
      "Loss: 11.687869949109313\n",
      "l2 norm of gradients: 0.058360769452941226\n",
      "l2 norm of weights: 3.30819381333524\n",
      "---------------------\n",
      "Iteration Number: 7510\n",
      "Loss: 11.687486738287285\n",
      "l2 norm of gradients: 0.058351432675539334\n",
      "l2 norm of weights: 3.308259804422026\n",
      "---------------------\n",
      "Iteration Number: 7511\n",
      "Loss: 11.687103689424537\n",
      "l2 norm of gradients: 0.05834210017585841\n",
      "l2 norm of weights: 3.308325798603048\n",
      "---------------------\n",
      "Iteration Number: 7512\n",
      "Loss: 11.686720802420647\n",
      "l2 norm of gradients: 0.05833277194993474\n",
      "l2 norm of weights: 3.3083917958690026\n",
      "---------------------\n",
      "Iteration Number: 7513\n",
      "Loss: 11.686338077175263\n",
      "l2 norm of gradients: 0.05832344799380922\n",
      "l2 norm of weights: 3.308457796210592\n",
      "---------------------\n",
      "Iteration Number: 7514\n",
      "Loss: 11.68595551358812\n",
      "l2 norm of gradients: 0.05831412830352762\n",
      "l2 norm of weights: 3.3085237996185297\n",
      "---------------------\n",
      "Iteration Number: 7515\n",
      "Loss: 11.685573111559087\n",
      "l2 norm of gradients: 0.05830481287514027\n",
      "l2 norm of weights: 3.3085898060835373\n",
      "---------------------\n",
      "Iteration Number: 7516\n",
      "Loss: 11.685190870988109\n",
      "l2 norm of gradients: 0.058295501704702275\n",
      "l2 norm of weights: 3.308655815596346\n",
      "---------------------\n",
      "Iteration Number: 7517\n",
      "Loss: 11.684808791775236\n",
      "l2 norm of gradients: 0.05828619478827338\n",
      "l2 norm of weights: 3.3087218281476947\n",
      "---------------------\n",
      "Iteration Number: 7518\n",
      "Loss: 11.684426873820614\n",
      "l2 norm of gradients: 0.05827689212191807\n",
      "l2 norm of weights: 3.3087878437283322\n",
      "---------------------\n",
      "Iteration Number: 7519\n",
      "Loss: 11.684045117024484\n",
      "l2 norm of gradients: 0.0582675937017055\n",
      "l2 norm of weights: 3.3088538623290162\n",
      "---------------------\n",
      "Iteration Number: 7520\n",
      "Loss: 11.683663521287212\n",
      "l2 norm of gradients: 0.0582582995237095\n",
      "l2 norm of weights: 3.3089198839405136\n",
      "---------------------\n",
      "Iteration Number: 7521\n",
      "Loss: 11.683282086509205\n",
      "l2 norm of gradients: 0.05824900958400857\n",
      "l2 norm of weights: 3.3089859085535993\n",
      "---------------------\n",
      "Iteration Number: 7522\n",
      "Loss: 11.68290081259103\n",
      "l2 norm of gradients: 0.058239723878685846\n",
      "l2 norm of weights: 3.309051936159057\n",
      "---------------------\n",
      "Iteration Number: 7523\n",
      "Loss: 11.682519699433321\n",
      "l2 norm of gradients: 0.058230442403829255\n",
      "l2 norm of weights: 3.3091179667476815\n",
      "---------------------\n",
      "Iteration Number: 7524\n",
      "Loss: 11.682138746936813\n",
      "l2 norm of gradients: 0.058221165155531246\n",
      "l2 norm of weights: 3.3091840003102737\n",
      "---------------------\n",
      "Iteration Number: 7525\n",
      "Loss: 11.681757955002325\n",
      "l2 norm of gradients: 0.05821189212988896\n",
      "l2 norm of weights: 3.3092500368376445\n",
      "---------------------\n",
      "Iteration Number: 7526\n",
      "Loss: 11.681377323530814\n",
      "l2 norm of gradients: 0.05820262332300421\n",
      "l2 norm of weights: 3.3093160763206146\n",
      "---------------------\n",
      "Iteration Number: 7527\n",
      "Loss: 11.680996852423283\n",
      "l2 norm of gradients: 0.05819335873098342\n",
      "l2 norm of weights: 3.309382118750012\n",
      "---------------------\n",
      "Iteration Number: 7528\n",
      "Loss: 11.680616541580887\n",
      "l2 norm of gradients: 0.05818409834993771\n",
      "l2 norm of weights: 3.3094481641166746\n",
      "---------------------\n",
      "Iteration Number: 7529\n",
      "Loss: 11.680236390904827\n",
      "l2 norm of gradients: 0.058174842175982754\n",
      "l2 norm of weights: 3.3095142124114485\n",
      "---------------------\n",
      "Iteration Number: 7530\n",
      "Loss: 11.679856400296424\n",
      "l2 norm of gradients: 0.05816559020523893\n",
      "l2 norm of weights: 3.309580263625189\n",
      "---------------------\n",
      "Iteration Number: 7531\n",
      "Loss: 11.679476569657101\n",
      "l2 norm of gradients: 0.05815634243383123\n",
      "l2 norm of weights: 3.309646317748761\n",
      "---------------------\n",
      "Iteration Number: 7532\n",
      "Loss: 11.679096898888375\n",
      "l2 norm of gradients: 0.05814709885788918\n",
      "l2 norm of weights: 3.3097123747730364\n",
      "---------------------\n",
      "Iteration Number: 7533\n",
      "Loss: 11.678717387891853\n",
      "l2 norm of gradients: 0.05813785947354702\n",
      "l2 norm of weights: 3.3097784346888974\n",
      "---------------------\n",
      "Iteration Number: 7534\n",
      "Loss: 11.67833803656924\n",
      "l2 norm of gradients: 0.058128624276943555\n",
      "l2 norm of weights: 3.3098444974872354\n",
      "---------------------\n",
      "Iteration Number: 7535\n",
      "Loss: 11.677958844822342\n",
      "l2 norm of gradients: 0.0581193932642222\n",
      "l2 norm of weights: 3.3099105631589483\n",
      "---------------------\n",
      "Iteration Number: 7536\n",
      "Loss: 11.677579812553056\n",
      "l2 norm of gradients: 0.05811016643153096\n",
      "l2 norm of weights: 3.309976631694946\n",
      "---------------------\n",
      "Iteration Number: 7537\n",
      "Loss: 11.677200939663386\n",
      "l2 norm of gradients: 0.058100943775022465\n",
      "l2 norm of weights: 3.3100427030861432\n",
      "---------------------\n",
      "Iteration Number: 7538\n",
      "Loss: 11.676822226055403\n",
      "l2 norm of gradients: 0.05809172529085389\n",
      "l2 norm of weights: 3.310108777323468\n",
      "---------------------\n",
      "Iteration Number: 7539\n",
      "Loss: 11.676443671631318\n",
      "l2 norm of gradients: 0.05808251097518702\n",
      "l2 norm of weights: 3.3101748543978533\n",
      "---------------------\n",
      "Iteration Number: 7540\n",
      "Loss: 11.676065276293404\n",
      "l2 norm of gradients: 0.0580733008241882\n",
      "l2 norm of weights: 3.3102409343002437\n",
      "---------------------\n",
      "Iteration Number: 7541\n",
      "Loss: 11.675687039944037\n",
      "l2 norm of gradients: 0.05806409483402832\n",
      "l2 norm of weights: 3.31030701702159\n",
      "---------------------\n",
      "Iteration Number: 7542\n",
      "Loss: 11.675308962485698\n",
      "l2 norm of gradients: 0.05805489300088298\n",
      "l2 norm of weights: 3.3103731025528536\n",
      "---------------------\n",
      "Iteration Number: 7543\n",
      "Loss: 11.674931043820948\n",
      "l2 norm of gradients: 0.058045695320932164\n",
      "l2 norm of weights: 3.3104391908850044\n",
      "---------------------\n",
      "Iteration Number: 7544\n",
      "Loss: 11.674553283852466\n",
      "l2 norm of gradients: 0.05803650179036051\n",
      "l2 norm of weights: 3.31050528200902\n",
      "---------------------\n",
      "Iteration Number: 7545\n",
      "Loss: 11.674175682483\n",
      "l2 norm of gradients: 0.05802731240535721\n",
      "l2 norm of weights: 3.310571375915888\n",
      "---------------------\n",
      "Iteration Number: 7546\n",
      "Loss: 11.673798239615401\n",
      "l2 norm of gradients: 0.05801812716211595\n",
      "l2 norm of weights: 3.3106374725966043\n",
      "---------------------\n",
      "Iteration Number: 7547\n",
      "Loss: 11.67342095515263\n",
      "l2 norm of gradients: 0.05800894605683505\n",
      "l2 norm of weights: 3.3107035720421725\n",
      "---------------------\n",
      "Iteration Number: 7548\n",
      "Loss: 11.673043828997734\n",
      "l2 norm of gradients: 0.05799976908571725\n",
      "l2 norm of weights: 3.3107696742436064\n",
      "---------------------\n",
      "Iteration Number: 7549\n",
      "Loss: 11.672666861053845\n",
      "l2 norm of gradients: 0.05799059624496995\n",
      "l2 norm of weights: 3.310835779191928\n",
      "---------------------\n",
      "Iteration Number: 7550\n",
      "Loss: 11.672290051224202\n",
      "l2 norm of gradients: 0.05798142753080494\n",
      "l2 norm of weights: 3.3109018868781668\n",
      "---------------------\n",
      "Iteration Number: 7551\n",
      "Loss: 11.67191339941212\n",
      "l2 norm of gradients: 0.057972262939438655\n",
      "l2 norm of weights: 3.310967997293363\n",
      "---------------------\n",
      "Iteration Number: 7552\n",
      "Loss: 11.671536905521036\n",
      "l2 norm of gradients: 0.057963102467092006\n",
      "l2 norm of weights: 3.3110341104285648\n",
      "---------------------\n",
      "Iteration Number: 7553\n",
      "Loss: 11.67116056945446\n",
      "l2 norm of gradients: 0.057953946109990405\n",
      "l2 norm of weights: 3.311100226274828\n",
      "---------------------\n",
      "Iteration Number: 7554\n",
      "Loss: 11.670784391116005\n",
      "l2 norm of gradients: 0.0579447938643638\n",
      "l2 norm of weights: 3.3111663448232176\n",
      "---------------------\n",
      "Iteration Number: 7555\n",
      "Loss: 11.670408370409369\n",
      "l2 norm of gradients: 0.05793564572644657\n",
      "l2 norm of weights: 3.3112324660648085\n",
      "---------------------\n",
      "Iteration Number: 7556\n",
      "Loss: 11.670032507238357\n",
      "l2 norm of gradients: 0.05792650169247769\n",
      "l2 norm of weights: 3.311298589990683\n",
      "---------------------\n",
      "Iteration Number: 7557\n",
      "Loss: 11.669656801506868\n",
      "l2 norm of gradients: 0.05791736175870058\n",
      "l2 norm of weights: 3.311364716591931\n",
      "---------------------\n",
      "Iteration Number: 7558\n",
      "Loss: 11.669281253118864\n",
      "l2 norm of gradients: 0.057908225921363174\n",
      "l2 norm of weights: 3.3114308458596535\n",
      "---------------------\n",
      "Iteration Number: 7559\n",
      "Loss: 11.668905861978446\n",
      "l2 norm of gradients: 0.0578990941767178\n",
      "l2 norm of weights: 3.3114969777849583\n",
      "---------------------\n",
      "Iteration Number: 7560\n",
      "Loss: 11.668530627989787\n",
      "l2 norm of gradients: 0.0578899665210214\n",
      "l2 norm of weights: 3.311563112358963\n",
      "---------------------\n",
      "Iteration Number: 7561\n",
      "Loss: 11.668155551057122\n",
      "l2 norm of gradients: 0.0578808429505353\n",
      "l2 norm of weights: 3.311629249572793\n",
      "---------------------\n",
      "Iteration Number: 7562\n",
      "Loss: 11.667780631084844\n",
      "l2 norm of gradients: 0.057871723461525325\n",
      "l2 norm of weights: 3.311695389417582\n",
      "---------------------\n",
      "Iteration Number: 7563\n",
      "Loss: 11.667405867977386\n",
      "l2 norm of gradients: 0.057862608050261735\n",
      "l2 norm of weights: 3.311761531884474\n",
      "---------------------\n",
      "Iteration Number: 7564\n",
      "Loss: 11.667031261639295\n",
      "l2 norm of gradients: 0.05785349671301931\n",
      "l2 norm of weights: 3.3118276769646187\n",
      "---------------------\n",
      "Iteration Number: 7565\n",
      "Loss: 11.666656811975205\n",
      "l2 norm of gradients: 0.05784438944607722\n",
      "l2 norm of weights: 3.3118938246491774\n",
      "---------------------\n",
      "Iteration Number: 7566\n",
      "Loss: 11.666282518889851\n",
      "l2 norm of gradients: 0.05783528624571912\n",
      "l2 norm of weights: 3.3119599749293176\n",
      "---------------------\n",
      "Iteration Number: 7567\n",
      "Loss: 11.665908382288041\n",
      "l2 norm of gradients: 0.05782618710823308\n",
      "l2 norm of weights: 3.3120261277962175\n",
      "---------------------\n",
      "Iteration Number: 7568\n",
      "Loss: 11.665534402074709\n",
      "l2 norm of gradients: 0.05781709202991164\n",
      "l2 norm of weights: 3.312092283241062\n",
      "---------------------\n",
      "Iteration Number: 7569\n",
      "Loss: 11.665160578154842\n",
      "l2 norm of gradients: 0.05780800100705178\n",
      "l2 norm of weights: 3.3121584412550455\n",
      "---------------------\n",
      "Iteration Number: 7570\n",
      "Loss: 11.664786910433554\n",
      "l2 norm of gradients: 0.05779891403595489\n",
      "l2 norm of weights: 3.312224601829371\n",
      "---------------------\n",
      "Iteration Number: 7571\n",
      "Loss: 11.664413398816016\n",
      "l2 norm of gradients: 0.05778983111292676\n",
      "l2 norm of weights: 3.3122907649552484\n",
      "---------------------\n",
      "Iteration Number: 7572\n",
      "Loss: 11.664040043207525\n",
      "l2 norm of gradients: 0.05778075223427764\n",
      "l2 norm of weights: 3.312356930623899\n",
      "---------------------\n",
      "Iteration Number: 7573\n",
      "Loss: 11.663666843513447\n",
      "l2 norm of gradients: 0.05777167739632219\n",
      "l2 norm of weights: 3.3124230988265504\n",
      "---------------------\n",
      "Iteration Number: 7574\n",
      "Loss: 11.663293799639252\n",
      "l2 norm of gradients: 0.05776260659537946\n",
      "l2 norm of weights: 3.3124892695544395\n",
      "---------------------\n",
      "Iteration Number: 7575\n",
      "Loss: 11.662920911490483\n",
      "l2 norm of gradients: 0.05775353982777293\n",
      "l2 norm of weights: 3.3125554427988115\n",
      "---------------------\n",
      "Iteration Number: 7576\n",
      "Loss: 11.6625481789728\n",
      "l2 norm of gradients: 0.05774447708983046\n",
      "l2 norm of weights: 3.3126216185509207\n",
      "---------------------\n",
      "Iteration Number: 7577\n",
      "Loss: 11.662175601991942\n",
      "l2 norm of gradients: 0.05773541837788429\n",
      "l2 norm of weights: 3.3126877968020283\n",
      "---------------------\n",
      "Iteration Number: 7578\n",
      "Loss: 11.661803180453735\n",
      "l2 norm of gradients: 0.0577263636882711\n",
      "l2 norm of weights: 3.3127539775434056\n",
      "---------------------\n",
      "Iteration Number: 7579\n",
      "Loss: 11.66143091426409\n",
      "l2 norm of gradients: 0.0577173130173319\n",
      "l2 norm of weights: 3.312820160766332\n",
      "---------------------\n",
      "Iteration Number: 7580\n",
      "Loss: 11.661058803329029\n",
      "l2 norm of gradients: 0.057708266361412144\n",
      "l2 norm of weights: 3.312886346462095\n",
      "---------------------\n",
      "Iteration Number: 7581\n",
      "Loss: 11.660686847554663\n",
      "l2 norm of gradients: 0.057699223716861606\n",
      "l2 norm of weights: 3.312952534621991\n",
      "---------------------\n",
      "Iteration Number: 7582\n",
      "Loss: 11.660315046847158\n",
      "l2 norm of gradients: 0.057690185080034456\n",
      "l2 norm of weights: 3.313018725237323\n",
      "---------------------\n",
      "Iteration Number: 7583\n",
      "Loss: 11.659943401112823\n",
      "l2 norm of gradients: 0.057681150447289185\n",
      "l2 norm of weights: 3.3130849182994058\n",
      "---------------------\n",
      "Iteration Number: 7584\n",
      "Loss: 11.659571910258029\n",
      "l2 norm of gradients: 0.05767211981498875\n",
      "l2 norm of weights: 3.31315111379956\n",
      "---------------------\n",
      "Iteration Number: 7585\n",
      "Loss: 11.659200574189224\n",
      "l2 norm of gradients: 0.05766309317950033\n",
      "l2 norm of weights: 3.313217311729116\n",
      "---------------------\n",
      "Iteration Number: 7586\n",
      "Loss: 11.65882939281298\n",
      "l2 norm of gradients: 0.057654070537195574\n",
      "l2 norm of weights: 3.3132835120794115\n",
      "---------------------\n",
      "Iteration Number: 7587\n",
      "Loss: 11.65845836603592\n",
      "l2 norm of gradients: 0.05764505188445042\n",
      "l2 norm of weights: 3.313349714841794\n",
      "---------------------\n",
      "Iteration Number: 7588\n",
      "Loss: 11.658087493764794\n",
      "l2 norm of gradients: 0.05763603721764511\n",
      "l2 norm of weights: 3.3134159200076168\n",
      "---------------------\n",
      "Iteration Number: 7589\n",
      "Loss: 11.657716775906422\n",
      "l2 norm of gradients: 0.05762702653316434\n",
      "l2 norm of weights: 3.313482127568245\n",
      "---------------------\n",
      "Iteration Number: 7590\n",
      "Loss: 11.65734621236771\n",
      "l2 norm of gradients: 0.057618019827397\n",
      "l2 norm of weights: 3.3135483375150496\n",
      "---------------------\n",
      "Iteration Number: 7591\n",
      "Loss: 11.65697580305568\n",
      "l2 norm of gradients: 0.057609017096736416\n",
      "l2 norm of weights: 3.313614549839411\n",
      "---------------------\n",
      "Iteration Number: 7592\n",
      "Loss: 11.656605547877412\n",
      "l2 norm of gradients: 0.05760001833758016\n",
      "l2 norm of weights: 3.313680764532718\n",
      "---------------------\n",
      "Iteration Number: 7593\n",
      "Loss: 11.656235446740077\n",
      "l2 norm of gradients: 0.057591023546330194\n",
      "l2 norm of weights: 3.3137469815863687\n",
      "---------------------\n",
      "Iteration Number: 7594\n",
      "Loss: 11.655865499550965\n",
      "l2 norm of gradients: 0.05758203271939271\n",
      "l2 norm of weights: 3.313813200991766\n",
      "---------------------\n",
      "Iteration Number: 7595\n",
      "Loss: 11.655495706217419\n",
      "l2 norm of gradients: 0.05757304585317827\n",
      "l2 norm of weights: 3.313879422740325\n",
      "---------------------\n",
      "Iteration Number: 7596\n",
      "Loss: 11.6551260666469\n",
      "l2 norm of gradients: 0.05756406294410171\n",
      "l2 norm of weights: 3.3139456468234676\n",
      "---------------------\n",
      "Iteration Number: 7597\n",
      "Loss: 11.65475658074694\n",
      "l2 norm of gradients: 0.05755508398858218\n",
      "l2 norm of weights: 3.3140118732326243\n",
      "---------------------\n",
      "Iteration Number: 7598\n",
      "Loss: 11.654387248425163\n",
      "l2 norm of gradients: 0.05754610898304311\n",
      "l2 norm of weights: 3.3140781019592334\n",
      "---------------------\n",
      "Iteration Number: 7599\n",
      "Loss: 11.654018069589291\n",
      "l2 norm of gradients: 0.057537137923912254\n",
      "l2 norm of weights: 3.3141443329947413\n",
      "---------------------\n",
      "Iteration Number: 7600\n",
      "Loss: 11.653649044147127\n",
      "l2 norm of gradients: 0.057528170807621584\n",
      "l2 norm of weights: 3.3142105663306047\n",
      "---------------------\n",
      "Iteration Number: 7601\n",
      "Loss: 11.653280172006546\n",
      "l2 norm of gradients: 0.057519207630607395\n",
      "l2 norm of weights: 3.314276801958286\n",
      "---------------------\n",
      "Iteration Number: 7602\n",
      "Loss: 11.652911453075555\n",
      "l2 norm of gradients: 0.05751024838931025\n",
      "l2 norm of weights: 3.3143430398692577\n",
      "---------------------\n",
      "Iteration Number: 7603\n",
      "Loss: 11.652542887262204\n",
      "l2 norm of gradients: 0.05750129308017499\n",
      "l2 norm of weights: 3.3144092800550005\n",
      "---------------------\n",
      "Iteration Number: 7604\n",
      "Loss: 11.65217447447466\n",
      "l2 norm of gradients: 0.05749234169965069\n",
      "l2 norm of weights: 3.3144755225070015\n",
      "---------------------\n",
      "Iteration Number: 7605\n",
      "Loss: 11.651806214621159\n",
      "l2 norm of gradients: 0.05748339424419075\n",
      "l2 norm of weights: 3.3145417672167587\n",
      "---------------------\n",
      "Iteration Number: 7606\n",
      "Loss: 11.651438107610028\n",
      "l2 norm of gradients: 0.05747445071025272\n",
      "l2 norm of weights: 3.3146080141757763\n",
      "---------------------\n",
      "Iteration Number: 7607\n",
      "Loss: 11.65107015334969\n",
      "l2 norm of gradients: 0.05746551109429848\n",
      "l2 norm of weights: 3.314674263375568\n",
      "---------------------\n",
      "Iteration Number: 7608\n",
      "Loss: 11.65070235174867\n",
      "l2 norm of gradients: 0.05745657539279418\n",
      "l2 norm of weights: 3.3147405148076556\n",
      "---------------------\n",
      "Iteration Number: 7609\n",
      "Loss: 11.650334702715533\n",
      "l2 norm of gradients: 0.0574476436022101\n",
      "l2 norm of weights: 3.3148067684635687\n",
      "---------------------\n",
      "Iteration Number: 7610\n",
      "Loss: 11.649967206158989\n",
      "l2 norm of gradients: 0.05743871571902085\n",
      "l2 norm of weights: 3.3148730243348448\n",
      "---------------------\n",
      "Iteration Number: 7611\n",
      "Loss: 11.64959986198778\n",
      "l2 norm of gradients: 0.057429791739705256\n",
      "l2 norm of weights: 3.31493928241303\n",
      "---------------------\n",
      "Iteration Number: 7612\n",
      "Loss: 11.64923267011079\n",
      "l2 norm of gradients: 0.057420871660746324\n",
      "l2 norm of weights: 3.31500554268968\n",
      "---------------------\n",
      "Iteration Number: 7613\n",
      "Loss: 11.64886563043695\n",
      "l2 norm of gradients: 0.05741195547863135\n",
      "l2 norm of weights: 3.3150718051563564\n",
      "---------------------\n",
      "Iteration Number: 7614\n",
      "Loss: 11.64849874287528\n",
      "l2 norm of gradients: 0.0574030431898518\n",
      "l2 norm of weights: 3.315138069804631\n",
      "---------------------\n",
      "Iteration Number: 7615\n",
      "Loss: 11.648132007334905\n",
      "l2 norm of gradients: 0.05739413479090332\n",
      "l2 norm of weights: 3.315204336626082\n",
      "---------------------\n",
      "Iteration Number: 7616\n",
      "Loss: 11.647765423725032\n",
      "l2 norm of gradients: 0.05738523027828587\n",
      "l2 norm of weights: 3.3152706056122976\n",
      "---------------------\n",
      "Iteration Number: 7617\n",
      "Loss: 11.647398991954947\n",
      "l2 norm of gradients: 0.0573763296485035\n",
      "l2 norm of weights: 3.315336876754873\n",
      "---------------------\n",
      "Iteration Number: 7618\n",
      "Loss: 11.647032711934035\n",
      "l2 norm of gradients: 0.05736743289806454\n",
      "l2 norm of weights: 3.315403150045411\n",
      "---------------------\n",
      "Iteration Number: 7619\n",
      "Loss: 11.64666658357175\n",
      "l2 norm of gradients: 0.057358540023481445\n",
      "l2 norm of weights: 3.315469425475525\n",
      "---------------------\n",
      "Iteration Number: 7620\n",
      "Loss: 11.646300606777634\n",
      "l2 norm of gradients: 0.05734965102127093\n",
      "l2 norm of weights: 3.3155357030368338\n",
      "---------------------\n",
      "Iteration Number: 7621\n",
      "Loss: 11.645934781461342\n",
      "l2 norm of gradients: 0.057340765887953817\n",
      "l2 norm of weights: 3.315601982720966\n",
      "---------------------\n",
      "Iteration Number: 7622\n",
      "Loss: 11.645569107532577\n",
      "l2 norm of gradients: 0.05733188462005516\n",
      "l2 norm of weights: 3.315668264519558\n",
      "---------------------\n",
      "Iteration Number: 7623\n",
      "Loss: 11.645203584901154\n",
      "l2 norm of gradients: 0.05732300721410415\n",
      "l2 norm of weights: 3.3157345484242544\n",
      "---------------------\n",
      "Iteration Number: 7624\n",
      "Loss: 11.644838213476973\n",
      "l2 norm of gradients: 0.05731413366663424\n",
      "l2 norm of weights: 3.315800834426707\n",
      "---------------------\n",
      "Iteration Number: 7625\n",
      "Loss: 11.64447299317\n",
      "l2 norm of gradients: 0.05730526397418292\n",
      "l2 norm of weights: 3.3158671225185783\n",
      "---------------------\n",
      "Iteration Number: 7626\n",
      "Loss: 11.644107923890305\n",
      "l2 norm of gradients: 0.0572963981332919\n",
      "l2 norm of weights: 3.3159334126915354\n",
      "---------------------\n",
      "Iteration Number: 7627\n",
      "Loss: 11.643743005548032\n",
      "l2 norm of gradients: 0.05728753614050704\n",
      "l2 norm of weights: 3.3159997049372563\n",
      "---------------------\n",
      "Iteration Number: 7628\n",
      "Loss: 11.643378238053423\n",
      "l2 norm of gradients: 0.05727867799237835\n",
      "l2 norm of weights: 3.316065999247426\n",
      "---------------------\n",
      "Iteration Number: 7629\n",
      "Loss: 11.643013621316792\n",
      "l2 norm of gradients: 0.057269823685460026\n",
      "l2 norm of weights: 3.3161322956137376\n",
      "---------------------\n",
      "Iteration Number: 7630\n",
      "Loss: 11.642649155248549\n",
      "l2 norm of gradients: 0.05726097321631032\n",
      "l2 norm of weights: 3.3161985940278917\n",
      "---------------------\n",
      "Iteration Number: 7631\n",
      "Loss: 11.642284839759178\n",
      "l2 norm of gradients: 0.057252126581491654\n",
      "l2 norm of weights: 3.3162648944815993\n",
      "---------------------\n",
      "Iteration Number: 7632\n",
      "Loss: 11.641920674759264\n",
      "l2 norm of gradients: 0.05724328377757063\n",
      "l2 norm of weights: 3.3163311969665763\n",
      "---------------------\n",
      "Iteration Number: 7633\n",
      "Loss: 11.641556660159452\n",
      "l2 norm of gradients: 0.05723444480111796\n",
      "l2 norm of weights: 3.3163975014745493\n",
      "---------------------\n",
      "Iteration Number: 7634\n",
      "Loss: 11.641192795870497\n",
      "l2 norm of gradients: 0.05722560964870842\n",
      "l2 norm of weights: 3.316463807997251\n",
      "---------------------\n",
      "Iteration Number: 7635\n",
      "Loss: 11.640829081803224\n",
      "l2 norm of gradients: 0.05721677831692096\n",
      "l2 norm of weights: 3.316530116526424\n",
      "---------------------\n",
      "Iteration Number: 7636\n",
      "Loss: 11.640465517868554\n",
      "l2 norm of gradients: 0.05720795080233861\n",
      "l2 norm of weights: 3.3165964270538177\n",
      "---------------------\n",
      "Iteration Number: 7637\n",
      "Loss: 11.64010210397747\n",
      "l2 norm of gradients: 0.05719912710154853\n",
      "l2 norm of weights: 3.3166627395711896\n",
      "---------------------\n",
      "Iteration Number: 7638\n",
      "Loss: 11.639738840041058\n",
      "l2 norm of gradients: 0.05719030721114199\n",
      "l2 norm of weights: 3.316729054070306\n",
      "---------------------\n",
      "Iteration Number: 7639\n",
      "Loss: 11.639375725970492\n",
      "l2 norm of gradients: 0.05718149112771432\n",
      "l2 norm of weights: 3.3167953705429407\n",
      "---------------------\n",
      "Iteration Number: 7640\n",
      "Loss: 11.639012761677023\n",
      "l2 norm of gradients: 0.057172678847865\n",
      "l2 norm of weights: 3.3168616889808753\n",
      "---------------------\n",
      "Iteration Number: 7641\n",
      "Loss: 11.638649947071977\n",
      "l2 norm of gradients: 0.057163870368197534\n",
      "l2 norm of weights: 3.3169280093759004\n",
      "---------------------\n",
      "Iteration Number: 7642\n",
      "Loss: 11.638287282066765\n",
      "l2 norm of gradients: 0.05715506568531955\n",
      "l2 norm of weights: 3.3169943317198127\n",
      "---------------------\n",
      "Iteration Number: 7643\n",
      "Loss: 11.637924766572903\n",
      "l2 norm of gradients: 0.05714626479584279\n",
      "l2 norm of weights: 3.3170606560044194\n",
      "---------------------\n",
      "Iteration Number: 7644\n",
      "Loss: 11.637562400501972\n",
      "l2 norm of gradients: 0.057137467696383004\n",
      "l2 norm of weights: 3.3171269822215335\n",
      "---------------------\n",
      "Iteration Number: 7645\n",
      "Loss: 11.637200183765627\n",
      "l2 norm of gradients: 0.057128674383560056\n",
      "l2 norm of weights: 3.317193310362977\n",
      "---------------------\n",
      "Iteration Number: 7646\n",
      "Loss: 11.636838116275628\n",
      "l2 norm of gradients: 0.05711988485399784\n",
      "l2 norm of weights: 3.317259640420581\n",
      "---------------------\n",
      "Iteration Number: 7647\n",
      "Loss: 11.636476197943823\n",
      "l2 norm of gradients: 0.05711109910432433\n",
      "l2 norm of weights: 3.317325972386182\n",
      "---------------------\n",
      "Iteration Number: 7648\n",
      "Loss: 11.636114428682111\n",
      "l2 norm of gradients: 0.05710231713117158\n",
      "l2 norm of weights: 3.317392306251626\n",
      "---------------------\n",
      "Iteration Number: 7649\n",
      "Loss: 11.635752808402502\n",
      "l2 norm of gradients: 0.05709353893117567\n",
      "l2 norm of weights: 3.3174586420087673\n",
      "---------------------\n",
      "Iteration Number: 7650\n",
      "Loss: 11.63539133701708\n",
      "l2 norm of gradients: 0.05708476450097672\n",
      "l2 norm of weights: 3.317524979649468\n",
      "---------------------\n",
      "Iteration Number: 7651\n",
      "Loss: 11.635030014438007\n",
      "l2 norm of gradients: 0.05707599383721893\n",
      "l2 norm of weights: 3.3175913191655964\n",
      "---------------------\n",
      "Iteration Number: 7652\n",
      "Loss: 11.63466884057754\n",
      "l2 norm of gradients: 0.05706722693655051\n",
      "l2 norm of weights: 3.3176576605490316\n",
      "---------------------\n",
      "Iteration Number: 7653\n",
      "Loss: 11.634307815347999\n",
      "l2 norm of gradients: 0.057058463795623675\n",
      "l2 norm of weights: 3.3177240037916587\n",
      "---------------------\n",
      "Iteration Number: 7654\n",
      "Loss: 11.633946938661804\n",
      "l2 norm of gradients: 0.05704970441109475\n",
      "l2 norm of weights: 3.3177903488853717\n",
      "---------------------\n",
      "Iteration Number: 7655\n",
      "Loss: 11.633586210431444\n",
      "l2 norm of gradients: 0.057040948779624\n",
      "l2 norm of weights: 3.3178566958220705\n",
      "---------------------\n",
      "Iteration Number: 7656\n",
      "Loss: 11.633225630569523\n",
      "l2 norm of gradients: 0.05703219689787576\n",
      "l2 norm of weights: 3.3179230445936665\n",
      "---------------------\n",
      "Iteration Number: 7657\n",
      "Loss: 11.632865198988672\n",
      "l2 norm of gradients: 0.057023448762518365\n",
      "l2 norm of weights: 3.317989395192076\n",
      "---------------------\n",
      "Iteration Number: 7658\n",
      "Loss: 11.63250491560165\n",
      "l2 norm of gradients: 0.057014704370224156\n",
      "l2 norm of weights: 3.3180557476092236\n",
      "---------------------\n",
      "Iteration Number: 7659\n",
      "Loss: 11.63214478032128\n",
      "l2 norm of gradients: 0.05700596371766949\n",
      "l2 norm of weights: 3.3181221018370435\n",
      "---------------------\n",
      "Iteration Number: 7660\n",
      "Loss: 11.631784793060474\n",
      "l2 norm of gradients: 0.05699722680153472\n",
      "l2 norm of weights: 3.3181884578674756\n",
      "---------------------\n",
      "Iteration Number: 7661\n",
      "Loss: 11.63142495373221\n",
      "l2 norm of gradients: 0.05698849361850421\n",
      "l2 norm of weights: 3.3182548156924696\n",
      "---------------------\n",
      "Iteration Number: 7662\n",
      "Loss: 11.631065262249559\n",
      "l2 norm of gradients: 0.05697976416526627\n",
      "l2 norm of weights: 3.318321175303982\n",
      "---------------------\n",
      "Iteration Number: 7663\n",
      "Loss: 11.63070571852568\n",
      "l2 norm of gradients: 0.05697103843851323\n",
      "l2 norm of weights: 3.3183875366939772\n",
      "---------------------\n",
      "Iteration Number: 7664\n",
      "Loss: 11.630346322473804\n",
      "l2 norm of gradients: 0.056962316434941436\n",
      "l2 norm of weights: 3.3184538998544277\n",
      "---------------------\n",
      "Iteration Number: 7665\n",
      "Loss: 11.629987074007246\n",
      "l2 norm of gradients: 0.05695359815125118\n",
      "l2 norm of weights: 3.3185202647773138\n",
      "---------------------\n",
      "Iteration Number: 7666\n",
      "Loss: 11.629627973039394\n",
      "l2 norm of gradients: 0.0569448835841467\n",
      "l2 norm of weights: 3.3185866314546235\n",
      "---------------------\n",
      "Iteration Number: 7667\n",
      "Loss: 11.62926901948373\n",
      "l2 norm of gradients: 0.05693617273033622\n",
      "l2 norm of weights: 3.3186529998783527\n",
      "---------------------\n",
      "Iteration Number: 7668\n",
      "Loss: 11.628910213253812\n",
      "l2 norm of gradients: 0.05692746558653196\n",
      "l2 norm of weights: 3.318719370040506\n",
      "---------------------\n",
      "Iteration Number: 7669\n",
      "Loss: 11.628551554263288\n",
      "l2 norm of gradients: 0.05691876214945008\n",
      "l2 norm of weights: 3.318785741933094\n",
      "---------------------\n",
      "Iteration Number: 7670\n",
      "Loss: 11.62819304242586\n",
      "l2 norm of gradients: 0.056910062415810696\n",
      "l2 norm of weights: 3.318852115548137\n",
      "---------------------\n",
      "Iteration Number: 7671\n",
      "Loss: 11.627834677655336\n",
      "l2 norm of gradients: 0.056901366382337865\n",
      "l2 norm of weights: 3.318918490877662\n",
      "---------------------\n",
      "Iteration Number: 7672\n",
      "Loss: 11.627476459865601\n",
      "l2 norm of gradients: 0.05689267404575958\n",
      "l2 norm of weights: 3.3189848679137035\n",
      "---------------------\n",
      "Iteration Number: 7673\n",
      "Loss: 11.627118388970608\n",
      "l2 norm of gradients: 0.056883985402807834\n",
      "l2 norm of weights: 3.3190512466483053\n",
      "---------------------\n",
      "Iteration Number: 7674\n",
      "Loss: 11.626760464884418\n",
      "l2 norm of gradients: 0.05687530045021847\n",
      "l2 norm of weights: 3.3191176270735174\n",
      "---------------------\n",
      "Iteration Number: 7675\n",
      "Loss: 11.626402687521127\n",
      "l2 norm of gradients: 0.05686661918473133\n",
      "l2 norm of weights: 3.319184009181399\n",
      "---------------------\n",
      "Iteration Number: 7676\n",
      "Loss: 11.62604505679494\n",
      "l2 norm of gradients: 0.05685794160309017\n",
      "l2 norm of weights: 3.3192503929640154\n",
      "---------------------\n",
      "Iteration Number: 7677\n",
      "Loss: 11.625687572620155\n",
      "l2 norm of gradients: 0.056849267702042655\n",
      "l2 norm of weights: 3.319316778413441\n",
      "---------------------\n",
      "Iteration Number: 7678\n",
      "Loss: 11.625330234911122\n",
      "l2 norm of gradients: 0.05684059747834039\n",
      "l2 norm of weights: 3.319383165521758\n",
      "---------------------\n",
      "Iteration Number: 7679\n",
      "Loss: 11.624973043582289\n",
      "l2 norm of gradients: 0.05683193092873886\n",
      "l2 norm of weights: 3.319449554281056\n",
      "---------------------\n",
      "Iteration Number: 7680\n",
      "Loss: 11.624615998548176\n",
      "l2 norm of gradients: 0.05682326804999748\n",
      "l2 norm of weights: 3.3195159446834306\n",
      "---------------------\n",
      "Iteration Number: 7681\n",
      "Loss: 11.624259099723375\n",
      "l2 norm of gradients: 0.05681460883887957\n",
      "l2 norm of weights: 3.319582336720989\n",
      "---------------------\n",
      "Iteration Number: 7682\n",
      "Loss: 11.623902347022579\n",
      "l2 norm of gradients: 0.056805953292152385\n",
      "l2 norm of weights: 3.319648730385843\n",
      "---------------------\n",
      "Iteration Number: 7683\n",
      "Loss: 11.62354574036054\n",
      "l2 norm of gradients: 0.05679730140658702\n",
      "l2 norm of weights: 3.3197151256701125\n",
      "---------------------\n",
      "Iteration Number: 7684\n",
      "Loss: 11.62318927965211\n",
      "l2 norm of gradients: 0.056788653178958495\n",
      "l2 norm of weights: 3.319781522565927\n",
      "---------------------\n",
      "Iteration Number: 7685\n",
      "Loss: 11.622832964812186\n",
      "l2 norm of gradients: 0.05678000860604568\n",
      "l2 norm of weights: 3.3198479210654215\n",
      "---------------------\n",
      "Iteration Number: 7686\n",
      "Loss: 11.62247679575579\n",
      "l2 norm of gradients: 0.05677136768463138\n",
      "l2 norm of weights: 3.31991432116074\n",
      "---------------------\n",
      "Iteration Number: 7687\n",
      "Loss: 11.622120772397977\n",
      "l2 norm of gradients: 0.05676273041150226\n",
      "l2 norm of weights: 3.3199807228440346\n",
      "---------------------\n",
      "Iteration Number: 7688\n",
      "Loss: 11.621764894653928\n",
      "l2 norm of gradients: 0.05675409678344883\n",
      "l2 norm of weights: 3.320047126107463\n",
      "---------------------\n",
      "Iteration Number: 7689\n",
      "Loss: 11.621409162438852\n",
      "l2 norm of gradients: 0.05674546679726552\n",
      "l2 norm of weights: 3.3201135309431926\n",
      "---------------------\n",
      "Iteration Number: 7690\n",
      "Loss: 11.621053575668075\n",
      "l2 norm of gradients: 0.056736840449750546\n",
      "l2 norm of weights: 3.3201799373433976\n",
      "---------------------\n",
      "Iteration Number: 7691\n",
      "Loss: 11.620698134256983\n",
      "l2 norm of gradients: 0.056728217737706074\n",
      "l2 norm of weights: 3.3202463453002604\n",
      "---------------------\n",
      "Iteration Number: 7692\n",
      "Loss: 11.62034283812106\n",
      "l2 norm of gradients: 0.056719598657938075\n",
      "l2 norm of weights: 3.320312754805971\n",
      "---------------------\n",
      "Iteration Number: 7693\n",
      "Loss: 11.619987687175833\n",
      "l2 norm of gradients: 0.05671098320725639\n",
      "l2 norm of weights: 3.320379165852726\n",
      "---------------------\n",
      "Iteration Number: 7694\n",
      "Loss: 11.619632681336952\n",
      "l2 norm of gradients: 0.056702371382474664\n",
      "l2 norm of weights: 3.320445578432732\n",
      "---------------------\n",
      "Iteration Number: 7695\n",
      "Loss: 11.619277820520104\n",
      "l2 norm of gradients: 0.056693763180410445\n",
      "l2 norm of weights: 3.3205119925382003\n",
      "---------------------\n",
      "Iteration Number: 7696\n",
      "Loss: 11.61892310464108\n",
      "l2 norm of gradients: 0.05668515859788508\n",
      "l2 norm of weights: 3.3205784081613516\n",
      "---------------------\n",
      "Iteration Number: 7697\n",
      "Loss: 11.618568533615752\n",
      "l2 norm of gradients: 0.0566765576317238\n",
      "l2 norm of weights: 3.3206448252944147\n",
      "---------------------\n",
      "Iteration Number: 7698\n",
      "Loss: 11.618214107360036\n",
      "l2 norm of gradients: 0.056667960278755564\n",
      "l2 norm of weights: 3.3207112439296247\n",
      "---------------------\n",
      "Iteration Number: 7699\n",
      "Loss: 11.617859825789967\n",
      "l2 norm of gradients: 0.05665936653581324\n",
      "l2 norm of weights: 3.320777664059225\n",
      "---------------------\n",
      "Iteration Number: 7700\n",
      "Loss: 11.61750568882164\n",
      "l2 norm of gradients: 0.056650776399733496\n",
      "l2 norm of weights: 3.3208440856754664\n",
      "---------------------\n",
      "Iteration Number: 7701\n",
      "Loss: 11.617151696371216\n",
      "l2 norm of gradients: 0.056642189867356814\n",
      "l2 norm of weights: 3.3209105087706074\n",
      "---------------------\n",
      "Iteration Number: 7702\n",
      "Loss: 11.61679784835496\n",
      "l2 norm of gradients: 0.056633606935527485\n",
      "l2 norm of weights: 3.3209769333369143\n",
      "---------------------\n",
      "Iteration Number: 7703\n",
      "Loss: 11.616444144689174\n",
      "l2 norm of gradients: 0.05662502760109359\n",
      "l2 norm of weights: 3.3210433593666617\n",
      "---------------------\n",
      "Iteration Number: 7704\n",
      "Loss: 11.6160905852903\n",
      "l2 norm of gradients: 0.05661645186090705\n",
      "l2 norm of weights: 3.321109786852129\n",
      "---------------------\n",
      "Iteration Number: 7705\n",
      "Loss: 11.615737170074786\n",
      "l2 norm of gradients: 0.05660787971182355\n",
      "l2 norm of weights: 3.321176215785607\n",
      "---------------------\n",
      "Iteration Number: 7706\n",
      "Loss: 11.615383898959207\n",
      "l2 norm of gradients: 0.056599311150702554\n",
      "l2 norm of weights: 3.321242646159391\n",
      "---------------------\n",
      "Iteration Number: 7707\n",
      "Loss: 11.615030771860207\n",
      "l2 norm of gradients: 0.056590746174407396\n",
      "l2 norm of weights: 3.3213090779657857\n",
      "---------------------\n",
      "Iteration Number: 7708\n",
      "Loss: 11.614677788694484\n",
      "l2 norm of gradients: 0.056582184779805075\n",
      "l2 norm of weights: 3.3213755111971026\n",
      "---------------------\n",
      "Iteration Number: 7709\n",
      "Loss: 11.614324949378828\n",
      "l2 norm of gradients: 0.05657362696376649\n",
      "l2 norm of weights: 3.3214419458456605\n",
      "---------------------\n",
      "Iteration Number: 7710\n",
      "Loss: 11.61397225383012\n",
      "l2 norm of gradients: 0.056565072723166215\n",
      "l2 norm of weights: 3.3215083819037874\n",
      "---------------------\n",
      "Iteration Number: 7711\n",
      "Loss: 11.613619701965288\n",
      "l2 norm of gradients: 0.05655652205488263\n",
      "l2 norm of weights: 3.321574819363816\n",
      "---------------------\n",
      "Iteration Number: 7712\n",
      "Loss: 11.613267293701353\n",
      "l2 norm of gradients: 0.056547974955797935\n",
      "l2 norm of weights: 3.3216412582180888\n",
      "---------------------\n",
      "Iteration Number: 7713\n",
      "Loss: 11.61291502895542\n",
      "l2 norm of gradients: 0.05653943142279798\n",
      "l2 norm of weights: 3.3217076984589555\n",
      "---------------------\n",
      "Iteration Number: 7714\n",
      "Loss: 11.61256290764466\n",
      "l2 norm of gradients: 0.05653089145277247\n",
      "l2 norm of weights: 3.321774140078773\n",
      "---------------------\n",
      "Iteration Number: 7715\n",
      "Loss: 11.612210929686317\n",
      "l2 norm of gradients: 0.05652235504261485\n",
      "l2 norm of weights: 3.321840583069905\n",
      "---------------------\n",
      "Iteration Number: 7716\n",
      "Loss: 11.61185909499771\n",
      "l2 norm of gradients: 0.05651382218922227\n",
      "l2 norm of weights: 3.321907027424724\n",
      "---------------------\n",
      "Iteration Number: 7717\n",
      "Loss: 11.611507403496255\n",
      "l2 norm of gradients: 0.05650529288949564\n",
      "l2 norm of weights: 3.32197347313561\n",
      "---------------------\n",
      "Iteration Number: 7718\n",
      "Loss: 11.611155855099419\n",
      "l2 norm of gradients: 0.05649676714033965\n",
      "l2 norm of weights: 3.322039920194949\n",
      "---------------------\n",
      "Iteration Number: 7719\n",
      "Loss: 11.61080444972476\n",
      "l2 norm of gradients: 0.05648824493866266\n",
      "l2 norm of weights: 3.3221063685951355\n",
      "---------------------\n",
      "Iteration Number: 7720\n",
      "Loss: 11.6104531872899\n",
      "l2 norm of gradients: 0.05647972628137681\n",
      "l2 norm of weights: 3.3221728183285717\n",
      "---------------------\n",
      "Iteration Number: 7721\n",
      "Loss: 11.610102067712553\n",
      "l2 norm of gradients: 0.05647121116539794\n",
      "l2 norm of weights: 3.322239269387668\n",
      "---------------------\n",
      "Iteration Number: 7722\n",
      "Loss: 11.60975109091048\n",
      "l2 norm of gradients: 0.05646269958764566\n",
      "l2 norm of weights: 3.322305721764839\n",
      "---------------------\n",
      "Iteration Number: 7723\n",
      "Loss: 11.609400256801548\n",
      "l2 norm of gradients: 0.05645419154504318\n",
      "l2 norm of weights: 3.3223721754525113\n",
      "---------------------\n",
      "Iteration Number: 7724\n",
      "Loss: 11.60904956530369\n",
      "l2 norm of gradients: 0.05644568703451757\n",
      "l2 norm of weights: 3.322438630443116\n",
      "---------------------\n",
      "Iteration Number: 7725\n",
      "Loss: 11.60869901633491\n",
      "l2 norm of gradients: 0.05643718605299953\n",
      "l2 norm of weights: 3.3225050867290915\n",
      "---------------------\n",
      "Iteration Number: 7726\n",
      "Loss: 11.60834860981329\n",
      "l2 norm of gradients: 0.05642868859742348\n",
      "l2 norm of weights: 3.322571544302886\n",
      "---------------------\n",
      "Iteration Number: 7727\n",
      "Loss: 11.607998345656975\n",
      "l2 norm of gradients: 0.05642019466472751\n",
      "l2 norm of weights: 3.3226380031569525\n",
      "---------------------\n",
      "Iteration Number: 7728\n",
      "Loss: 11.607648223784212\n",
      "l2 norm of gradients: 0.05641170425185345\n",
      "l2 norm of weights: 3.322704463283753\n",
      "---------------------\n",
      "Iteration Number: 7729\n",
      "Loss: 11.607298244113291\n",
      "l2 norm of gradients: 0.056403217355746806\n",
      "l2 norm of weights: 3.322770924675757\n",
      "---------------------\n",
      "Iteration Number: 7730\n",
      "Loss: 11.606948406562605\n",
      "l2 norm of gradients: 0.056394733973356766\n",
      "l2 norm of weights: 3.322837387325441\n",
      "---------------------\n",
      "Iteration Number: 7731\n",
      "Loss: 11.606598711050598\n",
      "l2 norm of gradients: 0.05638625410163621\n",
      "l2 norm of weights: 3.3229038512252873\n",
      "---------------------\n",
      "Iteration Number: 7732\n",
      "Loss: 11.606249157495808\n",
      "l2 norm of gradients: 0.05637777773754167\n",
      "l2 norm of weights: 3.322970316367789\n",
      "---------------------\n",
      "Iteration Number: 7733\n",
      "Loss: 11.605899745816837\n",
      "l2 norm of gradients: 0.0563693048780334\n",
      "l2 norm of weights: 3.3230367827454446\n",
      "---------------------\n",
      "Iteration Number: 7734\n",
      "Loss: 11.605550475932356\n",
      "l2 norm of gradients: 0.056360835520075286\n",
      "l2 norm of weights: 3.3231032503507594\n",
      "---------------------\n",
      "Iteration Number: 7735\n",
      "Loss: 11.605201347761131\n",
      "l2 norm of gradients: 0.056352369660634866\n",
      "l2 norm of weights: 3.3231697191762475\n",
      "---------------------\n",
      "Iteration Number: 7736\n",
      "Loss: 11.604852361221988\n",
      "l2 norm of gradients: 0.05634390729668339\n",
      "l2 norm of weights: 3.3232361892144295\n",
      "---------------------\n",
      "Iteration Number: 7737\n",
      "Loss: 11.604503516233812\n",
      "l2 norm of gradients: 0.05633544842519572\n",
      "l2 norm of weights: 3.3233026604578333\n",
      "---------------------\n",
      "Iteration Number: 7738\n",
      "Loss: 11.604154812715597\n",
      "l2 norm of gradients: 0.0563269930431504\n",
      "l2 norm of weights: 3.323369132898996\n",
      "---------------------\n",
      "Iteration Number: 7739\n",
      "Loss: 11.603806250586377\n",
      "l2 norm of gradients: 0.05631854114752961\n",
      "l2 norm of weights: 3.3234356065304587\n",
      "---------------------\n",
      "Iteration Number: 7740\n",
      "Loss: 11.603457829765283\n",
      "l2 norm of gradients: 0.05631009273531916\n",
      "l2 norm of weights: 3.323502081344773\n",
      "---------------------\n",
      "Iteration Number: 7741\n",
      "Loss: 11.603109550171512\n",
      "l2 norm of gradients: 0.056301647803508506\n",
      "l2 norm of weights: 3.3235685573344966\n",
      "---------------------\n",
      "Iteration Number: 7742\n",
      "Loss: 11.602761411724332\n",
      "l2 norm of gradients: 0.05629320634909075\n",
      "l2 norm of weights: 3.3236350344921943\n",
      "---------------------\n",
      "Iteration Number: 7743\n",
      "Loss: 11.602413414343086\n",
      "l2 norm of gradients: 0.05628476836906262\n",
      "l2 norm of weights: 3.323701512810438\n",
      "---------------------\n",
      "Iteration Number: 7744\n",
      "Loss: 11.602065557947197\n",
      "l2 norm of gradients: 0.056276333860424466\n",
      "l2 norm of weights: 3.3237679922818084\n",
      "---------------------\n",
      "Iteration Number: 7745\n",
      "Loss: 11.601717842456148\n",
      "l2 norm of gradients: 0.05626790282018024\n",
      "l2 norm of weights: 3.3238344728988922\n",
      "---------------------\n",
      "Iteration Number: 7746\n",
      "Loss: 11.60137026778951\n",
      "l2 norm of gradients: 0.05625947524533758\n",
      "l2 norm of weights: 3.3239009546542837\n",
      "---------------------\n",
      "Iteration Number: 7747\n",
      "Loss: 11.601022833866907\n",
      "l2 norm of gradients: 0.05625105113290766\n",
      "l2 norm of weights: 3.3239674375405843\n",
      "---------------------\n",
      "Iteration Number: 7748\n",
      "Loss: 11.600675540608062\n",
      "l2 norm of gradients: 0.056242630479905284\n",
      "l2 norm of weights: 3.324033921550404\n",
      "---------------------\n",
      "Iteration Number: 7749\n",
      "Loss: 11.600328387932754\n",
      "l2 norm of gradients: 0.05623421328334891\n",
      "l2 norm of weights: 3.324100406676358\n",
      "---------------------\n",
      "Iteration Number: 7750\n",
      "Loss: 11.599981375760839\n",
      "l2 norm of gradients: 0.05622579954026051\n",
      "l2 norm of weights: 3.3241668929110704\n",
      "---------------------\n",
      "Iteration Number: 7751\n",
      "Loss: 11.599634504012245\n",
      "l2 norm of gradients: 0.05621738924766569\n",
      "l2 norm of weights: 3.324233380247173\n",
      "---------------------\n",
      "Iteration Number: 7752\n",
      "Loss: 11.59928777260697\n",
      "l2 norm of gradients: 0.05620898240259373\n",
      "l2 norm of weights: 3.3242998686773024\n",
      "---------------------\n",
      "Iteration Number: 7753\n",
      "Loss: 11.598941181465095\n",
      "l2 norm of gradients: 0.056200579002077346\n",
      "l2 norm of weights: 3.3243663581941054\n",
      "---------------------\n",
      "Iteration Number: 7754\n",
      "Loss: 11.59859473050677\n",
      "l2 norm of gradients: 0.05619217904315296\n",
      "l2 norm of weights: 3.324432848790234\n",
      "---------------------\n",
      "Iteration Number: 7755\n",
      "Loss: 11.5982484196522\n",
      "l2 norm of gradients: 0.056183782522860506\n",
      "l2 norm of weights: 3.3244993404583485\n",
      "---------------------\n",
      "Iteration Number: 7756\n",
      "Loss: 11.597902248821681\n",
      "l2 norm of gradients: 0.056175389438243524\n",
      "l2 norm of weights: 3.324565833191116\n",
      "---------------------\n",
      "Iteration Number: 7757\n",
      "Loss: 11.597556217935587\n",
      "l2 norm of gradients: 0.05616699978634909\n",
      "l2 norm of weights: 3.3246323269812112\n",
      "---------------------\n",
      "Iteration Number: 7758\n",
      "Loss: 11.597210326914336\n",
      "l2 norm of gradients: 0.0561586135642279\n",
      "l2 norm of weights: 3.3246988218213165\n",
      "---------------------\n",
      "Iteration Number: 7759\n",
      "Loss: 11.596864575678456\n",
      "l2 norm of gradients: 0.056150230768934134\n",
      "l2 norm of weights: 3.324765317704119\n",
      "---------------------\n",
      "Iteration Number: 7760\n",
      "Loss: 11.596518964148505\n",
      "l2 norm of gradients: 0.05614185139752564\n",
      "l2 norm of weights: 3.324831814622318\n",
      "---------------------\n",
      "Iteration Number: 7761\n",
      "Loss: 11.596173492245144\n",
      "l2 norm of gradients: 0.056133475447063694\n",
      "l2 norm of weights: 3.3248983125686147\n",
      "---------------------\n",
      "Iteration Number: 7762\n",
      "Loss: 11.595828159889104\n",
      "l2 norm of gradients: 0.056125102914613215\n",
      "l2 norm of weights: 3.3249648115357204\n",
      "---------------------\n",
      "Iteration Number: 7763\n",
      "Loss: 11.595482967001175\n",
      "l2 norm of gradients: 0.05611673379724264\n",
      "l2 norm of weights: 3.3250313115163532\n",
      "---------------------\n",
      "Iteration Number: 7764\n",
      "Loss: 11.59513791350222\n",
      "l2 norm of gradients: 0.05610836809202393\n",
      "l2 norm of weights: 3.3250978125032375\n",
      "---------------------\n",
      "Iteration Number: 7765\n",
      "Loss: 11.594792999313183\n",
      "l2 norm of gradients: 0.05610000579603258\n",
      "l2 norm of weights: 3.325164314489107\n",
      "---------------------\n",
      "Iteration Number: 7766\n",
      "Loss: 11.594448224355068\n",
      "l2 norm of gradients: 0.05609164690634762\n",
      "l2 norm of weights: 3.325230817466701\n",
      "---------------------\n",
      "Iteration Number: 7767\n",
      "Loss: 11.594103588548949\n",
      "l2 norm of gradients: 0.05608329142005161\n",
      "l2 norm of weights: 3.325297321428765\n",
      "---------------------\n",
      "Iteration Number: 7768\n",
      "Loss: 11.593759091815993\n",
      "l2 norm of gradients: 0.0560749393342307\n",
      "l2 norm of weights: 3.325363826368054\n",
      "---------------------\n",
      "Iteration Number: 7769\n",
      "Loss: 11.593414734077422\n",
      "l2 norm of gradients: 0.05606659064597443\n",
      "l2 norm of weights: 3.3254303322773286\n",
      "---------------------\n",
      "Iteration Number: 7770\n",
      "Loss: 11.593070515254519\n",
      "l2 norm of gradients: 0.056058245352375954\n",
      "l2 norm of weights: 3.325496839149358\n",
      "---------------------\n",
      "Iteration Number: 7771\n",
      "Loss: 11.592726435268657\n",
      "l2 norm of gradients: 0.056049903450531915\n",
      "l2 norm of weights: 3.3255633469769164\n",
      "---------------------\n",
      "Iteration Number: 7772\n",
      "Loss: 11.592382494041265\n",
      "l2 norm of gradients: 0.056041564937542414\n",
      "l2 norm of weights: 3.3256298557527866\n",
      "---------------------\n",
      "Iteration Number: 7773\n",
      "Loss: 11.592038691493858\n",
      "l2 norm of gradients: 0.05603322981051113\n",
      "l2 norm of weights: 3.3256963654697596\n",
      "---------------------\n",
      "Iteration Number: 7774\n",
      "Loss: 11.591695027548013\n",
      "l2 norm of gradients: 0.05602489806654517\n",
      "l2 norm of weights: 3.3257628761206313\n",
      "---------------------\n",
      "Iteration Number: 7775\n",
      "Loss: 11.591351502125379\n",
      "l2 norm of gradients: 0.056016569702755176\n",
      "l2 norm of weights: 3.3258293876982057\n",
      "---------------------\n",
      "Iteration Number: 7776\n",
      "Loss: 11.591008115147662\n",
      "l2 norm of gradients: 0.05600824471625527\n",
      "l2 norm of weights: 3.325895900195294\n",
      "---------------------\n",
      "Iteration Number: 7777\n",
      "Loss: 11.590664866536656\n",
      "l2 norm of gradients: 0.0559999231041631\n",
      "l2 norm of weights: 3.325962413604715\n",
      "---------------------\n",
      "Iteration Number: 7778\n",
      "Loss: 11.59032175621424\n",
      "l2 norm of gradients: 0.05599160486359971\n",
      "l2 norm of weights: 3.3260289279192943\n",
      "---------------------\n",
      "Iteration Number: 7779\n",
      "Loss: 11.589978784102325\n",
      "l2 norm of gradients: 0.05598328999168965\n",
      "l2 norm of weights: 3.326095443131864\n",
      "---------------------\n",
      "Iteration Number: 7780\n",
      "Loss: 11.589635950122902\n",
      "l2 norm of gradients: 0.05597497848556098\n",
      "l2 norm of weights: 3.3261619592352636\n",
      "---------------------\n",
      "Iteration Number: 7781\n",
      "Loss: 11.589293254198063\n",
      "l2 norm of gradients: 0.05596667034234523\n",
      "l2 norm of weights: 3.32622847622234\n",
      "---------------------\n",
      "Iteration Number: 7782\n",
      "Loss: 11.588950696249935\n",
      "l2 norm of gradients: 0.055958365559177325\n",
      "l2 norm of weights: 3.326294994085948\n",
      "---------------------\n",
      "Iteration Number: 7783\n",
      "Loss: 11.588608276200732\n",
      "l2 norm of gradients: 0.0559500641331957\n",
      "l2 norm of weights: 3.3263615128189477\n",
      "---------------------\n",
      "Iteration Number: 7784\n",
      "Loss: 11.588265993972726\n",
      "l2 norm of gradients: 0.05594176606154227\n",
      "l2 norm of weights: 3.3264280324142064\n",
      "---------------------\n",
      "Iteration Number: 7785\n",
      "Loss: 11.587923849488284\n",
      "l2 norm of gradients: 0.05593347134136234\n",
      "l2 norm of weights: 3.326494552864601\n",
      "---------------------\n",
      "Iteration Number: 7786\n",
      "Loss: 11.587581842669808\n",
      "l2 norm of gradients: 0.05592517996980469\n",
      "l2 norm of weights: 3.326561074163013\n",
      "---------------------\n",
      "Iteration Number: 7787\n",
      "Loss: 11.58723997343978\n",
      "l2 norm of gradients: 0.05591689194402159\n",
      "l2 norm of weights: 3.3266275963023313\n",
      "---------------------\n",
      "Iteration Number: 7788\n",
      "Loss: 11.58689824172078\n",
      "l2 norm of gradients: 0.05590860726116864\n",
      "l2 norm of weights: 3.3266941192754524\n",
      "---------------------\n",
      "Iteration Number: 7789\n",
      "Loss: 11.586556647435422\n",
      "l2 norm of gradients: 0.05590032591840502\n",
      "l2 norm of weights: 3.3267606430752803\n",
      "---------------------\n",
      "Iteration Number: 7790\n",
      "Loss: 11.586215190506405\n",
      "l2 norm of gradients: 0.05589204791289319\n",
      "l2 norm of weights: 3.3268271676947254\n",
      "---------------------\n",
      "Iteration Number: 7791\n",
      "Loss: 11.5858738708565\n",
      "l2 norm of gradients: 0.055883773241799106\n",
      "l2 norm of weights: 3.3268936931267046\n",
      "---------------------\n",
      "Iteration Number: 7792\n",
      "Loss: 11.58553268840853\n",
      "l2 norm of gradients: 0.055875501902292186\n",
      "l2 norm of weights: 3.3269602193641425\n",
      "---------------------\n",
      "Iteration Number: 7793\n",
      "Loss: 11.5851916430854\n",
      "l2 norm of gradients: 0.05586723389154523\n",
      "l2 norm of weights: 3.3270267463999708\n",
      "---------------------\n",
      "Iteration Number: 7794\n",
      "Loss: 11.584850734810097\n",
      "l2 norm of gradients: 0.055858969206734436\n",
      "l2 norm of weights: 3.3270932742271286\n",
      "---------------------\n",
      "Iteration Number: 7795\n",
      "Loss: 11.584509963505653\n",
      "l2 norm of gradients: 0.0558507078450394\n",
      "l2 norm of weights: 3.3271598028385614\n",
      "---------------------\n",
      "Iteration Number: 7796\n",
      "Loss: 11.584169329095168\n",
      "l2 norm of gradients: 0.05584244980364319\n",
      "l2 norm of weights: 3.3272263322272213\n",
      "---------------------\n",
      "Iteration Number: 7797\n",
      "Loss: 11.583828831501835\n",
      "l2 norm of gradients: 0.05583419507973219\n",
      "l2 norm of weights: 3.327292862386069\n",
      "---------------------\n",
      "Iteration Number: 7798\n",
      "Loss: 11.583488470648902\n",
      "l2 norm of gradients: 0.05582594367049626\n",
      "l2 norm of weights: 3.32735939330807\n",
      "---------------------\n",
      "Iteration Number: 7799\n",
      "Loss: 11.583148246459677\n",
      "l2 norm of gradients: 0.0558176955731286\n",
      "l2 norm of weights: 3.3274259249861986\n",
      "---------------------\n",
      "Iteration Number: 7800\n",
      "Loss: 11.582808158857544\n",
      "l2 norm of gradients: 0.05580945078482579\n",
      "l2 norm of weights: 3.3274924574134346\n",
      "---------------------\n",
      "Iteration Number: 7801\n",
      "Loss: 11.58246820776597\n",
      "l2 norm of gradients: 0.05580120930278788\n",
      "l2 norm of weights: 3.3275589905827676\n",
      "---------------------\n",
      "Iteration Number: 7802\n",
      "Loss: 11.582128393108453\n",
      "l2 norm of gradients: 0.05579297112421821\n",
      "l2 norm of weights: 3.3276255244871904\n",
      "---------------------\n",
      "Iteration Number: 7803\n",
      "Loss: 11.581788714808601\n",
      "l2 norm of gradients: 0.05578473624632351\n",
      "l2 norm of weights: 3.327692059119705\n",
      "---------------------\n",
      "Iteration Number: 7804\n",
      "Loss: 11.581449172790064\n",
      "l2 norm of gradients: 0.05577650466631393\n",
      "l2 norm of weights: 3.3277585944733206\n",
      "---------------------\n",
      "Iteration Number: 7805\n",
      "Loss: 11.581109766976565\n",
      "l2 norm of gradients: 0.05576827638140294\n",
      "l2 norm of weights: 3.327825130541052\n",
      "---------------------\n",
      "Iteration Number: 7806\n",
      "Loss: 11.580770497291905\n",
      "l2 norm of gradients: 0.05576005138880742\n",
      "l2 norm of weights: 3.3278916673159222\n",
      "---------------------\n",
      "Iteration Number: 7807\n",
      "Loss: 11.580431363659928\n",
      "l2 norm of gradients: 0.055751829685747545\n",
      "l2 norm of weights: 3.3279582047909604\n",
      "---------------------\n",
      "Iteration Number: 7808\n",
      "Loss: 11.580092366004585\n",
      "l2 norm of gradients: 0.055743611269446894\n",
      "l2 norm of weights: 3.3280247429592027\n",
      "---------------------\n",
      "Iteration Number: 7809\n",
      "Loss: 11.579753504249856\n",
      "l2 norm of gradients: 0.055735396137132416\n",
      "l2 norm of weights: 3.3280912818136934\n",
      "---------------------\n",
      "Iteration Number: 7810\n",
      "Loss: 11.57941477831981\n",
      "l2 norm of gradients: 0.055727184286034355\n",
      "l2 norm of weights: 3.3281578213474816\n",
      "---------------------\n",
      "Iteration Number: 7811\n",
      "Loss: 11.579076188138565\n",
      "l2 norm of gradients: 0.055718975713386305\n",
      "l2 norm of weights: 3.3282243615536244\n",
      "---------------------\n",
      "Iteration Number: 7812\n",
      "Loss: 11.57873773363035\n",
      "l2 norm of gradients: 0.055710770416425255\n",
      "l2 norm of weights: 3.3282909024251874\n",
      "---------------------\n",
      "Iteration Number: 7813\n",
      "Loss: 11.578399414719398\n",
      "l2 norm of gradients: 0.055702568392391474\n",
      "l2 norm of weights: 3.3283574439552397\n",
      "---------------------\n",
      "Iteration Number: 7814\n",
      "Loss: 11.578061231330066\n",
      "l2 norm of gradients: 0.055694369638528564\n",
      "l2 norm of weights: 3.3284239861368605\n",
      "---------------------\n",
      "Iteration Number: 7815\n",
      "Loss: 11.577723183386732\n",
      "l2 norm of gradients: 0.05568617415208352\n",
      "l2 norm of weights: 3.3284905289631346\n",
      "---------------------\n",
      "Iteration Number: 7816\n",
      "Loss: 11.577385270813886\n",
      "l2 norm of gradients: 0.05567798193030652\n",
      "l2 norm of weights: 3.328557072427153\n",
      "---------------------\n",
      "Iteration Number: 7817\n",
      "Loss: 11.577047493536051\n",
      "l2 norm of gradients: 0.055669792970451236\n",
      "l2 norm of weights: 3.3286236165220147\n",
      "---------------------\n",
      "Iteration Number: 7818\n",
      "Loss: 11.576709851477828\n",
      "l2 norm of gradients: 0.05566160726977453\n",
      "l2 norm of weights: 3.3286901612408246\n",
      "---------------------\n",
      "Iteration Number: 7819\n",
      "Loss: 11.576372344563875\n",
      "l2 norm of gradients: 0.05565342482553661\n",
      "l2 norm of weights: 3.328756706576696\n",
      "---------------------\n",
      "Iteration Number: 7820\n",
      "Loss: 11.576034972718947\n",
      "l2 norm of gradients: 0.05564524563500102\n",
      "l2 norm of weights: 3.328823252522748\n",
      "---------------------\n",
      "Iteration Number: 7821\n",
      "Loss: 11.575697735867829\n",
      "l2 norm of gradients: 0.0556370696954346\n",
      "l2 norm of weights: 3.328889799072106\n",
      "---------------------\n",
      "Iteration Number: 7822\n",
      "Loss: 11.575360633935398\n",
      "l2 norm of gradients: 0.05562889700410739\n",
      "l2 norm of weights: 3.3289563462179035\n",
      "---------------------\n",
      "Iteration Number: 7823\n",
      "Loss: 11.575023666846576\n",
      "l2 norm of gradients: 0.05562072755829285\n",
      "l2 norm of weights: 3.32902289395328\n",
      "---------------------\n",
      "Iteration Number: 7824\n",
      "Loss: 11.57468683452638\n",
      "l2 norm of gradients: 0.05561256135526771\n",
      "l2 norm of weights: 3.329089442271383\n",
      "---------------------\n",
      "Iteration Number: 7825\n",
      "Loss: 11.574350136899856\n",
      "l2 norm of gradients: 0.055604398392311905\n",
      "l2 norm of weights: 3.3291559911653645\n",
      "---------------------\n",
      "Iteration Number: 7826\n",
      "Loss: 11.574013573892156\n",
      "l2 norm of gradients: 0.055596238666708736\n",
      "l2 norm of weights: 3.3292225406283857\n",
      "---------------------\n",
      "Iteration Number: 7827\n",
      "Loss: 11.57367714542847\n",
      "l2 norm of gradients: 0.05558808217574475\n",
      "l2 norm of weights: 3.3292890906536146\n",
      "---------------------\n",
      "Iteration Number: 7828\n",
      "Loss: 11.573340851434068\n",
      "l2 norm of gradients: 0.05557992891670982\n",
      "l2 norm of weights: 3.329355641234224\n",
      "---------------------\n",
      "Iteration Number: 7829\n",
      "Loss: 11.57300469183427\n",
      "l2 norm of gradients: 0.055571778886896954\n",
      "l2 norm of weights: 3.3294221923633947\n",
      "---------------------\n",
      "Iteration Number: 7830\n",
      "Loss: 11.572668666554478\n",
      "l2 norm of gradients: 0.0555636320836026\n",
      "l2 norm of weights: 3.329488744034315\n",
      "---------------------\n",
      "Iteration Number: 7831\n",
      "Loss: 11.572332775520165\n",
      "l2 norm of gradients: 0.055555488504126324\n",
      "l2 norm of weights: 3.329555296240179\n",
      "---------------------\n",
      "Iteration Number: 7832\n",
      "Loss: 11.571997018656846\n",
      "l2 norm of gradients: 0.05554734814577102\n",
      "l2 norm of weights: 3.3296218489741882\n",
      "---------------------\n",
      "Iteration Number: 7833\n",
      "Loss: 11.571661395890114\n",
      "l2 norm of gradients: 0.055539211005842824\n",
      "l2 norm of weights: 3.32968840222955\n",
      "---------------------\n",
      "Iteration Number: 7834\n",
      "Loss: 11.57132590714564\n",
      "l2 norm of gradients: 0.05553107708165112\n",
      "l2 norm of weights: 3.3297549559994803\n",
      "---------------------\n",
      "Iteration Number: 7835\n",
      "Loss: 11.570990552349137\n",
      "l2 norm of gradients: 0.05552294637050853\n",
      "l2 norm of weights: 3.329821510277199\n",
      "---------------------\n",
      "Iteration Number: 7836\n",
      "Loss: 11.570655331426401\n",
      "l2 norm of gradients: 0.05551481886973096\n",
      "l2 norm of weights: 3.3298880650559366\n",
      "---------------------\n",
      "Iteration Number: 7837\n",
      "Loss: 11.570320244303295\n",
      "l2 norm of gradients: 0.055506694576637504\n",
      "l2 norm of weights: 3.329954620328927\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 7838\n",
      "Loss: 11.569985290905722\n",
      "l2 norm of gradients: 0.055498573488550466\n",
      "l2 norm of weights: 3.3300211760894123\n",
      "---------------------\n",
      "Iteration Number: 7839\n",
      "Loss: 11.569650471159681\n",
      "l2 norm of gradients: 0.05549045560279547\n",
      "l2 norm of weights: 3.3300877323306417\n",
      "---------------------\n",
      "Iteration Number: 7840\n",
      "Loss: 11.569315784991222\n",
      "l2 norm of gradients: 0.055482340916701324\n",
      "l2 norm of weights: 3.3301542890458693\n",
      "---------------------\n",
      "Iteration Number: 7841\n",
      "Loss: 11.568981232326456\n",
      "l2 norm of gradients: 0.05547422942759999\n",
      "l2 norm of weights: 3.3302208462283587\n",
      "---------------------\n",
      "Iteration Number: 7842\n",
      "Loss: 11.568646813091565\n",
      "l2 norm of gradients: 0.05546612113282676\n",
      "l2 norm of weights: 3.3302874038713783\n",
      "---------------------\n",
      "Iteration Number: 7843\n",
      "Loss: 11.568312527212793\n",
      "l2 norm of gradients: 0.055458016029720066\n",
      "l2 norm of weights: 3.330353961968204\n",
      "---------------------\n",
      "Iteration Number: 7844\n",
      "Loss: 11.567978374616454\n",
      "l2 norm of gradients: 0.05544991411562155\n",
      "l2 norm of weights: 3.3304205205121176\n",
      "---------------------\n",
      "Iteration Number: 7845\n",
      "Loss: 11.567644355228921\n",
      "l2 norm of gradients: 0.05544181538787613\n",
      "l2 norm of weights: 3.330487079496409\n",
      "---------------------\n",
      "Iteration Number: 7846\n",
      "Loss: 11.567310468976633\n",
      "l2 norm of gradients: 0.055433719843831795\n",
      "l2 norm of weights: 3.330553638914374\n",
      "---------------------\n",
      "Iteration Number: 7847\n",
      "Loss: 11.566976715786097\n",
      "l2 norm of gradients: 0.05542562748083992\n",
      "l2 norm of weights: 3.3306201987593145\n",
      "---------------------\n",
      "Iteration Number: 7848\n",
      "Loss: 11.566643095583881\n",
      "l2 norm of gradients: 0.05541753829625489\n",
      "l2 norm of weights: 3.33068675902454\n",
      "---------------------\n",
      "Iteration Number: 7849\n",
      "Loss: 11.566309608296612\n",
      "l2 norm of gradients: 0.05540945228743436\n",
      "l2 norm of weights: 3.330753319703367\n",
      "---------------------\n",
      "Iteration Number: 7850\n",
      "Loss: 11.565976253850986\n",
      "l2 norm of gradients: 0.05540136945173921\n",
      "l2 norm of weights: 3.330819880789118\n",
      "---------------------\n",
      "Iteration Number: 7851\n",
      "Loss: 11.565643032173769\n",
      "l2 norm of gradients: 0.055393289786533406\n",
      "l2 norm of weights: 3.330886442275122\n",
      "---------------------\n",
      "Iteration Number: 7852\n",
      "Loss: 11.565309943191794\n",
      "l2 norm of gradients: 0.055385213289184165\n",
      "l2 norm of weights: 3.330953004154715\n",
      "---------------------\n",
      "Iteration Number: 7853\n",
      "Loss: 11.564976986831937\n",
      "l2 norm of gradients: 0.055377139957061866\n",
      "l2 norm of weights: 3.33101956642124\n",
      "---------------------\n",
      "Iteration Number: 7854\n",
      "Loss: 11.56464416302115\n",
      "l2 norm of gradients: 0.05536906978754004\n",
      "l2 norm of weights: 3.331086129068047\n",
      "---------------------\n",
      "Iteration Number: 7855\n",
      "Loss: 11.564311471686462\n",
      "l2 norm of gradients: 0.0553610027779954\n",
      "l2 norm of weights: 3.331152692088491\n",
      "---------------------\n",
      "Iteration Number: 7856\n",
      "Loss: 11.563978912754948\n",
      "l2 norm of gradients: 0.05535293892580778\n",
      "l2 norm of weights: 3.3312192554759354\n",
      "---------------------\n",
      "Iteration Number: 7857\n",
      "Loss: 11.563646486153743\n",
      "l2 norm of gradients: 0.05534487822836024\n",
      "l2 norm of weights: 3.3312858192237496\n",
      "---------------------\n",
      "Iteration Number: 7858\n",
      "Loss: 11.563314191810068\n",
      "l2 norm of gradients: 0.05533682068303893\n",
      "l2 norm of weights: 3.331352383325309\n",
      "---------------------\n",
      "Iteration Number: 7859\n",
      "Loss: 11.562982029651192\n",
      "l2 norm of gradients: 0.05532876628723324\n",
      "l2 norm of weights: 3.3314189477739973\n",
      "---------------------\n",
      "Iteration Number: 7860\n",
      "Loss: 11.562649999604442\n",
      "l2 norm of gradients: 0.05532071503833557\n",
      "l2 norm of weights: 3.3314855125632032\n",
      "---------------------\n",
      "Iteration Number: 7861\n",
      "Loss: 11.562318101597224\n",
      "l2 norm of gradients: 0.055312666933741594\n",
      "l2 norm of weights: 3.331552077686323\n",
      "---------------------\n",
      "Iteration Number: 7862\n",
      "Loss: 11.561986335556991\n",
      "l2 norm of gradients: 0.05530462197085\n",
      "l2 norm of weights: 3.3316186431367587\n",
      "---------------------\n",
      "Iteration Number: 7863\n",
      "Loss: 11.56165470141128\n",
      "l2 norm of gradients: 0.05529658014706275\n",
      "l2 norm of weights: 3.3316852089079205\n",
      "---------------------\n",
      "Iteration Number: 7864\n",
      "Loss: 11.561323199087665\n",
      "l2 norm of gradients: 0.05528854145978483\n",
      "l2 norm of weights: 3.331751774993223\n",
      "---------------------\n",
      "Iteration Number: 7865\n",
      "Loss: 11.560991828513806\n",
      "l2 norm of gradients: 0.055280505906424374\n",
      "l2 norm of weights: 3.33181834138609\n",
      "---------------------\n",
      "Iteration Number: 7866\n",
      "Loss: 11.560660589617422\n",
      "l2 norm of gradients: 0.05527247348439268\n",
      "l2 norm of weights: 3.3318849080799495\n",
      "---------------------\n",
      "Iteration Number: 7867\n",
      "Loss: 11.56032948232628\n",
      "l2 norm of gradients: 0.055264444191104115\n",
      "l2 norm of weights: 3.3319514750682373\n",
      "---------------------\n",
      "Iteration Number: 7868\n",
      "Loss: 11.559998506568217\n",
      "l2 norm of gradients: 0.05525641802397618\n",
      "l2 norm of weights: 3.3320180423443966\n",
      "---------------------\n",
      "Iteration Number: 7869\n",
      "Loss: 11.559667662271137\n",
      "l2 norm of gradients: 0.055248394980429494\n",
      "l2 norm of weights: 3.3320846099018753\n",
      "---------------------\n",
      "Iteration Number: 7870\n",
      "Loss: 11.559336949363013\n",
      "l2 norm of gradients: 0.05524037505788775\n",
      "l2 norm of weights: 3.3321511777341284\n",
      "---------------------\n",
      "Iteration Number: 7871\n",
      "Loss: 11.559006367771863\n",
      "l2 norm of gradients: 0.05523235825377784\n",
      "l2 norm of weights: 3.3322177458346194\n",
      "---------------------\n",
      "Iteration Number: 7872\n",
      "Loss: 11.558675917425788\n",
      "l2 norm of gradients: 0.05522434456552961\n",
      "l2 norm of weights: 3.3322843141968157\n",
      "---------------------\n",
      "Iteration Number: 7873\n",
      "Loss: 11.558345598252929\n",
      "l2 norm of gradients: 0.055216333990576086\n",
      "l2 norm of weights: 3.3323508828141932\n",
      "---------------------\n",
      "Iteration Number: 7874\n",
      "Loss: 11.558015410181511\n",
      "l2 norm of gradients: 0.05520832652635341\n",
      "l2 norm of weights: 3.3324174516802327\n",
      "---------------------\n",
      "Iteration Number: 7875\n",
      "Loss: 11.5576853531398\n",
      "l2 norm of gradients: 0.055200322170300735\n",
      "l2 norm of weights: 3.332484020788423\n",
      "---------------------\n",
      "Iteration Number: 7876\n",
      "Loss: 11.557355427056141\n",
      "l2 norm of gradients: 0.05519232091986037\n",
      "l2 norm of weights: 3.332550590132259\n",
      "---------------------\n",
      "Iteration Number: 7877\n",
      "Loss: 11.557025631858936\n",
      "l2 norm of gradients: 0.05518432277247766\n",
      "l2 norm of weights: 3.3326171597052414\n",
      "---------------------\n",
      "Iteration Number: 7878\n",
      "Loss: 11.556695967476646\n",
      "l2 norm of gradients: 0.055176327725601035\n",
      "l2 norm of weights: 3.332683729500879\n",
      "---------------------\n",
      "Iteration Number: 7879\n",
      "Loss: 11.556366433837795\n",
      "l2 norm of gradients: 0.05516833577668201\n",
      "l2 norm of weights: 3.332750299512686\n",
      "---------------------\n",
      "Iteration Number: 7880\n",
      "Loss: 11.556037030870979\n",
      "l2 norm of gradients: 0.05516034692317513\n",
      "l2 norm of weights: 3.332816869734183\n",
      "---------------------\n",
      "Iteration Number: 7881\n",
      "Loss: 11.555707758504832\n",
      "l2 norm of gradients: 0.05515236116253803\n",
      "l2 norm of weights: 3.3328834401588976\n",
      "---------------------\n",
      "Iteration Number: 7882\n",
      "Loss: 11.555378616668083\n",
      "l2 norm of gradients: 0.05514437849223141\n",
      "l2 norm of weights: 3.3329500107803636\n",
      "---------------------\n",
      "Iteration Number: 7883\n",
      "Loss: 11.555049605289495\n",
      "l2 norm of gradients: 0.05513639890971904\n",
      "l2 norm of weights: 3.333016581592122\n",
      "---------------------\n",
      "Iteration Number: 7884\n",
      "Loss: 11.55472072429789\n",
      "l2 norm of gradients: 0.055128422412467716\n",
      "l2 norm of weights: 3.333083152587719\n",
      "---------------------\n",
      "Iteration Number: 7885\n",
      "Loss: 11.554391973622186\n",
      "l2 norm of gradients: 0.05512044899794726\n",
      "l2 norm of weights: 3.333149723760709\n",
      "---------------------\n",
      "Iteration Number: 7886\n",
      "Loss: 11.55406335319132\n",
      "l2 norm of gradients: 0.0551124786636306\n",
      "l2 norm of weights: 3.333216295104651\n",
      "---------------------\n",
      "Iteration Number: 7887\n",
      "Loss: 11.553734862934327\n",
      "l2 norm of gradients: 0.05510451140699368\n",
      "l2 norm of weights: 3.3332828666131125\n",
      "---------------------\n",
      "Iteration Number: 7888\n",
      "Loss: 11.553406502780277\n",
      "l2 norm of gradients: 0.05509654722551545\n",
      "l2 norm of weights: 3.333349438279666\n",
      "---------------------\n",
      "Iteration Number: 7889\n",
      "Loss: 11.553078272658315\n",
      "l2 norm of gradients: 0.05508858611667789\n",
      "l2 norm of weights: 3.3334160100978902\n",
      "---------------------\n",
      "Iteration Number: 7890\n",
      "Loss: 11.552750172497634\n",
      "l2 norm of gradients: 0.05508062807796611\n",
      "l2 norm of weights: 3.333482582061372\n",
      "---------------------\n",
      "Iteration Number: 7891\n",
      "Loss: 11.552422202227506\n",
      "l2 norm of gradients: 0.05507267310686808\n",
      "l2 norm of weights: 3.333549154163704\n",
      "---------------------\n",
      "Iteration Number: 7892\n",
      "Loss: 11.552094361777248\n",
      "l2 norm of gradients: 0.05506472120087495\n",
      "l2 norm of weights: 3.333615726398484\n",
      "---------------------\n",
      "Iteration Number: 7893\n",
      "Loss: 11.55176665107625\n",
      "l2 norm of gradients: 0.05505677235748079\n",
      "l2 norm of weights: 3.3336822987593178\n",
      "---------------------\n",
      "Iteration Number: 7894\n",
      "Loss: 11.551439070053956\n",
      "l2 norm of gradients: 0.05504882657418268\n",
      "l2 norm of weights: 3.333748871239817\n",
      "---------------------\n",
      "Iteration Number: 7895\n",
      "Loss: 11.55111161863988\n",
      "l2 norm of gradients: 0.055040883848480805\n",
      "l2 norm of weights: 3.3338154438336\n",
      "---------------------\n",
      "Iteration Number: 7896\n",
      "Loss: 11.550784296763574\n",
      "l2 norm of gradients: 0.05503294417787826\n",
      "l2 norm of weights: 3.333882016534291\n",
      "---------------------\n",
      "Iteration Number: 7897\n",
      "Loss: 11.550457104354672\n",
      "l2 norm of gradients: 0.05502500755988118\n",
      "l2 norm of weights: 3.333948589335521\n",
      "---------------------\n",
      "Iteration Number: 7898\n",
      "Loss: 11.55013004134287\n",
      "l2 norm of gradients: 0.05501707399199869\n",
      "l2 norm of weights: 3.3340151622309286\n",
      "---------------------\n",
      "Iteration Number: 7899\n",
      "Loss: 11.549803107657903\n",
      "l2 norm of gradients: 0.055009143471742884\n",
      "l2 norm of weights: 3.334081735214156\n",
      "---------------------\n",
      "Iteration Number: 7900\n",
      "Loss: 11.54947630322959\n",
      "l2 norm of gradients: 0.05500121599662893\n",
      "l2 norm of weights: 3.3341483082788548\n",
      "---------------------\n",
      "Iteration Number: 7901\n",
      "Loss: 11.5491496279878\n",
      "l2 norm of gradients: 0.05499329156417491\n",
      "l2 norm of weights: 3.334214881418681\n",
      "---------------------\n",
      "Iteration Number: 7902\n",
      "Loss: 11.548823081862457\n",
      "l2 norm of gradients: 0.05498537017190188\n",
      "l2 norm of weights: 3.3342814546272983\n",
      "---------------------\n",
      "Iteration Number: 7903\n",
      "Loss: 11.548496664783546\n",
      "l2 norm of gradients: 0.05497745181733391\n",
      "l2 norm of weights: 3.3343480278983755\n",
      "---------------------\n",
      "Iteration Number: 7904\n",
      "Loss: 11.548170376681126\n",
      "l2 norm of gradients: 0.054969536497998045\n",
      "l2 norm of weights: 3.3344146012255886\n",
      "---------------------\n",
      "Iteration Number: 7905\n",
      "Loss: 11.547844217485313\n",
      "l2 norm of gradients: 0.0549616242114243\n",
      "l2 norm of weights: 3.3344811746026206\n",
      "---------------------\n",
      "Iteration Number: 7906\n",
      "Loss: 11.547518187126258\n",
      "l2 norm of gradients: 0.05495371495514565\n",
      "l2 norm of weights: 3.3345477480231596\n",
      "---------------------\n",
      "Iteration Number: 7907\n",
      "Loss: 11.54719228553421\n",
      "l2 norm of gradients: 0.05494580872669802\n",
      "l2 norm of weights: 3.3346143214809\n",
      "---------------------\n",
      "Iteration Number: 7908\n",
      "Loss: 11.546866512639436\n",
      "l2 norm of gradients: 0.05493790552362032\n",
      "l2 norm of weights: 3.334680894969545\n",
      "---------------------\n",
      "Iteration Number: 7909\n",
      "Loss: 11.546540868372304\n",
      "l2 norm of gradients: 0.054930005343454415\n",
      "l2 norm of weights: 3.3347474684828002\n",
      "---------------------\n",
      "Iteration Number: 7910\n",
      "Loss: 11.546215352663218\n",
      "l2 norm of gradients: 0.05492210818374509\n",
      "l2 norm of weights: 3.3348140420143815\n",
      "---------------------\n",
      "Iteration Number: 7911\n",
      "Loss: 11.545889965442644\n",
      "l2 norm of gradients: 0.05491421404204013\n",
      "l2 norm of weights: 3.334880615558008\n",
      "---------------------\n",
      "Iteration Number: 7912\n",
      "Loss: 11.545564706641102\n",
      "l2 norm of gradients: 0.054906322915890246\n",
      "l2 norm of weights: 3.3349471891074085\n",
      "---------------------\n",
      "Iteration Number: 7913\n",
      "Loss: 11.545239576189193\n",
      "l2 norm of gradients: 0.05489843480284904\n",
      "l2 norm of weights: 3.335013762656314\n",
      "---------------------\n",
      "Iteration Number: 7914\n",
      "Loss: 11.544914574017561\n",
      "l2 norm of gradients: 0.054890549700473135\n",
      "l2 norm of weights: 3.3350803361984656\n",
      "---------------------\n",
      "Iteration Number: 7915\n",
      "Loss: 11.544589700056896\n",
      "l2 norm of gradients: 0.05488266760632202\n",
      "l2 norm of weights: 3.335146909727608\n",
      "---------------------\n",
      "Iteration Number: 7916\n",
      "Loss: 11.544264954237988\n",
      "l2 norm of gradients: 0.05487478851795814\n",
      "l2 norm of weights: 3.335213483237494\n",
      "---------------------\n",
      "Iteration Number: 7917\n",
      "Loss: 11.543940336491634\n",
      "l2 norm of gradients: 0.05486691243294689\n",
      "l2 norm of weights: 3.3352800567218823\n",
      "---------------------\n",
      "Iteration Number: 7918\n",
      "Loss: 11.543615846748745\n",
      "l2 norm of gradients: 0.05485903934885654\n",
      "l2 norm of weights: 3.335346630174537\n",
      "---------------------\n",
      "Iteration Number: 7919\n",
      "Loss: 11.543291484940239\n",
      "l2 norm of gradients: 0.05485116926325831\n",
      "l2 norm of weights: 3.3354132035892294\n",
      "---------------------\n",
      "Iteration Number: 7920\n",
      "Loss: 11.542967250997124\n",
      "l2 norm of gradients: 0.054843302173726335\n",
      "l2 norm of weights: 3.3354797769597377\n",
      "---------------------\n",
      "Iteration Number: 7921\n",
      "Loss: 11.542643144850473\n",
      "l2 norm of gradients: 0.05483543807783766\n",
      "l2 norm of weights: 3.3355463502798446\n",
      "---------------------\n",
      "Iteration Number: 7922\n",
      "Loss: 11.542319166431382\n",
      "l2 norm of gradients: 0.05482757697317219\n",
      "l2 norm of weights: 3.3356129235433407\n",
      "---------------------\n",
      "Iteration Number: 7923\n",
      "Loss: 11.541995315671047\n",
      "l2 norm of gradients: 0.05481971885731282\n",
      "l2 norm of weights: 3.335679496744022\n",
      "---------------------\n",
      "Iteration Number: 7924\n",
      "Loss: 11.541671592500691\n",
      "l2 norm of gradients: 0.05481186372784526\n",
      "l2 norm of weights: 3.335746069875692\n",
      "---------------------\n",
      "Iteration Number: 7925\n",
      "Loss: 11.541347996851622\n",
      "l2 norm of gradients: 0.05480401158235819\n",
      "l2 norm of weights: 3.3358126429321584\n",
      "---------------------\n",
      "Iteration Number: 7926\n",
      "Loss: 11.541024528655182\n",
      "l2 norm of gradients: 0.05479616241844315\n",
      "l2 norm of weights: 3.335879215907237\n",
      "---------------------\n",
      "Iteration Number: 7927\n",
      "Loss: 11.540701187842783\n",
      "l2 norm of gradients: 0.05478831623369452\n",
      "l2 norm of weights: 3.3359457887947483\n",
      "---------------------\n",
      "Iteration Number: 7928\n",
      "Loss: 11.540377974345898\n",
      "l2 norm of gradients: 0.05478047302570963\n",
      "l2 norm of weights: 3.336012361588521\n",
      "---------------------\n",
      "Iteration Number: 7929\n",
      "Loss: 11.540054888096059\n",
      "l2 norm of gradients: 0.054772632792088716\n",
      "l2 norm of weights: 3.336078934282389\n",
      "---------------------\n",
      "Iteration Number: 7930\n",
      "Loss: 11.539731929024837\n",
      "l2 norm of gradients: 0.0547647955304348\n",
      "l2 norm of weights: 3.3361455068701913\n",
      "---------------------\n",
      "Iteration Number: 7931\n",
      "Loss: 11.539409097063889\n",
      "l2 norm of gradients: 0.05475696123835383\n",
      "l2 norm of weights: 3.336212079345776\n",
      "---------------------\n",
      "Iteration Number: 7932\n",
      "Loss: 11.539086392144915\n",
      "l2 norm of gradients: 0.054749129913454625\n",
      "l2 norm of weights: 3.336278651702994\n",
      "---------------------\n",
      "Iteration Number: 7933\n",
      "Loss: 11.538763814199672\n",
      "l2 norm of gradients: 0.054741301553348866\n",
      "l2 norm of weights: 3.3363452239357057\n",
      "---------------------\n",
      "Iteration Number: 7934\n",
      "Loss: 11.538441363159986\n",
      "l2 norm of gradients: 0.05473347615565113\n",
      "l2 norm of weights: 3.3364117960377753\n",
      "---------------------\n",
      "Iteration Number: 7935\n",
      "Loss: 11.538119038957722\n",
      "l2 norm of gradients: 0.054725653717978745\n",
      "l2 norm of weights: 3.3364783680030747\n",
      "---------------------\n",
      "Iteration Number: 7936\n",
      "Loss: 11.537796841524823\n",
      "l2 norm of gradients: 0.05471783423795201\n",
      "l2 norm of weights: 3.3365449398254805\n",
      "---------------------\n",
      "Iteration Number: 7937\n",
      "Loss: 11.537474770793276\n",
      "l2 norm of gradients: 0.05471001771319404\n",
      "l2 norm of weights: 3.3366115114988775\n",
      "---------------------\n",
      "Iteration Number: 7938\n",
      "Loss: 11.537152826695129\n",
      "l2 norm of gradients: 0.05470220414133076\n",
      "l2 norm of weights: 3.3366780830171554\n",
      "---------------------\n",
      "Iteration Number: 7939\n",
      "Loss: 11.536831009162489\n",
      "l2 norm of gradients: 0.05469439351999102\n",
      "l2 norm of weights: 3.3367446543742103\n",
      "---------------------\n",
      "Iteration Number: 7940\n",
      "Loss: 11.536509318127523\n",
      "l2 norm of gradients: 0.0546865858468064\n",
      "l2 norm of weights: 3.3368112255639444\n",
      "---------------------\n",
      "Iteration Number: 7941\n",
      "Loss: 11.536187753522444\n",
      "l2 norm of gradients: 0.0546787811194114\n",
      "l2 norm of weights: 3.336877796580266\n",
      "---------------------\n",
      "Iteration Number: 7942\n",
      "Loss: 11.535866315279547\n",
      "l2 norm of gradients: 0.05467097933544334\n",
      "l2 norm of weights: 3.336944367417091\n",
      "---------------------\n",
      "Iteration Number: 7943\n",
      "Loss: 11.535545003331155\n",
      "l2 norm of gradients: 0.054663180492542374\n",
      "l2 norm of weights: 3.337010938068339\n",
      "---------------------\n",
      "Iteration Number: 7944\n",
      "Loss: 11.535223817609667\n",
      "l2 norm of gradients: 0.05465538458835144\n",
      "l2 norm of weights: 3.3370775085279374\n",
      "---------------------\n",
      "Iteration Number: 7945\n",
      "Loss: 11.534902758047528\n",
      "l2 norm of gradients: 0.05464759162051631\n",
      "l2 norm of weights: 3.33714407878982\n",
      "---------------------\n",
      "Iteration Number: 7946\n",
      "Loss: 11.534581824577247\n",
      "l2 norm of gradients: 0.054639801586685605\n",
      "l2 norm of weights: 3.337210648847926\n",
      "---------------------\n",
      "Iteration Number: 7947\n",
      "Loss: 11.534261017131394\n",
      "l2 norm of gradients: 0.05463201448451076\n",
      "l2 norm of weights: 3.3372772186962005\n",
      "---------------------\n",
      "Iteration Number: 7948\n",
      "Loss: 11.533940335642585\n",
      "l2 norm of gradients: 0.05462423031164598\n",
      "l2 norm of weights: 3.3373437883285955\n",
      "---------------------\n",
      "Iteration Number: 7949\n",
      "Loss: 11.533619780043498\n",
      "l2 norm of gradients: 0.05461644906574833\n",
      "l2 norm of weights: 3.3374103577390692\n",
      "---------------------\n",
      "Iteration Number: 7950\n",
      "Loss: 11.53329935026687\n",
      "l2 norm of gradients: 0.05460867074447761\n",
      "l2 norm of weights: 3.3374769269215854\n",
      "---------------------\n",
      "Iteration Number: 7951\n",
      "Loss: 11.532979046245497\n",
      "l2 norm of gradients: 0.05460089534549648\n",
      "l2 norm of weights: 3.3375434958701145\n",
      "---------------------\n",
      "Iteration Number: 7952\n",
      "Loss: 11.532658867912222\n",
      "l2 norm of gradients: 0.05459312286647042\n",
      "l2 norm of weights: 3.3376100645786324\n",
      "---------------------\n",
      "Iteration Number: 7953\n",
      "Loss: 11.53233881519995\n",
      "l2 norm of gradients: 0.05458535330506759\n",
      "l2 norm of weights: 3.3376766330411214\n",
      "---------------------\n",
      "Iteration Number: 7954\n",
      "Loss: 11.532018888041645\n",
      "l2 norm of gradients: 0.054577586658959044\n",
      "l2 norm of weights: 3.3377432012515706\n",
      "---------------------\n",
      "Iteration Number: 7955\n",
      "Loss: 11.53169908637032\n",
      "l2 norm of gradients: 0.054569822925818585\n",
      "l2 norm of weights: 3.3378097692039743\n",
      "---------------------\n",
      "Iteration Number: 7956\n",
      "Loss: 11.531379410119058\n",
      "l2 norm of gradients: 0.05456206210332279\n",
      "l2 norm of weights: 3.337876336892333\n",
      "---------------------\n",
      "Iteration Number: 7957\n",
      "Loss: 11.53105985922098\n",
      "l2 norm of gradients: 0.05455430418915103\n",
      "l2 norm of weights: 3.3379429043106548\n",
      "---------------------\n",
      "Iteration Number: 7958\n",
      "Loss: 11.530740433609283\n",
      "l2 norm of gradients: 0.05454654918098545\n",
      "l2 norm of weights: 3.3380094714529505\n",
      "---------------------\n",
      "Iteration Number: 7959\n",
      "Loss: 11.530421133217208\n",
      "l2 norm of gradients: 0.05453879707651094\n",
      "l2 norm of weights: 3.338076038313241\n",
      "---------------------\n",
      "Iteration Number: 7960\n",
      "Loss: 11.530101957978047\n",
      "l2 norm of gradients: 0.05453104787341521\n",
      "l2 norm of weights: 3.338142604885551\n",
      "---------------------\n",
      "Iteration Number: 7961\n",
      "Loss: 11.529782907825162\n",
      "l2 norm of gradients: 0.05452330156938867\n",
      "l2 norm of weights: 3.338209171163911\n",
      "---------------------\n",
      "Iteration Number: 7962\n",
      "Loss: 11.529463982691961\n",
      "l2 norm of gradients: 0.05451555816212451\n",
      "l2 norm of weights: 3.338275737142359\n",
      "---------------------\n",
      "Iteration Number: 7963\n",
      "Loss: 11.529145182511904\n",
      "l2 norm of gradients: 0.05450781764931873\n",
      "l2 norm of weights: 3.3383423028149384\n",
      "---------------------\n",
      "Iteration Number: 7964\n",
      "Loss: 11.528826507218527\n",
      "l2 norm of gradients: 0.054500080028669975\n",
      "l2 norm of weights: 3.3384088681756987\n",
      "---------------------\n",
      "Iteration Number: 7965\n",
      "Loss: 11.528507956745402\n",
      "l2 norm of gradients: 0.05449234529787979\n",
      "l2 norm of weights: 3.3384754332186946\n",
      "---------------------\n",
      "Iteration Number: 7966\n",
      "Loss: 11.52818953102617\n",
      "l2 norm of gradients: 0.05448461345465229\n",
      "l2 norm of weights: 3.3385419979379884\n",
      "---------------------\n",
      "Iteration Number: 7967\n",
      "Loss: 11.527871229994512\n",
      "l2 norm of gradients: 0.054476884496694473\n",
      "l2 norm of weights: 3.3386085623276474\n",
      "---------------------\n",
      "Iteration Number: 7968\n",
      "Loss: 11.527553053584187\n",
      "l2 norm of gradients: 0.054469158421716\n",
      "l2 norm of weights: 3.338675126381746\n",
      "---------------------\n",
      "Iteration Number: 7969\n",
      "Loss: 11.527235001728979\n",
      "l2 norm of gradients: 0.054461435227429314\n",
      "l2 norm of weights: 3.3387416900943627\n",
      "---------------------\n",
      "Iteration Number: 7970\n",
      "Loss: 11.526917074362748\n",
      "l2 norm of gradients: 0.05445371491154955\n",
      "l2 norm of weights: 3.3388082534595838\n",
      "---------------------\n",
      "Iteration Number: 7971\n",
      "Loss: 11.526599271419421\n",
      "l2 norm of gradients: 0.054445997471794565\n",
      "l2 norm of weights: 3.3388748164715008\n",
      "---------------------\n",
      "Iteration Number: 7972\n",
      "Loss: 11.526281592832948\n",
      "l2 norm of gradients: 0.05443828290588498\n",
      "l2 norm of weights: 3.3389413791242117\n",
      "---------------------\n",
      "Iteration Number: 7973\n",
      "Loss: 11.525964038537362\n",
      "l2 norm of gradients: 0.05443057121154413\n",
      "l2 norm of weights: 3.339007941411821\n",
      "---------------------\n",
      "Iteration Number: 7974\n",
      "Loss: 11.525646608466731\n",
      "l2 norm of gradients: 0.054422862386498023\n",
      "l2 norm of weights: 3.3390745033284372\n",
      "---------------------\n",
      "Iteration Number: 7975\n",
      "Loss: 11.5253293025552\n",
      "l2 norm of gradients: 0.05441515642847542\n",
      "l2 norm of weights: 3.339141064868177\n",
      "---------------------\n",
      "Iteration Number: 7976\n",
      "Loss: 11.525012120736953\n",
      "l2 norm of gradients: 0.05440745333520778\n",
      "l2 norm of weights: 3.3392076260251624\n",
      "---------------------\n",
      "Iteration Number: 7977\n",
      "Loss: 11.524695062946218\n",
      "l2 norm of gradients: 0.05439975310442927\n",
      "l2 norm of weights: 3.3392741867935203\n",
      "---------------------\n",
      "Iteration Number: 7978\n",
      "Loss: 11.524378129117315\n",
      "l2 norm of gradients: 0.054392055733876794\n",
      "l2 norm of weights: 3.3393407471673853\n",
      "---------------------\n",
      "Iteration Number: 7979\n",
      "Loss: 11.524061319184586\n",
      "l2 norm of gradients: 0.05438436122128988\n",
      "l2 norm of weights: 3.3394073071408963\n",
      "---------------------\n",
      "Iteration Number: 7980\n",
      "Loss: 11.523744633082428\n",
      "l2 norm of gradients: 0.0543766695644108\n",
      "l2 norm of weights: 3.3394738667082002\n",
      "---------------------\n",
      "Iteration Number: 7981\n",
      "Loss: 11.523428070745318\n",
      "l2 norm of gradients: 0.0543689807609845\n",
      "l2 norm of weights: 3.339540425863448\n",
      "---------------------\n",
      "Iteration Number: 7982\n",
      "Loss: 11.523111632107776\n",
      "l2 norm of gradients: 0.054361294808758655\n",
      "l2 norm of weights: 3.339606984600798\n",
      "---------------------\n",
      "Iteration Number: 7983\n",
      "Loss: 11.522795317104361\n",
      "l2 norm of gradients: 0.054353611705483536\n",
      "l2 norm of weights: 3.3396735429144138\n",
      "---------------------\n",
      "Iteration Number: 7984\n",
      "Loss: 11.522479125669697\n",
      "l2 norm of gradients: 0.054345931448912206\n",
      "l2 norm of weights: 3.339740100798464\n",
      "---------------------\n",
      "Iteration Number: 7985\n",
      "Loss: 11.522163057738478\n",
      "l2 norm of gradients: 0.05433825403680033\n",
      "l2 norm of weights: 3.3398066582471255\n",
      "---------------------\n",
      "Iteration Number: 7986\n",
      "Loss: 11.521847113245421\n",
      "l2 norm of gradients: 0.05433057946690626\n",
      "l2 norm of weights: 3.33987321525458\n",
      "---------------------\n",
      "Iteration Number: 7987\n",
      "Loss: 11.521531292125333\n",
      "l2 norm of gradients: 0.05432290773699106\n",
      "l2 norm of weights: 3.3399397718150134\n",
      "---------------------\n",
      "Iteration Number: 7988\n",
      "Loss: 11.521215594313047\n",
      "l2 norm of gradients: 0.05431523884481839\n",
      "l2 norm of weights: 3.340006327922621\n",
      "---------------------\n",
      "Iteration Number: 7989\n",
      "Loss: 11.520900019743452\n",
      "l2 norm of gradients: 0.054307572788154616\n",
      "l2 norm of weights: 3.340072883571601\n",
      "---------------------\n",
      "Iteration Number: 7990\n",
      "Loss: 11.520584568351522\n",
      "l2 norm of gradients: 0.054299909564768795\n",
      "l2 norm of weights: 3.3401394387561587\n",
      "---------------------\n",
      "Iteration Number: 7991\n",
      "Loss: 11.520269240072247\n",
      "l2 norm of gradients: 0.05429224917243254\n",
      "l2 norm of weights: 3.3402059934705064\n",
      "---------------------\n",
      "Iteration Number: 7992\n",
      "Loss: 11.519954034840682\n",
      "l2 norm of gradients: 0.05428459160892026\n",
      "l2 norm of weights: 3.34027254770886\n",
      "---------------------\n",
      "Iteration Number: 7993\n",
      "Loss: 11.519638952591944\n",
      "l2 norm of gradients: 0.05427693687200888\n",
      "l2 norm of weights: 3.3403391014654433\n",
      "---------------------\n",
      "Iteration Number: 7994\n",
      "Loss: 11.519323993261212\n",
      "l2 norm of gradients: 0.05426928495947803\n",
      "l2 norm of weights: 3.3404056547344845\n",
      "---------------------\n",
      "Iteration Number: 7995\n",
      "Loss: 11.519009156783687\n",
      "l2 norm of gradients: 0.05426163586911002\n",
      "l2 norm of weights: 3.3404722075102202\n",
      "---------------------\n",
      "Iteration Number: 7996\n",
      "Loss: 11.518694443094667\n",
      "l2 norm of gradients: 0.054253989598689704\n",
      "l2 norm of weights: 3.3405387597868894\n",
      "---------------------\n",
      "Iteration Number: 7997\n",
      "Loss: 11.518379852129458\n",
      "l2 norm of gradients: 0.05424634614600464\n",
      "l2 norm of weights: 3.3406053115587393\n",
      "---------------------\n",
      "Iteration Number: 7998\n",
      "Loss: 11.518065383823455\n",
      "l2 norm of gradients: 0.05423870550884502\n",
      "l2 norm of weights: 3.3406718628200225\n",
      "---------------------\n",
      "Iteration Number: 7999\n",
      "Loss: 11.51775103811209\n",
      "l2 norm of gradients: 0.05423106768500364\n",
      "l2 norm of weights: 3.340738413564998\n",
      "---------------------\n",
      "Iteration Number: 8000\n",
      "Loss: 11.517436814930846\n",
      "l2 norm of gradients: 0.05422343267227589\n",
      "l2 norm of weights: 3.3408049637879293\n",
      "---------------------\n",
      "Iteration Number: 8001\n",
      "Loss: 11.517122714215276\n",
      "l2 norm of gradients: 0.05421580046845984\n",
      "l2 norm of weights: 3.340871513483087\n",
      "---------------------\n",
      "Iteration Number: 8002\n",
      "Loss: 11.516808735900971\n",
      "l2 norm of gradients: 0.05420817107135617\n",
      "l2 norm of weights: 3.3409380626447467\n",
      "---------------------\n",
      "Iteration Number: 8003\n",
      "Loss: 11.516494879923584\n",
      "l2 norm of gradients: 0.05420054447876814\n",
      "l2 norm of weights: 3.3410046112671914\n",
      "---------------------\n",
      "Iteration Number: 8004\n",
      "Loss: 11.516181146218813\n",
      "l2 norm of gradients: 0.054192920688501685\n",
      "l2 norm of weights: 3.3410711593447076\n",
      "---------------------\n",
      "Iteration Number: 8005\n",
      "Loss: 11.515867534722412\n",
      "l2 norm of gradients: 0.05418529969836525\n",
      "l2 norm of weights: 3.3411377068715895\n",
      "---------------------\n",
      "Iteration Number: 8006\n",
      "Loss: 11.515554045370182\n",
      "l2 norm of gradients: 0.05417768150616994\n",
      "l2 norm of weights: 3.341204253842137\n",
      "---------------------\n",
      "Iteration Number: 8007\n",
      "Loss: 11.515240678097998\n",
      "l2 norm of gradients: 0.05417006610972948\n",
      "l2 norm of weights: 3.3412708002506544\n",
      "---------------------\n",
      "Iteration Number: 8008\n",
      "Loss: 11.514927432841775\n",
      "l2 norm of gradients: 0.05416245350686016\n",
      "l2 norm of weights: 3.3413373460914535\n",
      "---------------------\n",
      "Iteration Number: 8009\n",
      "Loss: 11.514614309537473\n",
      "l2 norm of gradients: 0.05415484369538087\n",
      "l2 norm of weights: 3.3414038913588513\n",
      "---------------------\n",
      "Iteration Number: 8010\n",
      "Loss: 11.514301308121112\n",
      "l2 norm of gradients: 0.05414723667311311\n",
      "l2 norm of weights: 3.3414704360471705\n",
      "---------------------\n",
      "Iteration Number: 8011\n",
      "Loss: 11.513988428528764\n",
      "l2 norm of gradients: 0.05413963243788091\n",
      "l2 norm of weights: 3.3415369801507393\n",
      "---------------------\n",
      "Iteration Number: 8012\n",
      "Loss: 11.51367567069656\n",
      "l2 norm of gradients: 0.054132030987510946\n",
      "l2 norm of weights: 3.341603523663893\n",
      "---------------------\n",
      "Iteration Number: 8013\n",
      "Loss: 11.513363034560674\n",
      "l2 norm of gradients: 0.054124432319832466\n",
      "l2 norm of weights: 3.3416700665809707\n",
      "---------------------\n",
      "Iteration Number: 8014\n",
      "Loss: 11.513050520057336\n",
      "l2 norm of gradients: 0.05411683643267724\n",
      "l2 norm of weights: 3.3417366088963196\n",
      "---------------------\n",
      "Iteration Number: 8015\n",
      "Loss: 11.512738127122837\n",
      "l2 norm of gradients: 0.0541092433238797\n",
      "l2 norm of weights: 3.341803150604291\n",
      "---------------------\n",
      "Iteration Number: 8016\n",
      "Loss: 11.512425855693511\n",
      "l2 norm of gradients: 0.05410165299127673\n",
      "l2 norm of weights: 3.3418696916992423\n",
      "---------------------\n",
      "Iteration Number: 8017\n",
      "Loss: 11.512113705705735\n",
      "l2 norm of gradients: 0.05409406543270789\n",
      "l2 norm of weights: 3.3419362321755375\n",
      "---------------------\n",
      "Iteration Number: 8018\n",
      "Loss: 11.511801677095963\n",
      "l2 norm of gradients: 0.05408648064601527\n",
      "l2 norm of weights: 3.342002772027545\n",
      "---------------------\n",
      "Iteration Number: 8019\n",
      "Loss: 11.51148976980068\n",
      "l2 norm of gradients: 0.05407889862904345\n",
      "l2 norm of weights: 3.3420693112496407\n",
      "---------------------\n",
      "Iteration Number: 8020\n",
      "Loss: 11.511177983756435\n",
      "l2 norm of gradients: 0.05407131937963965\n",
      "l2 norm of weights: 3.342135849836205\n",
      "---------------------\n",
      "Iteration Number: 8021\n",
      "Loss: 11.51086631889982\n",
      "l2 norm of gradients: 0.05406374289565364\n",
      "l2 norm of weights: 3.3422023877816245\n",
      "---------------------\n",
      "Iteration Number: 8022\n",
      "Loss: 11.510554775167488\n",
      "l2 norm of gradients: 0.054056169174937656\n",
      "l2 norm of weights: 3.342268925080291\n",
      "---------------------\n",
      "Iteration Number: 8023\n",
      "Loss: 11.510243352496147\n",
      "l2 norm of gradients: 0.05404859821534662\n",
      "l2 norm of weights: 3.342335461726603\n",
      "---------------------\n",
      "Iteration Number: 8024\n",
      "Loss: 11.509932050822538\n",
      "l2 norm of gradients: 0.054041030014737826\n",
      "l2 norm of weights: 3.342401997714964\n",
      "---------------------\n",
      "Iteration Number: 8025\n",
      "Loss: 11.509620870083475\n",
      "l2 norm of gradients: 0.054033464570971246\n",
      "l2 norm of weights: 3.3424685330397845\n",
      "---------------------\n",
      "Iteration Number: 8026\n",
      "Loss: 11.5093098102158\n",
      "l2 norm of gradients: 0.05402590188190931\n",
      "l2 norm of weights: 3.342535067695479\n",
      "---------------------\n",
      "Iteration Number: 8027\n",
      "Loss: 11.508998871156448\n",
      "l2 norm of gradients: 0.054018341945417\n",
      "l2 norm of weights: 3.342601601676468\n",
      "---------------------\n",
      "Iteration Number: 8028\n",
      "Loss: 11.508688052842356\n",
      "l2 norm of gradients: 0.05401078475936185\n",
      "l2 norm of weights: 3.342668134977179\n",
      "---------------------\n",
      "Iteration Number: 8029\n",
      "Loss: 11.508377355210543\n",
      "l2 norm of gradients: 0.05400323032161387\n",
      "l2 norm of weights: 3.3427346675920444\n",
      "---------------------\n",
      "Iteration Number: 8030\n",
      "Loss: 11.508066778198089\n",
      "l2 norm of gradients: 0.053995678630045646\n",
      "l2 norm of weights: 3.3428011995155025\n",
      "---------------------\n",
      "Iteration Number: 8031\n",
      "Loss: 11.507756321742082\n",
      "l2 norm of gradients: 0.05398812968253225\n",
      "l2 norm of weights: 3.342867730741997\n",
      "---------------------\n",
      "Iteration Number: 8032\n",
      "Loss: 11.507445985779702\n",
      "l2 norm of gradients: 0.05398058347695124\n",
      "l2 norm of weights: 3.342934261265978\n",
      "---------------------\n",
      "Iteration Number: 8033\n",
      "Loss: 11.50713577024818\n",
      "l2 norm of gradients: 0.05397304001118275\n",
      "l2 norm of weights: 3.3430007910819004\n",
      "---------------------\n",
      "Iteration Number: 8034\n",
      "Loss: 11.506825675084762\n",
      "l2 norm of gradients: 0.05396549928310939\n",
      "l2 norm of weights: 3.343067320184225\n",
      "---------------------\n",
      "Iteration Number: 8035\n",
      "Loss: 11.506515700226775\n",
      "l2 norm of gradients: 0.05395796129061628\n",
      "l2 norm of weights: 3.343133848567419\n",
      "---------------------\n",
      "Iteration Number: 8036\n",
      "Loss: 11.506205845611609\n",
      "l2 norm of gradients: 0.053950426031591\n",
      "l2 norm of weights: 3.3432003762259543\n",
      "---------------------\n",
      "Iteration Number: 8037\n",
      "Loss: 11.505896111176664\n",
      "l2 norm of gradients: 0.05394289350392369\n",
      "l2 norm of weights: 3.3432669031543103\n",
      "---------------------\n",
      "Iteration Number: 8038\n",
      "Loss: 11.505586496859424\n",
      "l2 norm of gradients: 0.05393536370550697\n",
      "l2 norm of weights: 3.3433334293469694\n",
      "---------------------\n",
      "Iteration Number: 8039\n",
      "Loss: 11.505277002597426\n",
      "l2 norm of gradients: 0.053927836634235934\n",
      "l2 norm of weights: 3.3433999547984214\n",
      "---------------------\n",
      "Iteration Number: 8040\n",
      "Loss: 11.504967628328219\n",
      "l2 norm of gradients: 0.05392031228800813\n",
      "l2 norm of weights: 3.3434664795031623\n",
      "---------------------\n",
      "Iteration Number: 8041\n",
      "Loss: 11.504658373989447\n",
      "l2 norm of gradients: 0.05391279066472367\n",
      "l2 norm of weights: 3.3435330034556916\n",
      "---------------------\n",
      "Iteration Number: 8042\n",
      "Loss: 11.5043492395188\n",
      "l2 norm of gradients: 0.0539052717622851\n",
      "l2 norm of weights: 3.3435995266505167\n",
      "---------------------\n",
      "Iteration Number: 8043\n",
      "Loss: 11.504040224853986\n",
      "l2 norm of gradients: 0.05389775557859743\n",
      "l2 norm of weights: 3.3436660490821497\n",
      "---------------------\n",
      "Iteration Number: 8044\n",
      "Loss: 11.503731329932798\n",
      "l2 norm of gradients: 0.053890242111568176\n",
      "l2 norm of weights: 3.343732570745108\n",
      "---------------------\n",
      "Iteration Number: 8045\n",
      "Loss: 11.50342255469306\n",
      "l2 norm of gradients: 0.05388273135910729\n",
      "l2 norm of weights: 3.3437990916339153\n",
      "---------------------\n",
      "Iteration Number: 8046\n",
      "Loss: 11.503113899072652\n",
      "l2 norm of gradients: 0.05387522331912725\n",
      "l2 norm of weights: 3.3438656117431003\n",
      "---------------------\n",
      "Iteration Number: 8047\n",
      "Loss: 11.502805363009514\n",
      "l2 norm of gradients: 0.05386771798954294\n",
      "l2 norm of weights: 3.3439321310671977\n",
      "---------------------\n",
      "Iteration Number: 8048\n",
      "Loss: 11.502496946441614\n",
      "l2 norm of gradients: 0.05386021536827174\n",
      "l2 norm of weights: 3.3439986496007483\n",
      "---------------------\n",
      "Iteration Number: 8049\n",
      "Loss: 11.502188649306994\n",
      "l2 norm of gradients: 0.05385271545323344\n",
      "l2 norm of weights: 3.344065167338298\n",
      "---------------------\n",
      "Iteration Number: 8050\n",
      "Loss: 11.501880471543748\n",
      "l2 norm of gradients: 0.05384521824235034\n",
      "l2 norm of weights: 3.3441316842743984\n",
      "---------------------\n",
      "Iteration Number: 8051\n",
      "Loss: 11.501572413089988\n",
      "l2 norm of gradients: 0.05383772373354719\n",
      "l2 norm of weights: 3.344198200403606\n",
      "---------------------\n",
      "Iteration Number: 8052\n",
      "Loss: 11.501264473883905\n",
      "l2 norm of gradients: 0.053830231924751115\n",
      "l2 norm of weights: 3.344264715720484\n",
      "---------------------\n",
      "Iteration Number: 8053\n",
      "Loss: 11.500956653863739\n",
      "l2 norm of gradients: 0.05382274281389177\n",
      "l2 norm of weights: 3.3443312302196015\n",
      "---------------------\n",
      "Iteration Number: 8054\n",
      "Loss: 11.500648952967772\n",
      "l2 norm of gradients: 0.05381525639890124\n",
      "l2 norm of weights: 3.3443977438955317\n",
      "---------------------\n",
      "Iteration Number: 8055\n",
      "Loss: 11.500341371134331\n",
      "l2 norm of gradients: 0.053807772677713944\n",
      "l2 norm of weights: 3.3444642567428544\n",
      "---------------------\n",
      "Iteration Number: 8056\n",
      "Loss: 11.500033908301805\n",
      "l2 norm of gradients: 0.05380029164826687\n",
      "l2 norm of weights: 3.344530768756155\n",
      "---------------------\n",
      "Iteration Number: 8057\n",
      "Loss: 11.499726564408638\n",
      "l2 norm of gradients: 0.05379281330849939\n",
      "l2 norm of weights: 3.3445972799300234\n",
      "---------------------\n",
      "Iteration Number: 8058\n",
      "Loss: 11.499419339393292\n",
      "l2 norm of gradients: 0.053785337656353245\n",
      "l2 norm of weights: 3.3446637902590566\n",
      "---------------------\n",
      "Iteration Number: 8059\n",
      "Loss: 11.499112233194316\n",
      "l2 norm of gradients: 0.053777864689772706\n",
      "l2 norm of weights: 3.3447302997378565\n",
      "---------------------\n",
      "Iteration Number: 8060\n",
      "Loss: 11.4988052457503\n",
      "l2 norm of gradients: 0.053770394406704385\n",
      "l2 norm of weights: 3.344796808361031\n",
      "---------------------\n",
      "Iteration Number: 8061\n",
      "Loss: 11.498498376999862\n",
      "l2 norm of gradients: 0.053762926805097304\n",
      "l2 norm of weights: 3.3448633161231927\n",
      "---------------------\n",
      "Iteration Number: 8062\n",
      "Loss: 11.498191626881685\n",
      "l2 norm of gradients: 0.05375546188290298\n",
      "l2 norm of weights: 3.34492982301896\n",
      "---------------------\n",
      "Iteration Number: 8063\n",
      "Loss: 11.497884995334516\n",
      "l2 norm of gradients: 0.05374799963807525\n",
      "l2 norm of weights: 3.344996329042958\n",
      "---------------------\n",
      "Iteration Number: 8064\n",
      "Loss: 11.497578482297127\n",
      "l2 norm of gradients: 0.05374054006857042\n",
      "l2 norm of weights: 3.3450628341898154\n",
      "---------------------\n",
      "Iteration Number: 8065\n",
      "Loss: 11.49727208770836\n",
      "l2 norm of gradients: 0.05373308317234719\n",
      "l2 norm of weights: 3.3451293384541683\n",
      "---------------------\n",
      "Iteration Number: 8066\n",
      "Loss: 11.496965811507076\n",
      "l2 norm of gradients: 0.05372562894736665\n",
      "l2 norm of weights: 3.345195841830656\n",
      "---------------------\n",
      "Iteration Number: 8067\n",
      "Loss: 11.49665965363223\n",
      "l2 norm of gradients: 0.05371817739159227\n",
      "l2 norm of weights: 3.3452623443139267\n",
      "---------------------\n",
      "Iteration Number: 8068\n",
      "Loss: 11.496353614022787\n",
      "l2 norm of gradients: 0.05371072850298994\n",
      "l2 norm of weights: 3.3453288458986314\n",
      "---------------------\n",
      "Iteration Number: 8069\n",
      "Loss: 11.49604769261778\n",
      "l2 norm of gradients: 0.053703282279527954\n",
      "l2 norm of weights: 3.3453953465794273\n",
      "---------------------\n",
      "Iteration Number: 8070\n",
      "Loss: 11.495741889356289\n",
      "l2 norm of gradients: 0.053695838719176964\n",
      "l2 norm of weights: 3.345461846350978\n",
      "---------------------\n",
      "Iteration Number: 8071\n",
      "Loss: 11.495436204177434\n",
      "l2 norm of gradients: 0.053688397819910014\n",
      "l2 norm of weights: 3.345528345207951\n",
      "---------------------\n",
      "Iteration Number: 8072\n",
      "Loss: 11.495130637020402\n",
      "l2 norm of gradients: 0.05368095957970254\n",
      "l2 norm of weights: 3.3455948431450206\n",
      "---------------------\n",
      "Iteration Number: 8073\n",
      "Loss: 11.494825187824414\n",
      "l2 norm of gradients: 0.053673523996532316\n",
      "l2 norm of weights: 3.345661340156867\n",
      "---------------------\n",
      "Iteration Number: 8074\n",
      "Loss: 11.494519856528754\n",
      "l2 norm of gradients: 0.053666091068379586\n",
      "l2 norm of weights: 3.345727836238174\n",
      "---------------------\n",
      "Iteration Number: 8075\n",
      "Loss: 11.494214643072729\n",
      "l2 norm of gradients: 0.0536586607932269\n",
      "l2 norm of weights: 3.3457943313836327\n",
      "---------------------\n",
      "Iteration Number: 8076\n",
      "Loss: 11.493909547395722\n",
      "l2 norm of gradients: 0.053651233169059155\n",
      "l2 norm of weights: 3.3458608255879385\n",
      "---------------------\n",
      "Iteration Number: 8077\n",
      "Loss: 11.493604569437156\n",
      "l2 norm of gradients: 0.053643808193863614\n",
      "l2 norm of weights: 3.345927318845793\n",
      "---------------------\n",
      "Iteration Number: 8078\n",
      "Loss: 11.493299709136501\n",
      "l2 norm of gradients: 0.053636385865629986\n",
      "l2 norm of weights: 3.3459938111519034\n",
      "---------------------\n",
      "Iteration Number: 8079\n",
      "Loss: 11.492994966433265\n",
      "l2 norm of gradients: 0.05362896618235023\n",
      "l2 norm of weights: 3.346060302500982\n",
      "---------------------\n",
      "Iteration Number: 8080\n",
      "Loss: 11.492690341267028\n",
      "l2 norm of gradients: 0.05362154914201875\n",
      "l2 norm of weights: 3.3461267928877465\n",
      "---------------------\n",
      "Iteration Number: 8081\n",
      "Loss: 11.492385833577409\n",
      "l2 norm of gradients: 0.053614134742632255\n",
      "l2 norm of weights: 3.34619328230692\n",
      "---------------------\n",
      "Iteration Number: 8082\n",
      "Loss: 11.492081443304059\n",
      "l2 norm of gradients: 0.05360672298218982\n",
      "l2 norm of weights: 3.346259770753231\n",
      "---------------------\n",
      "Iteration Number: 8083\n",
      "Loss: 11.4917771703867\n",
      "l2 norm of gradients: 0.05359931385869285\n",
      "l2 norm of weights: 3.346326258221415\n",
      "---------------------\n",
      "Iteration Number: 8084\n",
      "Loss: 11.491473014765097\n",
      "l2 norm of gradients: 0.05359190737014507\n",
      "l2 norm of weights: 3.34639274470621\n",
      "---------------------\n",
      "Iteration Number: 8085\n",
      "Loss: 11.491168976379054\n",
      "l2 norm of gradients: 0.05358450351455262\n",
      "l2 norm of weights: 3.346459230202362\n",
      "---------------------\n",
      "Iteration Number: 8086\n",
      "Loss: 11.490865055168431\n",
      "l2 norm of gradients: 0.053577102289923906\n",
      "l2 norm of weights: 3.3465257147046215\n",
      "---------------------\n",
      "Iteration Number: 8087\n",
      "Loss: 11.490561251073135\n",
      "l2 norm of gradients: 0.05356970369426971\n",
      "l2 norm of weights: 3.3465921982077442\n",
      "---------------------\n",
      "Iteration Number: 8088\n",
      "Loss: 11.490257564033122\n",
      "l2 norm of gradients: 0.05356230772560311\n",
      "l2 norm of weights: 3.3466586807064918\n",
      "---------------------\n",
      "Iteration Number: 8089\n",
      "Loss: 11.48995399398839\n",
      "l2 norm of gradients: 0.05355491438193954\n",
      "l2 norm of weights: 3.346725162195631\n",
      "---------------------\n",
      "Iteration Number: 8090\n",
      "Loss: 11.489650540878994\n",
      "l2 norm of gradients: 0.05354752366129674\n",
      "l2 norm of weights: 3.3467916426699333\n",
      "---------------------\n",
      "Iteration Number: 8091\n",
      "Loss: 11.489347204645044\n",
      "l2 norm of gradients: 0.0535401355616948\n",
      "l2 norm of weights: 3.3468581221241775\n",
      "---------------------\n",
      "Iteration Number: 8092\n",
      "Loss: 11.489043985226663\n",
      "l2 norm of gradients: 0.053532750081156065\n",
      "l2 norm of weights: 3.346924600553146\n",
      "---------------------\n",
      "Iteration Number: 8093\n",
      "Loss: 11.488740882564068\n",
      "l2 norm of gradients: 0.05352536721770523\n",
      "l2 norm of weights: 3.346991077951628\n",
      "---------------------\n",
      "Iteration Number: 8094\n",
      "Loss: 11.488437896597492\n",
      "l2 norm of gradients: 0.05351798696936934\n",
      "l2 norm of weights: 3.347057554314416\n",
      "---------------------\n",
      "Iteration Number: 8095\n",
      "Loss: 11.48813502726723\n",
      "l2 norm of gradients: 0.05351060933417767\n",
      "l2 norm of weights: 3.3471240296363107\n",
      "---------------------\n",
      "Iteration Number: 8096\n",
      "Loss: 11.48783227451362\n",
      "l2 norm of gradients: 0.05350323431016185\n",
      "l2 norm of weights: 3.347190503912116\n",
      "---------------------\n",
      "Iteration Number: 8097\n",
      "Loss: 11.487529638277048\n",
      "l2 norm of gradients: 0.05349586189535583\n",
      "l2 norm of weights: 3.3472569771366416\n",
      "---------------------\n",
      "Iteration Number: 8098\n",
      "Loss: 11.487227118497936\n",
      "l2 norm of gradients: 0.0534884920877958\n",
      "l2 norm of weights: 3.3473234493047035\n",
      "---------------------\n",
      "Iteration Number: 8099\n",
      "Loss: 11.48692471511679\n",
      "l2 norm of gradients: 0.05348112488552025\n",
      "l2 norm of weights: 3.3473899204111226\n",
      "---------------------\n",
      "Iteration Number: 8100\n",
      "Loss: 11.486622428074115\n",
      "l2 norm of gradients: 0.05347376028657003\n",
      "l2 norm of weights: 3.347456390450725\n",
      "---------------------\n",
      "Iteration Number: 8101\n",
      "Loss: 11.486320257310503\n",
      "l2 norm of gradients: 0.053466398288988186\n",
      "l2 norm of weights: 3.347522859418341\n",
      "---------------------\n",
      "Iteration Number: 8102\n",
      "Loss: 11.486018202766573\n",
      "l2 norm of gradients: 0.053459038890820154\n",
      "l2 norm of weights: 3.347589327308809\n",
      "---------------------\n",
      "Iteration Number: 8103\n",
      "Loss: 11.485716264382992\n",
      "l2 norm of gradients: 0.05345168209011353\n",
      "l2 norm of weights: 3.3476557941169705\n",
      "---------------------\n",
      "Iteration Number: 8104\n",
      "Loss: 11.485414442100486\n",
      "l2 norm of gradients: 0.0534443278849183\n",
      "l2 norm of weights: 3.3477222598376732\n",
      "---------------------\n",
      "Iteration Number: 8105\n",
      "Loss: 11.485112735859815\n",
      "l2 norm of gradients: 0.05343697627328668\n",
      "l2 norm of weights: 3.3477887244657705\n",
      "---------------------\n",
      "Iteration Number: 8106\n",
      "Loss: 11.48481114560181\n",
      "l2 norm of gradients: 0.05342962725327312\n",
      "l2 norm of weights: 3.3478551879961196\n",
      "---------------------\n",
      "Iteration Number: 8107\n",
      "Loss: 11.484509671267304\n",
      "l2 norm of gradients: 0.05342228082293443\n",
      "l2 norm of weights: 3.347921650423585\n",
      "---------------------\n",
      "Iteration Number: 8108\n",
      "Loss: 11.484208312797222\n",
      "l2 norm of gradients: 0.05341493698032957\n",
      "l2 norm of weights: 3.3479881117430357\n",
      "---------------------\n",
      "Iteration Number: 8109\n",
      "Loss: 11.483907070132506\n",
      "l2 norm of gradients: 0.05340759572351988\n",
      "l2 norm of weights: 3.348054571949345\n",
      "---------------------\n",
      "Iteration Number: 8110\n",
      "Loss: 11.48360594321418\n",
      "l2 norm of gradients: 0.05340025705056887\n",
      "l2 norm of weights: 3.348121031037394\n",
      "---------------------\n",
      "Iteration Number: 8111\n",
      "Loss: 11.483304931983268\n",
      "l2 norm of gradients: 0.053392920959542366\n",
      "l2 norm of weights: 3.3481874890020658\n",
      "---------------------\n",
      "Iteration Number: 8112\n",
      "Loss: 11.483004036380876\n",
      "l2 norm of gradients: 0.05338558744850844\n",
      "l2 norm of weights: 3.3482539458382514\n",
      "---------------------\n",
      "Iteration Number: 8113\n",
      "Loss: 11.48270325634815\n",
      "l2 norm of gradients: 0.05337825651553734\n",
      "l2 norm of weights: 3.3483204015408474\n",
      "---------------------\n",
      "Iteration Number: 8114\n",
      "Loss: 11.482402591826274\n",
      "l2 norm of gradients: 0.053370928158701675\n",
      "l2 norm of weights: 3.348386856104753\n",
      "---------------------\n",
      "Iteration Number: 8115\n",
      "Loss: 11.48210204275648\n",
      "l2 norm of gradients: 0.053363602376076257\n",
      "l2 norm of weights: 3.3484533095248747\n",
      "---------------------\n",
      "Iteration Number: 8116\n",
      "Loss: 11.481801609080053\n",
      "l2 norm of gradients: 0.053356279165738096\n",
      "l2 norm of weights: 3.348519761796124\n",
      "---------------------\n",
      "Iteration Number: 8117\n",
      "Loss: 11.481501290738331\n",
      "l2 norm of gradients: 0.05334895852576647\n",
      "l2 norm of weights: 3.348586212913417\n",
      "---------------------\n",
      "Iteration Number: 8118\n",
      "Loss: 11.48120108767268\n",
      "l2 norm of gradients: 0.05334164045424291\n",
      "l2 norm of weights: 3.3486526628716775\n",
      "---------------------\n",
      "Iteration Number: 8119\n",
      "Loss: 11.480900999824527\n",
      "l2 norm of gradients: 0.05333432494925116\n",
      "l2 norm of weights: 3.3487191116658312\n",
      "---------------------\n",
      "Iteration Number: 8120\n",
      "Loss: 11.480601027135334\n",
      "l2 norm of gradients: 0.0533270120088772\n",
      "l2 norm of weights: 3.348785559290811\n",
      "---------------------\n",
      "Iteration Number: 8121\n",
      "Loss: 11.480301169546625\n",
      "l2 norm of gradients: 0.05331970163120924\n",
      "l2 norm of weights: 3.3488520057415543\n",
      "---------------------\n",
      "Iteration Number: 8122\n",
      "Loss: 11.48000142699995\n",
      "l2 norm of gradients: 0.053312393814337666\n",
      "l2 norm of weights: 3.3489184510130037\n",
      "---------------------\n",
      "Iteration Number: 8123\n",
      "Loss: 11.47970179943693\n",
      "l2 norm of gradients: 0.05330508855635519\n",
      "l2 norm of weights: 3.3489848951001093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Iteration Number: 8124\n",
      "Loss: 11.479402286799216\n",
      "l2 norm of gradients: 0.05329778585535659\n",
      "l2 norm of weights: 3.3490513379978233\n",
      "---------------------\n",
      "Iteration Number: 8125\n",
      "Loss: 11.479102889028496\n",
      "l2 norm of gradients: 0.05329048570943902\n",
      "l2 norm of weights: 3.3491177797011042\n",
      "---------------------\n",
      "Iteration Number: 8126\n",
      "Loss: 11.478803606066526\n",
      "l2 norm of gradients: 0.05328318811670171\n",
      "l2 norm of weights: 3.349184220204917\n",
      "---------------------\n",
      "Iteration Number: 8127\n",
      "Loss: 11.478504437855097\n",
      "l2 norm of gradients: 0.0532758930752462\n",
      "l2 norm of weights: 3.3492506595042304\n",
      "---------------------\n",
      "Iteration Number: 8128\n",
      "Loss: 11.478205384336054\n",
      "l2 norm of gradients: 0.05326860058317617\n",
      "l2 norm of weights: 3.349317097594019\n",
      "---------------------\n",
      "Iteration Number: 8129\n",
      "Loss: 11.47790644545127\n",
      "l2 norm of gradients: 0.0532613106385975\n",
      "l2 norm of weights: 3.3493835344692626\n",
      "---------------------\n",
      "Iteration Number: 8130\n",
      "Loss: 11.477607621142676\n",
      "l2 norm of gradients: 0.053254023239618274\n",
      "l2 norm of weights: 3.349449970124946\n",
      "---------------------\n",
      "Iteration Number: 8131\n",
      "Loss: 11.47730891135226\n",
      "l2 norm of gradients: 0.05324673838434882\n",
      "l2 norm of weights: 3.349516404556059\n",
      "---------------------\n",
      "Iteration Number: 8132\n",
      "Loss: 11.477010316022032\n",
      "l2 norm of gradients: 0.053239456070901595\n",
      "l2 norm of weights: 3.349582837757598\n",
      "---------------------\n",
      "Iteration Number: 8133\n",
      "Loss: 11.47671183509406\n",
      "l2 norm of gradients: 0.05323217629739132\n",
      "l2 norm of weights: 3.349649269724562\n",
      "---------------------\n",
      "Iteration Number: 8134\n",
      "Loss: 11.47641346851047\n",
      "l2 norm of gradients: 0.0532248990619348\n",
      "l2 norm of weights: 3.3497157004519584\n",
      "---------------------\n",
      "Iteration Number: 8135\n",
      "Loss: 11.476115216213406\n",
      "l2 norm of gradients: 0.05321762436265108\n",
      "l2 norm of weights: 3.3497821299347974\n",
      "---------------------\n",
      "Iteration Number: 8136\n",
      "Loss: 11.475817078145086\n",
      "l2 norm of gradients: 0.053210352197661404\n",
      "l2 norm of weights: 3.349848558168095\n",
      "---------------------\n",
      "Iteration Number: 8137\n",
      "Loss: 11.475519054247759\n",
      "l2 norm of gradients: 0.05320308256508916\n",
      "l2 norm of weights: 3.349914985146873\n",
      "---------------------\n",
      "Iteration Number: 8138\n",
      "Loss: 11.475221144463703\n",
      "l2 norm of gradients: 0.05319581546305995\n",
      "l2 norm of weights: 3.349981410866158\n",
      "---------------------\n",
      "Iteration Number: 8139\n",
      "Loss: 11.474923348735286\n",
      "l2 norm of gradients: 0.05318855088970149\n",
      "l2 norm of weights: 3.3500478353209813\n",
      "---------------------\n",
      "Iteration Number: 8140\n",
      "Loss: 11.474625667004869\n",
      "l2 norm of gradients: 0.053181288843143645\n",
      "l2 norm of weights: 3.35011425850638\n",
      "---------------------\n",
      "Iteration Number: 8141\n",
      "Loss: 11.474328099214903\n",
      "l2 norm of gradients: 0.05317402932151854\n",
      "l2 norm of weights: 3.3501806804173957\n",
      "---------------------\n",
      "Iteration Number: 8142\n",
      "Loss: 11.474030645307861\n",
      "l2 norm of gradients: 0.053166772322960416\n",
      "l2 norm of weights: 3.3502471010490766\n",
      "---------------------\n",
      "Iteration Number: 8143\n",
      "Loss: 11.473733305226254\n",
      "l2 norm of gradients: 0.053159517845605644\n",
      "l2 norm of weights: 3.3503135203964742\n",
      "---------------------\n",
      "Iteration Number: 8144\n",
      "Loss: 11.47343607891267\n",
      "l2 norm of gradients: 0.05315226588759282\n",
      "l2 norm of weights: 3.350379938454646\n",
      "---------------------\n",
      "Iteration Number: 8145\n",
      "Loss: 11.473138966309705\n",
      "l2 norm of gradients: 0.05314501644706259\n",
      "l2 norm of weights: 3.3504463552186556\n",
      "---------------------\n",
      "Iteration Number: 8146\n",
      "Loss: 11.47284196736003\n",
      "l2 norm of gradients: 0.05313776952215784\n",
      "l2 norm of weights: 3.3505127706835696\n",
      "---------------------\n",
      "Iteration Number: 8147\n",
      "Loss: 11.47254508200634\n",
      "l2 norm of gradients: 0.053130525111023556\n",
      "l2 norm of weights: 3.350579184844462\n",
      "---------------------\n",
      "Iteration Number: 8148\n",
      "Loss: 11.472248310191382\n",
      "l2 norm of gradients: 0.05312328321180687\n",
      "l2 norm of weights: 3.350645597696411\n",
      "---------------------\n",
      "Iteration Number: 8149\n",
      "Loss: 11.471951651857962\n",
      "l2 norm of gradients: 0.053116043822657076\n",
      "l2 norm of weights: 3.350712009234499\n",
      "---------------------\n",
      "Iteration Number: 8150\n",
      "Loss: 11.471655106948903\n",
      "l2 norm of gradients: 0.053108806941725616\n",
      "l2 norm of weights: 3.3507784194538144\n",
      "---------------------\n",
      "Iteration Number: 8151\n",
      "Loss: 11.4713586754071\n",
      "l2 norm of gradients: 0.053101572567165994\n",
      "l2 norm of weights: 3.350844828349451\n",
      "---------------------\n",
      "Iteration Number: 8152\n",
      "Loss: 11.471062357175464\n",
      "l2 norm of gradients: 0.05309434069713394\n",
      "l2 norm of weights: 3.350911235916508\n",
      "---------------------\n",
      "Iteration Number: 8153\n",
      "Loss: 11.470766152196987\n",
      "l2 norm of gradients: 0.05308711132978726\n",
      "l2 norm of weights: 3.350977642150088\n",
      "---------------------\n",
      "Iteration Number: 8154\n",
      "Loss: 11.470470060414678\n",
      "l2 norm of gradients: 0.0530798844632859\n",
      "l2 norm of weights: 3.3510440470453005\n",
      "---------------------\n",
      "Iteration Number: 8155\n",
      "Loss: 11.470174081771589\n",
      "l2 norm of gradients: 0.053072660095791875\n",
      "l2 norm of weights: 3.351110450597259\n",
      "---------------------\n",
      "Iteration Number: 8156\n",
      "Loss: 11.469878216210846\n",
      "l2 norm of gradients: 0.05306543822546942\n",
      "l2 norm of weights: 3.351176852801083\n",
      "---------------------\n",
      "Iteration Number: 8157\n",
      "Loss: 11.469582463675586\n",
      "l2 norm of gradients: 0.05305821885048484\n",
      "l2 norm of weights: 3.3512432536518957\n",
      "---------------------\n",
      "Iteration Number: 8158\n",
      "Loss: 11.469286824109018\n",
      "l2 norm of gradients: 0.053051001969006494\n",
      "l2 norm of weights: 3.351309653144827\n",
      "---------------------\n",
      "Iteration Number: 8159\n",
      "Loss: 11.468991297454362\n",
      "l2 norm of gradients: 0.05304378757920496\n",
      "l2 norm of weights: 3.351376051275011\n",
      "---------------------\n",
      "Iteration Number: 8160\n",
      "Loss: 11.468695883654915\n",
      "l2 norm of gradients: 0.05303657567925284\n",
      "l2 norm of weights: 3.351442448037587\n",
      "---------------------\n",
      "Iteration Number: 8161\n",
      "Loss: 11.468400582654006\n",
      "l2 norm of gradients: 0.05302936626732486\n",
      "l2 norm of weights: 3.3515088434277\n",
      "---------------------\n",
      "Iteration Number: 8162\n",
      "Loss: 11.468105394395002\n",
      "l2 norm of gradients: 0.05302215934159789\n",
      "l2 norm of weights: 3.3515752374404983\n",
      "---------------------\n",
      "Iteration Number: 8163\n",
      "Loss: 11.467810318821329\n",
      "l2 norm of gradients: 0.053014954900250835\n",
      "l2 norm of weights: 3.3516416300711374\n",
      "---------------------\n",
      "Iteration Number: 8164\n",
      "Loss: 11.467515355876442\n",
      "l2 norm of gradients: 0.05300775294146474\n",
      "l2 norm of weights: 3.3517080213147765\n",
      "---------------------\n",
      "Iteration Number: 8165\n",
      "Loss: 11.467220505503843\n",
      "l2 norm of gradients: 0.053000553463422716\n",
      "l2 norm of weights: 3.35177441116658\n",
      "---------------------\n",
      "Iteration Number: 8166\n",
      "Loss: 11.466925767647101\n",
      "l2 norm of gradients: 0.052993356464310006\n",
      "l2 norm of weights: 3.3518407996217183\n",
      "---------------------\n",
      "Iteration Number: 8167\n",
      "Loss: 11.466631142249785\n",
      "l2 norm of gradients: 0.05298616194231389\n",
      "l2 norm of weights: 3.351907186675366\n",
      "---------------------\n",
      "Iteration Number: 8168\n",
      "Loss: 11.466336629255547\n",
      "l2 norm of gradients: 0.05297896989562374\n",
      "l2 norm of weights: 3.351973572322702\n",
      "---------------------\n",
      "Iteration Number: 8169\n",
      "Loss: 11.466042228608066\n",
      "l2 norm of gradients: 0.052971780322431064\n",
      "l2 norm of weights: 3.3520399565589125\n",
      "---------------------\n",
      "Iteration Number: 8170\n",
      "Loss: 11.465747940251067\n",
      "l2 norm of gradients: 0.05296459322092937\n",
      "l2 norm of weights: 3.3521063393791857\n",
      "---------------------\n",
      "Iteration Number: 8171\n",
      "Loss: 11.465453764128318\n",
      "l2 norm of gradients: 0.0529574085893143\n",
      "l2 norm of weights: 3.3521727207787175\n",
      "---------------------\n",
      "Iteration Number: 8172\n",
      "Loss: 11.465159700183637\n",
      "l2 norm of gradients: 0.05295022642578353\n",
      "l2 norm of weights: 3.352239100752708\n",
      "---------------------\n",
      "Iteration Number: 8173\n",
      "Loss: 11.464865748360879\n",
      "l2 norm of gradients: 0.05294304672853682\n",
      "l2 norm of weights: 3.3523054792963616\n",
      "---------------------\n",
      "Iteration Number: 8174\n",
      "Loss: 11.46457190860394\n",
      "l2 norm of gradients: 0.052935869495776014\n",
      "l2 norm of weights: 3.3523718564048877\n",
      "---------------------\n",
      "Iteration Number: 8175\n",
      "Loss: 11.464278180856772\n",
      "l2 norm of gradients: 0.05292869472570499\n",
      "l2 norm of weights: 3.352438232073502\n",
      "---------------------\n",
      "Iteration Number: 8176\n",
      "Loss: 11.463984565063356\n",
      "l2 norm of gradients: 0.052921522416529705\n",
      "l2 norm of weights: 3.3525046062974244\n",
      "---------------------\n",
      "Iteration Number: 8177\n",
      "Loss: 11.463691061167731\n",
      "l2 norm of gradients: 0.05291435256645817\n",
      "l2 norm of weights: 3.35257097907188\n",
      "---------------------\n",
      "Iteration Number: 8178\n",
      "Loss: 11.463397669113961\n",
      "l2 norm of gradients: 0.05290718517370044\n",
      "l2 norm of weights: 3.3526373503920976\n",
      "---------------------\n",
      "Iteration Number: 8179\n",
      "Loss: 11.46310438884618\n",
      "l2 norm of gradients: 0.052900020236468635\n",
      "l2 norm of weights: 3.3527037202533134\n",
      "---------------------\n",
      "Iteration Number: 8180\n",
      "Loss: 11.462811220308538\n",
      "l2 norm of gradients: 0.05289285775297694\n",
      "l2 norm of weights: 3.352770088650766\n",
      "---------------------\n",
      "Iteration Number: 8181\n",
      "Loss: 11.462518163445244\n",
      "l2 norm of gradients: 0.052885697721441555\n",
      "l2 norm of weights: 3.3528364555797014\n",
      "---------------------\n",
      "Iteration Number: 8182\n",
      "Loss: 11.462225218200548\n",
      "l2 norm of gradients: 0.052878540140080706\n",
      "l2 norm of weights: 3.3529028210353684\n",
      "---------------------\n",
      "Iteration Number: 8183\n",
      "Loss: 11.461932384518738\n",
      "l2 norm of gradients: 0.052871385007114725\n",
      "l2 norm of weights: 3.3529691850130225\n",
      "---------------------\n",
      "Iteration Number: 8184\n",
      "Loss: 11.461639662344155\n",
      "l2 norm of gradients: 0.05286423232076594\n",
      "l2 norm of weights: 3.3530355475079237\n",
      "---------------------\n",
      "Iteration Number: 8185\n",
      "Loss: 11.461347051621171\n",
      "l2 norm of gradients: 0.05285708207925866\n",
      "l2 norm of weights: 3.3531019085153355\n",
      "---------------------\n",
      "Iteration Number: 8186\n",
      "Loss: 11.461054552294215\n",
      "l2 norm of gradients: 0.05284993428081936\n",
      "l2 norm of weights: 3.353168268030528\n",
      "---------------------\n",
      "Iteration Number: 8187\n",
      "Loss: 11.460762164307745\n",
      "l2 norm of gradients: 0.052842788923676456\n",
      "l2 norm of weights: 3.3532346260487764\n",
      "---------------------\n",
      "Iteration Number: 8188\n",
      "Loss: 11.46046988760627\n",
      "l2 norm of gradients: 0.052835646006060374\n",
      "l2 norm of weights: 3.3533009825653597\n",
      "---------------------\n",
      "Iteration Number: 8189\n",
      "Loss: 11.460177722134338\n",
      "l2 norm of gradients: 0.05282850552620358\n",
      "l2 norm of weights: 3.353367337575562\n",
      "---------------------\n",
      "Iteration Number: 8190\n",
      "Loss: 11.459885667836557\n",
      "l2 norm of gradients: 0.052821367482340606\n",
      "l2 norm of weights: 3.3534336910746743\n",
      "---------------------\n",
      "Iteration Number: 8191\n",
      "Loss: 11.459593724657545\n",
      "l2 norm of gradients: 0.052814231872707944\n",
      "l2 norm of weights: 3.353500043057989\n",
      "---------------------\n",
      "Iteration Number: 8192\n",
      "Loss: 11.45930189254199\n",
      "l2 norm of gradients: 0.05280709869554413\n",
      "l2 norm of weights: 3.353566393520806\n",
      "---------------------\n",
      "Iteration Number: 8193\n",
      "Loss: 11.459010171434613\n",
      "l2 norm of gradients: 0.05279996794908969\n",
      "l2 norm of weights: 3.3536327424584305\n",
      "---------------------\n",
      "Iteration Number: 8194\n",
      "Loss: 11.458718561280168\n",
      "l2 norm of gradients: 0.052792839631587214\n",
      "l2 norm of weights: 3.35369908986617\n",
      "---------------------\n",
      "Iteration Number: 8195\n",
      "Loss: 11.458427062023485\n",
      "l2 norm of gradients: 0.052785713741281194\n",
      "l2 norm of weights: 3.3537654357393394\n",
      "---------------------\n",
      "Iteration Number: 8196\n",
      "Loss: 11.458135673609396\n",
      "l2 norm of gradients: 0.0527785902764182\n",
      "l2 norm of weights: 3.353831780073257\n",
      "---------------------\n",
      "Iteration Number: 8197\n",
      "Loss: 11.457844395982804\n",
      "l2 norm of gradients: 0.05277146923524683\n",
      "l2 norm of weights: 3.3538981228632476\n",
      "---------------------\n",
      "Iteration Number: 8198\n",
      "Loss: 11.457553229088639\n",
      "l2 norm of gradients: 0.05276435061601762\n",
      "l2 norm of weights: 3.353964464104639\n",
      "---------------------\n",
      "Iteration Number: 8199\n",
      "Loss: 11.457262172871873\n",
      "l2 norm of gradients: 0.05275723441698309\n",
      "l2 norm of weights: 3.354030803792765\n",
      "---------------------\n",
      "Iteration Number: 8200\n",
      "Loss: 11.456971227277542\n",
      "l2 norm of gradients: 0.052750120636397814\n",
      "l2 norm of weights: 3.354097141922965\n",
      "---------------------\n",
      "Iteration Number: 8201\n",
      "Loss: 11.45668039225069\n",
      "l2 norm of gradients: 0.052743009272518314\n",
      "l2 norm of weights: 3.3541634784905803\n",
      "---------------------\n",
      "Iteration Number: 8202\n",
      "Loss: 11.45638966773643\n",
      "l2 norm of gradients: 0.05273590032360309\n",
      "l2 norm of weights: 3.354229813490961\n",
      "---------------------\n",
      "Iteration Number: 8203\n",
      "Loss: 11.456099053679923\n",
      "l2 norm of gradients: 0.05272879378791266\n",
      "l2 norm of weights: 3.3542961469194594\n",
      "---------------------\n",
      "Iteration Number: 8204\n",
      "Loss: 11.455808550026337\n",
      "l2 norm of gradients: 0.05272168966370949\n",
      "l2 norm of weights: 3.3543624787714332\n",
      "---------------------\n",
      "Iteration Number: 8205\n",
      "Loss: 11.455518156720915\n",
      "l2 norm of gradients: 0.052714587949258034\n",
      "l2 norm of weights: 3.354428809042246\n",
      "---------------------\n",
      "Iteration Number: 8206\n",
      "Loss: 11.455227873708926\n",
      "l2 norm of gradients: 0.05270748864282473\n",
      "l2 norm of weights: 3.3544951377272656\n",
      "---------------------\n",
      "Iteration Number: 8207\n",
      "Loss: 11.454937700935687\n",
      "l2 norm of gradients: 0.05270039174267799\n",
      "l2 norm of weights: 3.354561464821863\n",
      "---------------------\n",
      "Iteration Number: 8208\n",
      "Loss: 11.45464763834656\n",
      "l2 norm of gradients: 0.052693297247088156\n",
      "l2 norm of weights: 3.3546277903214174\n",
      "---------------------\n",
      "Iteration Number: 8209\n",
      "Loss: 11.45435768588694\n",
      "l2 norm of gradients: 0.052686205154327606\n",
      "l2 norm of weights: 3.35469411422131\n",
      "---------------------\n",
      "Iteration Number: 8210\n",
      "Loss: 11.454067843502276\n",
      "l2 norm of gradients: 0.0526791154626706\n",
      "l2 norm of weights: 3.354760436516928\n",
      "---------------------\n",
      "Iteration Number: 8211\n",
      "Loss: 11.453778111138048\n",
      "l2 norm of gradients: 0.05267202817039342\n",
      "l2 norm of weights: 3.354826757203664\n",
      "---------------------\n",
      "Iteration Number: 8212\n",
      "Loss: 11.453488488739781\n",
      "l2 norm of gradients: 0.05266494327577431\n",
      "l2 norm of weights: 3.3548930762769134\n",
      "---------------------\n",
      "Iteration Number: 8213\n",
      "Loss: 11.453198976253043\n",
      "l2 norm of gradients: 0.05265786077709342\n",
      "l2 norm of weights: 3.3549593937320785\n",
      "---------------------\n",
      "Iteration Number: 8214\n",
      "Loss: 11.452909573623453\n",
      "l2 norm of gradients: 0.05265078067263288\n",
      "l2 norm of weights: 3.3550257095645657\n",
      "---------------------\n",
      "Iteration Number: 8215\n",
      "Loss: 11.452620280796646\n",
      "l2 norm of gradients: 0.052643702960676754\n",
      "l2 norm of weights: 3.355092023769786\n",
      "---------------------\n",
      "Iteration Number: 8216\n",
      "Loss: 11.452331097718329\n",
      "l2 norm of gradients: 0.05263662763951108\n",
      "l2 norm of weights: 3.355158336343156\n",
      "---------------------\n",
      "Iteration Number: 8217\n",
      "Loss: 11.452042024334226\n",
      "l2 norm of gradients: 0.052629554707423816\n",
      "l2 norm of weights: 3.355224647280096\n",
      "---------------------\n",
      "Iteration Number: 8218\n",
      "Loss: 11.451753060590116\n",
      "l2 norm of gradients: 0.05262248416270489\n",
      "l2 norm of weights: 3.3552909565760314\n",
      "---------------------\n",
      "Iteration Number: 8219\n",
      "Loss: 11.45146420643183\n",
      "l2 norm of gradients: 0.05261541600364612\n",
      "l2 norm of weights: 3.355357264226393\n",
      "---------------------\n",
      "Iteration Number: 8220\n",
      "Loss: 11.451175461805214\n",
      "l2 norm of gradients: 0.05260835022854132\n",
      "l2 norm of weights: 3.3554235702266157\n",
      "---------------------\n",
      "Iteration Number: 8221\n",
      "Loss: 11.450886826656166\n",
      "l2 norm of gradients: 0.0526012868356862\n",
      "l2 norm of weights: 3.3554898745721395\n",
      "---------------------\n",
      "Iteration Number: 8222\n",
      "Loss: 11.450598300930638\n",
      "l2 norm of gradients: 0.05259422582337837\n",
      "l2 norm of weights: 3.3555561772584093\n",
      "---------------------\n",
      "Iteration Number: 8223\n",
      "Loss: 11.450309884574615\n",
      "l2 norm of gradients: 0.05258716718991746\n",
      "l2 norm of weights: 3.3556224782808743\n",
      "---------------------\n",
      "Iteration Number: 8224\n",
      "Loss: 11.450021577534114\n",
      "l2 norm of gradients: 0.05258011093360491\n",
      "l2 norm of weights: 3.3556887776349895\n",
      "---------------------\n",
      "Iteration Number: 8225\n",
      "Loss: 11.449733379755203\n",
      "l2 norm of gradients: 0.052573057052744177\n",
      "l2 norm of weights: 3.3557550753162135\n",
      "---------------------\n",
      "Iteration Number: 8226\n",
      "Loss: 11.449445291183999\n",
      "l2 norm of gradients: 0.0525660055456406\n",
      "l2 norm of weights: 3.3558213713200105\n",
      "---------------------\n",
      "Iteration Number: 8227\n",
      "Loss: 11.449157311766644\n",
      "l2 norm of gradients: 0.05255895641060143\n",
      "l2 norm of weights: 3.3558876656418484\n",
      "---------------------\n",
      "Iteration Number: 8228\n",
      "Loss: 11.448869441449318\n",
      "l2 norm of gradients: 0.052551909645935826\n",
      "l2 norm of weights: 3.355953958277201\n",
      "---------------------\n",
      "Iteration Number: 8229\n",
      "Loss: 11.448581680178272\n",
      "l2 norm of gradients: 0.05254486524995487\n",
      "l2 norm of weights: 3.356020249221546\n",
      "---------------------\n",
      "Iteration Number: 8230\n",
      "Loss: 11.44829402789977\n",
      "l2 norm of gradients: 0.052537823220971516\n",
      "l2 norm of weights: 3.3560865384703673\n",
      "---------------------\n",
      "Iteration Number: 8231\n",
      "Loss: 11.44800648456012\n",
      "l2 norm of gradients: 0.05253078355730072\n",
      "l2 norm of weights: 3.3561528260191515\n",
      "---------------------\n",
      "Iteration Number: 8232\n",
      "Loss: 11.447719050105679\n",
      "l2 norm of gradients: 0.05252374625725925\n",
      "l2 norm of weights: 3.356219111863391\n",
      "---------------------\n",
      "Iteration Number: 8233\n",
      "Loss: 11.447431724482847\n",
      "l2 norm of gradients: 0.05251671131916579\n",
      "l2 norm of weights: 3.356285395998583\n",
      "---------------------\n",
      "Iteration Number: 8234\n",
      "Loss: 11.447144507638061\n",
      "l2 norm of gradients: 0.052509678741340944\n",
      "l2 norm of weights: 3.35635167842023\n",
      "---------------------\n",
      "Iteration Number: 8235\n",
      "Loss: 11.446857399517786\n",
      "l2 norm of gradients: 0.05250264852210721\n",
      "l2 norm of weights: 3.356417959123837\n",
      "---------------------\n",
      "Iteration Number: 8236\n",
      "Loss: 11.44657040006856\n",
      "l2 norm of gradients: 0.05249562065978894\n",
      "l2 norm of weights: 3.356484238104916\n",
      "---------------------\n",
      "Iteration Number: 8237\n",
      "Loss: 11.446283509236922\n",
      "l2 norm of gradients: 0.052488595152712425\n",
      "l2 norm of weights: 3.356550515358983\n",
      "---------------------\n",
      "Iteration Number: 8238\n",
      "Loss: 11.445996726969481\n",
      "l2 norm of gradients: 0.0524815719992058\n",
      "l2 norm of weights: 3.3566167908815587\n",
      "---------------------\n",
      "Iteration Number: 8239\n",
      "Loss: 11.445710053212869\n",
      "l2 norm of gradients: 0.05247455119759913\n",
      "l2 norm of weights: 3.3566830646681685\n",
      "---------------------\n",
      "Iteration Number: 8240\n",
      "Loss: 11.445423487913784\n",
      "l2 norm of gradients: 0.052467532746224316\n",
      "l2 norm of weights: 3.356749336714342\n",
      "---------------------\n",
      "Iteration Number: 8241\n",
      "Loss: 11.445137031018927\n",
      "l2 norm of gradients: 0.052460516643415125\n",
      "l2 norm of weights: 3.3568156070156143\n",
      "---------------------\n",
      "Iteration Number: 8242\n",
      "Loss: 11.44485068247507\n",
      "l2 norm of gradients: 0.05245350288750728\n",
      "l2 norm of weights: 3.3568818755675247\n",
      "---------------------\n",
      "Iteration Number: 8243\n",
      "Loss: 11.444564442229021\n",
      "l2 norm of gradients: 0.05244649147683834\n",
      "l2 norm of weights: 3.356948142365617\n",
      "---------------------\n",
      "Iteration Number: 8244\n",
      "Loss: 11.444278310227615\n",
      "l2 norm of gradients: 0.05243948240974766\n",
      "l2 norm of weights: 3.3570144074054404\n",
      "---------------------\n",
      "Iteration Number: 8245\n",
      "Loss: 11.443992286417734\n",
      "l2 norm of gradients: 0.05243247568457659\n",
      "l2 norm of weights: 3.357080670682548\n",
      "---------------------\n",
      "Iteration Number: 8246\n",
      "Loss: 11.4437063707463\n",
      "l2 norm of gradients: 0.05242547129966818\n",
      "l2 norm of weights: 3.357146932192498\n",
      "---------------------\n",
      "Iteration Number: 8247\n",
      "Loss: 11.443420563160284\n",
      "l2 norm of gradients: 0.052418469253367544\n",
      "l2 norm of weights: 3.3572131919308537\n",
      "---------------------\n",
      "Iteration Number: 8248\n",
      "Loss: 11.44313486360669\n",
      "l2 norm of gradients: 0.05241146954402152\n",
      "l2 norm of weights: 3.3572794498931824\n",
      "---------------------\n",
      "Iteration Number: 8249\n",
      "Loss: 11.442849272032554\n",
      "l2 norm of gradients: 0.052404472169978795\n",
      "l2 norm of weights: 3.3573457060750553\n",
      "---------------------\n",
      "Iteration Number: 8250\n",
      "Loss: 11.442563788384973\n",
      "l2 norm of gradients: 0.052397477129589975\n",
      "l2 norm of weights: 3.35741196047205\n",
      "---------------------\n",
      "Iteration Number: 8251\n",
      "Loss: 11.442278412611056\n",
      "l2 norm of gradients: 0.05239048442120748\n",
      "l2 norm of weights: 3.3574782130797476\n",
      "---------------------\n",
      "Iteration Number: 8252\n",
      "Loss: 11.441993144657978\n",
      "l2 norm of gradients: 0.05238349404318564\n",
      "l2 norm of weights: 3.3575444638937344\n",
      "---------------------\n",
      "Iteration Number: 8253\n",
      "Loss: 11.441707984472945\n",
      "l2 norm of gradients: 0.052376505993880516\n",
      "l2 norm of weights: 3.3576107129096004\n",
      "---------------------\n",
      "Iteration Number: 8254\n",
      "Loss: 11.441422932003196\n",
      "l2 norm of gradients: 0.05236952027165014\n",
      "l2 norm of weights: 3.357676960122942\n",
      "---------------------\n",
      "Iteration Number: 8255\n",
      "Loss: 11.441137987196019\n",
      "l2 norm of gradients: 0.05236253687485426\n",
      "l2 norm of weights: 3.3577432055293586\n",
      "---------------------\n",
      "Iteration Number: 8256\n",
      "Loss: 11.440853149998741\n",
      "l2 norm of gradients: 0.052355555801854574\n",
      "l2 norm of weights: 3.3578094491244546\n",
      "---------------------\n",
      "Iteration Number: 8257\n",
      "Loss: 11.440568420358716\n",
      "l2 norm of gradients: 0.05234857705101456\n",
      "l2 norm of weights: 3.357875690903839\n",
      "---------------------\n",
      "Iteration Number: 8258\n",
      "Loss: 11.440283798223367\n",
      "l2 norm of gradients: 0.05234160062069952\n",
      "l2 norm of weights: 3.357941930863126\n",
      "---------------------\n",
      "Iteration Number: 8259\n",
      "Loss: 11.439999283540121\n",
      "l2 norm of gradients: 0.052334626509276616\n",
      "l2 norm of weights: 3.358008168997934\n",
      "---------------------\n",
      "Iteration Number: 8260\n",
      "Loss: 11.439714876256472\n",
      "l2 norm of gradients: 0.05232765471511481\n",
      "l2 norm of weights: 3.3580744053038862\n",
      "---------------------\n",
      "Iteration Number: 8261\n",
      "Loss: 11.439430576319943\n",
      "l2 norm of gradients: 0.05232068523658489\n",
      "l2 norm of weights: 3.3581406397766096\n",
      "---------------------\n",
      "Iteration Number: 8262\n",
      "Loss: 11.43914638367809\n",
      "l2 norm of gradients: 0.05231371807205951\n",
      "l2 norm of weights: 3.3582068724117367\n",
      "---------------------\n",
      "Iteration Number: 8263\n",
      "Loss: 11.438862298278528\n",
      "l2 norm of gradients: 0.052306753219913134\n",
      "l2 norm of weights: 3.3582731032049047\n",
      "---------------------\n",
      "Iteration Number: 8264\n",
      "Loss: 11.438578320068883\n",
      "l2 norm of gradients: 0.05229979067852195\n",
      "l2 norm of weights: 3.3583393321517545\n",
      "---------------------\n",
      "Iteration Number: 8265\n",
      "Loss: 11.438294448996855\n",
      "l2 norm of gradients: 0.05229283044626408\n",
      "l2 norm of weights: 3.358405559247932\n",
      "---------------------\n",
      "Iteration Number: 8266\n",
      "Loss: 11.438010685010159\n",
      "l2 norm of gradients: 0.052285872521519394\n",
      "l2 norm of weights: 3.358471784489088\n",
      "---------------------\n",
      "Iteration Number: 8267\n",
      "Loss: 11.437727028056548\n",
      "l2 norm of gradients: 0.052278916902669614\n",
      "l2 norm of weights: 3.3585380078708775\n",
      "---------------------\n",
      "Iteration Number: 8268\n",
      "Loss: 11.437443478083837\n",
      "l2 norm of gradients: 0.0522719635880982\n",
      "l2 norm of weights: 3.3586042293889604\n",
      "---------------------\n",
      "Iteration Number: 8269\n",
      "Loss: 11.437160035039856\n",
      "l2 norm of gradients: 0.052265012576190484\n",
      "l2 norm of weights: 3.3586704490390007\n",
      "---------------------\n",
      "Iteration Number: 8270\n",
      "Loss: 11.436876698872485\n",
      "l2 norm of gradients: 0.052258063865333576\n",
      "l2 norm of weights: 3.358736666816667\n",
      "---------------------\n",
      "Iteration Number: 8271\n",
      "Loss: 11.43659346952965\n",
      "l2 norm of gradients: 0.05225111745391635\n",
      "l2 norm of weights: 3.3588028827176335\n",
      "---------------------\n",
      "Iteration Number: 8272\n",
      "Loss: 11.436310346959296\n",
      "l2 norm of gradients: 0.052244173340329554\n",
      "l2 norm of weights: 3.3588690967375774\n",
      "---------------------\n",
      "Iteration Number: 8273\n",
      "Loss: 11.436027331109429\n",
      "l2 norm of gradients: 0.052237231522965666\n",
      "l2 norm of weights: 3.358935308872181\n",
      "---------------------\n",
      "Iteration Number: 8274\n",
      "Loss: 11.435744421928083\n",
      "l2 norm of gradients: 0.05223029200021897\n",
      "l2 norm of weights: 3.359001519117132\n",
      "---------------------\n",
      "Iteration Number: 8275\n",
      "Loss: 11.435461619363338\n",
      "l2 norm of gradients: 0.05222335477048552\n",
      "l2 norm of weights: 3.3590677274681213\n",
      "---------------------\n",
      "Iteration Number: 8276\n",
      "Loss: 11.435178923363305\n",
      "l2 norm of gradients: 0.05221641983216323\n",
      "l2 norm of weights: 3.3591339339208455\n",
      "---------------------\n",
      "Iteration Number: 8277\n",
      "Loss: 11.434896333876132\n",
      "l2 norm of gradients: 0.05220948718365168\n",
      "l2 norm of weights: 3.359200138471005\n",
      "---------------------\n",
      "Iteration Number: 8278\n",
      "Loss: 11.434613850850019\n",
      "l2 norm of gradients: 0.05220255682335234\n",
      "l2 norm of weights: 3.359266341114304\n",
      "---------------------\n",
      "Iteration Number: 8279\n",
      "Loss: 11.43433147423319\n",
      "l2 norm of gradients: 0.052195628749668425\n",
      "l2 norm of weights: 3.359332541846454\n",
      "---------------------\n",
      "Iteration Number: 8280\n",
      "Loss: 11.434049203973924\n",
      "l2 norm of gradients: 0.05218870296100484\n",
      "l2 norm of weights: 3.359398740663168\n",
      "---------------------\n",
      "Iteration Number: 8281\n",
      "Loss: 11.43376704002053\n",
      "l2 norm of gradients: 0.05218177945576844\n",
      "l2 norm of weights: 3.359464937560164\n",
      "---------------------\n",
      "Iteration Number: 8282\n",
      "Loss: 11.433484982321342\n",
      "l2 norm of gradients: 0.05217485823236767\n",
      "l2 norm of weights: 3.3595311325331667\n",
      "---------------------\n",
      "Iteration Number: 8283\n",
      "Loss: 11.433203030824762\n",
      "l2 norm of gradients: 0.052167939289212865\n",
      "l2 norm of weights: 3.359597325577903\n",
      "---------------------\n",
      "Iteration Number: 8284\n",
      "Loss: 11.432921185479213\n",
      "l2 norm of gradients: 0.05216102262471604\n",
      "l2 norm of weights: 3.3596635166901043\n",
      "---------------------\n",
      "Iteration Number: 8285\n",
      "Loss: 11.432639446233148\n",
      "l2 norm of gradients: 0.05215410823729108\n",
      "l2 norm of weights: 3.359729705865509\n",
      "---------------------\n",
      "Iteration Number: 8286\n",
      "Loss: 11.432357813035086\n",
      "l2 norm of gradients: 0.05214719612535348\n",
      "l2 norm of weights: 3.3597958930998564\n",
      "---------------------\n",
      "Iteration Number: 8287\n",
      "Loss: 11.432076285833563\n",
      "l2 norm of gradients: 0.052140286287320634\n",
      "l2 norm of weights: 3.3598620783888933\n",
      "---------------------\n",
      "Iteration Number: 8288\n",
      "Loss: 11.431794864577148\n",
      "l2 norm of gradients: 0.05213337872161159\n",
      "l2 norm of weights: 3.3599282617283692\n",
      "---------------------\n",
      "Iteration Number: 8289\n",
      "Loss: 11.431513549214468\n",
      "l2 norm of gradients: 0.05212647342664725\n",
      "l2 norm of weights: 3.359994443114039\n",
      "---------------------\n",
      "Iteration Number: 8290\n",
      "Loss: 11.43123233969419\n",
      "l2 norm of gradients: 0.05211957040085015\n",
      "l2 norm of weights: 3.3600606225416616\n",
      "---------------------\n",
      "Iteration Number: 8291\n",
      "Loss: 11.430951235964994\n",
      "l2 norm of gradients: 0.05211266964264465\n",
      "l2 norm of weights: 3.360126800007001\n",
      "---------------------\n",
      "Iteration Number: 8292\n",
      "Loss: 11.430670237975624\n",
      "l2 norm of gradients: 0.052105771150456885\n",
      "l2 norm of weights: 3.360192975505824\n",
      "---------------------\n",
      "Iteration Number: 8293\n",
      "Loss: 11.430389345674843\n",
      "l2 norm of gradients: 0.05209887492271462\n",
      "l2 norm of weights: 3.3602591490339035\n",
      "---------------------\n",
      "Iteration Number: 8294\n",
      "Loss: 11.430108559011472\n",
      "l2 norm of gradients: 0.05209198095784745\n",
      "l2 norm of weights: 3.360325320587017\n",
      "---------------------\n",
      "Iteration Number: 8295\n",
      "Loss: 11.42982787793435\n",
      "l2 norm of gradients: 0.05208508925428668\n",
      "l2 norm of weights: 3.360391490160945\n",
      "---------------------\n",
      "Iteration Number: 8296\n",
      "Loss: 11.429547302392375\n",
      "l2 norm of gradients: 0.05207819981046536\n",
      "l2 norm of weights: 3.3604576577514735\n",
      "---------------------\n",
      "Iteration Number: 8297\n",
      "Loss: 11.429266832334454\n",
      "l2 norm of gradients: 0.05207131262481825\n",
      "l2 norm of weights: 3.360523823354393\n",
      "---------------------\n",
      "Iteration Number: 8298\n",
      "Loss: 11.42898646770958\n",
      "l2 norm of gradients: 0.052064427695781845\n",
      "l2 norm of weights: 3.3605899869654974\n",
      "---------------------\n",
      "Iteration Number: 8299\n",
      "Loss: 11.428706208466725\n",
      "l2 norm of gradients: 0.05205754502179441\n",
      "l2 norm of weights: 3.3606561485805866\n",
      "---------------------\n",
      "Iteration Number: 8300\n",
      "Loss: 11.428426054554944\n",
      "l2 norm of gradients: 0.05205066460129588\n",
      "l2 norm of weights: 3.3607223081954634\n",
      "---------------------\n",
      "Iteration Number: 8301\n",
      "Loss: 11.428146005923317\n",
      "l2 norm of gradients: 0.052043786432727925\n",
      "l2 norm of weights: 3.3607884658059355\n",
      "---------------------\n",
      "Iteration Number: 8302\n",
      "Loss: 11.42786606252096\n",
      "l2 norm of gradients: 0.05203691051453396\n",
      "l2 norm of weights: 3.360854621407816\n",
      "---------------------\n",
      "Iteration Number: 8303\n",
      "Loss: 11.427586224297016\n",
      "l2 norm of gradients: 0.0520300368451591\n",
      "l2 norm of weights: 3.3609207749969205\n",
      "---------------------\n",
      "Iteration Number: 8304\n",
      "Loss: 11.427306491200689\n",
      "l2 norm of gradients: 0.052023165423050174\n",
      "l2 norm of weights: 3.3609869265690713\n",
      "---------------------\n",
      "Iteration Number: 8305\n",
      "Loss: 11.427026863181196\n",
      "l2 norm of gradients: 0.05201629624665573\n",
      "l2 norm of weights: 3.361053076120093\n",
      "---------------------\n",
      "Iteration Number: 8306\n",
      "Loss: 11.426747340187807\n",
      "l2 norm of gradients: 0.05200942931442597\n",
      "l2 norm of weights: 3.3611192236458156\n",
      "---------------------\n",
      "Iteration Number: 8307\n",
      "Loss: 11.426467922169836\n",
      "l2 norm of gradients: 0.05200256462481292\n",
      "l2 norm of weights: 3.3611853691420737\n",
      "---------------------\n",
      "Iteration Number: 8308\n",
      "Loss: 11.426188609076629\n",
      "l2 norm of gradients: 0.05199570217627023\n",
      "l2 norm of weights: 3.3612515126047064\n",
      "---------------------\n",
      "Iteration Number: 8309\n",
      "Loss: 11.42590940085755\n",
      "l2 norm of gradients: 0.05198884196725325\n",
      "l2 norm of weights: 3.3613176540295555\n",
      "---------------------\n",
      "Iteration Number: 8310\n",
      "Loss: 11.425630297462037\n",
      "l2 norm of gradients: 0.05198198399621906\n",
      "l2 norm of weights: 3.3613837934124695\n",
      "---------------------\n",
      "Iteration Number: 8311\n",
      "Loss: 11.425351298839526\n",
      "l2 norm of gradients: 0.05197512826162642\n",
      "l2 norm of weights: 3.3614499307492998\n",
      "---------------------\n",
      "Iteration Number: 8312\n",
      "Loss: 11.425072404939526\n",
      "l2 norm of gradients: 0.051968274761935794\n",
      "l2 norm of weights: 3.3615160660359025\n",
      "---------------------\n",
      "Iteration Number: 8313\n",
      "Loss: 11.42479361571156\n",
      "l2 norm of gradients: 0.05196142349560932\n",
      "l2 norm of weights: 3.361582199268138\n",
      "---------------------\n",
      "Iteration Number: 8314\n",
      "Loss: 11.424514931105202\n",
      "l2 norm of gradients: 0.051954574461110875\n",
      "l2 norm of weights: 3.361648330441871\n",
      "---------------------\n",
      "Iteration Number: 8315\n",
      "Loss: 11.424236351070059\n",
      "l2 norm of gradients: 0.051947727656905966\n",
      "l2 norm of weights: 3.3617144595529718\n",
      "---------------------\n",
      "Iteration Number: 8316\n",
      "Loss: 11.42395787555577\n",
      "l2 norm of gradients: 0.05194088308146181\n",
      "l2 norm of weights: 3.3617805865973134\n",
      "---------------------\n",
      "Iteration Number: 8317\n",
      "Loss: 11.423679504512018\n",
      "l2 norm of gradients: 0.0519340407332473\n",
      "l2 norm of weights: 3.361846711570773\n",
      "---------------------\n",
      "Iteration Number: 8318\n",
      "Loss: 11.423401237888514\n",
      "l2 norm of gradients: 0.05192720061073303\n",
      "l2 norm of weights: 3.3619128344692335\n",
      "---------------------\n",
      "Iteration Number: 8319\n",
      "Loss: 11.423123075635033\n",
      "l2 norm of gradients: 0.051920362712391255\n",
      "l2 norm of weights: 3.361978955288582\n",
      "---------------------\n",
      "Iteration Number: 8320\n",
      "Loss: 11.422845017701352\n",
      "l2 norm of gradients: 0.05191352703669591\n",
      "l2 norm of weights: 3.3620450740247083\n",
      "---------------------\n",
      "Iteration Number: 8321\n",
      "Loss: 11.422567064037313\n",
      "l2 norm of gradients: 0.051906693582122604\n",
      "l2 norm of weights: 3.362111190673509\n",
      "---------------------\n",
      "Iteration Number: 8322\n",
      "Loss: 11.42228921459277\n",
      "l2 norm of gradients: 0.05189986234714858\n",
      "l2 norm of weights: 3.3621773052308823\n",
      "---------------------\n",
      "Iteration Number: 8323\n",
      "Loss: 11.422011469317633\n",
      "l2 norm of gradients: 0.05189303333025283\n",
      "l2 norm of weights: 3.3622434176927336\n",
      "---------------------\n",
      "Iteration Number: 8324\n",
      "Loss: 11.421733828161852\n",
      "l2 norm of gradients: 0.05188620652991592\n",
      "l2 norm of weights: 3.36230952805497\n",
      "---------------------\n",
      "Iteration Number: 8325\n",
      "Loss: 11.421456291075389\n",
      "l2 norm of gradients: 0.051879381944620176\n",
      "l2 norm of weights: 3.362375636313504\n",
      "---------------------\n",
      "Iteration Number: 8326\n",
      "Loss: 11.42117885800828\n",
      "l2 norm of gradients: 0.05187255957284948\n",
      "l2 norm of weights: 3.362441742464253\n",
      "---------------------\n",
      "Iteration Number: 8327\n",
      "Loss: 11.420901528910564\n",
      "l2 norm of gradients: 0.05186573941308946\n",
      "l2 norm of weights: 3.3625078465031377\n",
      "---------------------\n",
      "Iteration Number: 8328\n",
      "Loss: 11.420624303732344\n",
      "l2 norm of gradients: 0.05185892146382735\n",
      "l2 norm of weights: 3.362573948426084\n",
      "---------------------\n",
      "Iteration Number: 8329\n",
      "Loss: 11.420347182423733\n",
      "l2 norm of gradients: 0.05185210572355205\n",
      "l2 norm of weights: 3.362640048229021\n",
      "---------------------\n",
      "Iteration Number: 8330\n",
      "Loss: 11.420070164934904\n",
      "l2 norm of gradients: 0.051845292190754136\n",
      "l2 norm of weights: 3.3627061459078837\n",
      "---------------------\n",
      "Iteration Number: 8331\n",
      "Loss: 11.419793251216046\n",
      "l2 norm of gradients: 0.05183848086392578\n",
      "l2 norm of weights: 3.362772241458609\n",
      "---------------------\n",
      "Iteration Number: 8332\n",
      "Loss: 11.41951644121741\n",
      "l2 norm of gradients: 0.051831671741560836\n",
      "l2 norm of weights: 3.362838334877141\n",
      "---------------------\n",
      "Iteration Number: 8333\n",
      "Loss: 11.419239734889274\n",
      "l2 norm of gradients: 0.05182486482215485\n",
      "l2 norm of weights: 3.362904426159426\n",
      "---------------------\n",
      "Iteration Number: 8334\n",
      "Loss: 11.418963132181934\n",
      "l2 norm of gradients: 0.0518180601042049\n",
      "l2 norm of weights: 3.362970515301414\n",
      "---------------------\n",
      "Iteration Number: 8335\n",
      "Loss: 11.418686633045743\n",
      "l2 norm of gradients: 0.05181125758620977\n",
      "l2 norm of weights: 3.3630366022990614\n",
      "---------------------\n",
      "Iteration Number: 8336\n",
      "Loss: 11.418410237431091\n",
      "l2 norm of gradients: 0.05180445726666989\n",
      "l2 norm of weights: 3.363102687148328\n",
      "---------------------\n",
      "Iteration Number: 8337\n",
      "Loss: 11.41813394528839\n",
      "l2 norm of gradients: 0.0517976591440873\n",
      "l2 norm of weights: 3.3631687698451773\n",
      "---------------------\n",
      "Iteration Number: 8338\n",
      "Loss: 11.417857756568113\n",
      "l2 norm of gradients: 0.05179086321696564\n",
      "l2 norm of weights: 3.363234850385578\n",
      "---------------------\n",
      "Iteration Number: 8339\n",
      "Loss: 11.417581671220734\n",
      "l2 norm of gradients: 0.051784069483810286\n",
      "l2 norm of weights: 3.363300928765501\n",
      "---------------------\n",
      "Iteration Number: 8340\n",
      "Loss: 11.417305689196795\n",
      "l2 norm of gradients: 0.051777277943128135\n",
      "l2 norm of weights: 3.3633670049809252\n",
      "---------------------\n",
      "Iteration Number: 8341\n",
      "Loss: 11.417029810446868\n",
      "l2 norm of gradients: 0.051770488593427734\n",
      "l2 norm of weights: 3.363433079027829\n",
      "---------------------\n",
      "Iteration Number: 8342\n",
      "Loss: 11.416754034921544\n",
      "l2 norm of gradients: 0.0517637014332193\n",
      "l2 norm of weights: 3.363499150902199\n",
      "---------------------\n",
      "Iteration Number: 8343\n",
      "Loss: 11.416478362571473\n",
      "l2 norm of gradients: 0.05175691646101463\n",
      "l2 norm of weights: 3.3635652206000244\n",
      "---------------------\n",
      "Iteration Number: 8344\n",
      "Loss: 11.416202793347338\n",
      "l2 norm of gradients: 0.05175013367532711\n",
      "l2 norm of weights: 3.3636312881172987\n",
      "---------------------\n",
      "Iteration Number: 8345\n",
      "Loss: 11.415927327199835\n",
      "l2 norm of gradients: 0.05174335307467184\n",
      "l2 norm of weights: 3.3636973534500196\n",
      "---------------------\n",
      "Iteration Number: 8346\n",
      "Loss: 11.415651964079723\n",
      "l2 norm of gradients: 0.05173657465756541\n",
      "l2 norm of weights: 3.363763416594189\n",
      "---------------------\n",
      "Iteration Number: 8347\n",
      "Loss: 11.41537670393778\n",
      "l2 norm of gradients: 0.05172979842252612\n",
      "l2 norm of weights: 3.3638294775458126\n",
      "---------------------\n",
      "Iteration Number: 8348\n",
      "Loss: 11.41510154672484\n",
      "l2 norm of gradients: 0.05172302436807381\n",
      "l2 norm of weights: 3.3638955363009018\n",
      "---------------------\n",
      "Iteration Number: 8349\n",
      "Loss: 11.414826492391748\n",
      "l2 norm of gradients: 0.051716252492729986\n",
      "l2 norm of weights: 3.3639615928554707\n",
      "---------------------\n",
      "Iteration Number: 8350\n",
      "Loss: 11.4145515408894\n",
      "l2 norm of gradients: 0.051709482795017685\n",
      "l2 norm of weights: 3.3640276472055377\n",
      "---------------------\n",
      "Iteration Number: 8351\n",
      "Loss: 11.414276692168732\n",
      "l2 norm of gradients: 0.051702715273461645\n",
      "l2 norm of weights: 3.364093699347127\n",
      "---------------------\n",
      "Iteration Number: 8352\n",
      "Loss: 11.414001946180711\n",
      "l2 norm of gradients: 0.051695949926588124\n",
      "l2 norm of weights: 3.3641597492762645\n",
      "---------------------\n",
      "Iteration Number: 8353\n",
      "Loss: 11.413727302876335\n",
      "l2 norm of gradients: 0.051689186752924984\n",
      "l2 norm of weights: 3.364225796988982\n",
      "---------------------\n",
      "Iteration Number: 8354\n",
      "Loss: 11.413452762206635\n",
      "l2 norm of gradients: 0.05168242575100171\n",
      "l2 norm of weights: 3.364291842481315\n",
      "---------------------\n",
      "Iteration Number: 8355\n",
      "Loss: 11.4131783241227\n",
      "l2 norm of gradients: 0.05167566691934935\n",
      "l2 norm of weights: 3.3643578857493037\n",
      "---------------------\n",
      "Iteration Number: 8356\n",
      "Loss: 11.412903988575634\n",
      "l2 norm of gradients: 0.05166891025650061\n",
      "l2 norm of weights: 3.3644239267889917\n",
      "---------------------\n",
      "Iteration Number: 8357\n",
      "Loss: 11.412629755516571\n",
      "l2 norm of gradients: 0.05166215576098968\n",
      "l2 norm of weights: 3.3644899655964267\n",
      "---------------------\n",
      "Iteration Number: 8358\n",
      "Loss: 11.412355624896716\n",
      "l2 norm of gradients: 0.05165540343135238\n",
      "l2 norm of weights: 3.364556002167662\n",
      "---------------------\n",
      "Iteration Number: 8359\n",
      "Loss: 11.412081596667267\n",
      "l2 norm of gradients: 0.05164865326612613\n",
      "l2 norm of weights: 3.3646220364987522\n",
      "---------------------\n",
      "Iteration Number: 8360\n",
      "Loss: 11.41180767077948\n",
      "l2 norm of gradients: 0.05164190526384996\n",
      "l2 norm of weights: 3.3646880685857594\n",
      "---------------------\n",
      "Iteration Number: 8361\n",
      "Loss: 11.411533847184646\n",
      "l2 norm of gradients: 0.05163515942306441\n",
      "l2 norm of weights: 3.3647540984247475\n",
      "---------------------\n",
      "Iteration Number: 8362\n",
      "Loss: 11.411260125834094\n",
      "l2 norm of gradients: 0.05162841574231161\n",
      "l2 norm of weights: 3.364820126011786\n",
      "---------------------\n",
      "Iteration Number: 8363\n",
      "Loss: 11.410986506679185\n",
      "l2 norm of gradients: 0.0516216742201353\n",
      "l2 norm of weights: 3.3648861513429464\n",
      "---------------------\n",
      "Iteration Number: 8364\n",
      "Loss: 11.41071298967131\n",
      "l2 norm of gradients: 0.051614934855080764\n",
      "l2 norm of weights: 3.3649521744143076\n",
      "---------------------\n",
      "Iteration Number: 8365\n",
      "Loss: 11.410439574761895\n",
      "l2 norm of gradients: 0.05160819764569485\n",
      "l2 norm of weights: 3.36501819522195\n",
      "---------------------\n",
      "Iteration Number: 8366\n",
      "Loss: 11.410166261902416\n",
      "l2 norm of gradients: 0.051601462590526025\n",
      "l2 norm of weights: 3.3650842137619583\n",
      "---------------------\n",
      "Iteration Number: 8367\n",
      "Loss: 11.40989305104438\n",
      "l2 norm of gradients: 0.051594729688124216\n",
      "l2 norm of weights: 3.3651502300304235\n",
      "---------------------\n",
      "Iteration Number: 8368\n",
      "Loss: 11.40961994213931\n",
      "l2 norm of gradients: 0.051587998937041024\n",
      "l2 norm of weights: 3.3652162440234377\n",
      "---------------------\n",
      "Iteration Number: 8369\n",
      "Loss: 11.409346935138792\n",
      "l2 norm of gradients: 0.05158127033582955\n",
      "l2 norm of weights: 3.3652822557370996\n",
      "---------------------\n",
      "Iteration Number: 8370\n",
      "Loss: 11.409074029994427\n",
      "l2 norm of gradients: 0.051574543883044464\n",
      "l2 norm of weights: 3.3653482651675106\n",
      "---------------------\n",
      "Iteration Number: 8371\n",
      "Loss: 11.408801226657861\n",
      "l2 norm of gradients: 0.05156781957724199\n",
      "l2 norm of weights: 3.365414272310777\n",
      "---------------------\n",
      "Iteration Number: 8372\n",
      "Loss: 11.408528525080781\n",
      "l2 norm of gradients: 0.051561097416979895\n",
      "l2 norm of weights: 3.365480277163008\n",
      "---------------------\n",
      "Iteration Number: 8373\n",
      "Loss: 11.408255925214892\n",
      "l2 norm of gradients: 0.051554377400817565\n",
      "l2 norm of weights: 3.365546279720318\n",
      "---------------------\n",
      "Iteration Number: 8374\n",
      "Loss: 11.407983427011946\n",
      "l2 norm of gradients: 0.051547659527315805\n",
      "l2 norm of weights: 3.365612279978826\n",
      "---------------------\n",
      "Iteration Number: 8375\n",
      "Loss: 11.40771103042373\n",
      "l2 norm of gradients: 0.05154094379503706\n",
      "l2 norm of weights: 3.365678277934654\n",
      "---------------------\n",
      "Iteration Number: 8376\n",
      "Loss: 11.407438735402067\n",
      "l2 norm of gradients: 0.05153423020254533\n",
      "l2 norm of weights: 3.365744273583928\n",
      "---------------------\n",
      "Iteration Number: 8377\n",
      "Loss: 11.407166541898805\n",
      "l2 norm of gradients: 0.05152751874840608\n",
      "l2 norm of weights: 3.365810266922778\n",
      "---------------------\n",
      "Iteration Number: 8378\n",
      "Loss: 11.40689444986584\n",
      "l2 norm of gradients: 0.05152080943118642\n",
      "l2 norm of weights: 3.3658762579473396\n",
      "---------------------\n",
      "Iteration Number: 8379\n",
      "Loss: 11.406622459255102\n",
      "l2 norm of gradients: 0.05151410224945491\n",
      "l2 norm of weights: 3.3659422466537507\n",
      "---------------------\n",
      "Iteration Number: 8380\n",
      "Loss: 11.406350570018542\n",
      "l2 norm of gradients: 0.05150739720178164\n",
      "l2 norm of weights: 3.366008233038154\n",
      "---------------------\n",
      "Iteration Number: 8381\n",
      "Loss: 11.406078782108166\n",
      "l2 norm of gradients: 0.05150069428673833\n",
      "l2 norm of weights: 3.3660742170966964\n",
      "---------------------\n",
      "Iteration Number: 8382\n",
      "Loss: 11.405807095476002\n",
      "l2 norm of gradients: 0.05149399350289811\n",
      "l2 norm of weights: 3.366140198825529\n",
      "---------------------\n",
      "Iteration Number: 8383\n",
      "Loss: 11.405535510074102\n",
      "l2 norm of gradients: 0.051487294848835734\n",
      "l2 norm of weights: 3.366206178220806\n",
      "---------------------\n",
      "Iteration Number: 8384\n",
      "Loss: 11.405264025854583\n",
      "l2 norm of gradients: 0.05148059832312743\n",
      "l2 norm of weights: 3.3662721552786867\n",
      "---------------------\n",
      "Iteration Number: 8385\n",
      "Loss: 11.404992642769576\n",
      "l2 norm of gradients: 0.051473903924350965\n",
      "l2 norm of weights: 3.366338129995334\n",
      "---------------------\n",
      "Iteration Number: 8386\n",
      "Loss: 11.404721360771251\n",
      "l2 norm of gradients: 0.05146721165108563\n",
      "l2 norm of weights: 3.3664041023669147\n",
      "---------------------\n",
      "Iteration Number: 8387\n",
      "Loss: 11.404450179811809\n",
      "l2 norm of gradients: 0.05146052150191222\n",
      "l2 norm of weights: 3.3664700723896\n",
      "---------------------\n",
      "Iteration Number: 8388\n",
      "Loss: 11.404179099843494\n",
      "l2 norm of gradients: 0.051453833475413074\n",
      "l2 norm of weights: 3.3665360400595645\n",
      "---------------------\n",
      "Iteration Number: 8389\n",
      "Loss: 11.403908120818581\n",
      "l2 norm of gradients: 0.05144714757017201\n",
      "l2 norm of weights: 3.3666020053729877\n",
      "---------------------\n",
      "Iteration Number: 8390\n",
      "Loss: 11.403637242689374\n",
      "l2 norm of gradients: 0.05144046378477442\n",
      "l2 norm of weights: 3.3666679683260523\n",
      "---------------------\n",
      "Iteration Number: 8391\n",
      "Loss: 11.403366465408222\n",
      "l2 norm of gradients: 0.05143378211780712\n",
      "l2 norm of weights: 3.3667339289149463\n",
      "---------------------\n",
      "Iteration Number: 8392\n",
      "Loss: 11.403095788927509\n",
      "l2 norm of gradients: 0.051427102567858526\n",
      "l2 norm of weights: 3.3667998871358593\n",
      "---------------------\n",
      "Iteration Number: 8393\n",
      "Loss: 11.40282521319963\n",
      "l2 norm of gradients: 0.05142042513351849\n",
      "l2 norm of weights: 3.3668658429849874\n",
      "---------------------\n",
      "Iteration Number: 8394\n",
      "Loss: 11.402554738177047\n",
      "l2 norm of gradients: 0.05141374981337838\n",
      "l2 norm of weights: 3.3669317964585295\n",
      "---------------------\n",
      "Iteration Number: 8395\n",
      "Loss: 11.402284363812232\n",
      "l2 norm of gradients: 0.05140707660603111\n",
      "l2 norm of weights: 3.3669977475526887\n",
      "---------------------\n",
      "Iteration Number: 8396\n",
      "Loss: 11.402014090057719\n",
      "l2 norm of gradients: 0.051400405510071065\n",
      "l2 norm of weights: 3.3670636962636724\n",
      "---------------------\n",
      "Iteration Number: 8397\n",
      "Loss: 11.401743916866042\n",
      "l2 norm of gradients: 0.05139373652409412\n",
      "l2 norm of weights: 3.367129642587691\n",
      "---------------------\n",
      "Iteration Number: 8398\n",
      "Loss: 11.401473844189788\n",
      "l2 norm of gradients: 0.05138706964669761\n",
      "l2 norm of weights: 3.3671955865209604\n",
      "---------------------\n",
      "Iteration Number: 8399\n",
      "Loss: 11.401203871981584\n",
      "l2 norm of gradients: 0.051380404876480455\n",
      "l2 norm of weights: 3.3672615280596987\n",
      "---------------------\n",
      "Iteration Number: 8400\n",
      "Loss: 11.400934000194075\n",
      "l2 norm of gradients: 0.05137374221204301\n",
      "l2 norm of weights: 3.3673274672001297\n",
      "---------------------\n",
      "Iteration Number: 8401\n",
      "Loss: 11.400664228779966\n",
      "l2 norm of gradients: 0.05136708165198712\n",
      "l2 norm of weights: 3.3673934039384807\n",
      "---------------------\n",
      "Iteration Number: 8402\n",
      "Loss: 11.400394557691957\n",
      "l2 norm of gradients: 0.051360423194916134\n",
      "l2 norm of weights: 3.3674593382709817\n",
      "---------------------\n",
      "Iteration Number: 8403\n",
      "Loss: 11.400124986882817\n",
      "l2 norm of gradients: 0.05135376683943483\n",
      "l2 norm of weights: 3.3675252701938683\n",
      "---------------------\n",
      "Iteration Number: 8404\n",
      "Loss: 11.399855516305337\n",
      "l2 norm of gradients: 0.05134711258414955\n",
      "l2 norm of weights: 3.367591199703379\n",
      "---------------------\n",
      "Iteration Number: 8405\n",
      "Loss: 11.399586145912348\n",
      "l2 norm of gradients: 0.05134046042766806\n",
      "l2 norm of weights: 3.3676571267957573\n",
      "---------------------\n",
      "Iteration Number: 8406\n",
      "Loss: 11.399316875656694\n",
      "l2 norm of gradients: 0.05133381036859962\n",
      "l2 norm of weights: 3.36772305146725\n",
      "---------------------\n",
      "Iteration Number: 8407\n",
      "Loss: 11.399047705491274\n",
      "l2 norm of gradients: 0.05132716240555497\n",
      "l2 norm of weights: 3.3677889737141062\n",
      "---------------------\n",
      "Iteration Number: 8408\n",
      "Loss: 11.39877863536902\n",
      "l2 norm of gradients: 0.05132051653714633\n",
      "l2 norm of weights: 3.367854893532583\n",
      "---------------------\n",
      "Iteration Number: 8409\n",
      "Loss: 11.398509665242889\n",
      "l2 norm of gradients: 0.051313872761987366\n",
      "l2 norm of weights: 3.3679208109189376\n",
      "---------------------\n",
      "Iteration Number: 8410\n",
      "Loss: 11.398240795065881\n",
      "l2 norm of gradients: 0.05130723107869325\n",
      "l2 norm of weights: 3.3679867258694327\n",
      "---------------------\n",
      "Iteration Number: 8411\n",
      "Loss: 11.39797202479102\n",
      "l2 norm of gradients: 0.05130059148588058\n",
      "l2 norm of weights: 3.368052638380335\n",
      "---------------------\n",
      "Iteration Number: 8412\n",
      "Loss: 11.397703354371373\n",
      "l2 norm of gradients: 0.051293953982167434\n",
      "l2 norm of weights: 3.3681185484479155\n",
      "---------------------\n",
      "Iteration Number: 8413\n",
      "Loss: 11.397434783760028\n",
      "l2 norm of gradients: 0.05128731856617339\n",
      "l2 norm of weights: 3.3681844560684477\n",
      "---------------------\n",
      "Iteration Number: 8414\n",
      "Loss: 11.397166312910134\n",
      "l2 norm of gradients: 0.05128068523651943\n",
      "l2 norm of weights: 3.3682503612382098\n",
      "---------------------\n",
      "Iteration Number: 8415\n",
      "Loss: 11.396897941774839\n",
      "l2 norm of gradients: 0.05127405399182803\n",
      "l2 norm of weights: 3.368316263953485\n",
      "---------------------\n",
      "Iteration Number: 8416\n",
      "Loss: 11.396629670307345\n",
      "l2 norm of gradients: 0.05126742483072311\n",
      "l2 norm of weights: 3.368382164210558\n",
      "---------------------\n",
      "Iteration Number: 8417\n",
      "Loss: 11.396361498460884\n",
      "l2 norm of gradients: 0.05126079775183003\n",
      "l2 norm of weights: 3.36844806200572\n",
      "---------------------\n",
      "Iteration Number: 8418\n",
      "Loss: 11.396093426188727\n",
      "l2 norm of gradients: 0.05125417275377566\n",
      "l2 norm of weights: 3.3685139573352645\n",
      "---------------------\n",
      "Iteration Number: 8419\n",
      "Loss: 11.395825453444173\n",
      "l2 norm of gradients: 0.05124754983518823\n",
      "l2 norm of weights: 3.368579850195489\n",
      "---------------------\n",
      "Iteration Number: 8420\n",
      "Loss: 11.395557580180549\n",
      "l2 norm of gradients: 0.05124092899469751\n",
      "l2 norm of weights: 3.368645740582696\n",
      "---------------------\n",
      "Iteration Number: 8421\n",
      "Loss: 11.395289806351228\n",
      "l2 norm of gradients: 0.05123431023093463\n",
      "l2 norm of weights: 3.3687116284931897\n",
      "---------------------\n",
      "Iteration Number: 8422\n",
      "Loss: 11.395022131909606\n",
      "l2 norm of gradients: 0.05122769354253226\n",
      "l2 norm of weights: 3.3687775139232805\n",
      "---------------------\n",
      "Iteration Number: 8423\n",
      "Loss: 11.394754556809113\n",
      "l2 norm of gradients: 0.05122107892812442\n",
      "l2 norm of weights: 3.3688433968692815\n",
      "---------------------\n",
      "Iteration Number: 8424\n",
      "Loss: 11.394487081003234\n",
      "l2 norm of gradients: 0.05121446638634661\n",
      "l2 norm of weights: 3.36890927732751\n",
      "---------------------\n",
      "Iteration Number: 8425\n",
      "Loss: 11.394219704445456\n",
      "l2 norm of gradients: 0.05120785591583578\n",
      "l2 norm of weights: 3.3689751552942875\n",
      "---------------------\n",
      "Iteration Number: 8426\n",
      "Loss: 11.393952427089301\n",
      "l2 norm of gradients: 0.05120124751523027\n",
      "l2 norm of weights: 3.369041030765938\n",
      "---------------------\n",
      "Iteration Number: 8427\n",
      "Loss: 11.39368524888836\n",
      "l2 norm of gradients: 0.05119464118316996\n",
      "l2 norm of weights: 3.369106903738791\n",
      "---------------------\n",
      "Iteration Number: 8428\n",
      "Loss: 11.393418169796226\n",
      "l2 norm of gradients: 0.051188036918296\n",
      "l2 norm of weights: 3.3691727742091784\n",
      "---------------------\n",
      "Iteration Number: 8429\n",
      "Loss: 11.393151189766535\n",
      "l2 norm of gradients: 0.05118143471925112\n",
      "l2 norm of weights: 3.3692386421734373\n",
      "---------------------\n",
      "Iteration Number: 8430\n",
      "Loss: 11.392884308752947\n",
      "l2 norm of gradients: 0.05117483458467936\n",
      "l2 norm of weights: 3.369304507627908\n",
      "---------------------\n",
      "Iteration Number: 8431\n",
      "Loss: 11.39261752670917\n",
      "l2 norm of gradients: 0.05116823651322628\n",
      "l2 norm of weights: 3.3693703705689346\n",
      "---------------------\n",
      "Iteration Number: 8432\n",
      "Loss: 11.392350843588925\n",
      "l2 norm of gradients: 0.05116164050353879\n",
      "l2 norm of weights: 3.369436230992865\n",
      "---------------------\n",
      "Iteration Number: 8433\n",
      "Loss: 11.392084259346001\n",
      "l2 norm of gradients: 0.05115504655426527\n",
      "l2 norm of weights: 3.3695020888960516\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 8434\n",
      "Loss: 11.391817773934179\n",
      "l2 norm of gradients: 0.05114845466405547\n",
      "l2 norm of weights: 3.36956794427485\n",
      "---------------------\n",
      "Iteration Number: 8435\n",
      "Loss: 11.391551387307308\n",
      "l2 norm of gradients: 0.051141864831560635\n",
      "l2 norm of weights: 3.3696337971256187\n",
      "---------------------\n",
      "Iteration Number: 8436\n",
      "Loss: 11.39128509941925\n",
      "l2 norm of gradients: 0.05113527705543332\n",
      "l2 norm of weights: 3.3696996474447225\n",
      "---------------------\n",
      "Iteration Number: 8437\n",
      "Loss: 11.391018910223895\n",
      "l2 norm of gradients: 0.05112869133432762\n",
      "l2 norm of weights: 3.3697654952285276\n",
      "---------------------\n",
      "Iteration Number: 8438\n",
      "Loss: 11.390752819675189\n",
      "l2 norm of gradients: 0.0511221076668989\n",
      "l2 norm of weights: 3.369831340473405\n",
      "---------------------\n",
      "Iteration Number: 8439\n",
      "Loss: 11.390486827727091\n",
      "l2 norm of gradients: 0.05111552605180402\n",
      "l2 norm of weights: 3.36989718317573\n",
      "---------------------\n",
      "Iteration Number: 8440\n",
      "Loss: 11.390220934333593\n",
      "l2 norm of gradients: 0.05110894648770126\n",
      "l2 norm of weights: 3.3699630233318807\n",
      "---------------------\n",
      "Iteration Number: 8441\n",
      "Loss: 11.389955139448745\n",
      "l2 norm of gradients: 0.05110236897325026\n",
      "l2 norm of weights: 3.3700288609382403\n",
      "---------------------\n",
      "Iteration Number: 8442\n",
      "Loss: 11.389689443026594\n",
      "l2 norm of gradients: 0.05109579350711204\n",
      "l2 norm of weights: 3.3700946959911935\n",
      "---------------------\n",
      "Iteration Number: 8443\n",
      "Loss: 11.389423845021243\n",
      "l2 norm of gradients: 0.05108922008794908\n",
      "l2 norm of weights: 3.370160528487132\n",
      "---------------------\n",
      "Iteration Number: 8444\n",
      "Loss: 11.38915834538683\n",
      "l2 norm of gradients: 0.05108264871442525\n",
      "l2 norm of weights: 3.3702263584224483\n",
      "---------------------\n",
      "Iteration Number: 8445\n",
      "Loss: 11.388892944077515\n",
      "l2 norm of gradients: 0.05107607938520577\n",
      "l2 norm of weights: 3.3702921857935406\n",
      "---------------------\n",
      "Iteration Number: 8446\n",
      "Loss: 11.388627641047481\n",
      "l2 norm of gradients: 0.05106951209895732\n",
      "l2 norm of weights: 3.3703580105968105\n",
      "---------------------\n",
      "Iteration Number: 8447\n",
      "Loss: 11.388362436250976\n",
      "l2 norm of gradients: 0.0510629468543479\n",
      "l2 norm of weights: 3.3704238328286618\n",
      "---------------------\n",
      "Iteration Number: 8448\n",
      "Loss: 11.388097329642246\n",
      "l2 norm of gradients: 0.051056383650046964\n",
      "l2 norm of weights: 3.3704896524855052\n",
      "---------------------\n",
      "Iteration Number: 8449\n",
      "Loss: 11.387832321175587\n",
      "l2 norm of gradients: 0.05104982248472532\n",
      "l2 norm of weights: 3.3705554695637514\n",
      "---------------------\n",
      "Iteration Number: 8450\n",
      "Loss: 11.387567410805334\n",
      "l2 norm of gradients: 0.05104326335705516\n",
      "l2 norm of weights: 3.3706212840598186\n",
      "---------------------\n",
      "Iteration Number: 8451\n",
      "Loss: 11.387302598485842\n",
      "l2 norm of gradients: 0.051036706265710094\n",
      "l2 norm of weights: 3.370687095970126\n",
      "---------------------\n",
      "Iteration Number: 8452\n",
      "Loss: 11.387037884171507\n",
      "l2 norm of gradients: 0.05103015120936504\n",
      "l2 norm of weights: 3.3707529052910976\n",
      "---------------------\n",
      "Iteration Number: 8453\n",
      "Loss: 11.386773267816737\n",
      "l2 norm of gradients: 0.05102359818669638\n",
      "l2 norm of weights: 3.370818712019161\n",
      "---------------------\n",
      "Iteration Number: 8454\n",
      "Loss: 11.386508749376008\n",
      "l2 norm of gradients: 0.05101704719638181\n",
      "l2 norm of weights: 3.3708845161507477\n",
      "---------------------\n",
      "Iteration Number: 8455\n",
      "Loss: 11.386244328803803\n",
      "l2 norm of gradients: 0.05101049823710047\n",
      "l2 norm of weights: 3.370950317682294\n",
      "---------------------\n",
      "Iteration Number: 8456\n",
      "Loss: 11.385980006054641\n",
      "l2 norm of gradients: 0.05100395130753282\n",
      "l2 norm of weights: 3.3710161166102366\n",
      "---------------------\n",
      "Iteration Number: 8457\n",
      "Loss: 11.385715781083077\n",
      "l2 norm of gradients: 0.0509974064063607\n",
      "l2 norm of weights: 3.3710819129310194\n",
      "---------------------\n",
      "Iteration Number: 8458\n",
      "Loss: 11.385451653843694\n",
      "l2 norm of gradients: 0.05099086353226731\n",
      "l2 norm of weights: 3.371147706641089\n",
      "---------------------\n",
      "Iteration Number: 8459\n",
      "Loss: 11.385187624291122\n",
      "l2 norm of gradients: 0.05098432268393729\n",
      "l2 norm of weights: 3.371213497736895\n",
      "---------------------\n",
      "Iteration Number: 8460\n",
      "Loss: 11.384923692380006\n",
      "l2 norm of gradients: 0.050977783860056525\n",
      "l2 norm of weights: 3.3712792862148913\n",
      "---------------------\n",
      "Iteration Number: 8461\n",
      "Loss: 11.384659858065023\n",
      "l2 norm of gradients: 0.05097124705931236\n",
      "l2 norm of weights: 3.371345072071535\n",
      "---------------------\n",
      "Iteration Number: 8462\n",
      "Loss: 11.3843961213009\n",
      "l2 norm of gradients: 0.05096471228039348\n",
      "l2 norm of weights: 3.371410855303288\n",
      "---------------------\n",
      "Iteration Number: 8463\n",
      "Loss: 11.38413248204237\n",
      "l2 norm of gradients: 0.05095817952198991\n",
      "l2 norm of weights: 3.3714766359066153\n",
      "---------------------\n",
      "Iteration Number: 8464\n",
      "Loss: 11.383868940244227\n",
      "l2 norm of gradients: 0.05095164878279303\n",
      "l2 norm of weights: 3.371542413877985\n",
      "---------------------\n",
      "Iteration Number: 8465\n",
      "Loss: 11.383605495861286\n",
      "l2 norm of gradients: 0.05094512006149564\n",
      "l2 norm of weights: 3.3716081892138696\n",
      "---------------------\n",
      "Iteration Number: 8466\n",
      "Loss: 11.38334214884837\n",
      "l2 norm of gradients: 0.05093859335679178\n",
      "l2 norm of weights: 3.3716739619107456\n",
      "---------------------\n",
      "Iteration Number: 8467\n",
      "Loss: 11.38307889916038\n",
      "l2 norm of gradients: 0.05093206866737694\n",
      "l2 norm of weights: 3.371739731965092\n",
      "---------------------\n",
      "Iteration Number: 8468\n",
      "Loss: 11.382815746752213\n",
      "l2 norm of gradients: 0.050925545991947936\n",
      "l2 norm of weights: 3.371805499373392\n",
      "---------------------\n",
      "Iteration Number: 8469\n",
      "Loss: 11.382552691578812\n",
      "l2 norm of gradients: 0.050919025329202915\n",
      "l2 norm of weights: 3.371871264132134\n",
      "---------------------\n",
      "Iteration Number: 8470\n",
      "Loss: 11.382289733595144\n",
      "l2 norm of gradients: 0.05091250667784133\n",
      "l2 norm of weights: 3.3719370262378074\n",
      "---------------------\n",
      "Iteration Number: 8471\n",
      "Loss: 11.382026872756214\n",
      "l2 norm of gradients: 0.050905990036564125\n",
      "l2 norm of weights: 3.3720027856869077\n",
      "---------------------\n",
      "Iteration Number: 8472\n",
      "Loss: 11.381764109017073\n",
      "l2 norm of gradients: 0.05089947540407341\n",
      "l2 norm of weights: 3.3720685424759322\n",
      "---------------------\n",
      "Iteration Number: 8473\n",
      "Loss: 11.381501442332771\n",
      "l2 norm of gradients: 0.050892962779072744\n",
      "l2 norm of weights: 3.3721342966013834\n",
      "---------------------\n",
      "Iteration Number: 8474\n",
      "Loss: 11.381238872658422\n",
      "l2 norm of gradients: 0.050886452160267\n",
      "l2 norm of weights: 3.3722000480597663\n",
      "---------------------\n",
      "Iteration Number: 8475\n",
      "Loss: 11.38097639994914\n",
      "l2 norm of gradients: 0.050879943546362334\n",
      "l2 norm of weights: 3.3722657968475898\n",
      "---------------------\n",
      "Iteration Number: 8476\n",
      "Loss: 11.380714024160113\n",
      "l2 norm of gradients: 0.05087343693606636\n",
      "l2 norm of weights: 3.372331542961367\n",
      "---------------------\n",
      "Iteration Number: 8477\n",
      "Loss: 11.380451745246516\n",
      "l2 norm of gradients: 0.0508669323280879\n",
      "l2 norm of weights: 3.3723972863976135\n",
      "---------------------\n",
      "Iteration Number: 8478\n",
      "Loss: 11.380189563163592\n",
      "l2 norm of gradients: 0.05086042972113716\n",
      "l2 norm of weights: 3.372463027152851\n",
      "---------------------\n",
      "Iteration Number: 8479\n",
      "Loss: 11.379927477866586\n",
      "l2 norm of gradients: 0.050853929113925674\n",
      "l2 norm of weights: 3.372528765223602\n",
      "---------------------\n",
      "Iteration Number: 8480\n",
      "Loss: 11.379665489310801\n",
      "l2 norm of gradients: 0.050847430505166304\n",
      "l2 norm of weights: 3.3725945006063935\n",
      "---------------------\n",
      "Iteration Number: 8481\n",
      "Loss: 11.379403597451551\n",
      "l2 norm of gradients: 0.050840933893573206\n",
      "l2 norm of weights: 3.3726602332977573\n",
      "---------------------\n",
      "Iteration Number: 8482\n",
      "Loss: 11.379141802244192\n",
      "l2 norm of gradients: 0.050834439277861916\n",
      "l2 norm of weights: 3.372725963294228\n",
      "---------------------\n",
      "Iteration Number: 8483\n",
      "Loss: 11.378880103644113\n",
      "l2 norm of gradients: 0.05082794665674925\n",
      "l2 norm of weights: 3.372791690592343\n",
      "---------------------\n",
      "Iteration Number: 8484\n",
      "Loss: 11.378618501606732\n",
      "l2 norm of gradients: 0.050821456028953355\n",
      "l2 norm of weights: 3.3728574151886446\n",
      "---------------------\n",
      "Iteration Number: 8485\n",
      "Loss: 11.378356996087488\n",
      "l2 norm of gradients: 0.05081496739319369\n",
      "l2 norm of weights: 3.372923137079678\n",
      "---------------------\n",
      "Iteration Number: 8486\n",
      "Loss: 11.37809558704187\n",
      "l2 norm of gradients: 0.05080848074819103\n",
      "l2 norm of weights: 3.3729888562619923\n",
      "---------------------\n",
      "Iteration Number: 8487\n",
      "Loss: 11.377834274425387\n",
      "l2 norm of gradients: 0.05080199609266749\n",
      "l2 norm of weights: 3.373054572732141\n",
      "---------------------\n",
      "Iteration Number: 8488\n",
      "Loss: 11.37757305819358\n",
      "l2 norm of gradients: 0.05079551342534643\n",
      "l2 norm of weights: 3.37312028648668\n",
      "---------------------\n",
      "Iteration Number: 8489\n",
      "Loss: 11.377311938302034\n",
      "l2 norm of gradients: 0.05078903274495259\n",
      "l2 norm of weights: 3.3731859975221674\n",
      "---------------------\n",
      "Iteration Number: 8490\n",
      "Loss: 11.37705091470634\n",
      "l2 norm of gradients: 0.05078255405021198\n",
      "l2 norm of weights: 3.3732517058351688\n",
      "---------------------\n",
      "Iteration Number: 8491\n",
      "Loss: 11.376789987362143\n",
      "l2 norm of gradients: 0.05077607733985193\n",
      "l2 norm of weights: 3.37331741142225\n",
      "---------------------\n",
      "Iteration Number: 8492\n",
      "Loss: 11.376529156225107\n",
      "l2 norm of gradients: 0.05076960261260105\n",
      "l2 norm of weights: 3.373383114279983\n",
      "---------------------\n",
      "Iteration Number: 8493\n",
      "Loss: 11.376268421250938\n",
      "l2 norm of gradients: 0.0507631298671893\n",
      "l2 norm of weights: 3.3734488144049406\n",
      "---------------------\n",
      "Iteration Number: 8494\n",
      "Loss: 11.37600778239537\n",
      "l2 norm of gradients: 0.05075665910234792\n",
      "l2 norm of weights: 3.373514511793701\n",
      "---------------------\n",
      "Iteration Number: 8495\n",
      "Loss: 11.375747239614151\n",
      "l2 norm of gradients: 0.050750190316809406\n",
      "l2 norm of weights: 3.3735802064428455\n",
      "---------------------\n",
      "Iteration Number: 8496\n",
      "Loss: 11.375486792863086\n",
      "l2 norm of gradients: 0.050743723509307594\n",
      "l2 norm of weights: 3.373645898348959\n",
      "---------------------\n",
      "Iteration Number: 8497\n",
      "Loss: 11.375226442098\n",
      "l2 norm of gradients: 0.050737258678577604\n",
      "l2 norm of weights: 3.3737115875086303\n",
      "---------------------\n",
      "Iteration Number: 8498\n",
      "Loss: 11.374966187274744\n",
      "l2 norm of gradients: 0.050730795823355865\n",
      "l2 norm of weights: 3.3737772739184515\n",
      "---------------------\n",
      "Iteration Number: 8499\n",
      "Loss: 11.374706028349197\n",
      "l2 norm of gradients: 0.05072433494238005\n",
      "l2 norm of weights: 3.3738429575750177\n",
      "---------------------\n",
      "Iteration Number: 8500\n",
      "Loss: 11.374445965277296\n",
      "l2 norm of gradients: 0.0507178760343892\n",
      "l2 norm of weights: 3.3739086384749286\n",
      "---------------------\n",
      "Iteration Number: 8501\n",
      "Loss: 11.374185998014976\n",
      "l2 norm of gradients: 0.05071141909812355\n",
      "l2 norm of weights: 3.3739743166147864\n",
      "---------------------\n",
      "Iteration Number: 8502\n",
      "Loss: 11.373926126518219\n",
      "l2 norm of gradients: 0.05070496413232469\n",
      "l2 norm of weights: 3.374039991991198\n",
      "---------------------\n",
      "Iteration Number: 8503\n",
      "Loss: 11.373666350743042\n",
      "l2 norm of gradients: 0.050698511135735466\n",
      "l2 norm of weights: 3.3741056646007723\n",
      "---------------------\n",
      "Iteration Number: 8504\n",
      "Loss: 11.373406670645478\n",
      "l2 norm of gradients: 0.0506920601071\n",
      "l2 norm of weights: 3.374171334440123\n",
      "---------------------\n",
      "Iteration Number: 8505\n",
      "Loss: 11.373147086181605\n",
      "l2 norm of gradients: 0.0506856110451637\n",
      "l2 norm of weights: 3.3742370015058674\n",
      "---------------------\n",
      "Iteration Number: 8506\n",
      "Loss: 11.372887597307527\n",
      "l2 norm of gradients: 0.05067916394867325\n",
      "l2 norm of weights: 3.3743026657946253\n",
      "---------------------\n",
      "Iteration Number: 8507\n",
      "Loss: 11.372628203979376\n",
      "l2 norm of gradients: 0.05067271881637662\n",
      "l2 norm of weights: 3.374368327303021\n",
      "---------------------\n",
      "Iteration Number: 8508\n",
      "Loss: 11.372368906153318\n",
      "l2 norm of gradients: 0.05066627564702305\n",
      "l2 norm of weights: 3.374433986027682\n",
      "---------------------\n",
      "Iteration Number: 8509\n",
      "Loss: 11.372109703785547\n",
      "l2 norm of gradients: 0.05065983443936303\n",
      "l2 norm of weights: 3.374499641965239\n",
      "---------------------\n",
      "Iteration Number: 8510\n",
      "Loss: 11.371850596832298\n",
      "l2 norm of gradients: 0.05065339519214837\n",
      "l2 norm of weights: 3.3745652951123257\n",
      "---------------------\n",
      "Iteration Number: 8511\n",
      "Loss: 11.371591585249817\n",
      "l2 norm of gradients: 0.05064695790413211\n",
      "l2 norm of weights: 3.374630945465581\n",
      "---------------------\n",
      "Iteration Number: 8512\n",
      "Loss: 11.371332668994402\n",
      "l2 norm of gradients: 0.05064052257406853\n",
      "l2 norm of weights: 3.374696593021647\n",
      "---------------------\n",
      "Iteration Number: 8513\n",
      "Loss: 11.371073848022366\n",
      "l2 norm of gradients: 0.05063408920071324\n",
      "l2 norm of weights: 3.3747622377771673\n",
      "---------------------\n",
      "Iteration Number: 8514\n",
      "Loss: 11.370815122290056\n",
      "l2 norm of gradients: 0.050627657782823104\n",
      "l2 norm of weights: 3.3748278797287914\n",
      "---------------------\n",
      "Iteration Number: 8515\n",
      "Loss: 11.37055649175386\n",
      "l2 norm of gradients: 0.050621228319156185\n",
      "l2 norm of weights: 3.3748935188731704\n",
      "---------------------\n",
      "Iteration Number: 8516\n",
      "Loss: 11.370297956370186\n",
      "l2 norm of gradients: 0.05061480080847186\n",
      "l2 norm of weights: 3.37495915520696\n",
      "---------------------\n",
      "Iteration Number: 8517\n",
      "Loss: 11.370039516095476\n",
      "l2 norm of gradients: 0.05060837524953075\n",
      "l2 norm of weights: 3.375024788726819\n",
      "---------------------\n",
      "Iteration Number: 8518\n",
      "Loss: 11.369781170886206\n",
      "l2 norm of gradients: 0.05060195164109473\n",
      "l2 norm of weights: 3.375090419429411\n",
      "---------------------\n",
      "Iteration Number: 8519\n",
      "Loss: 11.369522920698863\n",
      "l2 norm of gradients: 0.05059552998192696\n",
      "l2 norm of weights: 3.3751560473114\n",
      "---------------------\n",
      "Iteration Number: 8520\n",
      "Loss: 11.369264765489996\n",
      "l2 norm of gradients: 0.050589110270791764\n",
      "l2 norm of weights: 3.3752216723694564\n",
      "---------------------\n",
      "Iteration Number: 8521\n",
      "Loss: 11.369006705216156\n",
      "l2 norm of gradients: 0.05058269250645481\n",
      "l2 norm of weights: 3.375287294600253\n",
      "---------------------\n",
      "Iteration Number: 8522\n",
      "Loss: 11.368748739833947\n",
      "l2 norm of gradients: 0.05057627668768298\n",
      "l2 norm of weights: 3.375352914000466\n",
      "---------------------\n",
      "Iteration Number: 8523\n",
      "Loss: 11.368490869299986\n",
      "l2 norm of gradients: 0.05056986281324443\n",
      "l2 norm of weights: 3.3754185305667743\n",
      "---------------------\n",
      "Iteration Number: 8524\n",
      "Loss: 11.368233093570932\n",
      "l2 norm of gradients: 0.05056345088190847\n",
      "l2 norm of weights: 3.3754841442958625\n",
      "---------------------\n",
      "Iteration Number: 8525\n",
      "Loss: 11.367975412603462\n",
      "l2 norm of gradients: 0.05055704089244576\n",
      "l2 norm of weights: 3.3755497551844162\n",
      "---------------------\n",
      "Iteration Number: 8526\n",
      "Loss: 11.367717826354308\n",
      "l2 norm of gradients: 0.05055063284362813\n",
      "l2 norm of weights: 3.3756153632291257\n",
      "---------------------\n",
      "Iteration Number: 8527\n",
      "Loss: 11.367460334780196\n",
      "l2 norm of gradients: 0.05054422673422871\n",
      "l2 norm of weights: 3.375680968426685\n",
      "---------------------\n",
      "Iteration Number: 8528\n",
      "Loss: 11.367202937837911\n",
      "l2 norm of gradients: 0.05053782256302181\n",
      "l2 norm of weights: 3.375746570773791\n",
      "---------------------\n",
      "Iteration Number: 8529\n",
      "Loss: 11.366945635484257\n",
      "l2 norm of gradients: 0.050531420328783\n",
      "l2 norm of weights: 3.3758121702671424\n",
      "---------------------\n",
      "Iteration Number: 8530\n",
      "Loss: 11.366688427676069\n",
      "l2 norm of gradients: 0.05052502003028911\n",
      "l2 norm of weights: 3.375877766903445\n",
      "---------------------\n",
      "Iteration Number: 8531\n",
      "Loss: 11.36643131437021\n",
      "l2 norm of gradients: 0.05051862166631813\n",
      "l2 norm of weights: 3.375943360679406\n",
      "---------------------\n",
      "Iteration Number: 8532\n",
      "Loss: 11.366174295523589\n",
      "l2 norm of gradients: 0.0505122252356494\n",
      "l2 norm of weights: 3.3760089515917353\n",
      "---------------------\n",
      "Iteration Number: 8533\n",
      "Loss: 11.365917371093115\n",
      "l2 norm of gradients: 0.05050583073706336\n",
      "l2 norm of weights: 3.376074539637146\n",
      "---------------------\n",
      "Iteration Number: 8534\n",
      "Loss: 11.365660541035757\n",
      "l2 norm of gradients: 0.05049943816934174\n",
      "l2 norm of weights: 3.376140124812358\n",
      "---------------------\n",
      "Iteration Number: 8535\n",
      "Loss: 11.365403805308496\n",
      "l2 norm of gradients: 0.05049304753126751\n",
      "l2 norm of weights: 3.3762057071140905\n",
      "---------------------\n",
      "Iteration Number: 8536\n",
      "Loss: 11.365147163868347\n",
      "l2 norm of gradients: 0.050486658821624814\n",
      "l2 norm of weights: 3.3762712865390685\n",
      "---------------------\n",
      "Iteration Number: 8537\n",
      "Loss: 11.364890616672357\n",
      "l2 norm of gradients: 0.050480272039199074\n",
      "l2 norm of weights: 3.376336863084019\n",
      "---------------------\n",
      "Iteration Number: 8538\n",
      "Loss: 11.364634163677604\n",
      "l2 norm of gradients: 0.05047388718277692\n",
      "l2 norm of weights: 3.3764024367456735\n",
      "---------------------\n",
      "Iteration Number: 8539\n",
      "Loss: 11.364377804841192\n",
      "l2 norm of gradients: 0.05046750425114613\n",
      "l2 norm of weights: 3.376468007520767\n",
      "---------------------\n",
      "Iteration Number: 8540\n",
      "Loss: 11.364121540120253\n",
      "l2 norm of gradients: 0.0504611232430958\n",
      "l2 norm of weights: 3.376533575406037\n",
      "---------------------\n",
      "Iteration Number: 8541\n",
      "Loss: 11.363865369471965\n",
      "l2 norm of gradients: 0.050454744157416166\n",
      "l2 norm of weights: 3.3765991403982243\n",
      "---------------------\n",
      "Iteration Number: 8542\n",
      "Loss: 11.36360929285352\n",
      "l2 norm of gradients: 0.05044836699289874\n",
      "l2 norm of weights: 3.376664702494074\n",
      "---------------------\n",
      "Iteration Number: 8543\n",
      "Loss: 11.36335331022213\n",
      "l2 norm of gradients: 0.050441991748336176\n",
      "l2 norm of weights: 3.3767302616903336\n",
      "---------------------\n",
      "Iteration Number: 8544\n",
      "Loss: 11.363097421535063\n",
      "l2 norm of gradients: 0.050435618422522394\n",
      "l2 norm of weights: 3.3767958179837554\n",
      "---------------------\n",
      "Iteration Number: 8545\n",
      "Loss: 11.3628416267496\n",
      "l2 norm of gradients: 0.0504292470142525\n",
      "l2 norm of weights: 3.3768613713710933\n",
      "---------------------\n",
      "Iteration Number: 8546\n",
      "Loss: 11.36258592582306\n",
      "l2 norm of gradients: 0.05042287752232281\n",
      "l2 norm of weights: 3.376926921849107\n",
      "---------------------\n",
      "Iteration Number: 8547\n",
      "Loss: 11.362330318712779\n",
      "l2 norm of gradients: 0.050416509945530805\n",
      "l2 norm of weights: 3.3769924694145557\n",
      "---------------------\n",
      "Iteration Number: 8548\n",
      "Loss: 11.362074805376134\n",
      "l2 norm of gradients: 0.05041014428267524\n",
      "l2 norm of weights: 3.3770580140642052\n",
      "---------------------\n",
      "Iteration Number: 8549\n",
      "Loss: 11.361819385770533\n",
      "l2 norm of gradients: 0.05040378053255602\n",
      "l2 norm of weights: 3.377123555794825\n",
      "---------------------\n",
      "Iteration Number: 8550\n",
      "Loss: 11.361564059853405\n",
      "l2 norm of gradients: 0.05039741869397431\n",
      "l2 norm of weights: 3.3771890946031844\n",
      "---------------------\n",
      "Iteration Number: 8551\n",
      "Loss: 11.361308827582219\n",
      "l2 norm of gradients: 0.050391058765732355\n",
      "l2 norm of weights: 3.3772546304860596\n",
      "---------------------\n",
      "Iteration Number: 8552\n",
      "Loss: 11.361053688914454\n",
      "l2 norm of gradients: 0.050384700746633725\n",
      "l2 norm of weights: 3.377320163440229\n",
      "---------------------\n",
      "Iteration Number: 8553\n",
      "Loss: 11.360798643807643\n",
      "l2 norm of gradients: 0.0503783446354831\n",
      "l2 norm of weights: 3.377385693462473\n",
      "---------------------\n",
      "Iteration Number: 8554\n",
      "Loss: 11.36054369221933\n",
      "l2 norm of gradients: 0.05037199043108637\n",
      "l2 norm of weights: 3.3774512205495784\n",
      "---------------------\n",
      "Iteration Number: 8555\n",
      "Loss: 11.360288834107113\n",
      "l2 norm of gradients: 0.05036563813225066\n",
      "l2 norm of weights: 3.3775167446983314\n",
      "---------------------\n",
      "Iteration Number: 8556\n",
      "Loss: 11.36003406942858\n",
      "l2 norm of gradients: 0.05035928773778424\n",
      "l2 norm of weights: 3.377582265905525\n",
      "---------------------\n",
      "Iteration Number: 8557\n",
      "Loss: 11.359779398141377\n",
      "l2 norm of gradients: 0.05035293924649655\n",
      "l2 norm of weights: 3.3776477841679533\n",
      "---------------------\n",
      "Iteration Number: 8558\n",
      "Loss: 11.35952482020318\n",
      "l2 norm of gradients: 0.05034659265719827\n",
      "l2 norm of weights: 3.377713299482415\n",
      "---------------------\n",
      "Iteration Number: 8559\n",
      "Loss: 11.359270335571681\n",
      "l2 norm of gradients: 0.05034024796870123\n",
      "l2 norm of weights: 3.3777788118457104\n",
      "---------------------\n",
      "Iteration Number: 8560\n",
      "Loss: 11.35901594420461\n",
      "l2 norm of gradients: 0.05033390517981844\n",
      "l2 norm of weights: 3.377844321254646\n",
      "---------------------\n",
      "Iteration Number: 8561\n",
      "Loss: 11.35876164605972\n",
      "l2 norm of gradients: 0.05032756428936411\n",
      "l2 norm of weights: 3.3779098277060284\n",
      "---------------------\n",
      "Iteration Number: 8562\n",
      "Loss: 11.358507441094797\n",
      "l2 norm of gradients: 0.0503212252961536\n",
      "l2 norm of weights: 3.3779753311966703\n",
      "---------------------\n",
      "Iteration Number: 8563\n",
      "Loss: 11.358253329267669\n",
      "l2 norm of gradients: 0.0503148881990035\n",
      "l2 norm of weights: 3.378040831723385\n",
      "---------------------\n",
      "Iteration Number: 8564\n",
      "Loss: 11.357999310536162\n",
      "l2 norm of gradients: 0.0503085529967315\n",
      "l2 norm of weights: 3.3781063292829923\n",
      "---------------------\n",
      "Iteration Number: 8565\n",
      "Loss: 11.357745384858163\n",
      "l2 norm of gradients: 0.05030221968815653\n",
      "l2 norm of weights: 3.378171823872312\n",
      "---------------------\n",
      "Iteration Number: 8566\n",
      "Loss: 11.357491552191572\n",
      "l2 norm of gradients: 0.05029588827209866\n",
      "l2 norm of weights: 3.3782373154881693\n",
      "---------------------\n",
      "Iteration Number: 8567\n",
      "Loss: 11.35723781249431\n",
      "l2 norm of gradients: 0.05028955874737915\n",
      "l2 norm of weights: 3.3783028041273915\n",
      "---------------------\n",
      "Iteration Number: 8568\n",
      "Loss: 11.356984165724358\n",
      "l2 norm of gradients: 0.050283231112820376\n",
      "l2 norm of weights: 3.37836828978681\n",
      "---------------------\n",
      "Iteration Number: 8569\n",
      "Loss: 11.356730611839684\n",
      "l2 norm of gradients: 0.050276905367245946\n",
      "l2 norm of weights: 3.37843377246326\n",
      "---------------------\n",
      "Iteration Number: 8570\n",
      "Loss: 11.356477150798328\n",
      "l2 norm of gradients: 0.050270581509480605\n",
      "l2 norm of weights: 3.378499252153578\n",
      "---------------------\n",
      "Iteration Number: 8571\n",
      "Loss: 11.356223782558322\n",
      "l2 norm of gradients: 0.05026425953835029\n",
      "l2 norm of weights: 3.3785647288546055\n",
      "---------------------\n",
      "Iteration Number: 8572\n",
      "Loss: 11.355970507077755\n",
      "l2 norm of gradients: 0.05025793945268202\n",
      "l2 norm of weights: 3.378630202563186\n",
      "---------------------\n",
      "Iteration Number: 8573\n",
      "Loss: 11.35571732431472\n",
      "l2 norm of gradients: 0.05025162125130409\n",
      "l2 norm of weights: 3.378695673276168\n",
      "---------------------\n",
      "Iteration Number: 8574\n",
      "Loss: 11.355464234227364\n",
      "l2 norm of gradients: 0.05024530493304586\n",
      "l2 norm of weights: 3.3787611409904015\n",
      "---------------------\n",
      "Iteration Number: 8575\n",
      "Loss: 11.355211236773847\n",
      "l2 norm of gradients: 0.05023899049673789\n",
      "l2 norm of weights: 3.378826605702741\n",
      "---------------------\n",
      "Iteration Number: 8576\n",
      "Loss: 11.354958331912362\n",
      "l2 norm of gradients: 0.05023267794121187\n",
      "l2 norm of weights: 3.378892067410043\n",
      "---------------------\n",
      "Iteration Number: 8577\n",
      "Loss: 11.35470551960113\n",
      "l2 norm of gradients: 0.05022636726530068\n",
      "l2 norm of weights: 3.378957526109168\n",
      "---------------------\n",
      "Iteration Number: 8578\n",
      "Loss: 11.354452799798397\n",
      "l2 norm of gradients: 0.050220058467838315\n",
      "l2 norm of weights: 3.37902298179698\n",
      "---------------------\n",
      "Iteration Number: 8579\n",
      "Loss: 11.354200172462448\n",
      "l2 norm of gradients: 0.05021375154765996\n",
      "l2 norm of weights: 3.3790884344703453\n",
      "---------------------\n",
      "Iteration Number: 8580\n",
      "Loss: 11.353947637551586\n",
      "l2 norm of gradients: 0.05020744650360189\n",
      "l2 norm of weights: 3.3791538841261355\n",
      "---------------------\n",
      "Iteration Number: 8581\n",
      "Loss: 11.353695195024155\n",
      "l2 norm of gradients: 0.05020114333450162\n",
      "l2 norm of weights: 3.379219330761222\n",
      "---------------------\n",
      "Iteration Number: 8582\n",
      "Loss: 11.353442844838515\n",
      "l2 norm of gradients: 0.050194842039197676\n",
      "l2 norm of weights: 3.3792847743724828\n",
      "---------------------\n",
      "Iteration Number: 8583\n",
      "Loss: 11.353190586953065\n",
      "l2 norm of gradients: 0.05018854261652989\n",
      "l2 norm of weights: 3.3793502149567973\n",
      "---------------------\n",
      "Iteration Number: 8584\n",
      "Loss: 11.35293842132622\n",
      "l2 norm of gradients: 0.050182245065339096\n",
      "l2 norm of weights: 3.379415652511048\n",
      "---------------------\n",
      "Iteration Number: 8585\n",
      "Loss: 11.352686347916428\n",
      "l2 norm of gradients: 0.05017594938446734\n",
      "l2 norm of weights: 3.3794810870321212\n",
      "---------------------\n",
      "Iteration Number: 8586\n",
      "Loss: 11.35243436668219\n",
      "l2 norm of gradients: 0.05016965557275782\n",
      "l2 norm of weights: 3.379546518516907\n",
      "---------------------\n",
      "Iteration Number: 8587\n",
      "Loss: 11.352182477581986\n",
      "l2 norm of gradients: 0.0501633636290548\n",
      "l2 norm of weights: 3.3796119469622967\n",
      "---------------------\n",
      "Iteration Number: 8588\n",
      "Loss: 11.351930680574378\n",
      "l2 norm of gradients: 0.05015707355220376\n",
      "l2 norm of weights: 3.3796773723651876\n",
      "---------------------\n",
      "Iteration Number: 8589\n",
      "Loss: 11.351678975617919\n",
      "l2 norm of gradients: 0.05015078534105126\n",
      "l2 norm of weights: 3.3797427947224783\n",
      "---------------------\n",
      "Iteration Number: 8590\n",
      "Loss: 11.3514273626712\n",
      "l2 norm of gradients: 0.05014449899444502\n",
      "l2 norm of weights: 3.3798082140310703\n",
      "---------------------\n",
      "Iteration Number: 8591\n",
      "Loss: 11.351175841692854\n",
      "l2 norm of gradients: 0.05013821451123388\n",
      "l2 norm of weights: 3.37987363028787\n",
      "---------------------\n",
      "Iteration Number: 8592\n",
      "Loss: 11.350924412641524\n",
      "l2 norm of gradients: 0.05013193189026782\n",
      "l2 norm of weights: 3.3799390434897845\n",
      "---------------------\n",
      "Iteration Number: 8593\n",
      "Loss: 11.350673075475896\n",
      "l2 norm of gradients: 0.05012565113039792\n",
      "l2 norm of weights: 3.3800044536337266\n",
      "---------------------\n",
      "Iteration Number: 8594\n",
      "Loss: 11.350421830154668\n",
      "l2 norm of gradients: 0.05011937223047644\n",
      "l2 norm of weights: 3.3800698607166106\n",
      "---------------------\n",
      "Iteration Number: 8595\n",
      "Loss: 11.350170676636585\n",
      "l2 norm of gradients: 0.05011309518935671\n",
      "l2 norm of weights: 3.3801352647353555\n",
      "---------------------\n",
      "Iteration Number: 8596\n",
      "Loss: 11.349919614880408\n",
      "l2 norm of gradients: 0.050106820005893206\n",
      "l2 norm of weights: 3.3802006656868815\n",
      "---------------------\n",
      "Iteration Number: 8597\n",
      "Loss: 11.349668644844927\n",
      "l2 norm of gradients: 0.05010054667894152\n",
      "l2 norm of weights: 3.3802660635681137\n",
      "---------------------\n",
      "Iteration Number: 8598\n",
      "Loss: 11.349417766488965\n",
      "l2 norm of gradients: 0.05009427520735838\n",
      "l2 norm of weights: 3.3803314583759785\n",
      "---------------------\n",
      "Iteration Number: 8599\n",
      "Loss: 11.349166979771372\n",
      "l2 norm of gradients: 0.05008800559000162\n",
      "l2 norm of weights: 3.3803968501074086\n",
      "---------------------\n",
      "Iteration Number: 8600\n",
      "Loss: 11.348916284651027\n",
      "l2 norm of gradients: 0.0500817378257302\n",
      "l2 norm of weights: 3.380462238759336\n",
      "---------------------\n",
      "Iteration Number: 8601\n",
      "Loss: 11.34866568108683\n",
      "l2 norm of gradients: 0.050075471913404154\n",
      "l2 norm of weights: 3.380527624328699\n",
      "---------------------\n",
      "Iteration Number: 8602\n",
      "Loss: 11.348415169037716\n",
      "l2 norm of gradients: 0.0500692078518847\n",
      "l2 norm of weights: 3.3805930068124366\n",
      "---------------------\n",
      "Iteration Number: 8603\n",
      "Loss: 11.348164748462654\n",
      "l2 norm of gradients: 0.05006294564003412\n",
      "l2 norm of weights: 3.380658386207492\n",
      "---------------------\n",
      "Iteration Number: 8604\n",
      "Loss: 11.347914419320631\n",
      "l2 norm of gradients: 0.050056685276715814\n",
      "l2 norm of weights: 3.3807237625108133\n",
      "---------------------\n",
      "Iteration Number: 8605\n",
      "Loss: 11.347664181570657\n",
      "l2 norm of gradients: 0.05005042676079426\n",
      "l2 norm of weights: 3.3807891357193487\n",
      "---------------------\n",
      "Iteration Number: 8606\n",
      "Loss: 11.347414035171793\n",
      "l2 norm of gradients: 0.050044170091135126\n",
      "l2 norm of weights: 3.3808545058300505\n",
      "---------------------\n",
      "Iteration Number: 8607\n",
      "Loss: 11.3471639800831\n",
      "l2 norm of gradients: 0.050037915266605136\n",
      "l2 norm of weights: 3.3809198728398755\n",
      "---------------------\n",
      "Iteration Number: 8608\n",
      "Loss: 11.34691401626368\n",
      "l2 norm of gradients: 0.05003166228607211\n",
      "l2 norm of weights: 3.380985236745782\n",
      "---------------------\n",
      "Iteration Number: 8609\n",
      "Loss: 11.346664143672674\n",
      "l2 norm of gradients: 0.05002541114840498\n",
      "l2 norm of weights: 3.3810505975447325\n",
      "---------------------\n",
      "Iteration Number: 8610\n",
      "Loss: 11.34641436226923\n",
      "l2 norm of gradients: 0.05001916185247379\n",
      "l2 norm of weights: 3.381115955233691\n",
      "---------------------\n",
      "Iteration Number: 8611\n",
      "Loss: 11.346164672012542\n",
      "l2 norm of gradients: 0.05001291439714965\n",
      "l2 norm of weights: 3.381181309809627\n",
      "---------------------\n",
      "Iteration Number: 8612\n",
      "Loss: 11.34591507286182\n",
      "l2 norm of gradients: 0.05000666878130481\n",
      "l2 norm of weights: 3.381246661269512\n",
      "---------------------\n",
      "Iteration Number: 8613\n",
      "Loss: 11.345665564776306\n",
      "l2 norm of gradients: 0.05000042500381256\n",
      "l2 norm of weights: 3.381312009610319\n",
      "---------------------\n",
      "Iteration Number: 8614\n",
      "Loss: 11.345416147715262\n",
      "l2 norm of gradients: 0.04999418306354741\n",
      "l2 norm of weights: 3.3813773548290267\n",
      "---------------------\n",
      "Iteration Number: 8615\n",
      "Loss: 11.345166821638\n",
      "l2 norm of gradients: 0.049987942959384794\n",
      "l2 norm of weights: 3.381442696922615\n",
      "---------------------\n",
      "Iteration Number: 8616\n",
      "Loss: 11.344917586503836\n",
      "l2 norm of gradients: 0.04998170469020136\n",
      "l2 norm of weights: 3.3815080358880683\n",
      "---------------------\n",
      "Iteration Number: 8617\n",
      "Loss: 11.34466844227213\n",
      "l2 norm of gradients: 0.049975468254874804\n",
      "l2 norm of weights: 3.3815733717223733\n",
      "---------------------\n",
      "Iteration Number: 8618\n",
      "Loss: 11.344419388902253\n",
      "l2 norm of gradients: 0.049969233652283916\n",
      "l2 norm of weights: 3.3816387044225187\n",
      "---------------------\n",
      "Iteration Number: 8619\n",
      "Loss: 11.344170426353621\n",
      "l2 norm of gradients: 0.04996300088130856\n",
      "l2 norm of weights: 3.381704033985499\n",
      "---------------------\n",
      "Iteration Number: 8620\n",
      "Loss: 11.343921554585666\n",
      "l2 norm of gradients: 0.04995676994082972\n",
      "l2 norm of weights: 3.381769360408309\n",
      "---------------------\n",
      "Iteration Number: 8621\n",
      "Loss: 11.343672773557856\n",
      "l2 norm of gradients: 0.04995054082972941\n",
      "l2 norm of weights: 3.3818346836879485\n",
      "---------------------\n",
      "Iteration Number: 8622\n",
      "Loss: 11.34342408322968\n",
      "l2 norm of gradients: 0.04994431354689078\n",
      "l2 norm of weights: 3.38190000382142\n",
      "---------------------\n",
      "Iteration Number: 8623\n",
      "Loss: 11.343175483560657\n",
      "l2 norm of gradients: 0.04993808809119805\n",
      "l2 norm of weights: 3.3819653208057274\n",
      "---------------------\n",
      "Iteration Number: 8624\n",
      "Loss: 11.342926974510336\n",
      "l2 norm of gradients: 0.049931864461536496\n",
      "l2 norm of weights: 3.3820306346378795\n",
      "---------------------\n",
      "Iteration Number: 8625\n",
      "Loss: 11.342678556038287\n",
      "l2 norm of gradients: 0.049925642656792474\n",
      "l2 norm of weights: 3.3820959453148887\n",
      "---------------------\n",
      "Iteration Number: 8626\n",
      "Loss: 11.342430228104117\n",
      "l2 norm of gradients: 0.04991942267585345\n",
      "l2 norm of weights: 3.3821612528337677\n",
      "---------------------\n",
      "Iteration Number: 8627\n",
      "Loss: 11.342181990667449\n",
      "l2 norm of gradients: 0.04991320451760793\n",
      "l2 norm of weights: 3.3822265571915353\n",
      "---------------------\n",
      "Iteration Number: 8628\n",
      "Loss: 11.341933843687944\n",
      "l2 norm of gradients: 0.049906988180945536\n",
      "l2 norm of weights: 3.382291858385211\n",
      "---------------------\n",
      "Iteration Number: 8629\n",
      "Loss: 11.341685787125286\n",
      "l2 norm of gradients: 0.04990077366475689\n",
      "l2 norm of weights: 3.3823571564118184\n",
      "---------------------\n",
      "Iteration Number: 8630\n",
      "Loss: 11.341437820939188\n",
      "l2 norm of gradients: 0.04989456096793378\n",
      "l2 norm of weights: 3.3824224512683845\n",
      "---------------------\n",
      "Iteration Number: 8631\n",
      "Loss: 11.341189945089383\n",
      "l2 norm of gradients: 0.04988835008936896\n",
      "l2 norm of weights: 3.3824877429519384\n",
      "---------------------\n",
      "Iteration Number: 8632\n",
      "Loss: 11.34094215953565\n",
      "l2 norm of gradients: 0.04988214102795635\n",
      "l2 norm of weights: 3.3825530314595125\n",
      "---------------------\n",
      "Iteration Number: 8633\n",
      "Loss: 11.34069446423777\n",
      "l2 norm of gradients: 0.04987593378259087\n",
      "l2 norm of weights: 3.382618316788143\n",
      "---------------------\n",
      "Iteration Number: 8634\n",
      "Loss: 11.340446859155572\n",
      "l2 norm of gradients: 0.04986972835216852\n",
      "l2 norm of weights: 3.382683598934868\n",
      "---------------------\n",
      "Iteration Number: 8635\n",
      "Loss: 11.340199344248896\n",
      "l2 norm of gradients: 0.04986352473558639\n",
      "l2 norm of weights: 3.38274887789673\n",
      "---------------------\n",
      "Iteration Number: 8636\n",
      "Loss: 11.339951919477633\n",
      "l2 norm of gradients: 0.049857322931742575\n",
      "l2 norm of weights: 3.3828141536707723\n",
      "---------------------\n",
      "Iteration Number: 8637\n",
      "Loss: 11.339704584801671\n",
      "l2 norm of gradients: 0.049851122939536315\n",
      "l2 norm of weights: 3.382879426254043\n",
      "---------------------\n",
      "Iteration Number: 8638\n",
      "Loss: 11.339457340180946\n",
      "l2 norm of gradients: 0.04984492475786781\n",
      "l2 norm of weights: 3.3829446956435936\n",
      "---------------------\n",
      "Iteration Number: 8639\n",
      "Loss: 11.339210185575418\n",
      "l2 norm of gradients: 0.049838728385638395\n",
      "l2 norm of weights: 3.3830099618364766\n",
      "---------------------\n",
      "Iteration Number: 8640\n",
      "Loss: 11.338963120945063\n",
      "l2 norm of gradients: 0.049832533821750465\n",
      "l2 norm of weights: 3.383075224829749\n",
      "---------------------\n",
      "Iteration Number: 8641\n",
      "Loss: 11.338716146249897\n",
      "l2 norm of gradients: 0.04982634106510736\n",
      "l2 norm of weights: 3.383140484620471\n",
      "---------------------\n",
      "Iteration Number: 8642\n",
      "Loss: 11.338469261449971\n",
      "l2 norm of gradients: 0.04982015011461362\n",
      "l2 norm of weights: 3.3832057412057037\n",
      "---------------------\n",
      "Iteration Number: 8643\n",
      "Loss: 11.338222466505334\n",
      "l2 norm of gradients: 0.049813960969174745\n",
      "l2 norm of weights: 3.383270994582514\n",
      "---------------------\n",
      "Iteration Number: 8644\n",
      "Loss: 11.337975761376086\n",
      "l2 norm of gradients: 0.049807773627697303\n",
      "l2 norm of weights: 3.3833362447479702\n",
      "---------------------\n",
      "Iteration Number: 8645\n",
      "Loss: 11.337729146022344\n",
      "l2 norm of gradients: 0.04980158808908888\n",
      "l2 norm of weights: 3.383401491699144\n",
      "---------------------\n",
      "Iteration Number: 8646\n",
      "Loss: 11.337482620404268\n",
      "l2 norm of gradients: 0.04979540435225824\n",
      "l2 norm of weights: 3.3834667354331094\n",
      "---------------------\n",
      "Iteration Number: 8647\n",
      "Loss: 11.337236184482014\n",
      "l2 norm of gradients: 0.04978922241611502\n",
      "l2 norm of weights: 3.383531975946944\n",
      "---------------------\n",
      "Iteration Number: 8648\n",
      "Loss: 11.336989838215791\n",
      "l2 norm of gradients: 0.049783042279569985\n",
      "l2 norm of weights: 3.383597213237729\n",
      "---------------------\n",
      "Iteration Number: 8649\n",
      "Loss: 11.336743581565838\n",
      "l2 norm of gradients: 0.04977686394153496\n",
      "l2 norm of weights: 3.383662447302547\n",
      "---------------------\n",
      "Iteration Number: 8650\n",
      "Loss: 11.33649741449239\n",
      "l2 norm of gradients: 0.04977068740092276\n",
      "l2 norm of weights: 3.383727678138485\n",
      "---------------------\n",
      "Iteration Number: 8651\n",
      "Loss: 11.33625133695574\n",
      "l2 norm of gradients: 0.04976451265664732\n",
      "l2 norm of weights: 3.383792905742631\n",
      "---------------------\n",
      "Iteration Number: 8652\n",
      "Loss: 11.3360053489162\n",
      "l2 norm of gradients: 0.0497583397076235\n",
      "l2 norm of weights: 3.3838581301120785\n",
      "---------------------\n",
      "Iteration Number: 8653\n",
      "Loss: 11.3357594503341\n",
      "l2 norm of gradients: 0.049752168552767304\n",
      "l2 norm of weights: 3.3839233512439226\n",
      "---------------------\n",
      "Iteration Number: 8654\n",
      "Loss: 11.335513641169802\n",
      "l2 norm of gradients: 0.04974599919099571\n",
      "l2 norm of weights: 3.383988569135261\n",
      "---------------------\n",
      "Iteration Number: 8655\n",
      "Loss: 11.335267921383696\n",
      "l2 norm of gradients: 0.04973983162122675\n",
      "l2 norm of weights: 3.384053783783195\n",
      "---------------------\n",
      "Iteration Number: 8656\n",
      "Loss: 11.335022290936205\n",
      "l2 norm of gradients: 0.04973366584237946\n",
      "l2 norm of weights: 3.3841189951848287\n",
      "---------------------\n",
      "Iteration Number: 8657\n",
      "Loss: 11.334776749787757\n",
      "l2 norm of gradients: 0.04972750185337395\n",
      "l2 norm of weights: 3.3841842033372687\n",
      "---------------------\n",
      "Iteration Number: 8658\n",
      "Loss: 11.334531297898838\n",
      "l2 norm of gradients: 0.04972133965313137\n",
      "l2 norm of weights: 3.384249408237625\n",
      "---------------------\n",
      "Iteration Number: 8659\n",
      "Loss: 11.334285935229936\n",
      "l2 norm of gradients: 0.04971517924057385\n",
      "l2 norm of weights: 3.3843146098830115\n",
      "---------------------\n",
      "Iteration Number: 8660\n",
      "Loss: 11.334040661741573\n",
      "l2 norm of gradients: 0.04970902061462454\n",
      "l2 norm of weights: 3.3843798082705416\n",
      "---------------------\n",
      "Iteration Number: 8661\n",
      "Loss: 11.333795477394302\n",
      "l2 norm of gradients: 0.04970286377420769\n",
      "l2 norm of weights: 3.3844450033973366\n",
      "---------------------\n",
      "Iteration Number: 8662\n",
      "Loss: 11.333550382148697\n",
      "l2 norm of gradients: 0.04969670871824852\n",
      "l2 norm of weights: 3.384510195260516\n",
      "---------------------\n",
      "Iteration Number: 8663\n",
      "Loss: 11.333305375965363\n",
      "l2 norm of gradients: 0.04969055544567326\n",
      "l2 norm of weights: 3.384575383857205\n",
      "---------------------\n",
      "Iteration Number: 8664\n",
      "Loss: 11.333060458804926\n",
      "l2 norm of gradients: 0.04968440395540919\n",
      "l2 norm of weights: 3.3846405691845307\n",
      "---------------------\n",
      "Iteration Number: 8665\n",
      "Loss: 11.332815630628046\n",
      "l2 norm of gradients: 0.04967825424638461\n",
      "l2 norm of weights: 3.384705751239624\n",
      "---------------------\n",
      "Iteration Number: 8666\n",
      "Loss: 11.332570891395404\n",
      "l2 norm of gradients: 0.0496721063175288\n",
      "l2 norm of weights: 3.3847709300196174\n",
      "---------------------\n",
      "Iteration Number: 8667\n",
      "Loss: 11.332326241067706\n",
      "l2 norm of gradients: 0.04966596016777217\n",
      "l2 norm of weights: 3.3848361055216474\n",
      "---------------------\n",
      "Iteration Number: 8668\n",
      "Loss: 11.332081679605691\n",
      "l2 norm of gradients: 0.04965981579604598\n",
      "l2 norm of weights: 3.3849012777428524\n",
      "---------------------\n",
      "Iteration Number: 8669\n",
      "Loss: 11.331837206970121\n",
      "l2 norm of gradients: 0.049653673201282625\n",
      "l2 norm of weights: 3.384966446680375\n",
      "---------------------\n",
      "Iteration Number: 8670\n",
      "Loss: 11.331592823121781\n",
      "l2 norm of gradients: 0.04964753238241549\n",
      "l2 norm of weights: 3.385031612331359\n",
      "---------------------\n",
      "Iteration Number: 8671\n",
      "Loss: 11.331348528021493\n",
      "l2 norm of gradients: 0.04964139333837893\n",
      "l2 norm of weights: 3.3850967746929523\n",
      "---------------------\n",
      "Iteration Number: 8672\n",
      "Loss: 11.331104321630088\n",
      "l2 norm of gradients: 0.04963525606810836\n",
      "l2 norm of weights: 3.385161933762306\n",
      "---------------------\n",
      "Iteration Number: 8673\n",
      "Loss: 11.330860203908442\n",
      "l2 norm of gradients: 0.04962912057054016\n",
      "l2 norm of weights: 3.385227089536572\n",
      "---------------------\n",
      "Iteration Number: 8674\n",
      "Loss: 11.330616174817454\n",
      "l2 norm of gradients: 0.04962298684461178\n",
      "l2 norm of weights: 3.385292242012908\n",
      "---------------------\n",
      "Iteration Number: 8675\n",
      "Loss: 11.330372234318022\n",
      "l2 norm of gradients: 0.049616854889261604\n",
      "l2 norm of weights: 3.3853573911884722\n",
      "---------------------\n",
      "Iteration Number: 8676\n",
      "Loss: 11.330128382371118\n",
      "l2 norm of gradients: 0.04961072470342906\n",
      "l2 norm of weights: 3.3854225370604265\n",
      "---------------------\n",
      "Iteration Number: 8677\n",
      "Loss: 11.329884618937697\n",
      "l2 norm of gradients: 0.0496045962860546\n",
      "l2 norm of weights: 3.385487679625936\n",
      "---------------------\n",
      "Iteration Number: 8678\n",
      "Loss: 11.329640943978765\n",
      "l2 norm of gradients: 0.049598469636079605\n",
      "l2 norm of weights: 3.385552818882168\n",
      "---------------------\n",
      "Iteration Number: 8679\n",
      "Loss: 11.329397357455349\n",
      "l2 norm of gradients: 0.049592344752446546\n",
      "l2 norm of weights: 3.385617954826293\n",
      "---------------------\n",
      "Iteration Number: 8680\n",
      "Loss: 11.329153859328496\n",
      "l2 norm of gradients: 0.04958622163409883\n",
      "l2 norm of weights: 3.385683087455485\n",
      "---------------------\n",
      "Iteration Number: 8681\n",
      "Loss: 11.328910449559281\n",
      "l2 norm of gradients: 0.04958010027998087\n",
      "l2 norm of weights: 3.385748216766919\n",
      "---------------------\n",
      "Iteration Number: 8682\n",
      "Loss: 11.328667128108819\n",
      "l2 norm of gradients: 0.0495739806890381\n",
      "l2 norm of weights: 3.3858133427577743\n",
      "---------------------\n",
      "Iteration Number: 8683\n",
      "Loss: 11.328423894938227\n",
      "l2 norm of gradients: 0.049567862860216964\n",
      "l2 norm of weights: 3.3858784654252334\n",
      "---------------------\n",
      "Iteration Number: 8684\n",
      "Loss: 11.328180750008675\n",
      "l2 norm of gradients: 0.049561746792464854\n",
      "l2 norm of weights: 3.3859435847664803\n",
      "---------------------\n",
      "Iteration Number: 8685\n",
      "Loss: 11.327937693281337\n",
      "l2 norm of gradients: 0.049555632484730175\n",
      "l2 norm of weights: 3.386008700778703\n",
      "---------------------\n",
      "Iteration Number: 8686\n",
      "Loss: 11.327694724717416\n",
      "l2 norm of gradients: 0.04954951993596229\n",
      "l2 norm of weights: 3.386073813459091\n",
      "---------------------\n",
      "Iteration Number: 8687\n",
      "Loss: 11.327451844278151\n",
      "l2 norm of gradients: 0.04954340914511164\n",
      "l2 norm of weights: 3.3861389228048377\n",
      "---------------------\n",
      "Iteration Number: 8688\n",
      "Loss: 11.327209051924804\n",
      "l2 norm of gradients: 0.04953730011112956\n",
      "l2 norm of weights: 3.386204028813139\n",
      "---------------------\n",
      "Iteration Number: 8689\n",
      "Loss: 11.326966347618654\n",
      "l2 norm of gradients: 0.04953119283296844\n",
      "l2 norm of weights: 3.3862691314811943\n",
      "---------------------\n",
      "Iteration Number: 8690\n",
      "Loss: 11.326723731321032\n",
      "l2 norm of gradients: 0.049525087309581564\n",
      "l2 norm of weights: 3.386334230806204\n",
      "---------------------\n",
      "Iteration Number: 8691\n",
      "Loss: 11.326481202993245\n",
      "l2 norm of gradients: 0.04951898353992334\n",
      "l2 norm of weights: 3.386399326785374\n",
      "---------------------\n",
      "Iteration Number: 8692\n",
      "Loss: 11.326238762596683\n",
      "l2 norm of gradients: 0.049512881522949054\n",
      "l2 norm of weights: 3.3864644194159097\n",
      "---------------------\n",
      "Iteration Number: 8693\n",
      "Loss: 11.32599641009273\n",
      "l2 norm of gradients: 0.049506781257615\n",
      "l2 norm of weights: 3.386529508695022\n",
      "---------------------\n",
      "Iteration Number: 8694\n",
      "Loss: 11.32575414544278\n",
      "l2 norm of gradients: 0.049500682742878443\n",
      "l2 norm of weights: 3.386594594619923\n",
      "---------------------\n",
      "Iteration Number: 8695\n",
      "Loss: 11.325511968608307\n",
      "l2 norm of gradients: 0.04949458597769763\n",
      "l2 norm of weights: 3.3866596771878292\n",
      "---------------------\n",
      "Iteration Number: 8696\n",
      "Loss: 11.325269879550763\n",
      "l2 norm of gradients: 0.04948849096103184\n",
      "l2 norm of weights: 3.3867247563959584\n",
      "---------------------\n",
      "Iteration Number: 8697\n",
      "Loss: 11.32502787823164\n",
      "l2 norm of gradients: 0.04948239769184125\n",
      "l2 norm of weights: 3.386789832241531\n",
      "---------------------\n",
      "Iteration Number: 8698\n",
      "Loss: 11.324785964612456\n",
      "l2 norm of gradients: 0.04947630616908707\n",
      "l2 norm of weights: 3.386854904721772\n",
      "---------------------\n",
      "Iteration Number: 8699\n",
      "Loss: 11.324544138654758\n",
      "l2 norm of gradients: 0.04947021639173141\n",
      "l2 norm of weights: 3.386919973833907\n",
      "---------------------\n",
      "Iteration Number: 8700\n",
      "Loss: 11.32430240032012\n",
      "l2 norm of gradients: 0.049464128358737466\n",
      "l2 norm of weights: 3.3869850395751664\n",
      "---------------------\n",
      "Iteration Number: 8701\n",
      "Loss: 11.32406074957013\n",
      "l2 norm of gradients: 0.0494580420690693\n",
      "l2 norm of weights: 3.387050101942782\n",
      "---------------------\n",
      "Iteration Number: 8702\n",
      "Loss: 11.323819186366409\n",
      "l2 norm of gradients: 0.049451957521691965\n",
      "l2 norm of weights: 3.387115160933988\n",
      "---------------------\n",
      "Iteration Number: 8703\n",
      "Loss: 11.323577710670616\n",
      "l2 norm of gradients: 0.04944587471557158\n",
      "l2 norm of weights: 3.3871802165460227\n",
      "---------------------\n",
      "Iteration Number: 8704\n",
      "Loss: 11.323336322444412\n",
      "l2 norm of gradients: 0.049439793649675046\n",
      "l2 norm of weights: 3.3872452687761267\n",
      "---------------------\n",
      "Iteration Number: 8705\n",
      "Loss: 11.3230950216495\n",
      "l2 norm of gradients: 0.04943371432297043\n",
      "l2 norm of weights: 3.387310317621543\n",
      "---------------------\n",
      "Iteration Number: 8706\n",
      "Loss: 11.3228538082476\n",
      "l2 norm of gradients: 0.04942763673442664\n",
      "l2 norm of weights: 3.387375363079518\n",
      "---------------------\n",
      "Iteration Number: 8707\n",
      "Loss: 11.322612682200464\n",
      "l2 norm of gradients: 0.049421560883013596\n",
      "l2 norm of weights: 3.3874404051472995\n",
      "---------------------\n",
      "Iteration Number: 8708\n",
      "Loss: 11.322371643469877\n",
      "l2 norm of gradients: 0.04941548676770211\n",
      "l2 norm of weights: 3.3875054438221395\n",
      "---------------------\n",
      "Iteration Number: 8709\n",
      "Loss: 11.322130692017621\n",
      "l2 norm of gradients: 0.04940941438746408\n",
      "l2 norm of weights: 3.3875704791012917\n",
      "---------------------\n",
      "Iteration Number: 8710\n",
      "Loss: 11.321889827805538\n",
      "l2 norm of gradients: 0.049403343741272226\n",
      "l2 norm of weights: 3.387635510982014\n",
      "---------------------\n",
      "Iteration Number: 8711\n",
      "Loss: 11.321649050795468\n",
      "l2 norm of gradients: 0.04939727482810034\n",
      "l2 norm of weights: 3.387700539461565\n",
      "---------------------\n",
      "Iteration Number: 8712\n",
      "Loss: 11.3214083609493\n",
      "l2 norm of gradients: 0.049391207646923105\n",
      "l2 norm of weights: 3.3877655645372067\n",
      "---------------------\n",
      "Iteration Number: 8713\n",
      "Loss: 11.321167758228919\n",
      "l2 norm of gradients: 0.04938514219671618\n",
      "l2 norm of weights: 3.387830586206206\n",
      "---------------------\n",
      "Iteration Number: 8714\n",
      "Loss: 11.320927242596266\n",
      "l2 norm of gradients: 0.04937907847645616\n",
      "l2 norm of weights: 3.387895604465829\n",
      "---------------------\n",
      "Iteration Number: 8715\n",
      "Loss: 11.320686814013293\n",
      "l2 norm of gradients: 0.049373016485120644\n",
      "l2 norm of weights: 3.387960619313347\n",
      "---------------------\n",
      "Iteration Number: 8716\n",
      "Loss: 11.32044647244198\n",
      "l2 norm of gradients: 0.049366956221688116\n",
      "l2 norm of weights: 3.388025630746033\n",
      "---------------------\n",
      "Iteration Number: 8717\n",
      "Loss: 11.320206217844323\n",
      "l2 norm of gradients: 0.04936089768513805\n",
      "l2 norm of weights: 3.3880906387611627\n",
      "---------------------\n",
      "Iteration Number: 8718\n",
      "Loss: 11.319966050182346\n",
      "l2 norm of gradients: 0.04935484087445089\n",
      "l2 norm of weights: 3.3881556433560154\n",
      "---------------------\n",
      "Iteration Number: 8719\n",
      "Loss: 11.31972596941812\n",
      "l2 norm of gradients: 0.04934878578860796\n",
      "l2 norm of weights: 3.3882206445278715\n",
      "---------------------\n",
      "Iteration Number: 8720\n",
      "Loss: 11.31948597551372\n",
      "l2 norm of gradients: 0.04934273242659159\n",
      "l2 norm of weights: 3.3882856422740164\n",
      "---------------------\n",
      "Iteration Number: 8721\n",
      "Loss: 11.319246068431244\n",
      "l2 norm of gradients: 0.04933668078738504\n",
      "l2 norm of weights: 3.3883506365917353\n",
      "---------------------\n",
      "Iteration Number: 8722\n",
      "Loss: 11.319006248132816\n",
      "l2 norm of gradients: 0.04933063086997251\n",
      "l2 norm of weights: 3.3884156274783184\n",
      "---------------------\n",
      "Iteration Number: 8723\n",
      "Loss: 11.318766514580615\n",
      "l2 norm of gradients: 0.04932458267333913\n",
      "l2 norm of weights: 3.3884806149310585\n",
      "---------------------\n",
      "Iteration Number: 8724\n",
      "Loss: 11.318526867736797\n",
      "l2 norm of gradients: 0.04931853619647101\n",
      "l2 norm of weights: 3.388545598947249\n",
      "---------------------\n",
      "Iteration Number: 8725\n",
      "Loss: 11.318287307563581\n",
      "l2 norm of gradients: 0.04931249143835516\n",
      "l2 norm of weights: 3.3886105795241876\n",
      "---------------------\n",
      "Iteration Number: 8726\n",
      "Loss: 11.318047834023186\n",
      "l2 norm of gradients: 0.04930644839797953\n",
      "l2 norm of weights: 3.388675556659175\n",
      "---------------------\n",
      "Iteration Number: 8727\n",
      "Loss: 11.317808447077883\n",
      "l2 norm of gradients: 0.049300407074333044\n",
      "l2 norm of weights: 3.388740530349514\n",
      "---------------------\n",
      "Iteration Number: 8728\n",
      "Loss: 11.317569146689946\n",
      "l2 norm of gradients: 0.04929436746640557\n",
      "l2 norm of weights: 3.3888055005925097\n",
      "---------------------\n",
      "Iteration Number: 8729\n",
      "Loss: 11.317329932821671\n",
      "l2 norm of gradients: 0.04928832957318783\n",
      "l2 norm of weights: 3.388870467385471\n",
      "---------------------\n",
      "Iteration Number: 8730\n",
      "Loss: 11.317090805435406\n",
      "l2 norm of gradients: 0.04928229339367157\n",
      "l2 norm of weights: 3.3889354307257076\n",
      "---------------------\n",
      "Iteration Number: 8731\n",
      "Loss: 11.31685176449349\n",
      "l2 norm of gradients: 0.04927625892684942\n",
      "l2 norm of weights: 3.389000390610534\n",
      "---------------------\n",
      "Iteration Number: 8732\n",
      "Loss: 11.316612809958311\n",
      "l2 norm of gradients: 0.04927022617171495\n",
      "l2 norm of weights: 3.3890653470372656\n",
      "---------------------\n",
      "Iteration Number: 8733\n",
      "Loss: 11.316373941792282\n",
      "l2 norm of gradients: 0.049264195127262655\n",
      "l2 norm of weights: 3.3891303000032216\n",
      "---------------------\n",
      "Iteration Number: 8734\n",
      "Loss: 11.316135159957824\n",
      "l2 norm of gradients: 0.049258165792488026\n",
      "l2 norm of weights: 3.3891952495057236\n",
      "---------------------\n",
      "Iteration Number: 8735\n",
      "Loss: 11.315896464417392\n",
      "l2 norm of gradients: 0.04925213816638734\n",
      "l2 norm of weights: 3.3892601955420947\n",
      "---------------------\n",
      "Iteration Number: 8736\n",
      "Loss: 11.315657855133471\n",
      "l2 norm of gradients: 0.04924611224795798\n",
      "l2 norm of weights: 3.3893251381096627\n",
      "---------------------\n",
      "Iteration Number: 8737\n",
      "Loss: 11.315419332068565\n",
      "l2 norm of gradients: 0.04924008803619808\n",
      "l2 norm of weights: 3.389390077205756\n",
      "---------------------\n",
      "Iteration Number: 8738\n",
      "Loss: 11.315180895185204\n",
      "l2 norm of gradients: 0.049234065530106834\n",
      "l2 norm of weights: 3.3894550128277077\n",
      "---------------------\n",
      "Iteration Number: 8739\n",
      "Loss: 11.314942544445934\n",
      "l2 norm of gradients: 0.04922804472868427\n",
      "l2 norm of weights: 3.3895199449728515\n",
      "---------------------\n",
      "Iteration Number: 8740\n",
      "Loss: 11.314704279813352\n",
      "l2 norm of gradients: 0.04922202563093142\n",
      "l2 norm of weights: 3.389584873638525\n",
      "---------------------\n",
      "Iteration Number: 8741\n",
      "Loss: 11.314466101250051\n",
      "l2 norm of gradients: 0.04921600823585013\n",
      "l2 norm of weights: 3.3896497988220675\n",
      "---------------------\n",
      "Iteration Number: 8742\n",
      "Loss: 11.314228008718667\n",
      "l2 norm of gradients: 0.049209992542443265\n",
      "l2 norm of weights: 3.3897147205208222\n",
      "---------------------\n",
      "Iteration Number: 8743\n",
      "Loss: 11.313990002181843\n",
      "l2 norm of gradients: 0.049203978549714554\n",
      "l2 norm of weights: 3.3897796387321337\n",
      "---------------------\n",
      "Iteration Number: 8744\n",
      "Loss: 11.313752081602265\n",
      "l2 norm of gradients: 0.04919796625666867\n",
      "l2 norm of weights: 3.38984455345335\n",
      "---------------------\n",
      "Iteration Number: 8745\n",
      "Loss: 11.313514246942631\n",
      "l2 norm of gradients: 0.049191955662311174\n",
      "l2 norm of weights: 3.3899094646818213\n",
      "---------------------\n",
      "Iteration Number: 8746\n",
      "Loss: 11.313276498165687\n",
      "l2 norm of gradients: 0.04918594676564855\n",
      "l2 norm of weights: 3.3899743724149003\n",
      "---------------------\n",
      "Iteration Number: 8747\n",
      "Loss: 11.313038835234158\n",
      "l2 norm of gradients: 0.04917993956568826\n",
      "l2 norm of weights: 3.390039276649943\n",
      "---------------------\n",
      "Iteration Number: 8748\n",
      "Loss: 11.312801258110834\n",
      "l2 norm of gradients: 0.049173934061438565\n",
      "l2 norm of weights: 3.3901041773843072\n",
      "---------------------\n",
      "Iteration Number: 8749\n",
      "Loss: 11.312563766758522\n",
      "l2 norm of gradients: 0.049167930251908715\n",
      "l2 norm of weights: 3.390169074615354\n",
      "---------------------\n",
      "Iteration Number: 8750\n",
      "Loss: 11.312326361140052\n",
      "l2 norm of gradients: 0.049161928136108826\n",
      "l2 norm of weights: 3.3902339683404463\n",
      "---------------------\n",
      "Iteration Number: 8751\n",
      "Loss: 11.312089041218254\n",
      "l2 norm of gradients: 0.04915592771304998\n",
      "l2 norm of weights: 3.3902988585569496\n",
      "---------------------\n",
      "Iteration Number: 8752\n",
      "Loss: 11.311851806956016\n",
      "l2 norm of gradients: 0.0491499289817441\n",
      "l2 norm of weights: 3.3903637452622326\n",
      "---------------------\n",
      "Iteration Number: 8753\n",
      "Loss: 11.311614658316246\n",
      "l2 norm of gradients: 0.049143931941204075\n",
      "l2 norm of weights: 3.390428628453667\n",
      "---------------------\n",
      "Iteration Number: 8754\n",
      "Loss: 11.31137759526185\n",
      "l2 norm of gradients: 0.04913793659044365\n",
      "l2 norm of weights: 3.3904935081286256\n",
      "---------------------\n",
      "Iteration Number: 8755\n",
      "Loss: 11.311140617755802\n",
      "l2 norm of gradients: 0.0491319429284775\n",
      "l2 norm of weights: 3.3905583842844855\n",
      "---------------------\n",
      "Iteration Number: 8756\n",
      "Loss: 11.310903725761042\n",
      "l2 norm of gradients: 0.04912595095432121\n",
      "l2 norm of weights: 3.3906232569186243\n",
      "---------------------\n",
      "Iteration Number: 8757\n",
      "Loss: 11.3106669192406\n",
      "l2 norm of gradients: 0.04911996066699126\n",
      "l2 norm of weights: 3.3906881260284236\n",
      "---------------------\n",
      "Iteration Number: 8758\n",
      "Loss: 11.310430198157484\n",
      "l2 norm of gradients: 0.04911397206550502\n",
      "l2 norm of weights: 3.390752991611268\n",
      "---------------------\n",
      "Iteration Number: 8759\n",
      "Loss: 11.310193562474728\n",
      "l2 norm of gradients: 0.04910798514888074\n",
      "l2 norm of weights: 3.3908178536645437\n",
      "---------------------\n",
      "Iteration Number: 8760\n",
      "Loss: 11.30995701215542\n",
      "l2 norm of gradients: 0.049101999916137624\n",
      "l2 norm of weights: 3.3908827121856397\n",
      "---------------------\n",
      "Iteration Number: 8761\n",
      "Loss: 11.309720547162652\n",
      "l2 norm of gradients: 0.049096016366295715\n",
      "l2 norm of weights: 3.3909475671719473\n",
      "---------------------\n",
      "Iteration Number: 8762\n",
      "Loss: 11.30948416745954\n",
      "l2 norm of gradients: 0.049090034498376005\n",
      "l2 norm of weights: 3.3910124186208606\n",
      "---------------------\n",
      "Iteration Number: 8763\n",
      "Loss: 11.309247873009227\n",
      "l2 norm of gradients: 0.049084054311400345\n",
      "l2 norm of weights: 3.391077266529776\n",
      "---------------------\n",
      "Iteration Number: 8764\n",
      "Loss: 11.30901166377489\n",
      "l2 norm of gradients: 0.04907807580439148\n",
      "l2 norm of weights: 3.3911421108960935\n",
      "---------------------\n",
      "Iteration Number: 8765\n",
      "Loss: 11.30877553971971\n",
      "l2 norm of gradients: 0.04907209897637309\n",
      "l2 norm of weights: 3.391206951717215\n",
      "---------------------\n",
      "Iteration Number: 8766\n",
      "Loss: 11.308539500806903\n",
      "l2 norm of gradients: 0.049066123826369656\n",
      "l2 norm of weights: 3.3912717889905433\n",
      "---------------------\n",
      "Iteration Number: 8767\n",
      "Loss: 11.30830354699972\n",
      "l2 norm of gradients: 0.049060150353406655\n",
      "l2 norm of weights: 3.391336622713487\n",
      "---------------------\n",
      "Iteration Number: 8768\n",
      "Loss: 11.308067678261416\n",
      "l2 norm of gradients: 0.04905417855651036\n",
      "l2 norm of weights: 3.391401452883454\n",
      "---------------------\n",
      "Iteration Number: 8769\n",
      "Loss: 11.30783189455528\n",
      "l2 norm of gradients: 0.04904820843470804\n",
      "l2 norm of weights: 3.391466279497856\n",
      "---------------------\n",
      "Iteration Number: 8770\n",
      "Loss: 11.307596195844638\n",
      "l2 norm of gradients: 0.049042239987027725\n",
      "l2 norm of weights: 3.3915311025541084\n",
      "---------------------\n",
      "Iteration Number: 8771\n",
      "Loss: 11.307360582092807\n",
      "l2 norm of gradients: 0.04903627321249841\n",
      "l2 norm of weights: 3.391595922049628\n",
      "---------------------\n",
      "Iteration Number: 8772\n",
      "Loss: 11.307125053263167\n",
      "l2 norm of gradients: 0.04903030811014999\n",
      "l2 norm of weights: 3.3916607379818338\n",
      "---------------------\n",
      "Iteration Number: 8773\n",
      "Loss: 11.30688960931909\n",
      "l2 norm of gradients: 0.04902434467901315\n",
      "l2 norm of weights: 3.3917255503481476\n",
      "---------------------\n",
      "Iteration Number: 8774\n",
      "Loss: 11.306654250223994\n",
      "l2 norm of gradients: 0.04901838291811954\n",
      "l2 norm of weights: 3.3917903591459946\n",
      "---------------------\n",
      "Iteration Number: 8775\n",
      "Loss: 11.3064189759413\n",
      "l2 norm of gradients: 0.04901242282650166\n",
      "l2 norm of weights: 3.3918551643728003\n",
      "---------------------\n",
      "Iteration Number: 8776\n",
      "Loss: 11.306183786434486\n",
      "l2 norm of gradients: 0.04900646440319294\n",
      "l2 norm of weights: 3.3919199660259958\n",
      "---------------------\n",
      "Iteration Number: 8777\n",
      "Loss: 11.305948681667015\n",
      "l2 norm of gradients: 0.049000507647227555\n",
      "l2 norm of weights: 3.391984764103012\n",
      "---------------------\n",
      "Iteration Number: 8778\n",
      "Loss: 11.305713661602395\n",
      "l2 norm of gradients: 0.04899455255764075\n",
      "l2 norm of weights: 3.392049558601283\n",
      "---------------------\n",
      "Iteration Number: 8779\n",
      "Loss: 11.305478726204157\n",
      "l2 norm of gradients: 0.04898859913346847\n",
      "l2 norm of weights: 3.3921143495182466\n",
      "---------------------\n",
      "Iteration Number: 8780\n",
      "Loss: 11.305243875435863\n",
      "l2 norm of gradients: 0.04898264737374765\n",
      "l2 norm of weights: 3.3921791368513414\n",
      "---------------------\n",
      "Iteration Number: 8781\n",
      "Loss: 11.305009109261077\n",
      "l2 norm of gradients: 0.04897669727751606\n",
      "l2 norm of weights: 3.3922439205980104\n",
      "---------------------\n",
      "Iteration Number: 8782\n",
      "Loss: 11.304774427643403\n",
      "l2 norm of gradients: 0.04897074884381227\n",
      "l2 norm of weights: 3.3923087007556965\n",
      "---------------------\n",
      "Iteration Number: 8783\n",
      "Loss: 11.304539830546473\n",
      "l2 norm of gradients: 0.048964802071675854\n",
      "l2 norm of weights: 3.3923734773218475\n",
      "---------------------\n",
      "Iteration Number: 8784\n",
      "Loss: 11.30430531793393\n",
      "l2 norm of gradients: 0.048958856960147214\n",
      "l2 norm of weights: 3.3924382502939125\n",
      "---------------------\n",
      "Iteration Number: 8785\n",
      "Loss: 11.304070889769443\n",
      "l2 norm of gradients: 0.048952913508267575\n",
      "l2 norm of weights: 3.3925030196693426\n",
      "---------------------\n",
      "Iteration Number: 8786\n",
      "Loss: 11.303836546016711\n",
      "l2 norm of gradients: 0.048946971715079036\n",
      "l2 norm of weights: 3.3925677854455936\n",
      "---------------------\n",
      "Iteration Number: 8787\n",
      "Loss: 11.303602286639455\n",
      "l2 norm of gradients: 0.04894103157962463\n",
      "l2 norm of weights: 3.3926325476201202\n",
      "---------------------\n",
      "Iteration Number: 8788\n",
      "Loss: 11.303368111601422\n",
      "l2 norm of gradients: 0.04893509310094817\n",
      "l2 norm of weights: 3.3926973061903833\n",
      "---------------------\n",
      "Iteration Number: 8789\n",
      "Loss: 11.303134020866374\n",
      "l2 norm of gradients: 0.04892915627809441\n",
      "l2 norm of weights: 3.3927620611538436\n",
      "---------------------\n",
      "Iteration Number: 8790\n",
      "Loss: 11.302900014398098\n",
      "l2 norm of gradients: 0.04892322111010891\n",
      "l2 norm of weights: 3.3928268125079657\n",
      "---------------------\n",
      "Iteration Number: 8791\n",
      "Loss: 11.302666092160418\n",
      "l2 norm of gradients: 0.04891728759603815\n",
      "l2 norm of weights: 3.3928915602502148\n",
      "---------------------\n",
      "Iteration Number: 8792\n",
      "Loss: 11.30243225411716\n",
      "l2 norm of gradients: 0.04891135573492939\n",
      "l2 norm of weights: 3.3929563043780613\n",
      "---------------------\n",
      "Iteration Number: 8793\n",
      "Loss: 11.3021985002322\n",
      "l2 norm of gradients: 0.048905425525830847\n",
      "l2 norm of weights: 3.393021044888976\n",
      "---------------------\n",
      "Iteration Number: 8794\n",
      "Loss: 11.301964830469416\n",
      "l2 norm of gradients: 0.04889949696779153\n",
      "l2 norm of weights: 3.3930857817804334\n",
      "---------------------\n",
      "Iteration Number: 8795\n",
      "Loss: 11.301731244792716\n",
      "l2 norm of gradients: 0.04889357005986132\n",
      "l2 norm of weights: 3.3931505150499084\n",
      "---------------------\n",
      "Iteration Number: 8796\n",
      "Loss: 11.30149774316604\n",
      "l2 norm of gradients: 0.048887644801091\n",
      "l2 norm of weights: 3.3932152446948813\n",
      "---------------------\n",
      "Iteration Number: 8797\n",
      "Loss: 11.301264325553333\n",
      "l2 norm of gradients: 0.048881721190532135\n",
      "l2 norm of weights: 3.3932799707128316\n",
      "---------------------\n",
      "Iteration Number: 8798\n",
      "Loss: 11.301030991918585\n",
      "l2 norm of gradients: 0.04887579922723718\n",
      "l2 norm of weights: 3.393344693101244\n",
      "---------------------\n",
      "Iteration Number: 8799\n",
      "Loss: 11.300797742225797\n",
      "l2 norm of gradients: 0.04886987891025946\n",
      "l2 norm of weights: 3.3934094118576046\n",
      "---------------------\n",
      "Iteration Number: 8800\n",
      "Loss: 11.300564576438997\n",
      "l2 norm of gradients: 0.04886396023865314\n",
      "l2 norm of weights: 3.393474126979401\n",
      "---------------------\n",
      "Iteration Number: 8801\n",
      "Loss: 11.30033149452222\n",
      "l2 norm of gradients: 0.048858043211473245\n",
      "l2 norm of weights: 3.393538838464124\n",
      "---------------------\n",
      "Iteration Number: 8802\n",
      "Loss: 11.30009849643956\n",
      "l2 norm of gradients: 0.048852127827775635\n",
      "l2 norm of weights: 3.3936035463092677\n",
      "---------------------\n",
      "Iteration Number: 8803\n",
      "Loss: 11.2998655821551\n",
      "l2 norm of gradients: 0.048846214086617\n",
      "l2 norm of weights: 3.3936682505123272\n",
      "---------------------\n",
      "Iteration Number: 8804\n",
      "Loss: 11.29963275163297\n",
      "l2 norm of gradients: 0.04884030198705497\n",
      "l2 norm of weights: 3.3937329510708008\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 8805\n",
      "Loss: 11.299400004837313\n",
      "l2 norm of gradients: 0.048834391528147905\n",
      "l2 norm of weights: 3.393797647982188\n",
      "---------------------\n",
      "Iteration Number: 8806\n",
      "Loss: 11.299167341732282\n",
      "l2 norm of gradients: 0.048828482708955095\n",
      "l2 norm of weights: 3.3938623412439926\n",
      "---------------------\n",
      "Iteration Number: 8807\n",
      "Loss: 11.29893476228209\n",
      "l2 norm of gradients: 0.04882257552853664\n",
      "l2 norm of weights: 3.3939270308537193\n",
      "---------------------\n",
      "Iteration Number: 8808\n",
      "Loss: 11.298702266450936\n",
      "l2 norm of gradients: 0.0488166699859535\n",
      "l2 norm of weights: 3.393991716808877\n",
      "---------------------\n",
      "Iteration Number: 8809\n",
      "Loss: 11.29846985420305\n",
      "l2 norm of gradients: 0.04881076608026744\n",
      "l2 norm of weights: 3.3940563991069737\n",
      "---------------------\n",
      "Iteration Number: 8810\n",
      "Loss: 11.298237525502714\n",
      "l2 norm of gradients: 0.048804863810541124\n",
      "l2 norm of weights: 3.394121077745523\n",
      "---------------------\n",
      "Iteration Number: 8811\n",
      "Loss: 11.2980052803142\n",
      "l2 norm of gradients: 0.048798963175838046\n",
      "l2 norm of weights: 3.394185752722039\n",
      "---------------------\n",
      "Iteration Number: 8812\n",
      "Loss: 11.297773118601816\n",
      "l2 norm of gradients: 0.04879306417522249\n",
      "l2 norm of weights: 3.3942504240340403\n",
      "---------------------\n",
      "Iteration Number: 8813\n",
      "Loss: 11.297541040329886\n",
      "l2 norm of gradients: 0.04878716680775961\n",
      "l2 norm of weights: 3.3943150916790454\n",
      "---------------------\n",
      "Iteration Number: 8814\n",
      "Loss: 11.29730904546277\n",
      "l2 norm of gradients: 0.04878127107251546\n",
      "l2 norm of weights: 3.3943797556545756\n",
      "---------------------\n",
      "Iteration Number: 8815\n",
      "Loss: 11.297077133964848\n",
      "l2 norm of gradients: 0.04877537696855683\n",
      "l2 norm of weights: 3.394444415958156\n",
      "---------------------\n",
      "Iteration Number: 8816\n",
      "Loss: 11.296845305800513\n",
      "l2 norm of gradients: 0.048769484494951366\n",
      "l2 norm of weights: 3.394509072587313\n",
      "---------------------\n",
      "Iteration Number: 8817\n",
      "Loss: 11.296613560934187\n",
      "l2 norm of gradients: 0.048763593650767635\n",
      "l2 norm of weights: 3.3945737255395754\n",
      "---------------------\n",
      "Iteration Number: 8818\n",
      "Loss: 11.296381899330314\n",
      "l2 norm of gradients: 0.048757704435074954\n",
      "l2 norm of weights: 3.3946383748124753\n",
      "---------------------\n",
      "Iteration Number: 8819\n",
      "Loss: 11.296150320953377\n",
      "l2 norm of gradients: 0.04875181684694346\n",
      "l2 norm of weights: 3.3947030204035458\n",
      "---------------------\n",
      "Iteration Number: 8820\n",
      "Loss: 11.295918825767846\n",
      "l2 norm of gradients: 0.04874593088544421\n",
      "l2 norm of weights: 3.3947676623103225\n",
      "---------------------\n",
      "Iteration Number: 8821\n",
      "Loss: 11.29568741373826\n",
      "l2 norm of gradients: 0.048740046549649016\n",
      "l2 norm of weights: 3.3948323005303447\n",
      "---------------------\n",
      "Iteration Number: 8822\n",
      "Loss: 11.295456084829134\n",
      "l2 norm of gradients: 0.04873416383863056\n",
      "l2 norm of weights: 3.394896935061153\n",
      "---------------------\n",
      "Iteration Number: 8823\n",
      "Loss: 11.295224839005044\n",
      "l2 norm of gradients: 0.04872828275146229\n",
      "l2 norm of weights: 3.39496156590029\n",
      "---------------------\n",
      "Iteration Number: 8824\n",
      "Loss: 11.294993676230574\n",
      "l2 norm of gradients: 0.048722403287218594\n",
      "l2 norm of weights: 3.3950261930453016\n",
      "---------------------\n",
      "Iteration Number: 8825\n",
      "Loss: 11.29476259647032\n",
      "l2 norm of gradients: 0.04871652544497457\n",
      "l2 norm of weights: 3.3950908164937346\n",
      "---------------------\n",
      "Iteration Number: 8826\n",
      "Loss: 11.294531599688918\n",
      "l2 norm of gradients: 0.048710649223806204\n",
      "l2 norm of weights: 3.3951554362431406\n",
      "---------------------\n",
      "Iteration Number: 8827\n",
      "Loss: 11.29430068585103\n",
      "l2 norm of gradients: 0.04870477462279031\n",
      "l2 norm of weights: 3.395220052291071\n",
      "---------------------\n",
      "Iteration Number: 8828\n",
      "Loss: 11.294069854921313\n",
      "l2 norm of gradients: 0.048698901641004516\n",
      "l2 norm of weights: 3.395284664635081\n",
      "---------------------\n",
      "Iteration Number: 8829\n",
      "Loss: 11.293839106864487\n",
      "l2 norm of gradients: 0.04869303027752725\n",
      "l2 norm of weights: 3.3953492732727275\n",
      "---------------------\n",
      "Iteration Number: 8830\n",
      "Loss: 11.293608441645253\n",
      "l2 norm of gradients: 0.04868716053143777\n",
      "l2 norm of weights: 3.3954138782015697\n",
      "---------------------\n",
      "Iteration Number: 8831\n",
      "Loss: 11.293377859228368\n",
      "l2 norm of gradients: 0.04868129240181621\n",
      "l2 norm of weights: 3.3954784794191704\n",
      "---------------------\n",
      "Iteration Number: 8832\n",
      "Loss: 11.29314735957859\n",
      "l2 norm of gradients: 0.04867542588774343\n",
      "l2 norm of weights: 3.395543076923092\n",
      "---------------------\n",
      "Iteration Number: 8833\n",
      "Loss: 11.292916942660721\n",
      "l2 norm of gradients: 0.04866956098830117\n",
      "l2 norm of weights: 3.395607670710901\n",
      "---------------------\n",
      "Iteration Number: 8834\n",
      "Loss: 11.29268660843957\n",
      "l2 norm of gradients: 0.04866369770257199\n",
      "l2 norm of weights: 3.395672260780167\n",
      "---------------------\n",
      "Iteration Number: 8835\n",
      "Loss: 11.292456356879967\n",
      "l2 norm of gradients: 0.04865783602963925\n",
      "l2 norm of weights: 3.395736847128461\n",
      "---------------------\n",
      "Iteration Number: 8836\n",
      "Loss: 11.292226187946772\n",
      "l2 norm of gradients: 0.04865197596858712\n",
      "l2 norm of weights: 3.395801429753355\n",
      "---------------------\n",
      "Iteration Number: 8837\n",
      "Loss: 11.291996101604862\n",
      "l2 norm of gradients: 0.048646117518500565\n",
      "l2 norm of weights: 3.3958660086524257\n",
      "---------------------\n",
      "Iteration Number: 8838\n",
      "Loss: 11.29176609781915\n",
      "l2 norm of gradients: 0.048640260678465476\n",
      "l2 norm of weights: 3.3959305838232505\n",
      "---------------------\n",
      "Iteration Number: 8839\n",
      "Loss: 11.291536176554557\n",
      "l2 norm of gradients: 0.048634405447568384\n",
      "l2 norm of weights: 3.3959951552634093\n",
      "---------------------\n",
      "Iteration Number: 8840\n",
      "Loss: 11.291306337776025\n",
      "l2 norm of gradients: 0.04862855182489676\n",
      "l2 norm of weights: 3.396059722970485\n",
      "---------------------\n",
      "Iteration Number: 8841\n",
      "Loss: 11.291076581448545\n",
      "l2 norm of gradients: 0.04862269980953881\n",
      "l2 norm of weights: 3.3961242869420616\n",
      "---------------------\n",
      "Iteration Number: 8842\n",
      "Loss: 11.290846907537086\n",
      "l2 norm of gradients: 0.04861684940058364\n",
      "l2 norm of weights: 3.396188847175727\n",
      "---------------------\n",
      "Iteration Number: 8843\n",
      "Loss: 11.290617316006687\n",
      "l2 norm of gradients: 0.04861100059712107\n",
      "l2 norm of weights: 3.396253403669069\n",
      "---------------------\n",
      "Iteration Number: 8844\n",
      "Loss: 11.290387806822377\n",
      "l2 norm of gradients: 0.04860515339824178\n",
      "l2 norm of weights: 3.3963179564196806\n",
      "---------------------\n",
      "Iteration Number: 8845\n",
      "Loss: 11.290158379949222\n",
      "l2 norm of gradients: 0.04859930780303723\n",
      "l2 norm of weights: 3.3963825054251546\n",
      "---------------------\n",
      "Iteration Number: 8846\n",
      "Loss: 11.28992903535229\n",
      "l2 norm of gradients: 0.0485934638105997\n",
      "l2 norm of weights: 3.3964470506830877\n",
      "---------------------\n",
      "Iteration Number: 8847\n",
      "Loss: 11.289699772996713\n",
      "l2 norm of gradients: 0.04858762142002229\n",
      "l2 norm of weights: 3.3965115921910773\n",
      "---------------------\n",
      "Iteration Number: 8848\n",
      "Loss: 11.289470592847607\n",
      "l2 norm of gradients: 0.04858178063039888\n",
      "l2 norm of weights: 3.396576129946725\n",
      "---------------------\n",
      "Iteration Number: 8849\n",
      "Loss: 11.289241494870126\n",
      "l2 norm of gradients: 0.04857594144082419\n",
      "l2 norm of weights: 3.3966406639476325\n",
      "---------------------\n",
      "Iteration Number: 8850\n",
      "Loss: 11.289012479029438\n",
      "l2 norm of gradients: 0.04857010385039364\n",
      "l2 norm of weights: 3.396705194191407\n",
      "---------------------\n",
      "Iteration Number: 8851\n",
      "Loss: 11.288783545290757\n",
      "l2 norm of gradients: 0.048564267858203586\n",
      "l2 norm of weights: 3.3967697206756533\n",
      "---------------------\n",
      "Iteration Number: 8852\n",
      "Loss: 11.288554693619282\n",
      "l2 norm of gradients: 0.0485584334633511\n",
      "l2 norm of weights: 3.396834243397982\n",
      "---------------------\n",
      "Iteration Number: 8853\n",
      "Loss: 11.288325923980265\n",
      "l2 norm of gradients: 0.04855260066493407\n",
      "l2 norm of weights: 3.396898762356005\n",
      "---------------------\n",
      "Iteration Number: 8854\n",
      "Loss: 11.288097236338965\n",
      "l2 norm of gradients: 0.048546769462051186\n",
      "l2 norm of weights: 3.396963277547336\n",
      "---------------------\n",
      "Iteration Number: 8855\n",
      "Loss: 11.287868630660677\n",
      "l2 norm of gradients: 0.048540939853801965\n",
      "l2 norm of weights: 3.397027788969593\n",
      "---------------------\n",
      "Iteration Number: 8856\n",
      "Loss: 11.287640106910697\n",
      "l2 norm of gradients: 0.04853511183928666\n",
      "l2 norm of weights: 3.3970922966203916\n",
      "---------------------\n",
      "Iteration Number: 8857\n",
      "Loss: 11.28741166505437\n",
      "l2 norm of gradients: 0.04852928541760633\n",
      "l2 norm of weights: 3.397156800497355\n",
      "---------------------\n",
      "Iteration Number: 8858\n",
      "Loss: 11.287183305057042\n",
      "l2 norm of gradients: 0.0485234605878629\n",
      "l2 norm of weights: 3.3972213005981047\n",
      "---------------------\n",
      "Iteration Number: 8859\n",
      "Loss: 11.286955026884087\n",
      "l2 norm of gradients: 0.04851763734915897\n",
      "l2 norm of weights: 3.3972857969202668\n",
      "---------------------\n",
      "Iteration Number: 8860\n",
      "Loss: 11.286726830500902\n",
      "l2 norm of gradients: 0.04851181570059807\n",
      "l2 norm of weights: 3.397350289461468\n",
      "---------------------\n",
      "Iteration Number: 8861\n",
      "Loss: 11.286498715872913\n",
      "l2 norm of gradients: 0.04850599564128441\n",
      "l2 norm of weights: 3.3974147782193387\n",
      "---------------------\n",
      "Iteration Number: 8862\n",
      "Loss: 11.286270682965556\n",
      "l2 norm of gradients: 0.04850017717032303\n",
      "l2 norm of weights: 3.397479263191511\n",
      "---------------------\n",
      "Iteration Number: 8863\n",
      "Loss: 11.2860427317443\n",
      "l2 norm of gradients: 0.04849436028681975\n",
      "l2 norm of weights: 3.397543744375618\n",
      "---------------------\n",
      "Iteration Number: 8864\n",
      "Loss: 11.285814862174634\n",
      "l2 norm of gradients: 0.0484885449898812\n",
      "l2 norm of weights: 3.3976082217692958\n",
      "---------------------\n",
      "Iteration Number: 8865\n",
      "Loss: 11.285587074222052\n",
      "l2 norm of gradients: 0.04848273127861478\n",
      "l2 norm of weights: 3.3976726953701837\n",
      "---------------------\n",
      "Iteration Number: 8866\n",
      "Loss: 11.285359367852102\n",
      "l2 norm of gradients: 0.04847691915212867\n",
      "l2 norm of weights: 3.3977371651759225\n",
      "---------------------\n",
      "Iteration Number: 8867\n",
      "Loss: 11.28513174303033\n",
      "l2 norm of gradients: 0.04847110860953188\n",
      "l2 norm of weights: 3.397801631184155\n",
      "---------------------\n",
      "Iteration Number: 8868\n",
      "Loss: 11.28490419972231\n",
      "l2 norm of gradients: 0.048465299649934124\n",
      "l2 norm of weights: 3.3978660933925258\n",
      "---------------------\n",
      "Iteration Number: 8869\n",
      "Loss: 11.284676737893644\n",
      "l2 norm of gradients: 0.04845949227244598\n",
      "l2 norm of weights: 3.3979305517986824\n",
      "---------------------\n",
      "Iteration Number: 8870\n",
      "Loss: 11.284449357509935\n",
      "l2 norm of gradients: 0.04845368647617878\n",
      "l2 norm of weights: 3.3979950064002744\n",
      "---------------------\n",
      "Iteration Number: 8871\n",
      "Loss: 11.28422205853685\n",
      "l2 norm of gradients: 0.048447882260244594\n",
      "l2 norm of weights: 3.3980594571949534\n",
      "---------------------\n",
      "Iteration Number: 8872\n",
      "Loss: 11.283994840940034\n",
      "l2 norm of gradients: 0.048442079623756344\n",
      "l2 norm of weights: 3.3981239041803732\n",
      "---------------------\n",
      "Iteration Number: 8873\n",
      "Loss: 11.283767704685177\n",
      "l2 norm of gradients: 0.048436278565827696\n",
      "l2 norm of weights: 3.39818834735419\n",
      "---------------------\n",
      "Iteration Number: 8874\n",
      "Loss: 11.283540649737986\n",
      "l2 norm of gradients: 0.04843047908557306\n",
      "l2 norm of weights: 3.398252786714062\n",
      "---------------------\n",
      "Iteration Number: 8875\n",
      "Loss: 11.283313676064191\n",
      "l2 norm of gradients: 0.04842468118210773\n",
      "l2 norm of weights: 3.3983172222576497\n",
      "---------------------\n",
      "Iteration Number: 8876\n",
      "Loss: 11.283086783629543\n",
      "l2 norm of gradients: 0.048418884854547664\n",
      "l2 norm of weights: 3.398381653982615\n",
      "---------------------\n",
      "Iteration Number: 8877\n",
      "Loss: 11.282859972399821\n",
      "l2 norm of gradients: 0.048413090102009664\n",
      "l2 norm of weights: 3.3984460818866227\n",
      "---------------------\n",
      "Iteration Number: 8878\n",
      "Loss: 11.282633242340806\n",
      "l2 norm of gradients: 0.04840729692361125\n",
      "l2 norm of weights: 3.3985105059673404\n",
      "---------------------\n",
      "Iteration Number: 8879\n",
      "Loss: 11.282406593418324\n",
      "l2 norm of gradients: 0.048401505318470775\n",
      "l2 norm of weights: 3.3985749262224365\n",
      "---------------------\n",
      "Iteration Number: 8880\n",
      "Loss: 11.282180025598212\n",
      "l2 norm of gradients: 0.04839571528570737\n",
      "l2 norm of weights: 3.3986393426495827\n",
      "---------------------\n",
      "Iteration Number: 8881\n",
      "Loss: 11.281953538846329\n",
      "l2 norm of gradients: 0.04838992682444086\n",
      "l2 norm of weights: 3.398703755246452\n",
      "---------------------\n",
      "Iteration Number: 8882\n",
      "Loss: 11.281727133128562\n",
      "l2 norm of gradients: 0.048384139933791905\n",
      "l2 norm of weights: 3.39876816401072\n",
      "---------------------\n",
      "Iteration Number: 8883\n",
      "Loss: 11.281500808410806\n",
      "l2 norm of gradients: 0.04837835461288195\n",
      "l2 norm of weights: 3.3988325689400645\n",
      "---------------------\n",
      "Iteration Number: 8884\n",
      "Loss: 11.281274564659\n",
      "l2 norm of gradients: 0.048372570860833145\n",
      "l2 norm of weights: 3.3988969700321645\n",
      "---------------------\n",
      "Iteration Number: 8885\n",
      "Loss: 11.281048401839083\n",
      "l2 norm of gradients: 0.04836678867676849\n",
      "l2 norm of weights: 3.3989613672847034\n",
      "---------------------\n",
      "Iteration Number: 8886\n",
      "Loss: 11.280822319917021\n",
      "l2 norm of gradients: 0.04836100805981166\n",
      "l2 norm of weights: 3.3990257606953644\n",
      "---------------------\n",
      "Iteration Number: 8887\n",
      "Loss: 11.280596318858812\n",
      "l2 norm of gradients: 0.04835522900908717\n",
      "l2 norm of weights: 3.3990901502618334\n",
      "---------------------\n",
      "Iteration Number: 8888\n",
      "Loss: 11.28037039863047\n",
      "l2 norm of gradients: 0.04834945152372031\n",
      "l2 norm of weights: 3.3991545359817996\n",
      "---------------------\n",
      "Iteration Number: 8889\n",
      "Loss: 11.280144559198021\n",
      "l2 norm of gradients: 0.048343675602837054\n",
      "l2 norm of weights: 3.399218917852953\n",
      "---------------------\n",
      "Iteration Number: 8890\n",
      "Loss: 11.279918800527536\n",
      "l2 norm of gradients: 0.048337901245564203\n",
      "l2 norm of weights: 3.399283295872986\n",
      "---------------------\n",
      "Iteration Number: 8891\n",
      "Loss: 11.279693122585075\n",
      "l2 norm of gradients: 0.0483321284510293\n",
      "l2 norm of weights: 3.3993476700395937\n",
      "---------------------\n",
      "Iteration Number: 8892\n",
      "Loss: 11.279467525336749\n",
      "l2 norm of gradients: 0.04832635721836069\n",
      "l2 norm of weights: 3.3994120403504735\n",
      "---------------------\n",
      "Iteration Number: 8893\n",
      "Loss: 11.279242008748676\n",
      "l2 norm of gradients: 0.04832058754668742\n",
      "l2 norm of weights: 3.399476406803323\n",
      "---------------------\n",
      "Iteration Number: 8894\n",
      "Loss: 11.279016572786997\n",
      "l2 norm of gradients: 0.04831481943513932\n",
      "l2 norm of weights: 3.399540769395844\n",
      "---------------------\n",
      "Iteration Number: 8895\n",
      "Loss: 11.278791217417872\n",
      "l2 norm of gradients: 0.048309052882847016\n",
      "l2 norm of weights: 3.3996051281257404\n",
      "---------------------\n",
      "Iteration Number: 8896\n",
      "Loss: 11.2785659426075\n",
      "l2 norm of gradients: 0.04830328788894185\n",
      "l2 norm of weights: 3.399669482990716\n",
      "---------------------\n",
      "Iteration Number: 8897\n",
      "Loss: 11.27834074832208\n",
      "l2 norm of gradients: 0.04829752445255592\n",
      "l2 norm of weights: 3.39973383398848\n",
      "---------------------\n",
      "Iteration Number: 8898\n",
      "Loss: 11.278115634527838\n",
      "l2 norm of gradients: 0.04829176257282212\n",
      "l2 norm of weights: 3.3997981811167404\n",
      "---------------------\n",
      "Iteration Number: 8899\n",
      "Loss: 11.277890601191027\n",
      "l2 norm of gradients: 0.048286002248874034\n",
      "l2 norm of weights: 3.3998625243732095\n",
      "---------------------\n",
      "Iteration Number: 8900\n",
      "Loss: 11.277665648277925\n",
      "l2 norm of gradients: 0.048280243479846094\n",
      "l2 norm of weights: 3.399926863755601\n",
      "---------------------\n",
      "Iteration Number: 8901\n",
      "Loss: 11.277440775754817\n",
      "l2 norm of gradients: 0.04827448626487345\n",
      "l2 norm of weights: 3.3999911992616307\n",
      "---------------------\n",
      "Iteration Number: 8902\n",
      "Loss: 11.277215983588018\n",
      "l2 norm of gradients: 0.048268730603091906\n",
      "l2 norm of weights: 3.4000555308890164\n",
      "---------------------\n",
      "Iteration Number: 8903\n",
      "Loss: 11.276991271743865\n",
      "l2 norm of gradients: 0.048262976493638164\n",
      "l2 norm of weights: 3.4001198586354784\n",
      "---------------------\n",
      "Iteration Number: 8904\n",
      "Loss: 11.276766640188722\n",
      "l2 norm of gradients: 0.04825722393564962\n",
      "l2 norm of weights: 3.400184182498738\n",
      "---------------------\n",
      "Iteration Number: 8905\n",
      "Loss: 11.276542088888966\n",
      "l2 norm of gradients: 0.048251472928264434\n",
      "l2 norm of weights: 3.4002485024765203\n",
      "---------------------\n",
      "Iteration Number: 8906\n",
      "Loss: 11.276317617810985\n",
      "l2 norm of gradients: 0.04824572347062144\n",
      "l2 norm of weights: 3.4003128185665514\n",
      "---------------------\n",
      "Iteration Number: 8907\n",
      "Loss: 11.276093226921208\n",
      "l2 norm of gradients: 0.048239975561860354\n",
      "l2 norm of weights: 3.4003771307665587\n",
      "---------------------\n",
      "Iteration Number: 8908\n",
      "Loss: 11.275868916186086\n",
      "l2 norm of gradients: 0.04823422920112154\n",
      "l2 norm of weights: 3.4004414390742737\n",
      "---------------------\n",
      "Iteration Number: 8909\n",
      "Loss: 11.275644685572072\n",
      "l2 norm of gradients: 0.04822848438754615\n",
      "l2 norm of weights: 3.400505743487428\n",
      "---------------------\n",
      "Iteration Number: 8910\n",
      "Loss: 11.275420535045667\n",
      "l2 norm of gradients: 0.04822274112027605\n",
      "l2 norm of weights: 3.4005700440037567\n",
      "---------------------\n",
      "Iteration Number: 8911\n",
      "Loss: 11.275196464573348\n",
      "l2 norm of gradients: 0.04821699939845392\n",
      "l2 norm of weights: 3.4006343406209965\n",
      "---------------------\n",
      "Iteration Number: 8912\n",
      "Loss: 11.274972474121672\n",
      "l2 norm of gradients: 0.04821125922122304\n",
      "l2 norm of weights: 3.400698633336885\n",
      "---------------------\n",
      "Iteration Number: 8913\n",
      "Loss: 11.274748563657171\n",
      "l2 norm of gradients: 0.04820552058772765\n",
      "l2 norm of weights: 3.400762922149164\n",
      "---------------------\n",
      "Iteration Number: 8914\n",
      "Loss: 11.274524733146432\n",
      "l2 norm of gradients: 0.048199783497112555\n",
      "l2 norm of weights: 3.4008272070555763\n",
      "---------------------\n",
      "Iteration Number: 8915\n",
      "Loss: 11.27430098255603\n",
      "l2 norm of gradients: 0.0481940479485234\n",
      "l2 norm of weights: 3.400891488053866\n",
      "---------------------\n",
      "Iteration Number: 8916\n",
      "Loss: 11.274077311852587\n",
      "l2 norm of gradients: 0.0481883139411065\n",
      "l2 norm of weights: 3.40095576514178\n",
      "---------------------\n",
      "Iteration Number: 8917\n",
      "Loss: 11.273853721002729\n",
      "l2 norm of gradients: 0.04818258147400898\n",
      "l2 norm of weights: 3.4010200383170677\n",
      "---------------------\n",
      "Iteration Number: 8918\n",
      "Loss: 11.273630209973122\n",
      "l2 norm of gradients: 0.04817685054637866\n",
      "l2 norm of weights: 3.40108430757748\n",
      "---------------------\n",
      "Iteration Number: 8919\n",
      "Loss: 11.273406778730433\n",
      "l2 norm of gradients: 0.04817112115736411\n",
      "l2 norm of weights: 3.4011485729207687\n",
      "---------------------\n",
      "Iteration Number: 8920\n",
      "Loss: 11.273183427241364\n",
      "l2 norm of gradients: 0.048165393306114614\n",
      "l2 norm of weights: 3.401212834344691\n",
      "---------------------\n",
      "Iteration Number: 8921\n",
      "Loss: 11.272960155472628\n",
      "l2 norm of gradients: 0.04815966699178026\n",
      "l2 norm of weights: 3.401277091847002\n",
      "---------------------\n",
      "Iteration Number: 8922\n",
      "Loss: 11.272736963390972\n",
      "l2 norm of gradients: 0.0481539422135118\n",
      "l2 norm of weights: 3.4013413454254615\n",
      "---------------------\n",
      "Iteration Number: 8923\n",
      "Loss: 11.272513850963147\n",
      "l2 norm of gradients: 0.04814821897046079\n",
      "l2 norm of weights: 3.4014055950778306\n",
      "---------------------\n",
      "Iteration Number: 8924\n",
      "Loss: 11.272290818155945\n",
      "l2 norm of gradients: 0.048142497261779445\n",
      "l2 norm of weights: 3.4014698408018726\n",
      "---------------------\n",
      "Iteration Number: 8925\n",
      "Loss: 11.272067864936165\n",
      "l2 norm of gradients: 0.0481367770866208\n",
      "l2 norm of weights: 3.4015340825953526\n",
      "---------------------\n",
      "Iteration Number: 8926\n",
      "Loss: 11.271844991270624\n",
      "l2 norm of gradients: 0.048131058444138515\n",
      "l2 norm of weights: 3.4015983204560376\n",
      "---------------------\n",
      "Iteration Number: 8927\n",
      "Loss: 11.27162219712617\n",
      "l2 norm of gradients: 0.048125341333487094\n",
      "l2 norm of weights: 3.4016625543816974\n",
      "---------------------\n",
      "Iteration Number: 8928\n",
      "Loss: 11.271399482469668\n",
      "l2 norm of gradients: 0.04811962575382171\n",
      "l2 norm of weights: 3.401726784370102\n",
      "---------------------\n",
      "Iteration Number: 8929\n",
      "Loss: 11.27117684726801\n",
      "l2 norm of gradients: 0.04811391170429827\n",
      "l2 norm of weights: 3.4017910104190254\n",
      "---------------------\n",
      "Iteration Number: 8930\n",
      "Loss: 11.270954291488096\n",
      "l2 norm of gradients: 0.04810819918407342\n",
      "l2 norm of weights: 3.401855232526243\n",
      "---------------------\n",
      "Iteration Number: 8931\n",
      "Loss: 11.270731815096857\n",
      "l2 norm of gradients: 0.04810248819230456\n",
      "l2 norm of weights: 3.401919450689531\n",
      "---------------------\n",
      "Iteration Number: 8932\n",
      "Loss: 11.270509418061241\n",
      "l2 norm of gradients: 0.04809677872814976\n",
      "l2 norm of weights: 3.40198366490667\n",
      "---------------------\n",
      "Iteration Number: 8933\n",
      "Loss: 11.270287100348222\n",
      "l2 norm of gradients: 0.04809107079076788\n",
      "l2 norm of weights: 3.4020478751754406\n",
      "---------------------\n",
      "Iteration Number: 8934\n",
      "Loss: 11.270064861924778\n",
      "l2 norm of gradients: 0.04808536437931845\n",
      "l2 norm of weights: 3.402112081493626\n",
      "---------------------\n",
      "Iteration Number: 8935\n",
      "Loss: 11.269842702757934\n",
      "l2 norm of gradients: 0.04807965949296178\n",
      "l2 norm of weights: 3.4021762838590104\n",
      "---------------------\n",
      "Iteration Number: 8936\n",
      "Loss: 11.269620622814713\n",
      "l2 norm of gradients: 0.04807395613085886\n",
      "l2 norm of weights: 3.402240482269383\n",
      "---------------------\n",
      "Iteration Number: 8937\n",
      "Loss: 11.269398622062182\n",
      "l2 norm of gradients: 0.048068254292171424\n",
      "l2 norm of weights: 3.402304676722532\n",
      "---------------------\n",
      "Iteration Number: 8938\n",
      "Loss: 11.269176700467398\n",
      "l2 norm of gradients: 0.04806255397606195\n",
      "l2 norm of weights: 3.402368867216248\n",
      "---------------------\n",
      "Iteration Number: 8939\n",
      "Loss: 11.268954857997455\n",
      "l2 norm of gradients: 0.04805685518169358\n",
      "l2 norm of weights: 3.4024330537483247\n",
      "---------------------\n",
      "Iteration Number: 8940\n",
      "Loss: 11.268733094619488\n",
      "l2 norm of gradients: 0.04805115790823025\n",
      "l2 norm of weights: 3.4024972363165578\n",
      "---------------------\n",
      "Iteration Number: 8941\n",
      "Loss: 11.268511410300613\n",
      "l2 norm of gradients: 0.048045462154836564\n",
      "l2 norm of weights: 3.4025614149187433\n",
      "---------------------\n",
      "Iteration Number: 8942\n",
      "Loss: 11.268289805007994\n",
      "l2 norm of gradients: 0.04803976792067788\n",
      "l2 norm of weights: 3.402625589552681\n",
      "---------------------\n",
      "Iteration Number: 8943\n",
      "Loss: 11.268068278708807\n",
      "l2 norm of gradients: 0.04803407520492024\n",
      "l2 norm of weights: 3.4026897602161714\n",
      "---------------------\n",
      "Iteration Number: 8944\n",
      "Loss: 11.267846831370257\n",
      "l2 norm of gradients: 0.04802838400673042\n",
      "l2 norm of weights: 3.4027539269070184\n",
      "---------------------\n",
      "Iteration Number: 8945\n",
      "Loss: 11.267625462959549\n",
      "l2 norm of gradients: 0.04802269432527593\n",
      "l2 norm of weights: 3.402818089623026\n",
      "---------------------\n",
      "Iteration Number: 8946\n",
      "Loss: 11.26740417344393\n",
      "l2 norm of gradients: 0.04801700615972499\n",
      "l2 norm of weights: 3.402882248362001\n",
      "---------------------\n",
      "Iteration Number: 8947\n",
      "Loss: 11.26718296279066\n",
      "l2 norm of gradients: 0.048011319509246524\n",
      "l2 norm of weights: 3.402946403121753\n",
      "---------------------\n",
      "Iteration Number: 8948\n",
      "Loss: 11.266961830967022\n",
      "l2 norm of gradients: 0.04800563437301018\n",
      "l2 norm of weights: 3.4030105539000925\n",
      "---------------------\n",
      "Iteration Number: 8949\n",
      "Loss: 11.26674077794031\n",
      "l2 norm of gradients: 0.04799995075018628\n",
      "l2 norm of weights: 3.403074700694833\n",
      "---------------------\n",
      "Iteration Number: 8950\n",
      "Loss: 11.266519803677848\n",
      "l2 norm of gradients: 0.04799426863994595\n",
      "l2 norm of weights: 3.4031388435037884\n",
      "---------------------\n",
      "Iteration Number: 8951\n",
      "Loss: 11.266298908146979\n",
      "l2 norm of gradients: 0.04798858804146098\n",
      "l2 norm of weights: 3.4032029823247756\n",
      "---------------------\n",
      "Iteration Number: 8952\n",
      "Loss: 11.266078091315059\n",
      "l2 norm of gradients: 0.04798290895390382\n",
      "l2 norm of weights: 3.403267117155613\n",
      "---------------------\n",
      "Iteration Number: 8953\n",
      "Loss: 11.26585735314948\n",
      "l2 norm of gradients: 0.047977231376447715\n",
      "l2 norm of weights: 3.403331247994121\n",
      "---------------------\n",
      "Iteration Number: 8954\n",
      "Loss: 11.26563669361765\n",
      "l2 norm of gradients: 0.04797155530826659\n",
      "l2 norm of weights: 3.403395374838123\n",
      "---------------------\n",
      "Iteration Number: 8955\n",
      "Loss: 11.265416112686973\n",
      "l2 norm of gradients: 0.04796588074853505\n",
      "l2 norm of weights: 3.403459497685443\n",
      "---------------------\n",
      "Iteration Number: 8956\n",
      "Loss: 11.265195610324913\n",
      "l2 norm of gradients: 0.04796020769642849\n",
      "l2 norm of weights: 3.403523616533907\n",
      "---------------------\n",
      "Iteration Number: 8957\n",
      "Loss: 11.26497518649892\n",
      "l2 norm of gradients: 0.04795453615112291\n",
      "l2 norm of weights: 3.403587731381344\n",
      "---------------------\n",
      "Iteration Number: 8958\n",
      "Loss: 11.264754841176487\n",
      "l2 norm of gradients: 0.04794886611179507\n",
      "l2 norm of weights: 3.4036518422255835\n",
      "---------------------\n",
      "Iteration Number: 8959\n",
      "Loss: 11.264534574325118\n",
      "l2 norm of gradients: 0.047943197577622415\n",
      "l2 norm of weights: 3.4037159490644577\n",
      "---------------------\n",
      "Iteration Number: 8960\n",
      "Loss: 11.264314385912343\n",
      "l2 norm of gradients: 0.04793753054778317\n",
      "l2 norm of weights: 3.4037800518958012\n",
      "---------------------\n",
      "Iteration Number: 8961\n",
      "Loss: 11.264094275905698\n",
      "l2 norm of gradients: 0.04793186502145615\n",
      "l2 norm of weights: 3.4038441507174495\n",
      "---------------------\n",
      "Iteration Number: 8962\n",
      "Loss: 11.263874244272749\n",
      "l2 norm of gradients: 0.04792620099782098\n",
      "l2 norm of weights: 3.4039082455272407\n",
      "---------------------\n",
      "Iteration Number: 8963\n",
      "Loss: 11.263654290981098\n",
      "l2 norm of gradients: 0.04792053847605791\n",
      "l2 norm of weights: 3.4039723363230148\n",
      "---------------------\n",
      "Iteration Number: 8964\n",
      "Loss: 11.263434415998342\n",
      "l2 norm of gradients: 0.04791487745534793\n",
      "l2 norm of weights: 3.4040364231026126\n",
      "---------------------\n",
      "Iteration Number: 8965\n",
      "Loss: 11.263214619292103\n",
      "l2 norm of gradients: 0.047909217934872725\n",
      "l2 norm of weights: 3.404100505863879\n",
      "---------------------\n",
      "Iteration Number: 8966\n",
      "Loss: 11.262994900830034\n",
      "l2 norm of gradients: 0.047903559913814724\n",
      "l2 norm of weights: 3.4041645846046586\n",
      "---------------------\n",
      "Iteration Number: 8967\n",
      "Loss: 11.262775260579806\n",
      "l2 norm of gradients: 0.047897903391356966\n",
      "l2 norm of weights: 3.4042286593227993\n",
      "---------------------\n",
      "Iteration Number: 8968\n",
      "Loss: 11.262555698509102\n",
      "l2 norm of gradients: 0.04789224836668326\n",
      "l2 norm of weights: 3.4042927300161496\n",
      "---------------------\n",
      "Iteration Number: 8969\n",
      "Loss: 11.26233621458563\n",
      "l2 norm of gradients: 0.04788659483897806\n",
      "l2 norm of weights: 3.4043567966825616\n",
      "---------------------\n",
      "Iteration Number: 8970\n",
      "Loss: 11.262116808777126\n",
      "l2 norm of gradients: 0.04788094280742659\n",
      "l2 norm of weights: 3.4044208593198886\n",
      "---------------------\n",
      "Iteration Number: 8971\n",
      "Loss: 11.261897481051331\n",
      "l2 norm of gradients: 0.04787529227121474\n",
      "l2 norm of weights: 3.4044849179259846\n",
      "---------------------\n",
      "Iteration Number: 8972\n",
      "Loss: 11.261678231376008\n",
      "l2 norm of gradients: 0.04786964322952908\n",
      "l2 norm of weights: 3.404548972498707\n",
      "---------------------\n",
      "Iteration Number: 8973\n",
      "Loss: 11.261459059718959\n",
      "l2 norm of gradients: 0.04786399568155686\n",
      "l2 norm of weights: 3.404613023035915\n",
      "---------------------\n",
      "Iteration Number: 8974\n",
      "Loss: 11.261239966047986\n",
      "l2 norm of gradients: 0.04785834962648608\n",
      "l2 norm of weights: 3.4046770695354676\n",
      "---------------------\n",
      "Iteration Number: 8975\n",
      "Loss: 11.261020950330918\n",
      "l2 norm of gradients: 0.04785270506350538\n",
      "l2 norm of weights: 3.4047411119952296\n",
      "---------------------\n",
      "Iteration Number: 8976\n",
      "Loss: 11.260802012535605\n",
      "l2 norm of gradients: 0.04784706199180417\n",
      "l2 norm of weights: 3.4048051504130634\n",
      "---------------------\n",
      "Iteration Number: 8977\n",
      "Loss: 11.260583152629916\n",
      "l2 norm of gradients: 0.047841420410572426\n",
      "l2 norm of weights: 3.4048691847868366\n",
      "---------------------\n",
      "Iteration Number: 8978\n",
      "Loss: 11.260364370581733\n",
      "l2 norm of gradients: 0.04783578031900096\n",
      "l2 norm of weights: 3.4049332151144163\n",
      "---------------------\n",
      "Iteration Number: 8979\n",
      "Loss: 11.260145666358973\n",
      "l2 norm of gradients: 0.047830141716281166\n",
      "l2 norm of weights: 3.4049972413936738\n",
      "---------------------\n",
      "Iteration Number: 8980\n",
      "Loss: 11.259927039929568\n",
      "l2 norm of gradients: 0.047824504601605186\n",
      "l2 norm of weights: 3.4050612636224793\n",
      "---------------------\n",
      "Iteration Number: 8981\n",
      "Loss: 11.259708491261463\n",
      "l2 norm of gradients: 0.047818868974165867\n",
      "l2 norm of weights: 3.405125281798708\n",
      "---------------------\n",
      "Iteration Number: 8982\n",
      "Loss: 11.25949002032262\n",
      "l2 norm of gradients: 0.047813234833156665\n",
      "l2 norm of weights: 3.405189295920235\n",
      "---------------------\n",
      "Iteration Number: 8983\n",
      "Loss: 11.25927162708104\n",
      "l2 norm of gradients: 0.047807602177771795\n",
      "l2 norm of weights: 3.4052533059849366\n",
      "---------------------\n",
      "Iteration Number: 8984\n",
      "Loss: 11.259053311504724\n",
      "l2 norm of gradients: 0.047801971007206144\n",
      "l2 norm of weights: 3.405317311990694\n",
      "---------------------\n",
      "Iteration Number: 8985\n",
      "Loss: 11.258835073561702\n",
      "l2 norm of gradients: 0.04779634132065528\n",
      "l2 norm of weights: 3.4053813139353872\n",
      "---------------------\n",
      "Iteration Number: 8986\n",
      "Loss: 11.258616913220024\n",
      "l2 norm of gradients: 0.04779071311731546\n",
      "l2 norm of weights: 3.4054453118168992\n",
      "---------------------\n",
      "Iteration Number: 8987\n",
      "Loss: 11.258398830447758\n",
      "l2 norm of gradients: 0.047785086396383604\n",
      "l2 norm of weights: 3.405509305633115\n",
      "---------------------\n",
      "Iteration Number: 8988\n",
      "Loss: 11.25818082521299\n",
      "l2 norm of gradients: 0.04777946115705739\n",
      "l2 norm of weights: 3.405573295381921\n",
      "---------------------\n",
      "Iteration Number: 8989\n",
      "Loss: 11.257962897483832\n",
      "l2 norm of gradients: 0.04777383739853506\n",
      "l2 norm of weights: 3.4056372810612063\n",
      "---------------------\n",
      "Iteration Number: 8990\n",
      "Loss: 11.25774504722841\n",
      "l2 norm of gradients: 0.047768215120015674\n",
      "l2 norm of weights: 3.4057012626688605\n",
      "---------------------\n",
      "Iteration Number: 8991\n",
      "Loss: 11.257527274414876\n",
      "l2 norm of gradients: 0.047762594320698896\n",
      "l2 norm of weights: 3.405765240202777\n",
      "---------------------\n",
      "Iteration Number: 8992\n",
      "Loss: 11.257309579011393\n",
      "l2 norm of gradients: 0.04775697499978504\n",
      "l2 norm of weights: 3.405829213660848\n",
      "---------------------\n",
      "Iteration Number: 8993\n",
      "Loss: 11.257091960986148\n",
      "l2 norm of gradients: 0.047751357156475174\n",
      "l2 norm of weights: 3.4058931830409707\n",
      "---------------------\n",
      "Iteration Number: 8994\n",
      "Loss: 11.25687442030735\n",
      "l2 norm of gradients: 0.047745740789971014\n",
      "l2 norm of weights: 3.4059571483410416\n",
      "---------------------\n",
      "Iteration Number: 8995\n",
      "Loss: 11.256656956943225\n",
      "l2 norm of gradients: 0.04774012589947498\n",
      "l2 norm of weights: 3.406021109558962\n",
      "---------------------\n",
      "Iteration Number: 8996\n",
      "Loss: 11.256439570862018\n",
      "l2 norm of gradients: 0.04773451248419013\n",
      "l2 norm of weights: 3.406085066692631\n",
      "---------------------\n",
      "Iteration Number: 8997\n",
      "Loss: 11.256222262031999\n",
      "l2 norm of gradients: 0.04772890054332022\n",
      "l2 norm of weights: 3.406149019739953\n",
      "---------------------\n",
      "Iteration Number: 8998\n",
      "Loss: 11.256005030421447\n",
      "l2 norm of gradients: 0.04772329007606972\n",
      "l2 norm of weights: 3.4062129686988327\n",
      "---------------------\n",
      "Iteration Number: 8999\n",
      "Loss: 11.255787875998678\n",
      "l2 norm of gradients: 0.0477176810816437\n",
      "l2 norm of weights: 3.4062769135671775\n",
      "---------------------\n",
      "Iteration Number: 9000\n",
      "Loss: 11.255570798732013\n",
      "l2 norm of gradients: 0.04771207355924796\n",
      "l2 norm of weights: 3.4063408543428944\n",
      "---------------------\n",
      "Iteration Number: 9001\n",
      "Loss: 11.25535379858979\n",
      "l2 norm of gradients: 0.04770646750808897\n",
      "l2 norm of weights: 3.4064047910238946\n",
      "---------------------\n",
      "Iteration Number: 9002\n",
      "Loss: 11.255136875540382\n",
      "l2 norm of gradients: 0.047700862927373876\n",
      "l2 norm of weights: 3.40646872360809\n",
      "---------------------\n",
      "Iteration Number: 9003\n",
      "Loss: 11.254920029552176\n",
      "l2 norm of gradients: 0.04769525981631048\n",
      "l2 norm of weights: 3.4065326520933956\n",
      "---------------------\n",
      "Iteration Number: 9004\n",
      "Loss: 11.254703260593558\n",
      "l2 norm of gradients: 0.04768965817410726\n",
      "l2 norm of weights: 3.4065965764777255\n",
      "---------------------\n",
      "Iteration Number: 9005\n",
      "Loss: 11.254486568632966\n",
      "l2 norm of gradients: 0.047684057999973405\n",
      "l2 norm of weights: 3.4066604967589984\n",
      "---------------------\n",
      "Iteration Number: 9006\n",
      "Loss: 11.25426995363885\n",
      "l2 norm of gradients: 0.04767845929311872\n",
      "l2 norm of weights: 3.4067244129351333\n",
      "---------------------\n",
      "Iteration Number: 9007\n",
      "Loss: 11.254053415579646\n",
      "l2 norm of gradients: 0.047672862052753714\n",
      "l2 norm of weights: 3.4067883250040514\n",
      "---------------------\n",
      "Iteration Number: 9008\n",
      "Loss: 11.253836954423868\n",
      "l2 norm of gradients: 0.04766726627808958\n",
      "l2 norm of weights: 3.4068522329636752\n",
      "---------------------\n",
      "Iteration Number: 9009\n",
      "Loss: 11.25362057013999\n",
      "l2 norm of gradients: 0.04766167196833811\n",
      "l2 norm of weights: 3.4069161368119296\n",
      "---------------------\n",
      "Iteration Number: 9010\n",
      "Loss: 11.253404262696549\n",
      "l2 norm of gradients: 0.047656079122711865\n",
      "l2 norm of weights: 3.4069800365467415\n",
      "---------------------\n",
      "Iteration Number: 9011\n",
      "Loss: 11.253188032062074\n",
      "l2 norm of gradients: 0.047650487740424\n",
      "l2 norm of weights: 3.4070439321660384\n",
      "---------------------\n",
      "Iteration Number: 9012\n",
      "Loss: 11.252971878205136\n",
      "l2 norm of gradients: 0.047644897820688405\n",
      "l2 norm of weights: 3.4071078236677512\n",
      "---------------------\n",
      "Iteration Number: 9013\n",
      "Loss: 11.252755801094295\n",
      "l2 norm of gradients: 0.04763930936271951\n",
      "l2 norm of weights: 3.4071717110498105\n",
      "---------------------\n",
      "Iteration Number: 9014\n",
      "Loss: 11.252539800698175\n",
      "l2 norm of gradients: 0.04763372236573258\n",
      "l2 norm of weights: 3.4072355943101518\n",
      "---------------------\n",
      "Iteration Number: 9015\n",
      "Loss: 11.252323876985379\n",
      "l2 norm of gradients: 0.04762813682894345\n",
      "l2 norm of weights: 3.4072994734467077\n",
      "---------------------\n",
      "Iteration Number: 9016\n",
      "Loss: 11.25210802992455\n",
      "l2 norm of gradients: 0.04762255275156859\n",
      "l2 norm of weights: 3.4073633484574177\n",
      "---------------------\n",
      "Iteration Number: 9017\n",
      "Loss: 11.251892259484338\n",
      "l2 norm of gradients: 0.04761697013282522\n",
      "l2 norm of weights: 3.40742721934022\n",
      "---------------------\n",
      "Iteration Number: 9018\n",
      "Loss: 11.251676565633417\n",
      "l2 norm of gradients: 0.047611388971931144\n",
      "l2 norm of weights: 3.4074910860930543\n",
      "---------------------\n",
      "Iteration Number: 9019\n",
      "Loss: 11.251460948340494\n",
      "l2 norm of gradients: 0.0476058092681049\n",
      "l2 norm of weights: 3.407554948713864\n",
      "---------------------\n",
      "Iteration Number: 9020\n",
      "Loss: 11.25124540757427\n",
      "l2 norm of gradients: 0.04760023102056565\n",
      "l2 norm of weights: 3.407618807200593\n",
      "---------------------\n",
      "Iteration Number: 9021\n",
      "Loss: 11.25102994330349\n",
      "l2 norm of gradients: 0.0475946542285332\n",
      "l2 norm of weights: 3.4076826615511866\n",
      "---------------------\n",
      "Iteration Number: 9022\n",
      "Loss: 11.250814555496902\n",
      "l2 norm of gradients: 0.047589078891228015\n",
      "l2 norm of weights: 3.4077465117635932\n",
      "---------------------\n",
      "Iteration Number: 9023\n",
      "Loss: 11.250599244123269\n",
      "l2 norm of gradients: 0.04758350500787131\n",
      "l2 norm of weights: 3.4078103578357624\n",
      "---------------------\n",
      "Iteration Number: 9024\n",
      "Loss: 11.250384009151404\n",
      "l2 norm of gradients: 0.04757793257768487\n",
      "l2 norm of weights: 3.407874199765644\n",
      "---------------------\n",
      "Iteration Number: 9025\n",
      "Loss: 11.2501688505501\n",
      "l2 norm of gradients: 0.04757236159989116\n",
      "l2 norm of weights: 3.407938037551192\n",
      "---------------------\n",
      "Iteration Number: 9026\n",
      "Loss: 11.249953768288188\n",
      "l2 norm of gradients: 0.04756679207371328\n",
      "l2 norm of weights: 3.4080018711903612\n",
      "---------------------\n",
      "Iteration Number: 9027\n",
      "Loss: 11.249738762334527\n",
      "l2 norm of gradients: 0.047561223998375035\n",
      "l2 norm of weights: 3.408065700681107\n",
      "---------------------\n",
      "Iteration Number: 9028\n",
      "Loss: 11.249523832657975\n",
      "l2 norm of gradients: 0.047555657373100864\n",
      "l2 norm of weights: 3.408129526021388\n",
      "---------------------\n",
      "Iteration Number: 9029\n",
      "Loss: 11.249308979227425\n",
      "l2 norm of gradients: 0.04755009219711587\n",
      "l2 norm of weights: 3.408193347209164\n",
      "---------------------\n",
      "Iteration Number: 9030\n",
      "Loss: 11.249094202011776\n",
      "l2 norm of gradients: 0.047544528469645776\n",
      "l2 norm of weights: 3.4082571642423964\n",
      "---------------------\n",
      "Iteration Number: 9031\n",
      "Loss: 11.248879500979971\n",
      "l2 norm of gradients: 0.047538966189917035\n",
      "l2 norm of weights: 3.4083209771190486\n",
      "---------------------\n",
      "Iteration Number: 9032\n",
      "Loss: 11.248664876100937\n",
      "l2 norm of gradients: 0.047533405357156666\n",
      "l2 norm of weights: 3.4083847858370855\n",
      "---------------------\n",
      "Iteration Number: 9033\n",
      "Loss: 11.248450327343644\n",
      "l2 norm of gradients: 0.04752784597059241\n",
      "l2 norm of weights: 3.4084485903944737\n",
      "---------------------\n",
      "Iteration Number: 9034\n",
      "Loss: 11.24823585467707\n",
      "l2 norm of gradients: 0.0475222880294526\n",
      "l2 norm of weights: 3.4085123907891814\n",
      "---------------------\n",
      "Iteration Number: 9035\n",
      "Loss: 11.248021458070225\n",
      "l2 norm of gradients: 0.04751673153296633\n",
      "l2 norm of weights: 3.4085761870191797\n",
      "---------------------\n",
      "Iteration Number: 9036\n",
      "Loss: 11.247807137492128\n",
      "l2 norm of gradients: 0.04751117648036317\n",
      "l2 norm of weights: 3.4086399790824395\n",
      "---------------------\n",
      "Iteration Number: 9037\n",
      "Loss: 11.247592892911811\n",
      "l2 norm of gradients: 0.04750562287087352\n",
      "l2 norm of weights: 3.408703766976935\n",
      "---------------------\n",
      "Iteration Number: 9038\n",
      "Loss: 11.247378724298343\n",
      "l2 norm of gradients: 0.047500070703728324\n",
      "l2 norm of weights: 3.4087675507006407\n",
      "---------------------\n",
      "Iteration Number: 9039\n",
      "Loss: 11.247164631620798\n",
      "l2 norm of gradients: 0.047494519978159226\n",
      "l2 norm of weights: 3.408831330251534\n",
      "---------------------\n",
      "Iteration Number: 9040\n",
      "Loss: 11.246950614848272\n",
      "l2 norm of gradients: 0.047488970693398434\n",
      "l2 norm of weights: 3.408895105627594\n",
      "---------------------\n",
      "Iteration Number: 9041\n",
      "Loss: 11.246736673949872\n",
      "l2 norm of gradients: 0.047483422848678944\n",
      "l2 norm of weights: 3.4089588768268\n",
      "---------------------\n",
      "Iteration Number: 9042\n",
      "Loss: 11.24652280889475\n",
      "l2 norm of gradients: 0.04747787644323428\n",
      "l2 norm of weights: 3.4090226438471354\n",
      "---------------------\n",
      "Iteration Number: 9043\n",
      "Loss: 11.24630901965205\n",
      "l2 norm of gradients: 0.047472331476298674\n",
      "l2 norm of weights: 3.4090864066865834\n",
      "---------------------\n",
      "Iteration Number: 9044\n",
      "Loss: 11.246095306190943\n",
      "l2 norm of gradients: 0.04746678794710697\n",
      "l2 norm of weights: 3.409150165343129\n",
      "---------------------\n",
      "Iteration Number: 9045\n",
      "Loss: 11.245881668480623\n",
      "l2 norm of gradients: 0.047461245854894675\n",
      "l2 norm of weights: 3.4092139198147597\n",
      "---------------------\n",
      "Iteration Number: 9046\n",
      "Loss: 11.245668106490294\n",
      "l2 norm of gradients: 0.04745570519889795\n",
      "l2 norm of weights: 3.4092776700994643\n",
      "---------------------\n",
      "Iteration Number: 9047\n",
      "Loss: 11.2454546201892\n",
      "l2 norm of gradients: 0.0474501659783536\n",
      "l2 norm of weights: 3.4093414161952333\n",
      "---------------------\n",
      "Iteration Number: 9048\n",
      "Loss: 11.245241209546574\n",
      "l2 norm of gradients: 0.04744462819249905\n",
      "l2 norm of weights: 3.409405158100059\n",
      "---------------------\n",
      "Iteration Number: 9049\n",
      "Loss: 11.245027874531687\n",
      "l2 norm of gradients: 0.047439091840572385\n",
      "l2 norm of weights: 3.4094688958119352\n",
      "---------------------\n",
      "Iteration Number: 9050\n",
      "Loss: 11.244814615113825\n",
      "l2 norm of gradients: 0.04743355692181231\n",
      "l2 norm of weights: 3.409532629328858\n",
      "---------------------\n",
      "Iteration Number: 9051\n",
      "Loss: 11.244601431262295\n",
      "l2 norm of gradients: 0.04742802343545822\n",
      "l2 norm of weights: 3.4095963586488236\n",
      "---------------------\n",
      "Iteration Number: 9052\n",
      "Loss: 11.24438832294641\n",
      "l2 norm of gradients: 0.04742249138075011\n",
      "l2 norm of weights: 3.4096600837698317\n",
      "---------------------\n",
      "Iteration Number: 9053\n",
      "Loss: 11.244175290135525\n",
      "l2 norm of gradients: 0.04741696075692865\n",
      "l2 norm of weights: 3.4097238046898823\n",
      "---------------------\n",
      "Iteration Number: 9054\n",
      "Loss: 11.243962332798983\n",
      "l2 norm of gradients: 0.047411431563235074\n",
      "l2 norm of weights: 3.409787521406978\n",
      "---------------------\n",
      "Iteration Number: 9055\n",
      "Loss: 11.243749450906181\n",
      "l2 norm of gradients: 0.04740590379891138\n",
      "l2 norm of weights: 3.409851233919123\n",
      "---------------------\n",
      "Iteration Number: 9056\n",
      "Loss: 11.24353664442651\n",
      "l2 norm of gradients: 0.047400377463200076\n",
      "l2 norm of weights: 3.4099149422243222\n",
      "---------------------\n",
      "Iteration Number: 9057\n",
      "Loss: 11.243323913329384\n",
      "l2 norm of gradients: 0.0473948525553444\n",
      "l2 norm of weights: 3.409978646320583\n",
      "---------------------\n",
      "Iteration Number: 9058\n",
      "Loss: 11.24311125758424\n",
      "l2 norm of gradients: 0.04738932907458819\n",
      "l2 norm of weights: 3.410042346205915\n",
      "---------------------\n",
      "Iteration Number: 9059\n",
      "Loss: 11.242898677160532\n",
      "l2 norm of gradients: 0.04738380702017592\n",
      "l2 norm of weights: 3.4101060418783278\n",
      "---------------------\n",
      "Iteration Number: 9060\n",
      "Loss: 11.242686172027733\n",
      "l2 norm of gradients: 0.047378286391352706\n",
      "l2 norm of weights: 3.4101697333358336\n",
      "---------------------\n",
      "Iteration Number: 9061\n",
      "Loss: 11.242473742155322\n",
      "l2 norm of gradients: 0.04737276718736429\n",
      "l2 norm of weights: 3.4102334205764464\n",
      "---------------------\n",
      "Iteration Number: 9062\n",
      "Loss: 11.242261387512828\n",
      "l2 norm of gradients: 0.04736724940745706\n",
      "l2 norm of weights: 3.410297103598183\n",
      "---------------------\n",
      "Iteration Number: 9063\n",
      "Loss: 11.242049108069764\n",
      "l2 norm of gradients: 0.04736173305087806\n",
      "l2 norm of weights: 3.4103607823990583\n",
      "---------------------\n",
      "Iteration Number: 9064\n",
      "Loss: 11.241836903795683\n",
      "l2 norm of gradients: 0.0473562181168749\n",
      "l2 norm of weights: 3.410424456977092\n",
      "---------------------\n",
      "Iteration Number: 9065\n",
      "Loss: 11.24162477466015\n",
      "l2 norm of gradients: 0.047350704604695905\n",
      "l2 norm of weights: 3.4104881273303054\n",
      "---------------------\n",
      "Iteration Number: 9066\n",
      "Loss: 11.241412720632743\n",
      "l2 norm of gradients: 0.04734519251358996\n",
      "l2 norm of weights: 3.4105517934567198\n",
      "---------------------\n",
      "Iteration Number: 9067\n",
      "Loss: 11.24120074168308\n",
      "l2 norm of gradients: 0.047339681842806625\n",
      "l2 norm of weights: 3.410615455354358\n",
      "---------------------\n",
      "Iteration Number: 9068\n",
      "Loss: 11.240988837780764\n",
      "l2 norm of gradients: 0.04733417259159613\n",
      "l2 norm of weights: 3.410679113021247\n",
      "---------------------\n",
      "Iteration Number: 9069\n",
      "Loss: 11.240777008895437\n",
      "l2 norm of gradients: 0.0473286647592092\n",
      "l2 norm of weights: 3.410742766455413\n",
      "---------------------\n",
      "Iteration Number: 9070\n",
      "Loss: 11.240565254996763\n",
      "l2 norm of gradients: 0.04732315834489735\n",
      "l2 norm of weights: 3.410806415654884\n",
      "---------------------\n",
      "Iteration Number: 9071\n",
      "Loss: 11.240353576054414\n",
      "l2 norm of gradients: 0.04731765334791263\n",
      "l2 norm of weights: 3.4108700606176905\n",
      "---------------------\n",
      "Iteration Number: 9072\n",
      "Loss: 11.240141972038083\n",
      "l2 norm of gradients: 0.04731214976750773\n",
      "l2 norm of weights: 3.4109337013418646\n",
      "---------------------\n",
      "Iteration Number: 9073\n",
      "Loss: 11.239930442917483\n",
      "l2 norm of gradients: 0.04730664760293598\n",
      "l2 norm of weights: 3.4109973378254397\n",
      "---------------------\n",
      "Iteration Number: 9074\n",
      "Loss: 11.239718988662348\n",
      "l2 norm of gradients: 0.04730114685345133\n",
      "l2 norm of weights: 3.4110609700664507\n",
      "---------------------\n",
      "Iteration Number: 9075\n",
      "Loss: 11.239507609242429\n",
      "l2 norm of gradients: 0.04729564751830839\n",
      "l2 norm of weights: 3.4111245980629348\n",
      "---------------------\n",
      "Iteration Number: 9076\n",
      "Loss: 11.23929630462749\n",
      "l2 norm of gradients: 0.047290149596762344\n",
      "l2 norm of weights: 3.411188221812929\n",
      "---------------------\n",
      "Iteration Number: 9077\n",
      "Loss: 11.239085074787319\n",
      "l2 norm of gradients: 0.047284653088069034\n",
      "l2 norm of weights: 3.4112518413144746\n",
      "---------------------\n",
      "Iteration Number: 9078\n",
      "Loss: 11.238873919691716\n",
      "l2 norm of gradients: 0.04727915799148493\n",
      "l2 norm of weights: 3.411315456565612\n",
      "---------------------\n",
      "Iteration Number: 9079\n",
      "Loss: 11.238662839310516\n",
      "l2 norm of gradients: 0.04727366430626709\n",
      "l2 norm of weights: 3.411379067564385\n",
      "---------------------\n",
      "Iteration Number: 9080\n",
      "Loss: 11.238451833613547\n",
      "l2 norm of gradients: 0.047268172031673245\n",
      "l2 norm of weights: 3.411442674308838\n",
      "---------------------\n",
      "Iteration Number: 9081\n",
      "Loss: 11.23824090257067\n",
      "l2 norm of gradients: 0.04726268116696174\n",
      "l2 norm of weights: 3.4115062767970175\n",
      "---------------------\n",
      "Iteration Number: 9082\n",
      "Loss: 11.238030046151767\n",
      "l2 norm of gradients: 0.0472571917113915\n",
      "l2 norm of weights: 3.4115698750269705\n",
      "---------------------\n",
      "Iteration Number: 9083\n",
      "Loss: 11.237819264326731\n",
      "l2 norm of gradients: 0.04725170366422211\n",
      "l2 norm of weights: 3.411633468996748\n",
      "---------------------\n",
      "Iteration Number: 9084\n",
      "Loss: 11.237608557065482\n",
      "l2 norm of gradients: 0.04724621702471378\n",
      "l2 norm of weights: 3.4116970587043998\n",
      "---------------------\n",
      "Iteration Number: 9085\n",
      "Loss: 11.237397924337943\n",
      "l2 norm of gradients: 0.0472407317921273\n",
      "l2 norm of weights: 3.4117606441479795\n",
      "---------------------\n",
      "Iteration Number: 9086\n",
      "Loss: 11.237187366114066\n",
      "l2 norm of gradients: 0.04723524796572411\n",
      "l2 norm of weights: 3.411824225325541\n",
      "---------------------\n",
      "Iteration Number: 9087\n",
      "Loss: 11.236976882363829\n",
      "l2 norm of gradients: 0.04722976554476633\n",
      "l2 norm of weights: 3.4118878022351393\n",
      "---------------------\n",
      "Iteration Number: 9088\n",
      "Loss: 11.236766473057209\n",
      "l2 norm of gradients: 0.0472242845285166\n",
      "l2 norm of weights: 3.4119513748748336\n",
      "---------------------\n",
      "Iteration Number: 9089\n",
      "Loss: 11.236556138164215\n",
      "l2 norm of gradients: 0.04721880491623819\n",
      "l2 norm of weights: 3.412014943242681\n",
      "---------------------\n",
      "Iteration Number: 9090\n",
      "Loss: 11.23634587765487\n",
      "l2 norm of gradients: 0.04721332670719505\n",
      "l2 norm of weights: 3.4120785073367434\n",
      "---------------------\n",
      "Iteration Number: 9091\n",
      "Loss: 11.236135691499214\n",
      "l2 norm of gradients: 0.04720784990065168\n",
      "l2 norm of weights: 3.412142067155083\n",
      "---------------------\n",
      "Iteration Number: 9092\n",
      "Loss: 11.235925579667304\n",
      "l2 norm of gradients: 0.04720237449587325\n",
      "l2 norm of weights: 3.412205622695762\n",
      "---------------------\n",
      "Iteration Number: 9093\n",
      "Loss: 11.235715542129217\n",
      "l2 norm of gradients: 0.04719690049212554\n",
      "l2 norm of weights: 3.412269173956848\n",
      "---------------------\n",
      "Iteration Number: 9094\n",
      "Loss: 11.235505578855054\n",
      "l2 norm of gradients: 0.047191427888674906\n",
      "l2 norm of weights: 3.412332720936406\n",
      "---------------------\n",
      "Iteration Number: 9095\n",
      "Loss: 11.235295689814933\n",
      "l2 norm of gradients: 0.04718595668478833\n",
      "l2 norm of weights: 3.412396263632505\n",
      "---------------------\n",
      "Iteration Number: 9096\n",
      "Loss: 11.235085874978973\n",
      "l2 norm of gradients: 0.047180486879733446\n",
      "l2 norm of weights: 3.4124598020432155\n",
      "---------------------\n",
      "Iteration Number: 9097\n",
      "Loss: 11.234876134317322\n",
      "l2 norm of gradients: 0.047175018472778496\n",
      "l2 norm of weights: 3.4125233361666085\n",
      "---------------------\n",
      "Iteration Number: 9098\n",
      "Loss: 11.234666467800158\n",
      "l2 norm of gradients: 0.04716955146319228\n",
      "l2 norm of weights: 3.4125868660007574\n",
      "---------------------\n",
      "Iteration Number: 9099\n",
      "Loss: 11.234456875397669\n",
      "l2 norm of gradients: 0.04716408585024426\n",
      "l2 norm of weights: 3.4126503915437367\n",
      "---------------------\n",
      "Iteration Number: 9100\n",
      "Loss: 11.23424735708004\n",
      "l2 norm of gradients: 0.047158621633204494\n",
      "l2 norm of weights: 3.4127139127936226\n",
      "---------------------\n",
      "Iteration Number: 9101\n",
      "Loss: 11.234037912817513\n",
      "l2 norm of gradients: 0.04715315881134367\n",
      "l2 norm of weights: 3.4127774297484934\n",
      "---------------------\n",
      "Iteration Number: 9102\n",
      "Loss: 11.233828542580316\n",
      "l2 norm of gradients: 0.047147697383933075\n",
      "l2 norm of weights: 3.4128409424064277\n",
      "---------------------\n",
      "Iteration Number: 9103\n",
      "Loss: 11.233619246338703\n",
      "l2 norm of gradients: 0.04714223735024459\n",
      "l2 norm of weights: 3.412904450765507\n",
      "---------------------\n",
      "Iteration Number: 9104\n",
      "Loss: 11.23341002406296\n",
      "l2 norm of gradients: 0.047136778709550714\n",
      "l2 norm of weights: 3.412967954823813\n",
      "---------------------\n",
      "Iteration Number: 9105\n",
      "Loss: 11.233200875723366\n",
      "l2 norm of gradients: 0.04713132146112456\n",
      "l2 norm of weights: 3.4130314545794307\n",
      "---------------------\n",
      "Iteration Number: 9106\n",
      "Loss: 11.232991801290245\n",
      "l2 norm of gradients: 0.047125865604239904\n",
      "l2 norm of weights: 3.4130949500304446\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 9107\n",
      "Loss: 11.232782800733915\n",
      "l2 norm of gradients: 0.047120411138170996\n",
      "l2 norm of weights: 3.4131584411749425\n",
      "---------------------\n",
      "Iteration Number: 9108\n",
      "Loss: 11.232573874024734\n",
      "l2 norm of gradients: 0.04711495806219282\n",
      "l2 norm of weights: 3.413221928011012\n",
      "---------------------\n",
      "Iteration Number: 9109\n",
      "Loss: 11.23236502113306\n",
      "l2 norm of gradients: 0.04710950637558091\n",
      "l2 norm of weights: 3.4132854105367447\n",
      "---------------------\n",
      "Iteration Number: 9110\n",
      "Loss: 11.232156242029275\n",
      "l2 norm of gradients: 0.047104056077611436\n",
      "l2 norm of weights: 3.4133488887502312\n",
      "---------------------\n",
      "Iteration Number: 9111\n",
      "Loss: 11.23194753668377\n",
      "l2 norm of gradients: 0.04709860716756113\n",
      "l2 norm of weights: 3.413412362649565\n",
      "---------------------\n",
      "Iteration Number: 9112\n",
      "Loss: 11.23173890506698\n",
      "l2 norm of gradients: 0.047093159644707366\n",
      "l2 norm of weights: 3.4134758322328405\n",
      "---------------------\n",
      "Iteration Number: 9113\n",
      "Loss: 11.231530347149327\n",
      "l2 norm of gradients: 0.047087713508328105\n",
      "l2 norm of weights: 3.413539297498154\n",
      "---------------------\n",
      "Iteration Number: 9114\n",
      "Loss: 11.231321862901272\n",
      "l2 norm of gradients: 0.047082268757701946\n",
      "l2 norm of weights: 3.413602758443604\n",
      "---------------------\n",
      "Iteration Number: 9115\n",
      "Loss: 11.231113452293286\n",
      "l2 norm of gradients: 0.04707682539210803\n",
      "l2 norm of weights: 3.413666215067289\n",
      "---------------------\n",
      "Iteration Number: 9116\n",
      "Loss: 11.230905115295846\n",
      "l2 norm of gradients: 0.04707138341082617\n",
      "l2 norm of weights: 3.41372966736731\n",
      "---------------------\n",
      "Iteration Number: 9117\n",
      "Loss: 11.230696851879468\n",
      "l2 norm of gradients: 0.04706594281313672\n",
      "l2 norm of weights: 3.413793115341769\n",
      "---------------------\n",
      "Iteration Number: 9118\n",
      "Loss: 11.230488662014677\n",
      "l2 norm of gradients: 0.04706050359832067\n",
      "l2 norm of weights: 3.4138565589887704\n",
      "---------------------\n",
      "Iteration Number: 9119\n",
      "Loss: 11.230280545672011\n",
      "l2 norm of gradients: 0.047055065765659584\n",
      "l2 norm of weights: 3.413919998306419\n",
      "---------------------\n",
      "Iteration Number: 9120\n",
      "Loss: 11.230072502822027\n",
      "l2 norm of gradients: 0.0470496293144357\n",
      "l2 norm of weights: 3.4139834332928216\n",
      "---------------------\n",
      "Iteration Number: 9121\n",
      "Loss: 11.229864533435306\n",
      "l2 norm of gradients: 0.04704419424393176\n",
      "l2 norm of weights: 3.414046863946087\n",
      "---------------------\n",
      "Iteration Number: 9122\n",
      "Loss: 11.22965663748244\n",
      "l2 norm of gradients: 0.04703876055343117\n",
      "l2 norm of weights: 3.4141102902643246\n",
      "---------------------\n",
      "Iteration Number: 9123\n",
      "Loss: 11.22944881493404\n",
      "l2 norm of gradients: 0.04703332824221791\n",
      "l2 norm of weights: 3.414173712245646\n",
      "---------------------\n",
      "Iteration Number: 9124\n",
      "Loss: 11.229241065760739\n",
      "l2 norm of gradients: 0.047027897309576544\n",
      "l2 norm of weights: 3.414237129888164\n",
      "---------------------\n",
      "Iteration Number: 9125\n",
      "Loss: 11.229033389933186\n",
      "l2 norm of gradients: 0.047022467754792296\n",
      "l2 norm of weights: 3.4143005431899933\n",
      "---------------------\n",
      "Iteration Number: 9126\n",
      "Loss: 11.228825787422036\n",
      "l2 norm of gradients: 0.04701703957715091\n",
      "l2 norm of weights: 3.4143639521492495\n",
      "---------------------\n",
      "Iteration Number: 9127\n",
      "Loss: 11.228618258197976\n",
      "l2 norm of gradients: 0.047011612775938745\n",
      "l2 norm of weights: 3.414427356764049\n",
      "---------------------\n",
      "Iteration Number: 9128\n",
      "Loss: 11.22841080223171\n",
      "l2 norm of gradients: 0.04700618735044285\n",
      "l2 norm of weights: 3.4144907570325116\n",
      "---------------------\n",
      "Iteration Number: 9129\n",
      "Loss: 11.228203419493957\n",
      "l2 norm of gradients: 0.047000763299950704\n",
      "l2 norm of weights: 3.414554152952758\n",
      "---------------------\n",
      "Iteration Number: 9130\n",
      "Loss: 11.227996109955432\n",
      "l2 norm of gradients: 0.04699534062375051\n",
      "l2 norm of weights: 3.4146175445229088\n",
      "---------------------\n",
      "Iteration Number: 9131\n",
      "Loss: 11.227788873586913\n",
      "l2 norm of gradients: 0.04698991932113104\n",
      "l2 norm of weights: 3.414680931741088\n",
      "---------------------\n",
      "Iteration Number: 9132\n",
      "Loss: 11.227581710359155\n",
      "l2 norm of gradients: 0.04698449939138167\n",
      "l2 norm of weights: 3.41474431460542\n",
      "---------------------\n",
      "Iteration Number: 9133\n",
      "Loss: 11.227374620242948\n",
      "l2 norm of gradients: 0.04697908083379228\n",
      "l2 norm of weights: 3.414807693114031\n",
      "---------------------\n",
      "Iteration Number: 9134\n",
      "Loss: 11.227167603209095\n",
      "l2 norm of gradients: 0.046973663647653444\n",
      "l2 norm of weights: 3.4148710672650497\n",
      "---------------------\n",
      "Iteration Number: 9135\n",
      "Loss: 11.226960659228416\n",
      "l2 norm of gradients: 0.04696824783225629\n",
      "l2 norm of weights: 3.4149344370566044\n",
      "---------------------\n",
      "Iteration Number: 9136\n",
      "Loss: 11.22675378827176\n",
      "l2 norm of gradients: 0.046962833386892555\n",
      "l2 norm of weights: 3.4149978024868255\n",
      "---------------------\n",
      "Iteration Number: 9137\n",
      "Loss: 11.226546990309973\n",
      "l2 norm of gradients: 0.04695742031085454\n",
      "l2 norm of weights: 3.4150611635538457\n",
      "---------------------\n",
      "Iteration Number: 9138\n",
      "Loss: 11.226340265313937\n",
      "l2 norm of gradients: 0.046952008603435166\n",
      "l2 norm of weights: 3.415124520255798\n",
      "---------------------\n",
      "Iteration Number: 9139\n",
      "Loss: 11.226133613254532\n",
      "l2 norm of gradients: 0.04694659826392791\n",
      "l2 norm of weights: 3.4151878725908182\n",
      "---------------------\n",
      "Iteration Number: 9140\n",
      "Loss: 11.225927034102684\n",
      "l2 norm of gradients: 0.046941189291626884\n",
      "l2 norm of weights: 3.415251220557042\n",
      "---------------------\n",
      "Iteration Number: 9141\n",
      "Loss: 11.2257205278293\n",
      "l2 norm of gradients: 0.04693578168582677\n",
      "l2 norm of weights: 3.4153145641526086\n",
      "---------------------\n",
      "Iteration Number: 9142\n",
      "Loss: 11.225514094405327\n",
      "l2 norm of gradients: 0.04693037544582282\n",
      "l2 norm of weights: 3.415377903375656\n",
      "---------------------\n",
      "Iteration Number: 9143\n",
      "Loss: 11.225307733801737\n",
      "l2 norm of gradients: 0.046924970570910884\n",
      "l2 norm of weights: 3.415441238224326\n",
      "---------------------\n",
      "Iteration Number: 9144\n",
      "Loss: 11.225101445989502\n",
      "l2 norm of gradients: 0.046919567060387406\n",
      "l2 norm of weights: 3.4155045686967602\n",
      "---------------------\n",
      "Iteration Number: 9145\n",
      "Loss: 11.22489523093961\n",
      "l2 norm of gradients: 0.04691416491354942\n",
      "l2 norm of weights: 3.4155678947911032\n",
      "---------------------\n",
      "Iteration Number: 9146\n",
      "Loss: 11.22468908862308\n",
      "l2 norm of gradients: 0.04690876412969456\n",
      "l2 norm of weights: 3.4156312165054996\n",
      "---------------------\n",
      "Iteration Number: 9147\n",
      "Loss: 11.224483019010938\n",
      "l2 norm of gradients: 0.04690336470812101\n",
      "l2 norm of weights: 3.4156945338380966\n",
      "---------------------\n",
      "Iteration Number: 9148\n",
      "Loss: 11.224277022074238\n",
      "l2 norm of gradients: 0.04689796664812759\n",
      "l2 norm of weights: 3.4157578467870424\n",
      "---------------------\n",
      "Iteration Number: 9149\n",
      "Loss: 11.224071097784034\n",
      "l2 norm of gradients: 0.0468925699490136\n",
      "l2 norm of weights: 3.415821155350486\n",
      "---------------------\n",
      "Iteration Number: 9150\n",
      "Loss: 11.223865246111414\n",
      "l2 norm of gradients: 0.046887174610079095\n",
      "l2 norm of weights: 3.415884459526579\n",
      "---------------------\n",
      "Iteration Number: 9151\n",
      "Loss: 11.223659467027467\n",
      "l2 norm of gradients: 0.04688178063062457\n",
      "l2 norm of weights: 3.415947759313473\n",
      "---------------------\n",
      "Iteration Number: 9152\n",
      "Loss: 11.223453760503327\n",
      "l2 norm of gradients: 0.046876388009951156\n",
      "l2 norm of weights: 3.416011054709323\n",
      "---------------------\n",
      "Iteration Number: 9153\n",
      "Loss: 11.2232481265101\n",
      "l2 norm of gradients: 0.04687099674736057\n",
      "l2 norm of weights: 3.4160743457122837\n",
      "---------------------\n",
      "Iteration Number: 9154\n",
      "Loss: 11.223042565018956\n",
      "l2 norm of gradients: 0.046865606842155086\n",
      "l2 norm of weights: 3.416137632320512\n",
      "---------------------\n",
      "Iteration Number: 9155\n",
      "Loss: 11.222837076001053\n",
      "l2 norm of gradients: 0.046860218293637595\n",
      "l2 norm of weights: 3.416200914532166\n",
      "---------------------\n",
      "Iteration Number: 9156\n",
      "Loss: 11.222631659427574\n",
      "l2 norm of gradients: 0.04685483110111158\n",
      "l2 norm of weights: 3.416264192345405\n",
      "---------------------\n",
      "Iteration Number: 9157\n",
      "Loss: 11.222426315269729\n",
      "l2 norm of gradients: 0.04684944526388105\n",
      "l2 norm of weights: 3.416327465758391\n",
      "---------------------\n",
      "Iteration Number: 9158\n",
      "Loss: 11.222221043498717\n",
      "l2 norm of gradients: 0.04684406078125062\n",
      "l2 norm of weights: 3.4163907347692857\n",
      "---------------------\n",
      "Iteration Number: 9159\n",
      "Loss: 11.22201584408579\n",
      "l2 norm of gradients: 0.046838677652525515\n",
      "l2 norm of weights: 3.416453999376253\n",
      "---------------------\n",
      "Iteration Number: 9160\n",
      "Loss: 11.221810717002187\n",
      "l2 norm of gradients: 0.046833295877011474\n",
      "l2 norm of weights: 3.4165172595774584\n",
      "---------------------\n",
      "Iteration Number: 9161\n",
      "Loss: 11.22160566221919\n",
      "l2 norm of gradients: 0.046827915454014914\n",
      "l2 norm of weights: 3.416580515371069\n",
      "---------------------\n",
      "Iteration Number: 9162\n",
      "Loss: 11.221400679708077\n",
      "l2 norm of gradients: 0.04682253638284274\n",
      "l2 norm of weights: 3.4166437667552523\n",
      "---------------------\n",
      "Iteration Number: 9163\n",
      "Loss: 11.221195769440156\n",
      "l2 norm of gradients: 0.046817158662802436\n",
      "l2 norm of weights: 3.4167070137281783\n",
      "---------------------\n",
      "Iteration Number: 9164\n",
      "Loss: 11.220990931386737\n",
      "l2 norm of gradients: 0.04681178229320216\n",
      "l2 norm of weights: 3.4167702562880176\n",
      "---------------------\n",
      "Iteration Number: 9165\n",
      "Loss: 11.22078616551916\n",
      "l2 norm of gradients: 0.04680640727335052\n",
      "l2 norm of weights: 3.416833494432943\n",
      "---------------------\n",
      "Iteration Number: 9166\n",
      "Loss: 11.220581471808783\n",
      "l2 norm of gradients: 0.04680103360255681\n",
      "l2 norm of weights: 3.4168967281611278\n",
      "---------------------\n",
      "Iteration Number: 9167\n",
      "Loss: 11.22037685022697\n",
      "l2 norm of gradients: 0.046795661280130844\n",
      "l2 norm of weights: 3.4169599574707474\n",
      "---------------------\n",
      "Iteration Number: 9168\n",
      "Loss: 11.220172300745116\n",
      "l2 norm of gradients: 0.046790290305382974\n",
      "l2 norm of weights: 3.417023182359978\n",
      "---------------------\n",
      "Iteration Number: 9169\n",
      "Loss: 11.219967823334619\n",
      "l2 norm of gradients: 0.046784920677624256\n",
      "l2 norm of weights: 3.4170864028269987\n",
      "---------------------\n",
      "Iteration Number: 9170\n",
      "Loss: 11.219763417966893\n",
      "l2 norm of gradients: 0.046779552396166164\n",
      "l2 norm of weights: 3.417149618869988\n",
      "---------------------\n",
      "Iteration Number: 9171\n",
      "Loss: 11.219559084613394\n",
      "l2 norm of gradients: 0.046774185460320854\n",
      "l2 norm of weights: 3.417212830487127\n",
      "---------------------\n",
      "Iteration Number: 9172\n",
      "Loss: 11.219354823245563\n",
      "l2 norm of gradients: 0.04676881986940103\n",
      "l2 norm of weights: 3.4172760376765976\n",
      "---------------------\n",
      "Iteration Number: 9173\n",
      "Loss: 11.219150633834873\n",
      "l2 norm of gradients: 0.04676345562271994\n",
      "l2 norm of weights: 3.4173392404365837\n",
      "---------------------\n",
      "Iteration Number: 9174\n",
      "Loss: 11.218946516352819\n",
      "l2 norm of gradients: 0.04675809271959144\n",
      "l2 norm of weights: 3.41740243876527\n",
      "---------------------\n",
      "Iteration Number: 9175\n",
      "Loss: 11.218742470770898\n",
      "l2 norm of gradients: 0.0467527311593299\n",
      "l2 norm of weights: 3.4174656326608424\n",
      "---------------------\n",
      "Iteration Number: 9176\n",
      "Loss: 11.218538497060637\n",
      "l2 norm of gradients: 0.0467473709412504\n",
      "l2 norm of weights: 3.41752882212149\n",
      "---------------------\n",
      "Iteration Number: 9177\n",
      "Loss: 11.21833459519357\n",
      "l2 norm of gradients: 0.046742012064668416\n",
      "l2 norm of weights: 3.4175920071454002\n",
      "---------------------\n",
      "Iteration Number: 9178\n",
      "Loss: 11.218130765141261\n",
      "l2 norm of gradients: 0.04673665452890012\n",
      "l2 norm of weights: 3.417655187730765\n",
      "---------------------\n",
      "Iteration Number: 9179\n",
      "Loss: 11.217927006875264\n",
      "l2 norm of gradients: 0.04673129833326215\n",
      "l2 norm of weights: 3.4177183638757755\n",
      "---------------------\n",
      "Iteration Number: 9180\n",
      "Loss: 11.217723320367186\n",
      "l2 norm of gradients: 0.046725943477071825\n",
      "l2 norm of weights: 3.417781535578625\n",
      "---------------------\n",
      "Iteration Number: 9181\n",
      "Loss: 11.217519705588625\n",
      "l2 norm of gradients: 0.046720589959646945\n",
      "l2 norm of weights: 3.417844702837508\n",
      "---------------------\n",
      "Iteration Number: 9182\n",
      "Loss: 11.217316162511207\n",
      "l2 norm of gradients: 0.046715237780306\n",
      "l2 norm of weights: 3.417907865650621\n",
      "---------------------\n",
      "Iteration Number: 9183\n",
      "Loss: 11.217112691106562\n",
      "l2 norm of gradients: 0.046709886938367876\n",
      "l2 norm of weights: 3.417971024016161\n",
      "---------------------\n",
      "Iteration Number: 9184\n",
      "Loss: 11.216909291346347\n",
      "l2 norm of gradients: 0.04670453743315213\n",
      "l2 norm of weights: 3.418034177932327\n",
      "---------------------\n",
      "Iteration Number: 9185\n",
      "Loss: 11.216705963202246\n",
      "l2 norm of gradients: 0.04669918926397889\n",
      "l2 norm of weights: 3.4180973273973185\n",
      "---------------------\n",
      "Iteration Number: 9186\n",
      "Loss: 11.216502706645931\n",
      "l2 norm of gradients: 0.04669384243016882\n",
      "l2 norm of weights: 3.4181604724093373\n",
      "---------------------\n",
      "Iteration Number: 9187\n",
      "Loss: 11.216299521649116\n",
      "l2 norm of gradients: 0.046688496931043155\n",
      "l2 norm of weights: 3.418223612966587\n",
      "---------------------\n",
      "Iteration Number: 9188\n",
      "Loss: 11.216096408183528\n",
      "l2 norm of gradients: 0.046683152765923726\n",
      "l2 norm of weights: 3.4182867490672706\n",
      "---------------------\n",
      "Iteration Number: 9189\n",
      "Loss: 11.21589336622089\n",
      "l2 norm of gradients: 0.0466778099341329\n",
      "l2 norm of weights: 3.4183498807095942\n",
      "---------------------\n",
      "Iteration Number: 9190\n",
      "Loss: 11.21569039573297\n",
      "l2 norm of gradients: 0.046672468434993564\n",
      "l2 norm of weights: 3.4184130078917647\n",
      "---------------------\n",
      "Iteration Number: 9191\n",
      "Loss: 11.215487496691534\n",
      "l2 norm of gradients: 0.046667128267829296\n",
      "l2 norm of weights: 3.4184761306119906\n",
      "---------------------\n",
      "Iteration Number: 9192\n",
      "Loss: 11.215284669068367\n",
      "l2 norm of gradients: 0.046661789431964126\n",
      "l2 norm of weights: 3.418539248868481\n",
      "---------------------\n",
      "Iteration Number: 9193\n",
      "Loss: 11.215081912835284\n",
      "l2 norm of gradients: 0.046656451926722686\n",
      "l2 norm of weights: 3.4186023626594473\n",
      "---------------------\n",
      "Iteration Number: 9194\n",
      "Loss: 11.214879227964095\n",
      "l2 norm of gradients: 0.046651115751430154\n",
      "l2 norm of weights: 3.4186654719831013\n",
      "---------------------\n",
      "Iteration Number: 9195\n",
      "Loss: 11.214676614426635\n",
      "l2 norm of gradients: 0.04664578090541229\n",
      "l2 norm of weights: 3.418728576837657\n",
      "---------------------\n",
      "Iteration Number: 9196\n",
      "Loss: 11.214474072194774\n",
      "l2 norm of gradients: 0.046640447387995436\n",
      "l2 norm of weights: 3.4187916772213294\n",
      "---------------------\n",
      "Iteration Number: 9197\n",
      "Loss: 11.214271601240362\n",
      "l2 norm of gradients: 0.04663511519850643\n",
      "l2 norm of weights: 3.4188547731323347\n",
      "---------------------\n",
      "Iteration Number: 9198\n",
      "Loss: 11.214069201535306\n",
      "l2 norm of gradients: 0.046629784336272746\n",
      "l2 norm of weights: 3.4189178645688916\n",
      "---------------------\n",
      "Iteration Number: 9199\n",
      "Loss: 11.21386687305149\n",
      "l2 norm of gradients: 0.04662445480062236\n",
      "l2 norm of weights: 3.4189809515292175\n",
      "---------------------\n",
      "Iteration Number: 9200\n",
      "Loss: 11.213664615760852\n",
      "l2 norm of gradients: 0.046619126590883846\n",
      "l2 norm of weights: 3.4190440340115336\n",
      "---------------------\n",
      "Iteration Number: 9201\n",
      "Loss: 11.21346242963531\n",
      "l2 norm of gradients: 0.046613799706386315\n",
      "l2 norm of weights: 3.4191071120140615\n",
      "---------------------\n",
      "Iteration Number: 9202\n",
      "Loss: 11.213260314646819\n",
      "l2 norm of gradients: 0.046608474146459446\n",
      "l2 norm of weights: 3.419170185535024\n",
      "---------------------\n",
      "Iteration Number: 9203\n",
      "Loss: 11.21305827076736\n",
      "l2 norm of gradients: 0.046603149910433496\n",
      "l2 norm of weights: 3.4192332545726463\n",
      "---------------------\n",
      "Iteration Number: 9204\n",
      "Loss: 11.212856297968914\n",
      "l2 norm of gradients: 0.04659782699763924\n",
      "l2 norm of weights: 3.4192963191251535\n",
      "---------------------\n",
      "Iteration Number: 9205\n",
      "Loss: 11.212654396223474\n",
      "l2 norm of gradients: 0.046592505407408016\n",
      "l2 norm of weights: 3.419359379190772\n",
      "---------------------\n",
      "Iteration Number: 9206\n",
      "Loss: 11.212452565503066\n",
      "l2 norm of gradients: 0.046587185139071764\n",
      "l2 norm of weights: 3.419422434767732\n",
      "---------------------\n",
      "Iteration Number: 9207\n",
      "Loss: 11.212250805779716\n",
      "l2 norm of gradients: 0.04658186619196295\n",
      "l2 norm of weights: 3.4194854858542607\n",
      "---------------------\n",
      "Iteration Number: 9208\n",
      "Loss: 11.21204911702548\n",
      "l2 norm of gradients: 0.04657654856541456\n",
      "l2 norm of weights: 3.419548532448591\n",
      "---------------------\n",
      "Iteration Number: 9209\n",
      "Loss: 11.21184749921242\n",
      "l2 norm of gradients: 0.046571232258760226\n",
      "l2 norm of weights: 3.4196115745489544\n",
      "---------------------\n",
      "Iteration Number: 9210\n",
      "Loss: 11.21164595231262\n",
      "l2 norm of gradients: 0.04656591727133406\n",
      "l2 norm of weights: 3.4196746121535844\n",
      "---------------------\n",
      "Iteration Number: 9211\n",
      "Loss: 11.211444476298182\n",
      "l2 norm of gradients: 0.04656060360247074\n",
      "l2 norm of weights: 3.419737645260717\n",
      "---------------------\n",
      "Iteration Number: 9212\n",
      "Loss: 11.211243071141219\n",
      "l2 norm of gradients: 0.04655529125150551\n",
      "l2 norm of weights: 3.4198006738685867\n",
      "---------------------\n",
      "Iteration Number: 9213\n",
      "Loss: 11.211041736813863\n",
      "l2 norm of gradients: 0.046549980217774185\n",
      "l2 norm of weights: 3.4198636979754324\n",
      "---------------------\n",
      "Iteration Number: 9214\n",
      "Loss: 11.210840473288258\n",
      "l2 norm of gradients: 0.04654467050061312\n",
      "l2 norm of weights: 3.419926717579492\n",
      "---------------------\n",
      "Iteration Number: 9215\n",
      "Loss: 11.210639280536569\n",
      "l2 norm of gradients: 0.04653936209935923\n",
      "l2 norm of weights: 3.4199897326790065\n",
      "---------------------\n",
      "Iteration Number: 9216\n",
      "Loss: 11.210438158530984\n",
      "l2 norm of gradients: 0.04653405501334992\n",
      "l2 norm of weights: 3.420052743272217\n",
      "---------------------\n",
      "Iteration Number: 9217\n",
      "Loss: 11.210237107243682\n",
      "l2 norm of gradients: 0.04652874924192327\n",
      "l2 norm of weights: 3.4201157493573673\n",
      "---------------------\n",
      "Iteration Number: 9218\n",
      "Loss: 11.210036126646896\n",
      "l2 norm of gradients: 0.04652344478441778\n",
      "l2 norm of weights: 3.4201787509327\n",
      "---------------------\n",
      "Iteration Number: 9219\n",
      "Loss: 11.209835216712834\n",
      "l2 norm of gradients: 0.046518141640172594\n",
      "l2 norm of weights: 3.420241747996461\n",
      "---------------------\n",
      "Iteration Number: 9220\n",
      "Loss: 11.209634377413751\n",
      "l2 norm of gradients: 0.0465128398085274\n",
      "l2 norm of weights: 3.420304740546897\n",
      "---------------------\n",
      "Iteration Number: 9221\n",
      "Loss: 11.20943360872191\n",
      "l2 norm of gradients: 0.046507539288822376\n",
      "l2 norm of weights: 3.4203677285822556\n",
      "---------------------\n",
      "Iteration Number: 9222\n",
      "Loss: 11.209232910609579\n",
      "l2 norm of gradients: 0.046502240080398285\n",
      "l2 norm of weights: 3.420430712100787\n",
      "---------------------\n",
      "Iteration Number: 9223\n",
      "Loss: 11.209032283049053\n",
      "l2 norm of gradients: 0.046496942182596465\n",
      "l2 norm of weights: 3.420493691100741\n",
      "---------------------\n",
      "Iteration Number: 9224\n",
      "Loss: 11.208831726012647\n",
      "l2 norm of gradients: 0.04649164559475877\n",
      "l2 norm of weights: 3.42055666558037\n",
      "---------------------\n",
      "Iteration Number: 9225\n",
      "Loss: 11.208631239472677\n",
      "l2 norm of gradients: 0.046486350316227616\n",
      "l2 norm of weights: 3.4206196355379266\n",
      "---------------------\n",
      "Iteration Number: 9226\n",
      "Loss: 11.208430823401489\n",
      "l2 norm of gradients: 0.04648105634634595\n",
      "l2 norm of weights: 3.4206826009716655\n",
      "---------------------\n",
      "Iteration Number: 9227\n",
      "Loss: 11.208230477771437\n",
      "l2 norm of gradients: 0.0464757636844573\n",
      "l2 norm of weights: 3.4207455618798424\n",
      "---------------------\n",
      "Iteration Number: 9228\n",
      "Loss: 11.208030202554891\n",
      "l2 norm of gradients: 0.046470472329905706\n",
      "l2 norm of weights: 3.4208085182607144\n",
      "---------------------\n",
      "Iteration Number: 9229\n",
      "Loss: 11.207829997724243\n",
      "l2 norm of gradients: 0.04646518228203577\n",
      "l2 norm of weights: 3.420871470112539\n",
      "---------------------\n",
      "Iteration Number: 9230\n",
      "Loss: 11.207629863251901\n",
      "l2 norm of gradients: 0.04645989354019266\n",
      "l2 norm of weights: 3.420934417433577\n",
      "---------------------\n",
      "Iteration Number: 9231\n",
      "Loss: 11.207429799110281\n",
      "l2 norm of gradients: 0.04645460610372205\n",
      "l2 norm of weights: 3.420997360222088\n",
      "---------------------\n",
      "Iteration Number: 9232\n",
      "Loss: 11.207229805271812\n",
      "l2 norm of gradients: 0.04644931997197017\n",
      "l2 norm of weights: 3.4210602984763354\n",
      "---------------------\n",
      "Iteration Number: 9233\n",
      "Loss: 11.207029881708962\n",
      "l2 norm of gradients: 0.04644403514428383\n",
      "l2 norm of weights: 3.4211232321945815\n",
      "---------------------\n",
      "Iteration Number: 9234\n",
      "Loss: 11.206830028394187\n",
      "l2 norm of gradients: 0.04643875162001034\n",
      "l2 norm of weights: 3.421186161375091\n",
      "---------------------\n",
      "Iteration Number: 9235\n",
      "Loss: 11.206630245299976\n",
      "l2 norm of gradients: 0.04643346939849761\n",
      "l2 norm of weights: 3.4212490860161306\n",
      "---------------------\n",
      "Iteration Number: 9236\n",
      "Loss: 11.206430532398826\n",
      "l2 norm of gradients: 0.046428188479094\n",
      "l2 norm of weights: 3.421312006115967\n",
      "---------------------\n",
      "Iteration Number: 9237\n",
      "Loss: 11.206230889663255\n",
      "l2 norm of gradients: 0.04642290886114854\n",
      "l2 norm of weights: 3.421374921672868\n",
      "---------------------\n",
      "Iteration Number: 9238\n",
      "Loss: 11.206031317065795\n",
      "l2 norm of gradients: 0.04641763054401065\n",
      "l2 norm of weights: 3.4214378326851045\n",
      "---------------------\n",
      "Iteration Number: 9239\n",
      "Loss: 11.205831814578985\n",
      "l2 norm of gradients: 0.04641235352703048\n",
      "l2 norm of weights: 3.4215007391509467\n",
      "---------------------\n",
      "Iteration Number: 9240\n",
      "Loss: 11.205632382175406\n",
      "l2 norm of gradients: 0.046407077809558515\n",
      "l2 norm of weights: 3.4215636410686674\n",
      "---------------------\n",
      "Iteration Number: 9241\n",
      "Loss: 11.205433019827623\n",
      "l2 norm of gradients: 0.04640180339094596\n",
      "l2 norm of weights: 3.421626538436539\n",
      "---------------------\n",
      "Iteration Number: 9242\n",
      "Loss: 11.205233727508228\n",
      "l2 norm of gradients: 0.04639653027054444\n",
      "l2 norm of weights: 3.4216894312528376\n",
      "---------------------\n",
      "Iteration Number: 9243\n",
      "Loss: 11.205034505189843\n",
      "l2 norm of gradients: 0.0463912584477062\n",
      "l2 norm of weights: 3.4217523195158384\n",
      "---------------------\n",
      "Iteration Number: 9244\n",
      "Loss: 11.20483535284508\n",
      "l2 norm of gradients: 0.04638598792178397\n",
      "l2 norm of weights: 3.4218152032238187\n",
      "---------------------\n",
      "Iteration Number: 9245\n",
      "Loss: 11.204636270446594\n",
      "l2 norm of gradients: 0.04638071869213107\n",
      "l2 norm of weights: 3.421878082375057\n",
      "---------------------\n",
      "Iteration Number: 9246\n",
      "Loss: 11.204437257967045\n",
      "l2 norm of gradients: 0.04637545075810129\n",
      "l2 norm of weights: 3.421940956967833\n",
      "---------------------\n",
      "Iteration Number: 9247\n",
      "Loss: 11.204238315379087\n",
      "l2 norm of gradients: 0.04637018411904902\n",
      "l2 norm of weights: 3.4220038270004283\n",
      "---------------------\n",
      "Iteration Number: 9248\n",
      "Loss: 11.204039442655425\n",
      "l2 norm of gradients: 0.04636491877432919\n",
      "l2 norm of weights: 3.4220666924711245\n",
      "---------------------\n",
      "Iteration Number: 9249\n",
      "Loss: 11.203840639768762\n",
      "l2 norm of gradients: 0.046359654723297214\n",
      "l2 norm of weights: 3.422129553378205\n",
      "---------------------\n",
      "Iteration Number: 9250\n",
      "Loss: 11.203641906691818\n",
      "l2 norm of gradients: 0.0463543919653091\n",
      "l2 norm of weights: 3.422192409719955\n",
      "---------------------\n",
      "Iteration Number: 9251\n",
      "Loss: 11.203443243397327\n",
      "l2 norm of gradients: 0.046349130499721364\n",
      "l2 norm of weights: 3.42225526149466\n",
      "---------------------\n",
      "Iteration Number: 9252\n",
      "Loss: 11.203244649858032\n",
      "l2 norm of gradients: 0.04634387032589106\n",
      "l2 norm of weights: 3.422318108700608\n",
      "---------------------\n",
      "Iteration Number: 9253\n",
      "Loss: 11.203046126046718\n",
      "l2 norm of gradients: 0.0463386114431758\n",
      "l2 norm of weights: 3.422380951336086\n",
      "---------------------\n",
      "Iteration Number: 9254\n",
      "Loss: 11.202847671936153\n",
      "l2 norm of gradients: 0.04633335385093368\n",
      "l2 norm of weights: 3.4224437893993853\n",
      "---------------------\n",
      "Iteration Number: 9255\n",
      "Loss: 11.202649287499144\n",
      "l2 norm of gradients: 0.0463280975485234\n",
      "l2 norm of weights: 3.4225066228887955\n",
      "---------------------\n",
      "Iteration Number: 9256\n",
      "Loss: 11.202450972708506\n",
      "l2 norm of gradients: 0.04632284253530415\n",
      "l2 norm of weights: 3.4225694518026093\n",
      "---------------------\n",
      "Iteration Number: 9257\n",
      "Loss: 11.202252727537058\n",
      "l2 norm of gradients: 0.046317588810635676\n",
      "l2 norm of weights: 3.4226322761391206\n",
      "---------------------\n",
      "Iteration Number: 9258\n",
      "Loss: 11.202054551957659\n",
      "l2 norm of gradients: 0.04631233637387824\n",
      "l2 norm of weights: 3.422695095896623\n",
      "---------------------\n",
      "Iteration Number: 9259\n",
      "Loss: 11.20185644594315\n",
      "l2 norm of gradients: 0.046307085224392634\n",
      "l2 norm of weights: 3.422757911073413\n",
      "---------------------\n",
      "Iteration Number: 9260\n",
      "Loss: 11.201658409466432\n",
      "l2 norm of gradients: 0.046301835361540235\n",
      "l2 norm of weights: 3.422820721667787\n",
      "---------------------\n",
      "Iteration Number: 9261\n",
      "Loss: 11.201460442500377\n",
      "l2 norm of gradients: 0.046296586784682875\n",
      "l2 norm of weights: 3.4228835276780436\n",
      "---------------------\n",
      "Iteration Number: 9262\n",
      "Loss: 11.2012625450179\n",
      "l2 norm of gradients: 0.04629133949318299\n",
      "l2 norm of weights: 3.422946329102482\n",
      "---------------------\n",
      "Iteration Number: 9263\n",
      "Loss: 11.201064716991926\n",
      "l2 norm of gradients: 0.0462860934864035\n",
      "l2 norm of weights: 3.4230091259394038\n",
      "---------------------\n",
      "Iteration Number: 9264\n",
      "Loss: 11.200866958395387\n",
      "l2 norm of gradients: 0.046280848763707874\n",
      "l2 norm of weights: 3.42307191818711\n",
      "---------------------\n",
      "Iteration Number: 9265\n",
      "Loss: 11.200669269201239\n",
      "l2 norm of gradients: 0.0462756053244601\n",
      "l2 norm of weights: 3.4231347058439043\n",
      "---------------------\n",
      "Iteration Number: 9266\n",
      "Loss: 11.200471649382452\n",
      "l2 norm of gradients: 0.046270363168024735\n",
      "l2 norm of weights: 3.4231974889080905\n",
      "---------------------\n",
      "Iteration Number: 9267\n",
      "Loss: 11.200274098912013\n",
      "l2 norm of gradients: 0.046265122293766836\n",
      "l2 norm of weights: 3.4232602673779744\n",
      "---------------------\n",
      "Iteration Number: 9268\n",
      "Loss: 11.20007661776292\n",
      "l2 norm of gradients: 0.046259882701051976\n",
      "l2 norm of weights: 3.4233230412518623\n",
      "---------------------\n",
      "Iteration Number: 9269\n",
      "Loss: 11.199879205908179\n",
      "l2 norm of gradients: 0.0462546443892463\n",
      "l2 norm of weights: 3.423385810528063\n",
      "---------------------\n",
      "Iteration Number: 9270\n",
      "Loss: 11.199681863320826\n",
      "l2 norm of gradients: 0.04624940735771643\n",
      "l2 norm of weights: 3.4234485752048855\n",
      "---------------------\n",
      "Iteration Number: 9271\n",
      "Loss: 11.199484589973913\n",
      "l2 norm of gradients: 0.046244171605829575\n",
      "l2 norm of weights: 3.4235113352806397\n",
      "---------------------\n",
      "Iteration Number: 9272\n",
      "Loss: 11.199287385840497\n",
      "l2 norm of gradients: 0.04623893713295342\n",
      "l2 norm of weights: 3.4235740907536374\n",
      "---------------------\n",
      "Iteration Number: 9273\n",
      "Loss: 11.19909025089365\n",
      "l2 norm of gradients: 0.046233703938456217\n",
      "l2 norm of weights: 3.4236368416221916\n",
      "---------------------\n",
      "Iteration Number: 9274\n",
      "Loss: 11.198893185106469\n",
      "l2 norm of gradients: 0.046228472021706705\n",
      "l2 norm of weights: 3.423699587884616\n",
      "---------------------\n",
      "Iteration Number: 9275\n",
      "Loss: 11.198696188452063\n",
      "l2 norm of gradients: 0.046223241382074204\n",
      "l2 norm of weights: 3.4237623295392257\n",
      "---------------------\n",
      "Iteration Number: 9276\n",
      "Loss: 11.198499260903551\n",
      "l2 norm of gradients: 0.04621801201892853\n",
      "l2 norm of weights: 3.4238250665843375\n",
      "---------------------\n",
      "Iteration Number: 9277\n",
      "Loss: 11.198302402434074\n",
      "l2 norm of gradients: 0.04621278393164001\n",
      "l2 norm of weights: 3.4238877990182686\n",
      "---------------------\n",
      "Iteration Number: 9278\n",
      "Loss: 11.198105613016773\n",
      "l2 norm of gradients: 0.04620755711957954\n",
      "l2 norm of weights: 3.423950526839338\n",
      "---------------------\n",
      "Iteration Number: 9279\n",
      "Loss: 11.19790889262483\n",
      "l2 norm of gradients: 0.04620233158211848\n",
      "l2 norm of weights: 3.424013250045865\n",
      "---------------------\n",
      "Iteration Number: 9280\n",
      "Loss: 11.197712241231422\n",
      "l2 norm of gradients: 0.04619710731862879\n",
      "l2 norm of weights: 3.424075968636171\n",
      "---------------------\n",
      "Iteration Number: 9281\n",
      "Loss: 11.197515658809747\n",
      "l2 norm of gradients: 0.046191884328482896\n",
      "l2 norm of weights: 3.424138682608579\n",
      "---------------------\n",
      "Iteration Number: 9282\n",
      "Loss: 11.197319145333028\n",
      "l2 norm of gradients: 0.046186662611053746\n",
      "l2 norm of weights: 3.4242013919614123\n",
      "---------------------\n",
      "Iteration Number: 9283\n",
      "Loss: 11.19712270077448\n",
      "l2 norm of gradients: 0.046181442165714896\n",
      "l2 norm of weights: 3.424264096692995\n",
      "---------------------\n",
      "Iteration Number: 9284\n",
      "Loss: 11.196926325107356\n",
      "l2 norm of gradients: 0.04617622299184029\n",
      "l2 norm of weights: 3.4243267968016533\n",
      "---------------------\n",
      "Iteration Number: 9285\n",
      "Loss: 11.196730018304905\n",
      "l2 norm of gradients: 0.046171005088804534\n",
      "l2 norm of weights: 3.424389492285714\n",
      "---------------------\n",
      "Iteration Number: 9286\n",
      "Loss: 11.196533780340417\n",
      "l2 norm of gradients: 0.04616578845598263\n",
      "l2 norm of weights: 3.4244521831435057\n",
      "---------------------\n",
      "Iteration Number: 9287\n",
      "Loss: 11.196337611187175\n",
      "l2 norm of gradients: 0.046160573092750234\n",
      "l2 norm of weights: 3.424514869373358\n",
      "---------------------\n",
      "Iteration Number: 9288\n",
      "Loss: 11.196141510818482\n",
      "l2 norm of gradients: 0.04615535899848341\n",
      "l2 norm of weights: 3.424577550973601\n",
      "---------------------\n",
      "Iteration Number: 9289\n",
      "Loss: 11.195945479207655\n",
      "l2 norm of gradients: 0.0461501461725588\n",
      "l2 norm of weights: 3.4246402279425663\n",
      "---------------------\n",
      "Iteration Number: 9290\n",
      "Loss: 11.195749516328036\n",
      "l2 norm of gradients: 0.04614493461435355\n",
      "l2 norm of weights: 3.4247029002785876\n",
      "---------------------\n",
      "Iteration Number: 9291\n",
      "Loss: 11.19555362215296\n",
      "l2 norm of gradients: 0.04613972432324534\n",
      "l2 norm of weights: 3.424765567979998\n",
      "---------------------\n",
      "Iteration Number: 9292\n",
      "Loss: 11.195357796655813\n",
      "l2 norm of gradients: 0.04613451529861235\n",
      "l2 norm of weights: 3.4248282310451335\n",
      "---------------------\n",
      "Iteration Number: 9293\n",
      "Loss: 11.195162039809958\n",
      "l2 norm of gradients: 0.046129307539833346\n",
      "l2 norm of weights: 3.4248908894723296\n",
      "---------------------\n",
      "Iteration Number: 9294\n",
      "Loss: 11.194966351588803\n",
      "l2 norm of gradients: 0.04612410104628749\n",
      "l2 norm of weights: 3.4249535432599254\n",
      "---------------------\n",
      "Iteration Number: 9295\n",
      "Loss: 11.19477073196575\n",
      "l2 norm of gradients: 0.04611889581735455\n",
      "l2 norm of weights: 3.4250161924062588\n",
      "---------------------\n",
      "Iteration Number: 9296\n",
      "Loss: 11.194575180914224\n",
      "l2 norm of gradients: 0.046113691852414825\n",
      "l2 norm of weights: 3.425078836909669\n",
      "---------------------\n",
      "Iteration Number: 9297\n",
      "Loss: 11.194379698407669\n",
      "l2 norm of gradients: 0.04610848915084909\n",
      "l2 norm of weights: 3.4251414767684984\n",
      "---------------------\n",
      "Iteration Number: 9298\n",
      "Loss: 11.194184284419535\n",
      "l2 norm of gradients: 0.04610328771203866\n",
      "l2 norm of weights: 3.4252041119810888\n",
      "---------------------\n",
      "Iteration Number: 9299\n",
      "Loss: 11.193988938923297\n",
      "l2 norm of gradients: 0.046098087535365324\n",
      "l2 norm of weights: 3.4252667425457832\n",
      "---------------------\n",
      "Iteration Number: 9300\n",
      "Loss: 11.193793661892434\n",
      "l2 norm of gradients: 0.046092888620211485\n",
      "l2 norm of weights: 3.4253293684609263\n",
      "---------------------\n",
      "Iteration Number: 9301\n",
      "Loss: 11.193598453300448\n",
      "l2 norm of gradients: 0.046087690965959924\n",
      "l2 norm of weights: 3.4253919897248646\n",
      "---------------------\n",
      "Iteration Number: 9302\n",
      "Loss: 11.19340331312086\n",
      "l2 norm of gradients: 0.04608249457199409\n",
      "l2 norm of weights: 3.4254546063359435\n",
      "---------------------\n",
      "Iteration Number: 9303\n",
      "Loss: 11.193208241327191\n",
      "l2 norm of gradients: 0.04607729943769785\n",
      "l2 norm of weights: 3.4255172182925118\n",
      "---------------------\n",
      "Iteration Number: 9304\n",
      "Loss: 11.19301323789299\n",
      "l2 norm of gradients: 0.0460721055624556\n",
      "l2 norm of weights: 3.4255798255929184\n",
      "---------------------\n",
      "Iteration Number: 9305\n",
      "Loss: 11.192818302791817\n",
      "l2 norm of gradients: 0.04606691294565228\n",
      "l2 norm of weights: 3.425642428235514\n",
      "---------------------\n",
      "Iteration Number: 9306\n",
      "Loss: 11.19262343599724\n",
      "l2 norm of gradients: 0.04606172158667332\n",
      "l2 norm of weights: 3.4257050262186497\n",
      "---------------------\n",
      "Iteration Number: 9307\n",
      "Loss: 11.19242863748286\n",
      "l2 norm of gradients: 0.04605653148490467\n",
      "l2 norm of weights: 3.4257676195406788\n",
      "---------------------\n",
      "Iteration Number: 9308\n",
      "Loss: 11.19223390722227\n",
      "l2 norm of gradients: 0.04605134263973279\n",
      "l2 norm of weights: 3.425830208199954\n",
      "---------------------\n",
      "Iteration Number: 9309\n",
      "Loss: 11.192039245189086\n",
      "l2 norm of gradients: 0.04604615505054468\n",
      "l2 norm of weights: 3.425892792194831\n",
      "---------------------\n",
      "Iteration Number: 9310\n",
      "Loss: 11.191844651356957\n",
      "l2 norm of gradients: 0.046040968716727824\n",
      "l2 norm of weights: 3.425955371523665\n",
      "---------------------\n",
      "Iteration Number: 9311\n",
      "Loss: 11.191650125699516\n",
      "l2 norm of gradients: 0.04603578363767025\n",
      "l2 norm of weights: 3.426017946184813\n",
      "---------------------\n",
      "Iteration Number: 9312\n",
      "Loss: 11.191455668190432\n",
      "l2 norm of gradients: 0.046030599812760406\n",
      "l2 norm of weights: 3.4260805161766346\n",
      "---------------------\n",
      "Iteration Number: 9313\n",
      "Loss: 11.191261278803387\n",
      "l2 norm of gradients: 0.046025417241387416\n",
      "l2 norm of weights: 3.426143081497488\n",
      "---------------------\n",
      "Iteration Number: 9314\n",
      "Loss: 11.191066957512072\n",
      "l2 norm of gradients: 0.046020235922940765\n",
      "l2 norm of weights: 3.4262056421457343\n",
      "---------------------\n",
      "Iteration Number: 9315\n",
      "Loss: 11.190872704290182\n",
      "l2 norm of gradients: 0.04601505585681052\n",
      "l2 norm of weights: 3.4262681981197356\n",
      "---------------------\n",
      "Iteration Number: 9316\n",
      "Loss: 11.190678519111453\n",
      "l2 norm of gradients: 0.04600987704238726\n",
      "l2 norm of weights: 3.4263307494178536\n",
      "---------------------\n",
      "Iteration Number: 9317\n",
      "Loss: 11.190484401949618\n",
      "l2 norm of gradients: 0.046004699479062064\n",
      "l2 norm of weights: 3.426393296038453\n",
      "---------------------\n",
      "Iteration Number: 9318\n",
      "Loss: 11.190290352778437\n",
      "l2 norm of gradients: 0.045999523166226505\n",
      "l2 norm of weights: 3.426455837979899\n",
      "---------------------\n",
      "Iteration Number: 9319\n",
      "Loss: 11.190096371571652\n",
      "l2 norm of gradients: 0.04599434810327269\n",
      "l2 norm of weights: 3.4265183752405566\n",
      "---------------------\n",
      "Iteration Number: 9320\n",
      "Loss: 11.18990245830307\n",
      "l2 norm of gradients: 0.045989174289593225\n",
      "l2 norm of weights: 3.4265809078187948\n",
      "---------------------\n",
      "Iteration Number: 9321\n",
      "Loss: 11.189708612946468\n",
      "l2 norm of gradients: 0.04598400172458122\n",
      "l2 norm of weights: 3.426643435712981\n",
      "---------------------\n",
      "Iteration Number: 9322\n",
      "Loss: 11.189514835475668\n",
      "l2 norm of gradients: 0.04597883040763029\n",
      "l2 norm of weights: 3.4267059589214846\n",
      "---------------------\n",
      "Iteration Number: 9323\n",
      "Loss: 11.189321125864483\n",
      "l2 norm of gradients: 0.0459736603381346\n",
      "l2 norm of weights: 3.426768477442677\n",
      "---------------------\n",
      "Iteration Number: 9324\n",
      "Loss: 11.189127484086763\n",
      "l2 norm of gradients: 0.04596849151548877\n",
      "l2 norm of weights: 3.4268309912749295\n",
      "---------------------\n",
      "Iteration Number: 9325\n",
      "Loss: 11.188933910116361\n",
      "l2 norm of gradients: 0.045963323939087954\n",
      "l2 norm of weights: 3.4268935004166154\n",
      "---------------------\n",
      "Iteration Number: 9326\n",
      "Loss: 11.18874040392714\n",
      "l2 norm of gradients: 0.04595815760832784\n",
      "l2 norm of weights: 3.4269560048661076\n",
      "---------------------\n",
      "Iteration Number: 9327\n",
      "Loss: 11.188546965492987\n",
      "l2 norm of gradients: 0.04595299252260453\n",
      "l2 norm of weights: 3.4270185046217825\n",
      "---------------------\n",
      "Iteration Number: 9328\n",
      "Loss: 11.188353594787799\n",
      "l2 norm of gradients: 0.045947828681314744\n",
      "l2 norm of weights: 3.427080999682016\n",
      "---------------------\n",
      "Iteration Number: 9329\n",
      "Loss: 11.188160291785483\n",
      "l2 norm of gradients: 0.045942666083855624\n",
      "l2 norm of weights: 3.427143490045185\n",
      "---------------------\n",
      "Iteration Number: 9330\n",
      "Loss: 11.187967056459973\n",
      "l2 norm of gradients: 0.0459375047296249\n",
      "l2 norm of weights: 3.427205975709669\n",
      "---------------------\n",
      "Iteration Number: 9331\n",
      "Loss: 11.187773888785204\n",
      "l2 norm of gradients: 0.04593234461802071\n",
      "l2 norm of weights: 3.4272684566738456\n",
      "---------------------\n",
      "Iteration Number: 9332\n",
      "Loss: 11.187580788735138\n",
      "l2 norm of gradients: 0.045927185748441776\n",
      "l2 norm of weights: 3.4273309329360977\n",
      "---------------------\n",
      "Iteration Number: 9333\n",
      "Loss: 11.187387756283737\n",
      "l2 norm of gradients: 0.045922028120287284\n",
      "l2 norm of weights: 3.4273934044948056\n",
      "---------------------\n",
      "Iteration Number: 9334\n",
      "Loss: 11.18719479140499\n",
      "l2 norm of gradients: 0.04591687173295694\n",
      "l2 norm of weights: 3.4274558713483527\n",
      "---------------------\n",
      "Iteration Number: 9335\n",
      "Loss: 11.187001894072901\n",
      "l2 norm of gradients: 0.04591171658585097\n",
      "l2 norm of weights: 3.4275183334951222\n",
      "---------------------\n",
      "Iteration Number: 9336\n",
      "Loss: 11.186809064261476\n",
      "l2 norm of gradients: 0.04590656267837006\n",
      "l2 norm of weights: 3.427580790933501\n",
      "---------------------\n",
      "Iteration Number: 9337\n",
      "Loss: 11.186616301944744\n",
      "l2 norm of gradients: 0.045901410009915426\n",
      "l2 norm of weights: 3.427643243661874\n",
      "---------------------\n",
      "Iteration Number: 9338\n",
      "Loss: 11.186423607096751\n",
      "l2 norm of gradients: 0.045896258579888806\n",
      "l2 norm of weights: 3.427705691678628\n",
      "---------------------\n",
      "Iteration Number: 9339\n",
      "Loss: 11.186230979691548\n",
      "l2 norm of gradients: 0.04589110838769238\n",
      "l2 norm of weights: 3.427768134982152\n",
      "---------------------\n",
      "Iteration Number: 9340\n",
      "Loss: 11.186038419703214\n",
      "l2 norm of gradients: 0.04588595943272888\n",
      "l2 norm of weights: 3.427830573570836\n",
      "---------------------\n",
      "Iteration Number: 9341\n",
      "Loss: 11.185845927105827\n",
      "l2 norm of gradients: 0.04588081171440159\n",
      "l2 norm of weights: 3.4278930074430694\n",
      "---------------------\n",
      "Iteration Number: 9342\n",
      "Loss: 11.185653501873496\n",
      "l2 norm of gradients: 0.04587566523211413\n",
      "l2 norm of weights: 3.4279554365972444\n",
      "---------------------\n",
      "Iteration Number: 9343\n",
      "Loss: 11.185461143980325\n",
      "l2 norm of gradients: 0.045870519985270815\n",
      "l2 norm of weights: 3.4280178610317535\n",
      "---------------------\n",
      "Iteration Number: 9344\n",
      "Loss: 11.18526885340044\n",
      "l2 norm of gradients: 0.045865375973276334\n",
      "l2 norm of weights: 3.428080280744991\n",
      "---------------------\n",
      "Iteration Number: 9345\n",
      "Loss: 11.185076630108\n",
      "l2 norm of gradients: 0.04586023319553588\n",
      "l2 norm of weights: 3.428142695735351\n",
      "---------------------\n",
      "Iteration Number: 9346\n",
      "Loss: 11.184884474077146\n",
      "l2 norm of gradients: 0.045855091651455265\n",
      "l2 norm of weights: 3.4282051060012297\n",
      "---------------------\n",
      "Iteration Number: 9347\n",
      "Loss: 11.184692385282059\n",
      "l2 norm of gradients: 0.045849951340440634\n",
      "l2 norm of weights: 3.428267511541025\n",
      "---------------------\n",
      "Iteration Number: 9348\n",
      "Loss: 11.18450036369692\n",
      "l2 norm of gradients: 0.04584481226189876\n",
      "l2 norm of weights: 3.428329912353134\n",
      "---------------------\n",
      "Iteration Number: 9349\n",
      "Loss: 11.184308409295932\n",
      "l2 norm of gradients: 0.04583967441523684\n",
      "l2 norm of weights: 3.4283923084359564\n",
      "---------------------\n",
      "Iteration Number: 9350\n",
      "Loss: 11.184116522053305\n",
      "l2 norm of gradients: 0.04583453779986264\n",
      "l2 norm of weights: 3.428454699787893\n",
      "---------------------\n",
      "Iteration Number: 9351\n",
      "Loss: 11.183924701943274\n",
      "l2 norm of gradients: 0.04582940241518431\n",
      "l2 norm of weights: 3.4285170864073433\n",
      "---------------------\n",
      "Iteration Number: 9352\n",
      "Loss: 11.183732948940076\n",
      "l2 norm of gradients: 0.045824268260610646\n",
      "l2 norm of weights: 3.4285794682927118\n",
      "---------------------\n",
      "Iteration Number: 9353\n",
      "Loss: 11.183541263017972\n",
      "l2 norm of gradients: 0.045819135335550806\n",
      "l2 norm of weights: 3.4286418454424012\n",
      "---------------------\n",
      "Iteration Number: 9354\n",
      "Loss: 11.183349644151226\n",
      "l2 norm of gradients: 0.04581400363941453\n",
      "l2 norm of weights: 3.428704217854816\n",
      "---------------------\n",
      "Iteration Number: 9355\n",
      "Loss: 11.183158092314127\n",
      "l2 norm of gradients: 0.045808873171612016\n",
      "l2 norm of weights: 3.4287665855283618\n",
      "---------------------\n",
      "Iteration Number: 9356\n",
      "Loss: 11.182966607480985\n",
      "l2 norm of gradients: 0.04580374393155401\n",
      "l2 norm of weights: 3.4288289484614456\n",
      "---------------------\n",
      "Iteration Number: 9357\n",
      "Loss: 11.182775189626097\n",
      "l2 norm of gradients: 0.045798615918651656\n",
      "l2 norm of weights: 3.428891306652475\n",
      "---------------------\n",
      "Iteration Number: 9358\n",
      "Loss: 11.182583838723794\n",
      "l2 norm of gradients: 0.04579348913231668\n",
      "l2 norm of weights: 3.428953660099859\n",
      "---------------------\n",
      "Iteration Number: 9359\n",
      "Loss: 11.182392554748429\n",
      "l2 norm of gradients: 0.045788363571961296\n",
      "l2 norm of weights: 3.4290160088020074\n",
      "---------------------\n",
      "Iteration Number: 9360\n",
      "Loss: 11.18220133767435\n",
      "l2 norm of gradients: 0.04578323923699816\n",
      "l2 norm of weights: 3.4290783527573314\n",
      "---------------------\n",
      "Iteration Number: 9361\n",
      "Loss: 11.182010187475925\n",
      "l2 norm of gradients: 0.045778116126840476\n",
      "l2 norm of weights: 3.429140691964242\n",
      "---------------------\n",
      "Iteration Number: 9362\n",
      "Loss: 11.181819104127541\n",
      "l2 norm of gradients: 0.045772994240901914\n",
      "l2 norm of weights: 3.4292030264211544\n",
      "---------------------\n",
      "Iteration Number: 9363\n",
      "Loss: 11.181628087603597\n",
      "l2 norm of gradients: 0.04576787357859665\n",
      "l2 norm of weights: 3.429265356126481\n",
      "---------------------\n",
      "Iteration Number: 9364\n",
      "Loss: 11.181437137878497\n",
      "l2 norm of gradients: 0.04576275413933933\n",
      "l2 norm of weights: 3.4293276810786377\n",
      "---------------------\n",
      "Iteration Number: 9365\n",
      "Loss: 11.181246254926679\n",
      "l2 norm of gradients: 0.04575763592254514\n",
      "l2 norm of weights: 3.4293900012760403\n",
      "---------------------\n",
      "Iteration Number: 9366\n",
      "Loss: 11.181055438722582\n",
      "l2 norm of gradients: 0.04575251892762974\n",
      "l2 norm of weights: 3.429452316717107\n",
      "---------------------\n",
      "Iteration Number: 9367\n",
      "Loss: 11.18086468924066\n",
      "l2 norm of gradients: 0.04574740315400923\n",
      "l2 norm of weights: 3.4295146274002555\n",
      "---------------------\n",
      "Iteration Number: 9368\n",
      "Loss: 11.180674006455373\n",
      "l2 norm of gradients: 0.04574228860110028\n",
      "l2 norm of weights: 3.4295769333239057\n",
      "---------------------\n",
      "Iteration Number: 9369\n",
      "Loss: 11.180483390341205\n",
      "l2 norm of gradients: 0.045737175268319996\n",
      "l2 norm of weights: 3.4296392344864772\n",
      "---------------------\n",
      "Iteration Number: 9370\n",
      "Loss: 11.180292840872669\n",
      "l2 norm of gradients: 0.045732063155086024\n",
      "l2 norm of weights: 3.4297015308863927\n",
      "---------------------\n",
      "Iteration Number: 9371\n",
      "Loss: 11.180102358024246\n",
      "l2 norm of gradients: 0.04572695226081647\n",
      "l2 norm of weights: 3.4297638225220743\n",
      "---------------------\n",
      "Iteration Number: 9372\n",
      "Loss: 11.179911941770492\n",
      "l2 norm of gradients: 0.04572184258492991\n",
      "l2 norm of weights: 3.4298261093919455\n",
      "---------------------\n",
      "Iteration Number: 9373\n",
      "Loss: 11.179721592085917\n",
      "l2 norm of gradients: 0.04571673412684547\n",
      "l2 norm of weights: 3.429888391494431\n",
      "---------------------\n",
      "Iteration Number: 9374\n",
      "Loss: 11.179531308945094\n",
      "l2 norm of gradients: 0.04571162688598268\n",
      "l2 norm of weights: 3.429950668827957\n",
      "---------------------\n",
      "Iteration Number: 9375\n",
      "Loss: 11.179341092322582\n",
      "l2 norm of gradients: 0.04570652086176166\n",
      "l2 norm of weights: 3.4300129413909493\n",
      "---------------------\n",
      "Iteration Number: 9376\n",
      "Loss: 11.179150942192955\n",
      "l2 norm of gradients: 0.04570141605360299\n",
      "l2 norm of weights: 3.4300752091818363\n",
      "---------------------\n",
      "Iteration Number: 9377\n",
      "Loss: 11.178960858530814\n",
      "l2 norm of gradients: 0.04569631246092765\n",
      "l2 norm of weights: 3.430137472199047\n",
      "---------------------\n",
      "Iteration Number: 9378\n",
      "Loss: 11.178770841310767\n",
      "l2 norm of gradients: 0.04569121008315722\n",
      "l2 norm of weights: 3.4301997304410117\n",
      "---------------------\n",
      "Iteration Number: 9379\n",
      "Loss: 11.178580890507435\n",
      "l2 norm of gradients: 0.04568610891971375\n",
      "l2 norm of weights: 3.43026198390616\n",
      "---------------------\n",
      "Iteration Number: 9380\n",
      "Loss: 11.178391006095444\n",
      "l2 norm of gradients: 0.0456810089700197\n",
      "l2 norm of weights: 3.4303242325929246\n",
      "---------------------\n",
      "Iteration Number: 9381\n",
      "Loss: 11.178201188049458\n",
      "l2 norm of gradients: 0.04567591023349811\n",
      "l2 norm of weights: 3.430386476499739\n",
      "---------------------\n",
      "Iteration Number: 9382\n",
      "Loss: 11.178011436344132\n",
      "l2 norm of gradients: 0.045670812709572475\n",
      "l2 norm of weights: 3.4304487156250367\n",
      "---------------------\n",
      "Iteration Number: 9383\n",
      "Loss: 11.17782175095414\n",
      "l2 norm of gradients: 0.04566571639766672\n",
      "l2 norm of weights: 3.430510949967253\n",
      "---------------------\n",
      "Iteration Number: 9384\n",
      "Loss: 11.177632131854178\n",
      "l2 norm of gradients: 0.045660621297205384\n",
      "l2 norm of weights: 3.430573179524823\n",
      "---------------------\n",
      "Iteration Number: 9385\n",
      "Loss: 11.17744257901895\n",
      "l2 norm of gradients: 0.04565552740761336\n",
      "l2 norm of weights: 3.430635404296185\n",
      "---------------------\n",
      "Iteration Number: 9386\n",
      "Loss: 11.177253092423165\n",
      "l2 norm of gradients: 0.045650434728316105\n",
      "l2 norm of weights: 3.4306976242797766\n",
      "---------------------\n",
      "Iteration Number: 9387\n",
      "Loss: 11.17706367204157\n",
      "l2 norm of gradients: 0.045645343258739535\n",
      "l2 norm of weights: 3.430759839474037\n",
      "---------------------\n",
      "Iteration Number: 9388\n",
      "Loss: 11.176874317848904\n",
      "l2 norm of gradients: 0.045640252998310066\n",
      "l2 norm of weights: 3.430822049877407\n",
      "---------------------\n",
      "Iteration Number: 9389\n",
      "Loss: 11.176685029819927\n",
      "l2 norm of gradients: 0.04563516394645459\n",
      "l2 norm of weights: 3.4308842554883276\n",
      "---------------------\n",
      "Iteration Number: 9390\n",
      "Loss: 11.176495807929408\n",
      "l2 norm of gradients: 0.04563007610260047\n",
      "l2 norm of weights: 3.4309464563052403\n",
      "---------------------\n",
      "Iteration Number: 9391\n",
      "Loss: 11.176306652152144\n",
      "l2 norm of gradients: 0.04562498946617555\n",
      "l2 norm of weights: 3.431008652326589\n",
      "---------------------\n",
      "Iteration Number: 9392\n",
      "Loss: 11.176117562462917\n",
      "l2 norm of gradients: 0.045619904036608205\n",
      "l2 norm of weights: 3.431070843550818\n",
      "---------------------\n",
      "Iteration Number: 9393\n",
      "Loss: 11.17592853883656\n",
      "l2 norm of gradients: 0.04561481981332726\n",
      "l2 norm of weights: 3.431133029976372\n",
      "---------------------\n",
      "Iteration Number: 9394\n",
      "Loss: 11.175739581247893\n",
      "l2 norm of gradients: 0.04560973679576204\n",
      "l2 norm of weights: 3.4311952116016986\n",
      "---------------------\n",
      "Iteration Number: 9395\n",
      "Loss: 11.175550689671756\n",
      "l2 norm of gradients: 0.045604654983342305\n",
      "l2 norm of weights: 3.4312573884252435\n",
      "---------------------\n",
      "Iteration Number: 9396\n",
      "Loss: 11.175361864083008\n",
      "l2 norm of gradients: 0.045599574375498365\n",
      "l2 norm of weights: 3.431319560445456\n",
      "---------------------\n",
      "Iteration Number: 9397\n",
      "Loss: 11.175173104456512\n",
      "l2 norm of gradients: 0.045594494971660925\n",
      "l2 norm of weights: 3.4313817276607854\n",
      "---------------------\n",
      "Iteration Number: 9398\n",
      "Loss: 11.174984410767157\n",
      "l2 norm of gradients: 0.045589416771261305\n",
      "l2 norm of weights: 3.431443890069682\n",
      "---------------------\n",
      "Iteration Number: 9399\n",
      "Loss: 11.174795782989834\n",
      "l2 norm of gradients: 0.04558433977373117\n",
      "l2 norm of weights: 3.4315060476705974\n",
      "---------------------\n",
      "Iteration Number: 9400\n",
      "Loss: 11.174607221099457\n",
      "l2 norm of gradients: 0.045579263978502725\n",
      "l2 norm of weights: 3.4315682004619834\n",
      "---------------------\n",
      "Iteration Number: 9401\n",
      "Loss: 11.174418725070943\n",
      "l2 norm of gradients: 0.04557418938500867\n",
      "l2 norm of weights: 3.431630348442294\n",
      "---------------------\n",
      "Iteration Number: 9402\n",
      "Loss: 11.17423029487923\n",
      "l2 norm of gradients: 0.04556911599268218\n",
      "l2 norm of weights: 3.4316924916099834\n",
      "---------------------\n",
      "Iteration Number: 9403\n",
      "Loss: 11.174041930499282\n",
      "l2 norm of gradients: 0.045564043800956876\n",
      "l2 norm of weights: 3.431754629963507\n",
      "---------------------\n",
      "Iteration Number: 9404\n",
      "Loss: 11.17385363190604\n",
      "l2 norm of gradients: 0.04555897280926691\n",
      "l2 norm of weights: 3.4318167635013213\n",
      "---------------------\n",
      "Iteration Number: 9405\n",
      "Loss: 11.173665399074498\n",
      "l2 norm of gradients: 0.04555390301704684\n",
      "l2 norm of weights: 3.4318788922218837\n",
      "---------------------\n",
      "Iteration Number: 9406\n",
      "Loss: 11.173477231979643\n",
      "l2 norm of gradients: 0.04554883442373182\n",
      "l2 norm of weights: 3.4319410161236523\n",
      "---------------------\n",
      "Iteration Number: 9407\n",
      "Loss: 11.173289130596471\n",
      "l2 norm of gradients: 0.04554376702875736\n",
      "l2 norm of weights: 3.432003135205087\n",
      "---------------------\n",
      "Iteration Number: 9408\n",
      "Loss: 11.17310109490001\n",
      "l2 norm of gradients: 0.04553870083155951\n",
      "l2 norm of weights: 3.432065249464648\n",
      "---------------------\n",
      "Iteration Number: 9409\n",
      "Loss: 11.172913124865286\n",
      "l2 norm of gradients: 0.045533635831574805\n",
      "l2 norm of weights: 3.432127358900797\n",
      "---------------------\n",
      "Iteration Number: 9410\n",
      "Loss: 11.172725220467344\n",
      "l2 norm of gradients: 0.04552857202824021\n",
      "l2 norm of weights: 3.4321894635119956\n",
      "---------------------\n",
      "Iteration Number: 9411\n",
      "Loss: 11.172537381681241\n",
      "l2 norm of gradients: 0.04552350942099324\n",
      "l2 norm of weights: 3.4322515632967083\n",
      "---------------------\n",
      "Iteration Number: 9412\n",
      "Loss: 11.17234960848205\n",
      "l2 norm of gradients: 0.0455184480092718\n",
      "l2 norm of weights: 3.4323136582533986\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 9413\n",
      "Loss: 11.172161900844861\n",
      "l2 norm of gradients: 0.0455133877925144\n",
      "l2 norm of weights: 3.4323757483805326\n",
      "---------------------\n",
      "Iteration Number: 9414\n",
      "Loss: 11.171974258744758\n",
      "l2 norm of gradients: 0.045508328770159846\n",
      "l2 norm of weights: 3.4324378336765764\n",
      "---------------------\n",
      "Iteration Number: 9415\n",
      "Loss: 11.17178668215687\n",
      "l2 norm of gradients: 0.045503270941647606\n",
      "l2 norm of weights: 3.4324999141399974\n",
      "---------------------\n",
      "Iteration Number: 9416\n",
      "Loss: 11.171599171056307\n",
      "l2 norm of gradients: 0.045498214306417484\n",
      "l2 norm of weights: 3.4325619897692636\n",
      "---------------------\n",
      "Iteration Number: 9417\n",
      "Loss: 11.171411725418219\n",
      "l2 norm of gradients: 0.04549315886390982\n",
      "l2 norm of weights: 3.4326240605628455\n",
      "---------------------\n",
      "Iteration Number: 9418\n",
      "Loss: 11.171224345217754\n",
      "l2 norm of gradients: 0.045488104613565436\n",
      "l2 norm of weights: 3.432686126519212\n",
      "---------------------\n",
      "Iteration Number: 9419\n",
      "Loss: 11.171037030430076\n",
      "l2 norm of gradients: 0.04548305155482563\n",
      "l2 norm of weights: 3.432748187636835\n",
      "---------------------\n",
      "Iteration Number: 9420\n",
      "Loss: 11.170849781030356\n",
      "l2 norm of gradients: 0.04547799968713213\n",
      "l2 norm of weights: 3.4328102439141874\n",
      "---------------------\n",
      "Iteration Number: 9421\n",
      "Loss: 11.170662596993806\n",
      "l2 norm of gradients: 0.04547294900992717\n",
      "l2 norm of weights: 3.4328722953497417\n",
      "---------------------\n",
      "Iteration Number: 9422\n",
      "Loss: 11.170475478295604\n",
      "l2 norm of gradients: 0.04546789952265349\n",
      "l2 norm of weights: 3.432934341941973\n",
      "---------------------\n",
      "Iteration Number: 9423\n",
      "Loss: 11.17028842491099\n",
      "l2 norm of gradients: 0.04546285122475423\n",
      "l2 norm of weights: 3.432996383689355\n",
      "---------------------\n",
      "Iteration Number: 9424\n",
      "Loss: 11.170101436815184\n",
      "l2 norm of gradients: 0.04545780411567306\n",
      "l2 norm of weights: 3.433058420590366\n",
      "---------------------\n",
      "Iteration Number: 9425\n",
      "Loss: 11.16991451398344\n",
      "l2 norm of gradients: 0.0454527581948541\n",
      "l2 norm of weights: 3.4331204526434824\n",
      "---------------------\n",
      "Iteration Number: 9426\n",
      "Loss: 11.169727656391002\n",
      "l2 norm of gradients: 0.04544771346174196\n",
      "l2 norm of weights: 3.433182479847182\n",
      "---------------------\n",
      "Iteration Number: 9427\n",
      "Loss: 11.169540864013154\n",
      "l2 norm of gradients: 0.045442669915781726\n",
      "l2 norm of weights: 3.4332445021999436\n",
      "---------------------\n",
      "Iteration Number: 9428\n",
      "Loss: 11.169354136825175\n",
      "l2 norm of gradients: 0.04543762755641892\n",
      "l2 norm of weights: 3.4333065197002486\n",
      "---------------------\n",
      "Iteration Number: 9429\n",
      "Loss: 11.169167474802366\n",
      "l2 norm of gradients: 0.04543258638309954\n",
      "l2 norm of weights: 3.4333685323465772\n",
      "---------------------\n",
      "Iteration Number: 9430\n",
      "Loss: 11.168980877920035\n",
      "l2 norm of gradients: 0.04542754639527012\n",
      "l2 norm of weights: 3.4334305401374112\n",
      "---------------------\n",
      "Iteration Number: 9431\n",
      "Loss: 11.1687943461535\n",
      "l2 norm of gradients: 0.04542250759237758\n",
      "l2 norm of weights: 3.433492543071235\n",
      "---------------------\n",
      "Iteration Number: 9432\n",
      "Loss: 11.168607879478111\n",
      "l2 norm of gradients: 0.045417469973869355\n",
      "l2 norm of weights: 3.433554541146532\n",
      "---------------------\n",
      "Iteration Number: 9433\n",
      "Loss: 11.168421477869208\n",
      "l2 norm of gradients: 0.04541243353919334\n",
      "l2 norm of weights: 3.433616534361787\n",
      "---------------------\n",
      "Iteration Number: 9434\n",
      "Loss: 11.16823514130216\n",
      "l2 norm of gradients: 0.04540739828779792\n",
      "l2 norm of weights: 3.433678522715486\n",
      "---------------------\n",
      "Iteration Number: 9435\n",
      "Loss: 11.16804886975234\n",
      "l2 norm of gradients: 0.045402364219131915\n",
      "l2 norm of weights: 3.4337405062061164\n",
      "---------------------\n",
      "Iteration Number: 9436\n",
      "Loss: 11.167862663195145\n",
      "l2 norm of gradients: 0.04539733133264467\n",
      "l2 norm of weights: 3.433802484832165\n",
      "---------------------\n",
      "Iteration Number: 9437\n",
      "Loss: 11.167676521605966\n",
      "l2 norm of gradients: 0.04539229962778591\n",
      "l2 norm of weights: 3.433864458592122\n",
      "---------------------\n",
      "Iteration Number: 9438\n",
      "Loss: 11.167490444960228\n",
      "l2 norm of gradients: 0.04538726910400594\n",
      "l2 norm of weights: 3.4339264274844763\n",
      "---------------------\n",
      "Iteration Number: 9439\n",
      "Loss: 11.167304433233353\n",
      "l2 norm of gradients: 0.04538223976075541\n",
      "l2 norm of weights: 3.433988391507719\n",
      "---------------------\n",
      "Iteration Number: 9440\n",
      "Loss: 11.16711848640079\n",
      "l2 norm of gradients: 0.045377211597485534\n",
      "l2 norm of weights: 3.434050350660342\n",
      "---------------------\n",
      "Iteration Number: 9441\n",
      "Loss: 11.166932604437998\n",
      "l2 norm of gradients: 0.04537218461364798\n",
      "l2 norm of weights: 3.434112304940838\n",
      "---------------------\n",
      "Iteration Number: 9442\n",
      "Loss: 11.166746787320427\n",
      "l2 norm of gradients: 0.04536715880869484\n",
      "l2 norm of weights: 3.4341742543477007\n",
      "---------------------\n",
      "Iteration Number: 9443\n",
      "Loss: 11.166561035023578\n",
      "l2 norm of gradients: 0.0453621341820787\n",
      "l2 norm of weights: 3.4342361988794248\n",
      "---------------------\n",
      "Iteration Number: 9444\n",
      "Loss: 11.166375347522928\n",
      "l2 norm of gradients: 0.045357110733252604\n",
      "l2 norm of weights: 3.4342981385345053\n",
      "---------------------\n",
      "Iteration Number: 9445\n",
      "Loss: 11.166189724793998\n",
      "l2 norm of gradients: 0.045352088461670104\n",
      "l2 norm of weights: 3.4343600733114394\n",
      "---------------------\n",
      "Iteration Number: 9446\n",
      "Loss: 11.166004166812302\n",
      "l2 norm of gradients: 0.04534706736678517\n",
      "l2 norm of weights: 3.434422003208724\n",
      "---------------------\n",
      "Iteration Number: 9447\n",
      "Loss: 11.165818673553371\n",
      "l2 norm of gradients: 0.04534204744805226\n",
      "l2 norm of weights: 3.4344839282248585\n",
      "---------------------\n",
      "Iteration Number: 9448\n",
      "Loss: 11.16563324499276\n",
      "l2 norm of gradients: 0.04533702870492628\n",
      "l2 norm of weights: 3.4345458483583413\n",
      "---------------------\n",
      "Iteration Number: 9449\n",
      "Loss: 11.165447881106013\n",
      "l2 norm of gradients: 0.045332011136862596\n",
      "l2 norm of weights: 3.4346077636076733\n",
      "---------------------\n",
      "Iteration Number: 9450\n",
      "Loss: 11.16526258186872\n",
      "l2 norm of gradients: 0.04532699474331709\n",
      "l2 norm of weights: 3.4346696739713556\n",
      "---------------------\n",
      "Iteration Number: 9451\n",
      "Loss: 11.165077347256448\n",
      "l2 norm of gradients: 0.045321979523746045\n",
      "l2 norm of weights: 3.4347315794478903\n",
      "---------------------\n",
      "Iteration Number: 9452\n",
      "Loss: 11.164892177244804\n",
      "l2 norm of gradients: 0.045316965477606254\n",
      "l2 norm of weights: 3.4347934800357813\n",
      "---------------------\n",
      "Iteration Number: 9453\n",
      "Loss: 11.1647070718094\n",
      "l2 norm of gradients: 0.045311952604354944\n",
      "l2 norm of weights: 3.434855375733532\n",
      "---------------------\n",
      "Iteration Number: 9454\n",
      "Loss: 11.164522030925857\n",
      "l2 norm of gradients: 0.04530694090344984\n",
      "l2 norm of weights: 3.434917266539648\n",
      "---------------------\n",
      "Iteration Number: 9455\n",
      "Loss: 11.164337054569808\n",
      "l2 norm of gradients: 0.04530193037434908\n",
      "l2 norm of weights: 3.4349791524526347\n",
      "---------------------\n",
      "Iteration Number: 9456\n",
      "Loss: 11.16415214271691\n",
      "l2 norm of gradients: 0.045296921016511324\n",
      "l2 norm of weights: 3.4350410334709998\n",
      "---------------------\n",
      "Iteration Number: 9457\n",
      "Loss: 11.16396729534282\n",
      "l2 norm of gradients: 0.04529191282939563\n",
      "l2 norm of weights: 3.4351029095932506\n",
      "---------------------\n",
      "Iteration Number: 9458\n",
      "Loss: 11.16378251242321\n",
      "l2 norm of gradients: 0.045286905812461614\n",
      "l2 norm of weights: 3.435164780817897\n",
      "---------------------\n",
      "Iteration Number: 9459\n",
      "Loss: 11.163597793933775\n",
      "l2 norm of gradients: 0.045281899965169216\n",
      "l2 norm of weights: 3.435226647143448\n",
      "---------------------\n",
      "Iteration Number: 9460\n",
      "Loss: 11.16341313985021\n",
      "l2 norm of gradients: 0.04527689528697899\n",
      "l2 norm of weights: 3.435288508568413\n",
      "---------------------\n",
      "Iteration Number: 9461\n",
      "Loss: 11.163228550148233\n",
      "l2 norm of gradients: 0.04527189177735182\n",
      "l2 norm of weights: 3.4353503650913066\n",
      "---------------------\n",
      "Iteration Number: 9462\n",
      "Loss: 11.163044024803568\n",
      "l2 norm of gradients: 0.04526688943574913\n",
      "l2 norm of weights: 3.435412216710639\n",
      "---------------------\n",
      "Iteration Number: 9463\n",
      "Loss: 11.162859563791951\n",
      "l2 norm of gradients: 0.045261888261632806\n",
      "l2 norm of weights: 3.4354740634249255\n",
      "---------------------\n",
      "Iteration Number: 9464\n",
      "Loss: 11.162675167089132\n",
      "l2 norm of gradients: 0.045256888254465145\n",
      "l2 norm of weights: 3.435535905232679\n",
      "---------------------\n",
      "Iteration Number: 9465\n",
      "Loss: 11.16249083467088\n",
      "l2 norm of gradients: 0.04525188941370894\n",
      "l2 norm of weights: 3.4355977421324155\n",
      "---------------------\n",
      "Iteration Number: 9466\n",
      "Loss: 11.162306566512974\n",
      "l2 norm of gradients: 0.04524689173882745\n",
      "l2 norm of weights: 3.435659574122652\n",
      "---------------------\n",
      "Iteration Number: 9467\n",
      "Loss: 11.1621223625912\n",
      "l2 norm of gradients: 0.04524189522928433\n",
      "l2 norm of weights: 3.4357214012019055\n",
      "---------------------\n",
      "Iteration Number: 9468\n",
      "Loss: 11.161938222881355\n",
      "l2 norm of gradients: 0.045236899884543796\n",
      "l2 norm of weights: 3.435783223368694\n",
      "---------------------\n",
      "Iteration Number: 9469\n",
      "Loss: 11.161754147359273\n",
      "l2 norm of gradients: 0.045231905704070464\n",
      "l2 norm of weights: 3.435845040621536\n",
      "---------------------\n",
      "Iteration Number: 9470\n",
      "Loss: 11.161570136000758\n",
      "l2 norm of gradients: 0.04522691268732939\n",
      "l2 norm of weights: 3.4359068529589525\n",
      "---------------------\n",
      "Iteration Number: 9471\n",
      "Loss: 11.161386188781666\n",
      "l2 norm of gradients: 0.04522192083378613\n",
      "l2 norm of weights: 3.4359686603794644\n",
      "---------------------\n",
      "Iteration Number: 9472\n",
      "Loss: 11.161202305677849\n",
      "l2 norm of gradients: 0.04521693014290671\n",
      "l2 norm of weights: 3.436030462881593\n",
      "---------------------\n",
      "Iteration Number: 9473\n",
      "Loss: 11.161018486665169\n",
      "l2 norm of gradients: 0.04521194061415754\n",
      "l2 norm of weights: 3.436092260463862\n",
      "---------------------\n",
      "Iteration Number: 9474\n",
      "Loss: 11.1608347317195\n",
      "l2 norm of gradients: 0.045206952247005536\n",
      "l2 norm of weights: 3.4361540531247954\n",
      "---------------------\n",
      "Iteration Number: 9475\n",
      "Loss: 11.160651040816742\n",
      "l2 norm of gradients: 0.045201965040918106\n",
      "l2 norm of weights: 3.436215840862916\n",
      "---------------------\n",
      "Iteration Number: 9476\n",
      "Loss: 11.160467413932796\n",
      "l2 norm of gradients: 0.045196978995363055\n",
      "l2 norm of weights: 3.436277623676751\n",
      "---------------------\n",
      "Iteration Number: 9477\n",
      "Loss: 11.160283851043577\n",
      "l2 norm of gradients: 0.04519199410980867\n",
      "l2 norm of weights: 3.436339401564827\n",
      "---------------------\n",
      "Iteration Number: 9478\n",
      "Loss: 11.160100352125017\n",
      "l2 norm of gradients: 0.04518701038372372\n",
      "l2 norm of weights: 3.4364011745256704\n",
      "---------------------\n",
      "Iteration Number: 9479\n",
      "Loss: 11.15991691715305\n",
      "l2 norm of gradients: 0.04518202781657734\n",
      "l2 norm of weights: 3.436462942557811\n",
      "---------------------\n",
      "Iteration Number: 9480\n",
      "Loss: 11.159733546103636\n",
      "l2 norm of gradients: 0.045177046407839226\n",
      "l2 norm of weights: 3.4365247056597767\n",
      "---------------------\n",
      "Iteration Number: 9481\n",
      "Loss: 11.159550238952736\n",
      "l2 norm of gradients: 0.04517206615697951\n",
      "l2 norm of weights: 3.436586463830098\n",
      "---------------------\n",
      "Iteration Number: 9482\n",
      "Loss: 11.159366995676335\n",
      "l2 norm of gradients: 0.04516708706346869\n",
      "l2 norm of weights: 3.436648217067307\n",
      "---------------------\n",
      "Iteration Number: 9483\n",
      "Loss: 11.159183816250424\n",
      "l2 norm of gradients: 0.04516210912677783\n",
      "l2 norm of weights: 3.436709965369935\n",
      "---------------------\n",
      "Iteration Number: 9484\n",
      "Loss: 11.159000700651012\n",
      "l2 norm of gradients: 0.045157132346378394\n",
      "l2 norm of weights: 3.436771708736515\n",
      "---------------------\n",
      "Iteration Number: 9485\n",
      "Loss: 11.158817648854104\n",
      "l2 norm of gradients: 0.04515215672174232\n",
      "l2 norm of weights: 3.436833447165581\n",
      "---------------------\n",
      "Iteration Number: 9486\n",
      "Loss: 11.15863466083573\n",
      "l2 norm of gradients: 0.04514718225234198\n",
      "l2 norm of weights: 3.4368951806556676\n",
      "---------------------\n",
      "Iteration Number: 9487\n",
      "Loss: 11.158451736571942\n",
      "l2 norm of gradients: 0.045142208937650176\n",
      "l2 norm of weights: 3.436956909205311\n",
      "---------------------\n",
      "Iteration Number: 9488\n",
      "Loss: 11.158268876038784\n",
      "l2 norm of gradients: 0.04513723677714026\n",
      "l2 norm of weights: 3.4370186328130465\n",
      "---------------------\n",
      "Iteration Number: 9489\n",
      "Loss: 11.158086079212328\n",
      "l2 norm of gradients: 0.04513226577028592\n",
      "l2 norm of weights: 3.437080351477413\n",
      "---------------------\n",
      "Iteration Number: 9490\n",
      "Loss: 11.157903346068647\n",
      "l2 norm of gradients: 0.04512729591656138\n",
      "l2 norm of weights: 3.4371420651969484\n",
      "---------------------\n",
      "Iteration Number: 9491\n",
      "Loss: 11.157720676583843\n",
      "l2 norm of gradients: 0.04512232721544128\n",
      "l2 norm of weights: 3.437203773970192\n",
      "---------------------\n",
      "Iteration Number: 9492\n",
      "Loss: 11.157538070734004\n",
      "l2 norm of gradients: 0.045117359666400715\n",
      "l2 norm of weights: 3.4372654777956844\n",
      "---------------------\n",
      "Iteration Number: 9493\n",
      "Loss: 11.157355528495263\n",
      "l2 norm of gradients: 0.04511239326891522\n",
      "l2 norm of weights: 3.4373271766719657\n",
      "---------------------\n",
      "Iteration Number: 9494\n",
      "Loss: 11.15717304984374\n",
      "l2 norm of gradients: 0.04510742802246086\n",
      "l2 norm of weights: 3.437388870597579\n",
      "---------------------\n",
      "Iteration Number: 9495\n",
      "Loss: 11.15699063475557\n",
      "l2 norm of gradients: 0.045102463926513986\n",
      "l2 norm of weights: 3.437450559571067\n",
      "---------------------\n",
      "Iteration Number: 9496\n",
      "Loss: 11.156808283206914\n",
      "l2 norm of gradients: 0.045097500980551576\n",
      "l2 norm of weights: 3.437512243590973\n",
      "---------------------\n",
      "Iteration Number: 9497\n",
      "Loss: 11.156625995173936\n",
      "l2 norm of gradients: 0.04509253918405097\n",
      "l2 norm of weights: 3.437573922655842\n",
      "---------------------\n",
      "Iteration Number: 9498\n",
      "Loss: 11.156443770632814\n",
      "l2 norm of gradients: 0.04508757853648995\n",
      "l2 norm of weights: 3.4376355967642205\n",
      "---------------------\n",
      "Iteration Number: 9499\n",
      "Loss: 11.156261609559733\n",
      "l2 norm of gradients: 0.04508261903734679\n",
      "l2 norm of weights: 3.4376972659146543\n",
      "---------------------\n",
      "Iteration Number: 9500\n",
      "Loss: 11.156079511930907\n",
      "l2 norm of gradients: 0.04507766068610021\n",
      "l2 norm of weights: 3.437758930105691\n",
      "---------------------\n",
      "Iteration Number: 9501\n",
      "Loss: 11.15589747772254\n",
      "l2 norm of gradients: 0.045072703482229325\n",
      "l2 norm of weights: 3.437820589335878\n",
      "---------------------\n",
      "Iteration Number: 9502\n",
      "Loss: 11.15571550691086\n",
      "l2 norm of gradients: 0.04506774742521376\n",
      "l2 norm of weights: 3.437882243603766\n",
      "---------------------\n",
      "Iteration Number: 9503\n",
      "Loss: 11.155533599472111\n",
      "l2 norm of gradients: 0.045062792514533594\n",
      "l2 norm of weights: 3.4379438929079047\n",
      "---------------------\n",
      "Iteration Number: 9504\n",
      "Loss: 11.155351755382545\n",
      "l2 norm of gradients: 0.04505783874966929\n",
      "l2 norm of weights: 3.4380055372468443\n",
      "---------------------\n",
      "Iteration Number: 9505\n",
      "Loss: 11.15516997461842\n",
      "l2 norm of gradients: 0.0450528861301018\n",
      "l2 norm of weights: 3.4380671766191373\n",
      "---------------------\n",
      "Iteration Number: 9506\n",
      "Loss: 11.154988257156017\n",
      "l2 norm of gradients: 0.04504793465531254\n",
      "l2 norm of weights: 3.4381288110233372\n",
      "---------------------\n",
      "Iteration Number: 9507\n",
      "Loss: 11.154806602971625\n",
      "l2 norm of gradients: 0.04504298432478333\n",
      "l2 norm of weights: 3.4381904404579964\n",
      "---------------------\n",
      "Iteration Number: 9508\n",
      "Loss: 11.154625012041535\n",
      "l2 norm of gradients: 0.04503803513799648\n",
      "l2 norm of weights: 3.4382520649216706\n",
      "---------------------\n",
      "Iteration Number: 9509\n",
      "Loss: 11.15444348434207\n",
      "l2 norm of gradients: 0.04503308709443472\n",
      "l2 norm of weights: 3.4383136844129147\n",
      "---------------------\n",
      "Iteration Number: 9510\n",
      "Loss: 11.15426201984955\n",
      "l2 norm of gradients: 0.04502814019358124\n",
      "l2 norm of weights: 3.438375298930285\n",
      "---------------------\n",
      "Iteration Number: 9511\n",
      "Loss: 11.154080618540313\n",
      "l2 norm of gradients: 0.045023194434919686\n",
      "l2 norm of weights: 3.4384369084723394\n",
      "---------------------\n",
      "Iteration Number: 9512\n",
      "Loss: 11.15389928039071\n",
      "l2 norm of gradients: 0.045018249817934106\n",
      "l2 norm of weights: 3.4384985130376355\n",
      "---------------------\n",
      "Iteration Number: 9513\n",
      "Loss: 11.153718005377103\n",
      "l2 norm of gradients: 0.045013306342109025\n",
      "l2 norm of weights: 3.4385601126247325\n",
      "---------------------\n",
      "Iteration Number: 9514\n",
      "Loss: 11.153536793475855\n",
      "l2 norm of gradients: 0.045008364006929445\n",
      "l2 norm of weights: 3.43862170723219\n",
      "---------------------\n",
      "Iteration Number: 9515\n",
      "Loss: 11.153355644663366\n",
      "l2 norm of gradients: 0.04500342281188075\n",
      "l2 norm of weights: 3.4386832968585694\n",
      "---------------------\n",
      "Iteration Number: 9516\n",
      "Loss: 11.15317455891603\n",
      "l2 norm of gradients: 0.04499848275644883\n",
      "l2 norm of weights: 3.438744881502432\n",
      "---------------------\n",
      "Iteration Number: 9517\n",
      "Loss: 11.15299353621025\n",
      "l2 norm of gradients: 0.044993543840119964\n",
      "l2 norm of weights: 3.4388064611623403\n",
      "---------------------\n",
      "Iteration Number: 9518\n",
      "Loss: 11.152812576522443\n",
      "l2 norm of gradients: 0.04498860606238091\n",
      "l2 norm of weights: 3.438868035836858\n",
      "---------------------\n",
      "Iteration Number: 9519\n",
      "Loss: 11.152631679829062\n",
      "l2 norm of gradients: 0.04498366942271885\n",
      "l2 norm of weights: 3.4389296055245495\n",
      "---------------------\n",
      "Iteration Number: 9520\n",
      "Loss: 11.152450846106541\n",
      "l2 norm of gradients: 0.04497873392062144\n",
      "l2 norm of weights: 3.4389911702239795\n",
      "---------------------\n",
      "Iteration Number: 9521\n",
      "Loss: 11.152270075331339\n",
      "l2 norm of gradients: 0.04497379955557673\n",
      "l2 norm of weights: 3.4390527299337146\n",
      "---------------------\n",
      "Iteration Number: 9522\n",
      "Loss: 11.15208936747992\n",
      "l2 norm of gradients: 0.044968866327073295\n",
      "l2 norm of weights: 3.4391142846523213\n",
      "---------------------\n",
      "Iteration Number: 9523\n",
      "Loss: 11.151908722528782\n",
      "l2 norm of gradients: 0.04496393423460007\n",
      "l2 norm of weights: 3.4391758343783687\n",
      "---------------------\n",
      "Iteration Number: 9524\n",
      "Loss: 11.151728140454406\n",
      "l2 norm of gradients: 0.04495900327764646\n",
      "l2 norm of weights: 3.439237379110424\n",
      "---------------------\n",
      "Iteration Number: 9525\n",
      "Loss: 11.151547621233304\n",
      "l2 norm of gradients: 0.04495407345570232\n",
      "l2 norm of weights: 3.439298918847057\n",
      "---------------------\n",
      "Iteration Number: 9526\n",
      "Loss: 11.151367164841991\n",
      "l2 norm of gradients: 0.04494914476825795\n",
      "l2 norm of weights: 3.439360453586839\n",
      "---------------------\n",
      "Iteration Number: 9527\n",
      "Loss: 11.151186771256993\n",
      "l2 norm of gradients: 0.04494421721480408\n",
      "l2 norm of weights: 3.4394219833283413\n",
      "---------------------\n",
      "Iteration Number: 9528\n",
      "Loss: 11.151006440454866\n",
      "l2 norm of gradients: 0.044939290794831895\n",
      "l2 norm of weights: 3.439483508070135\n",
      "---------------------\n",
      "Iteration Number: 9529\n",
      "Loss: 11.150826172412147\n",
      "l2 norm of gradients: 0.04493436550783299\n",
      "l2 norm of weights: 3.439545027810794\n",
      "---------------------\n",
      "Iteration Number: 9530\n",
      "Loss: 11.150645967105412\n",
      "l2 norm of gradients: 0.04492944135329946\n",
      "l2 norm of weights: 3.4396065425488924\n",
      "---------------------\n",
      "Iteration Number: 9531\n",
      "Loss: 11.150465824511242\n",
      "l2 norm of gradients: 0.04492451833072376\n",
      "l2 norm of weights: 3.4396680522830048\n",
      "---------------------\n",
      "Iteration Number: 9532\n",
      "Loss: 11.150285744606208\n",
      "l2 norm of gradients: 0.044919596439598884\n",
      "l2 norm of weights: 3.439729557011707\n",
      "---------------------\n",
      "Iteration Number: 9533\n",
      "Loss: 11.150105727366935\n",
      "l2 norm of gradients: 0.04491467567941816\n",
      "l2 norm of weights: 3.4397910567335757\n",
      "---------------------\n",
      "Iteration Number: 9534\n",
      "Loss: 11.149925772770029\n",
      "l2 norm of gradients: 0.04490975604967546\n",
      "l2 norm of weights: 3.4398525514471876\n",
      "---------------------\n",
      "Iteration Number: 9535\n",
      "Loss: 11.14974588079211\n",
      "l2 norm of gradients: 0.044904837549865004\n",
      "l2 norm of weights: 3.4399140411511224\n",
      "---------------------\n",
      "Iteration Number: 9536\n",
      "Loss: 11.149566051409812\n",
      "l2 norm of gradients: 0.04489992017948148\n",
      "l2 norm of weights: 3.439975525843958\n",
      "---------------------\n",
      "Iteration Number: 9537\n",
      "Loss: 11.149386284599796\n",
      "l2 norm of gradients: 0.044895003938020074\n",
      "l2 norm of weights: 3.440037005524275\n",
      "---------------------\n",
      "Iteration Number: 9538\n",
      "Loss: 11.149206580338717\n",
      "l2 norm of gradients: 0.0448900888249763\n",
      "l2 norm of weights: 3.4400984801906542\n",
      "---------------------\n",
      "Iteration Number: 9539\n",
      "Loss: 11.14902693860325\n",
      "l2 norm of gradients: 0.04488517483984623\n",
      "l2 norm of weights: 3.4401599498416773\n",
      "---------------------\n",
      "Iteration Number: 9540\n",
      "Loss: 11.148847359370077\n",
      "l2 norm of gradients: 0.04488026198212629\n",
      "l2 norm of weights: 3.440221414475927\n",
      "---------------------\n",
      "Iteration Number: 9541\n",
      "Loss: 11.148667842615893\n",
      "l2 norm of gradients: 0.04487535025131337\n",
      "l2 norm of weights: 3.4402828740919875\n",
      "---------------------\n",
      "Iteration Number: 9542\n",
      "Loss: 11.148488388317409\n",
      "l2 norm of gradients: 0.0448704396469048\n",
      "l2 norm of weights: 3.440344328688442\n",
      "---------------------\n",
      "Iteration Number: 9543\n",
      "Loss: 11.148308996451345\n",
      "l2 norm of gradients: 0.044865530168398345\n",
      "l2 norm of weights: 3.440405778263876\n",
      "---------------------\n",
      "Iteration Number: 9544\n",
      "Loss: 11.148129666994432\n",
      "l2 norm of gradients: 0.0448606218152922\n",
      "l2 norm of weights: 3.440467222816876\n",
      "---------------------\n",
      "Iteration Number: 9545\n",
      "Loss: 11.147950399923415\n",
      "l2 norm of gradients: 0.044855714587085004\n",
      "l2 norm of weights: 3.4405286623460283\n",
      "---------------------\n",
      "Iteration Number: 9546\n",
      "Loss: 11.147771195215043\n",
      "l2 norm of gradients: 0.04485080848327584\n",
      "l2 norm of weights: 3.4405900968499217\n",
      "---------------------\n",
      "Iteration Number: 9547\n",
      "Loss: 11.147592052846091\n",
      "l2 norm of gradients: 0.044845903503364204\n",
      "l2 norm of weights: 3.4406515263271444\n",
      "---------------------\n",
      "Iteration Number: 9548\n",
      "Loss: 11.14741297279334\n",
      "l2 norm of gradients: 0.04484099964685007\n",
      "l2 norm of weights: 3.440712950776285\n",
      "---------------------\n",
      "Iteration Number: 9549\n",
      "Loss: 11.147233955033574\n",
      "l2 norm of gradients: 0.04483609691323379\n",
      "l2 norm of weights: 3.440774370195935\n",
      "---------------------\n",
      "Iteration Number: 9550\n",
      "Loss: 11.147054999543592\n",
      "l2 norm of gradients: 0.044831195302016194\n",
      "l2 norm of weights: 3.4408357845846855\n",
      "---------------------\n",
      "Iteration Number: 9551\n",
      "Loss: 11.146876106300212\n",
      "l2 norm of gradients: 0.04482629481269852\n",
      "l2 norm of weights: 3.440897193941128\n",
      "---------------------\n",
      "Iteration Number: 9552\n",
      "Loss: 11.146697275280264\n",
      "l2 norm of gradients: 0.044821395444782475\n",
      "l2 norm of weights: 3.4409585982638564\n",
      "---------------------\n",
      "Iteration Number: 9553\n",
      "Loss: 11.14651850646058\n",
      "l2 norm of gradients: 0.04481649719777016\n",
      "l2 norm of weights: 3.441019997551463\n",
      "---------------------\n",
      "Iteration Number: 9554\n",
      "Loss: 11.14633979981801\n",
      "l2 norm of gradients: 0.04481160007116415\n",
      "l2 norm of weights: 3.441081391802544\n",
      "---------------------\n",
      "Iteration Number: 9555\n",
      "Loss: 11.146161155329414\n",
      "l2 norm of gradients: 0.04480670406446739\n",
      "l2 norm of weights: 3.4411427810156936\n",
      "---------------------\n",
      "Iteration Number: 9556\n",
      "Loss: 11.145982572971661\n",
      "l2 norm of gradients: 0.04480180917718336\n",
      "l2 norm of weights: 3.441204165189509\n",
      "---------------------\n",
      "Iteration Number: 9557\n",
      "Loss: 11.145804052721644\n",
      "l2 norm of gradients: 0.044796915408815884\n",
      "l2 norm of weights: 3.441265544322588\n",
      "---------------------\n",
      "Iteration Number: 9558\n",
      "Loss: 11.145625594556252\n",
      "l2 norm of gradients: 0.04479202275886923\n",
      "l2 norm of weights: 3.4413269184135267\n",
      "---------------------\n",
      "Iteration Number: 9559\n",
      "Loss: 11.145447198452391\n",
      "l2 norm of gradients: 0.044787131226848145\n",
      "l2 norm of weights: 3.441388287460925\n",
      "---------------------\n",
      "Iteration Number: 9560\n",
      "Loss: 11.145268864386983\n",
      "l2 norm of gradients: 0.044782240812257776\n",
      "l2 norm of weights: 3.441449651463383\n",
      "---------------------\n",
      "Iteration Number: 9561\n",
      "Loss: 11.145090592336953\n",
      "l2 norm of gradients: 0.044777351514603714\n",
      "l2 norm of weights: 3.441511010419501\n",
      "---------------------\n",
      "Iteration Number: 9562\n",
      "Loss: 11.14491238227925\n",
      "l2 norm of gradients: 0.04477246333339194\n",
      "l2 norm of weights: 3.44157236432788\n",
      "---------------------\n",
      "Iteration Number: 9563\n",
      "Loss: 11.14473423419082\n",
      "l2 norm of gradients: 0.04476757626812895\n",
      "l2 norm of weights: 3.4416337131871226\n",
      "---------------------\n",
      "Iteration Number: 9564\n",
      "Loss: 11.144556148048633\n",
      "l2 norm of gradients: 0.04476269031832162\n",
      "l2 norm of weights: 3.4416950569958327\n",
      "---------------------\n",
      "Iteration Number: 9565\n",
      "Loss: 11.14437812382966\n",
      "l2 norm of gradients: 0.044757805483477194\n",
      "l2 norm of weights: 3.441756395752613\n",
      "---------------------\n",
      "Iteration Number: 9566\n",
      "Loss: 11.144200161510899\n",
      "l2 norm of gradients: 0.04475292176310349\n",
      "l2 norm of weights: 3.441817729456069\n",
      "---------------------\n",
      "Iteration Number: 9567\n",
      "Loss: 11.144022261069338\n",
      "l2 norm of gradients: 0.04474803915670863\n",
      "l2 norm of weights: 3.441879058104806\n",
      "---------------------\n",
      "Iteration Number: 9568\n",
      "Loss: 11.14384442248199\n",
      "l2 norm of gradients: 0.044743157663801235\n",
      "l2 norm of weights: 3.4419403816974303\n",
      "---------------------\n",
      "Iteration Number: 9569\n",
      "Loss: 11.143666645725876\n",
      "l2 norm of gradients: 0.04473827728389032\n",
      "l2 norm of weights: 3.4420017002325496\n",
      "---------------------\n",
      "Iteration Number: 9570\n",
      "Loss: 11.14348893077804\n",
      "l2 norm of gradients: 0.04473339801648537\n",
      "l2 norm of weights: 3.4420630137087724\n",
      "---------------------\n",
      "Iteration Number: 9571\n",
      "Loss: 11.143311277615513\n",
      "l2 norm of gradients: 0.04472851986109622\n",
      "l2 norm of weights: 3.442124322124707\n",
      "---------------------\n",
      "Iteration Number: 9572\n",
      "Loss: 11.143133686215362\n",
      "l2 norm of gradients: 0.04472364281723326\n",
      "l2 norm of weights: 3.442185625478964\n",
      "---------------------\n",
      "Iteration Number: 9573\n",
      "Loss: 11.142956156554654\n",
      "l2 norm of gradients: 0.044718766884407186\n",
      "l2 norm of weights: 3.4422469237701536\n",
      "---------------------\n",
      "Iteration Number: 9574\n",
      "Loss: 11.142778688610463\n",
      "l2 norm of gradients: 0.044713892062129196\n",
      "l2 norm of weights: 3.4423082169968873\n",
      "---------------------\n",
      "Iteration Number: 9575\n",
      "Loss: 11.142601282359886\n",
      "l2 norm of gradients: 0.04470901834991085\n",
      "l2 norm of weights: 3.442369505157777\n",
      "---------------------\n",
      "Iteration Number: 9576\n",
      "Loss: 11.14242393778002\n",
      "l2 norm of gradients: 0.04470414574726424\n",
      "l2 norm of weights: 3.4424307882514373\n",
      "---------------------\n",
      "Iteration Number: 9577\n",
      "Loss: 11.142246654847977\n",
      "l2 norm of gradients: 0.044699274253701786\n",
      "l2 norm of weights: 3.442492066276481\n",
      "---------------------\n",
      "Iteration Number: 9578\n",
      "Loss: 11.14206943354089\n",
      "l2 norm of gradients: 0.044694403868736396\n",
      "l2 norm of weights: 3.442553339231523\n",
      "---------------------\n",
      "Iteration Number: 9579\n",
      "Loss: 11.141892273835891\n",
      "l2 norm of gradients: 0.04468953459188135\n",
      "l2 norm of weights: 3.4426146071151797\n",
      "---------------------\n",
      "Iteration Number: 9580\n",
      "Loss: 11.141715175710123\n",
      "l2 norm of gradients: 0.044684666422650406\n",
      "l2 norm of weights: 3.4426758699260667\n",
      "---------------------\n",
      "Iteration Number: 9581\n",
      "Loss: 11.141538139140758\n",
      "l2 norm of gradients: 0.044679799360557736\n",
      "l2 norm of weights: 3.4427371276628023\n",
      "---------------------\n",
      "Iteration Number: 9582\n",
      "Loss: 11.141361164104953\n",
      "l2 norm of gradients: 0.044674933405117925\n",
      "l2 norm of weights: 3.4427983803240036\n",
      "---------------------\n",
      "Iteration Number: 9583\n",
      "Loss: 11.14118425057989\n",
      "l2 norm of gradients: 0.04467006855584596\n",
      "l2 norm of weights: 3.4428596279082915\n",
      "---------------------\n",
      "Iteration Number: 9584\n",
      "Loss: 11.141007398542772\n",
      "l2 norm of gradients: 0.04466520481225733\n",
      "l2 norm of weights: 3.4429208704142837\n",
      "---------------------\n",
      "Iteration Number: 9585\n",
      "Loss: 11.140830607970802\n",
      "l2 norm of gradients: 0.04466034217386788\n",
      "l2 norm of weights: 3.442982107840602\n",
      "---------------------\n",
      "Iteration Number: 9586\n",
      "Loss: 11.140653878841185\n",
      "l2 norm of gradients: 0.0446554806401939\n",
      "l2 norm of weights: 3.4430433401858678\n",
      "---------------------\n",
      "Iteration Number: 9587\n",
      "Loss: 11.140477211131152\n",
      "l2 norm of gradients: 0.04465062021075213\n",
      "l2 norm of weights: 3.4431045674487035\n",
      "---------------------\n",
      "Iteration Number: 9588\n",
      "Loss: 11.140300604817947\n",
      "l2 norm of gradients: 0.044645760885059664\n",
      "l2 norm of weights: 3.443165789627732\n",
      "---------------------\n",
      "Iteration Number: 9589\n",
      "Loss: 11.140124059878817\n",
      "l2 norm of gradients: 0.04464090266263413\n",
      "l2 norm of weights: 3.4432270067215778\n",
      "---------------------\n",
      "Iteration Number: 9590\n",
      "Loss: 11.139947576291014\n",
      "l2 norm of gradients: 0.04463604554299345\n",
      "l2 norm of weights: 3.443288218728865\n",
      "---------------------\n",
      "Iteration Number: 9591\n",
      "Loss: 11.139771154031825\n",
      "l2 norm of gradients: 0.04463118952565608\n",
      "l2 norm of weights: 3.4433494256482198\n",
      "---------------------\n",
      "Iteration Number: 9592\n",
      "Loss: 11.13959479307853\n",
      "l2 norm of gradients: 0.04462633461014086\n",
      "l2 norm of weights: 3.4434106274782676\n",
      "---------------------\n",
      "Iteration Number: 9593\n",
      "Loss: 11.139418493408408\n",
      "l2 norm of gradients: 0.04462148079596703\n",
      "l2 norm of weights: 3.4434718242176374\n",
      "---------------------\n",
      "Iteration Number: 9594\n",
      "Loss: 11.139242254998775\n",
      "l2 norm of gradients: 0.044616628082654264\n",
      "l2 norm of weights: 3.4435330158649564\n",
      "---------------------\n",
      "Iteration Number: 9595\n",
      "Loss: 11.139066077826945\n",
      "l2 norm of gradients: 0.044611776469722704\n",
      "l2 norm of weights: 3.4435942024188533\n",
      "---------------------\n",
      "Iteration Number: 9596\n",
      "Loss: 11.138889961870252\n",
      "l2 norm of gradients: 0.04460692595669285\n",
      "l2 norm of weights: 3.443655383877959\n",
      "---------------------\n",
      "Iteration Number: 9597\n",
      "Loss: 11.138713907106027\n",
      "l2 norm of gradients: 0.044602076543085616\n",
      "l2 norm of weights: 3.4437165602409023\n",
      "---------------------\n",
      "Iteration Number: 9598\n",
      "Loss: 11.138537913511623\n",
      "l2 norm of gradients: 0.04459722822842244\n",
      "l2 norm of weights: 3.443777731506316\n",
      "---------------------\n",
      "Iteration Number: 9599\n",
      "Loss: 11.138361981064406\n",
      "l2 norm of gradients: 0.0445923810122251\n",
      "l2 norm of weights: 3.4438388976728316\n",
      "---------------------\n",
      "Iteration Number: 9600\n",
      "Loss: 11.13818610974174\n",
      "l2 norm of gradients: 0.04458753489401577\n",
      "l2 norm of weights: 3.4439000587390827\n",
      "---------------------\n",
      "Iteration Number: 9601\n",
      "Loss: 11.13801029952101\n",
      "l2 norm of gradients: 0.04458268987331711\n",
      "l2 norm of weights: 3.4439612147037026\n",
      "---------------------\n",
      "Iteration Number: 9602\n",
      "Loss: 11.137834550379612\n",
      "l2 norm of gradients: 0.044577845949652174\n",
      "l2 norm of weights: 3.444022365565327\n",
      "---------------------\n",
      "Iteration Number: 9603\n",
      "Loss: 11.137658862294954\n",
      "l2 norm of gradients: 0.044573003122544426\n",
      "l2 norm of weights: 3.44408351132259\n",
      "---------------------\n",
      "Iteration Number: 9604\n",
      "Loss: 11.137483235244451\n",
      "l2 norm of gradients: 0.04456816139151777\n",
      "l2 norm of weights: 3.444144651974129\n",
      "---------------------\n",
      "Iteration Number: 9605\n",
      "Loss: 11.137307669205525\n",
      "l2 norm of gradients: 0.044563320756096515\n",
      "l2 norm of weights: 3.4442057875185803\n",
      "---------------------\n",
      "Iteration Number: 9606\n",
      "Loss: 11.137132164155624\n",
      "l2 norm of gradients: 0.04455848121580541\n",
      "l2 norm of weights: 3.444266917954583\n",
      "---------------------\n",
      "Iteration Number: 9607\n",
      "Loss: 11.13695672007219\n",
      "l2 norm of gradients: 0.044553642770169585\n",
      "l2 norm of weights: 3.4443280432807746\n",
      "---------------------\n",
      "Iteration Number: 9608\n",
      "Loss: 11.136781336932689\n",
      "l2 norm of gradients: 0.04454880541871462\n",
      "l2 norm of weights: 3.4443891634957953\n",
      "---------------------\n",
      "Iteration Number: 9609\n",
      "Loss: 11.136606014714587\n",
      "l2 norm of gradients: 0.044543969160966465\n",
      "l2 norm of weights: 3.444450278598285\n",
      "---------------------\n",
      "Iteration Number: 9610\n",
      "Loss: 11.136430753395375\n",
      "l2 norm of gradients: 0.04453913399645161\n",
      "l2 norm of weights: 3.444511388586886\n",
      "---------------------\n",
      "Iteration Number: 9611\n",
      "Loss: 11.136255552952537\n",
      "l2 norm of gradients: 0.04453429992469684\n",
      "l2 norm of weights: 3.44457249346024\n",
      "---------------------\n",
      "Iteration Number: 9612\n",
      "Loss: 11.13608041336359\n",
      "l2 norm of gradients: 0.044529466945229405\n",
      "l2 norm of weights: 3.4446335932169885\n",
      "---------------------\n",
      "Iteration Number: 9613\n",
      "Loss: 11.135905334606035\n",
      "l2 norm of gradients: 0.044524635057576915\n",
      "l2 norm of weights: 3.444694687855777\n",
      "---------------------\n",
      "Iteration Number: 9614\n",
      "Loss: 11.13573031665741\n",
      "l2 norm of gradients: 0.04451980426126752\n",
      "l2 norm of weights: 3.444755777375249\n",
      "---------------------\n",
      "Iteration Number: 9615\n",
      "Loss: 11.135555359495244\n",
      "l2 norm of gradients: 0.04451497455582968\n",
      "l2 norm of weights: 3.4448168617740493\n",
      "---------------------\n",
      "Iteration Number: 9616\n",
      "Loss: 11.135380463097091\n",
      "l2 norm of gradients: 0.04451014594079231\n",
      "l2 norm of weights: 3.4448779410508252\n",
      "---------------------\n",
      "Iteration Number: 9617\n",
      "Loss: 11.13520562744051\n",
      "l2 norm of gradients: 0.04450531841568473\n",
      "l2 norm of weights: 3.4449390152042225\n",
      "---------------------\n",
      "Iteration Number: 9618\n",
      "Loss: 11.135030852503075\n",
      "l2 norm of gradients: 0.044500491980036735\n",
      "l2 norm of weights: 3.4450000842328894\n",
      "---------------------\n",
      "Iteration Number: 9619\n",
      "Loss: 11.13485613826236\n",
      "l2 norm of gradients: 0.04449566663337842\n",
      "l2 norm of weights: 3.4450611481354745\n",
      "---------------------\n",
      "Iteration Number: 9620\n",
      "Loss: 11.134681484695957\n",
      "l2 norm of gradients: 0.044490842375240404\n",
      "l2 norm of weights: 3.4451222069106273\n",
      "---------------------\n",
      "Iteration Number: 9621\n",
      "Loss: 11.134506891781477\n",
      "l2 norm of gradients: 0.04448601920515366\n",
      "l2 norm of weights: 3.445183260556997\n",
      "---------------------\n",
      "Iteration Number: 9622\n",
      "Loss: 11.13433235949652\n",
      "l2 norm of gradients: 0.04448119712264957\n",
      "l2 norm of weights: 3.445244309073235\n",
      "---------------------\n",
      "Iteration Number: 9623\n",
      "Loss: 11.13415788781873\n",
      "l2 norm of gradients: 0.04447637612725999\n",
      "l2 norm of weights: 3.4453053524579937\n",
      "---------------------\n",
      "Iteration Number: 9624\n",
      "Loss: 11.133983476725732\n",
      "l2 norm of gradients: 0.04447155621851717\n",
      "l2 norm of weights: 3.445366390709925\n",
      "---------------------\n",
      "Iteration Number: 9625\n",
      "Loss: 11.133809126195166\n",
      "l2 norm of gradients: 0.04446673739595372\n",
      "l2 norm of weights: 3.445427423827682\n",
      "---------------------\n",
      "Iteration Number: 9626\n",
      "Loss: 11.133634836204696\n",
      "l2 norm of gradients: 0.044461919659102726\n",
      "l2 norm of weights: 3.445488451809919\n",
      "---------------------\n",
      "Iteration Number: 9627\n",
      "Loss: 11.13346060673199\n",
      "l2 norm of gradients: 0.04445710300749767\n",
      "l2 norm of weights: 3.4455494746552917\n",
      "---------------------\n",
      "Iteration Number: 9628\n",
      "Loss: 11.133286437754734\n",
      "l2 norm of gradients: 0.04445228744067242\n",
      "l2 norm of weights: 3.445610492362455\n",
      "---------------------\n",
      "Iteration Number: 9629\n",
      "Loss: 11.1331123292506\n",
      "l2 norm of gradients: 0.04444747295816129\n",
      "l2 norm of weights: 3.4456715049300657\n",
      "---------------------\n",
      "Iteration Number: 9630\n",
      "Loss: 11.132938281197301\n",
      "l2 norm of gradients: 0.04444265955949897\n",
      "l2 norm of weights: 3.445732512356781\n",
      "---------------------\n",
      "Iteration Number: 9631\n",
      "Loss: 11.132764293572553\n",
      "l2 norm of gradients: 0.04443784724422065\n",
      "l2 norm of weights: 3.44579351464126\n",
      "---------------------\n",
      "Iteration Number: 9632\n",
      "Loss: 11.132590366354066\n",
      "l2 norm of gradients: 0.044433036011861826\n",
      "l2 norm of weights: 3.4458545117821604\n",
      "---------------------\n",
      "Iteration Number: 9633\n",
      "Loss: 11.132416499519579\n",
      "l2 norm of gradients: 0.04442822586195846\n",
      "l2 norm of weights: 3.4459155037781426\n",
      "---------------------\n",
      "Iteration Number: 9634\n",
      "Loss: 11.132242693046827\n",
      "l2 norm of gradients: 0.04442341679404693\n",
      "l2 norm of weights: 3.4459764906278667\n",
      "---------------------\n",
      "Iteration Number: 9635\n",
      "Loss: 11.132068946913579\n",
      "l2 norm of gradients: 0.044418608807663996\n",
      "l2 norm of weights: 3.446037472329995\n",
      "---------------------\n",
      "Iteration Number: 9636\n",
      "Loss: 11.131895261097581\n",
      "l2 norm of gradients: 0.04441380190234682\n",
      "l2 norm of weights: 3.4460984488831885\n",
      "---------------------\n",
      "Iteration Number: 9637\n",
      "Loss: 11.131721635576628\n",
      "l2 norm of gradients: 0.04440899607763308\n",
      "l2 norm of weights: 3.446159420286111\n",
      "---------------------\n",
      "Iteration Number: 9638\n",
      "Loss: 11.13154807032849\n",
      "l2 norm of gradients: 0.044404191333060714\n",
      "l2 norm of weights: 3.446220386537426\n",
      "---------------------\n",
      "Iteration Number: 9639\n",
      "Loss: 11.13137456533098\n",
      "l2 norm of gradients: 0.044399387668168144\n",
      "l2 norm of weights: 3.4462813476357983\n",
      "---------------------\n",
      "Iteration Number: 9640\n",
      "Loss: 11.131201120561888\n",
      "l2 norm of gradients: 0.04439458508249424\n",
      "l2 norm of weights: 3.446342303579893\n",
      "---------------------\n",
      "Iteration Number: 9641\n",
      "Loss: 11.13102773599905\n",
      "l2 norm of gradients: 0.044389783575578205\n",
      "l2 norm of weights: 3.446403254368376\n",
      "---------------------\n",
      "Iteration Number: 9642\n",
      "Loss: 11.130854411620271\n",
      "l2 norm of gradients: 0.04438498314695971\n",
      "l2 norm of weights: 3.4464641999999137\n",
      "---------------------\n",
      "Iteration Number: 9643\n",
      "Loss: 11.130681147403424\n",
      "l2 norm of gradients: 0.0443801837961788\n",
      "l2 norm of weights: 3.446525140473176\n",
      "---------------------\n",
      "Iteration Number: 9644\n",
      "Loss: 11.130507943326327\n",
      "l2 norm of gradients: 0.044375385522775944\n",
      "l2 norm of weights: 3.446586075786829\n",
      "---------------------\n",
      "Iteration Number: 9645\n",
      "Loss: 11.130334799366851\n",
      "l2 norm of gradients: 0.04437058832629203\n",
      "l2 norm of weights: 3.446647005939544\n",
      "---------------------\n",
      "Iteration Number: 9646\n",
      "Loss: 11.130161715502881\n",
      "l2 norm of gradients: 0.04436579220626833\n",
      "l2 norm of weights: 3.4467079309299895\n",
      "---------------------\n",
      "Iteration Number: 9647\n",
      "Loss: 11.129988691712281\n",
      "l2 norm of gradients: 0.04436099716224654\n",
      "l2 norm of weights: 3.446768850756838\n",
      "---------------------\n",
      "Iteration Number: 9648\n",
      "Loss: 11.129815727972952\n",
      "l2 norm of gradients: 0.044356203193768734\n",
      "l2 norm of weights: 3.4468297654187596\n",
      "---------------------\n",
      "Iteration Number: 9649\n",
      "Loss: 11.129642824262797\n",
      "l2 norm of gradients: 0.04435141030037744\n",
      "l2 norm of weights: 3.4468906749144277\n",
      "---------------------\n",
      "Iteration Number: 9650\n",
      "Loss: 11.129469980559723\n",
      "l2 norm of gradients: 0.0443466184816156\n",
      "l2 norm of weights: 3.4469515792425156\n",
      "---------------------\n",
      "Iteration Number: 9651\n",
      "Loss: 11.129297196841655\n",
      "l2 norm of gradients: 0.04434182773702652\n",
      "l2 norm of weights: 3.447012478401697\n",
      "---------------------\n",
      "Iteration Number: 9652\n",
      "Loss: 11.129124473086545\n",
      "l2 norm of gradients: 0.04433703806615391\n",
      "l2 norm of weights: 3.4470733723906473\n",
      "---------------------\n",
      "Iteration Number: 9653\n",
      "Loss: 11.128951809272314\n",
      "l2 norm of gradients: 0.04433224946854191\n",
      "l2 norm of weights: 3.447134261208042\n",
      "---------------------\n",
      "Iteration Number: 9654\n",
      "Loss: 11.128779205376935\n",
      "l2 norm of gradients: 0.044327461943735075\n",
      "l2 norm of weights: 3.447195144852557\n",
      "---------------------\n",
      "Iteration Number: 9655\n",
      "Loss: 11.128606661378361\n",
      "l2 norm of gradients: 0.044322675491278336\n",
      "l2 norm of weights: 3.4472560233228697\n",
      "---------------------\n",
      "Iteration Number: 9656\n",
      "Loss: 11.128434177254583\n",
      "l2 norm of gradients: 0.04431789011071706\n",
      "l2 norm of weights: 3.447316896617659\n",
      "---------------------\n",
      "Iteration Number: 9657\n",
      "Loss: 11.128261752983573\n",
      "l2 norm of gradients: 0.044313105801597\n",
      "l2 norm of weights: 3.4473777647356036\n",
      "---------------------\n",
      "Iteration Number: 9658\n",
      "Loss: 11.128089388543334\n",
      "l2 norm of gradients: 0.04430832256346432\n",
      "l2 norm of weights: 3.447438627675382\n",
      "---------------------\n",
      "Iteration Number: 9659\n",
      "Loss: 11.127917083911875\n",
      "l2 norm of gradients: 0.0443035403958656\n",
      "l2 norm of weights: 3.4474994854356753\n",
      "---------------------\n",
      "Iteration Number: 9660\n",
      "Loss: 11.127744839067223\n",
      "l2 norm of gradients: 0.04429875929834781\n",
      "l2 norm of weights: 3.4475603380151645\n",
      "---------------------\n",
      "Iteration Number: 9661\n",
      "Loss: 11.127572653987395\n",
      "l2 norm of gradients: 0.04429397927045833\n",
      "l2 norm of weights: 3.447621185412532\n",
      "---------------------\n",
      "Iteration Number: 9662\n",
      "Loss: 11.127400528650433\n",
      "l2 norm of gradients: 0.04428920031174492\n",
      "l2 norm of weights: 3.44768202762646\n",
      "---------------------\n",
      "Iteration Number: 9663\n",
      "Loss: 11.12722846303439\n",
      "l2 norm of gradients: 0.044284422421755766\n",
      "l2 norm of weights: 3.4477428646556327\n",
      "---------------------\n",
      "Iteration Number: 9664\n",
      "Loss: 11.127056457117325\n",
      "l2 norm of gradients: 0.04427964560003952\n",
      "l2 norm of weights: 3.4478036964987337\n",
      "---------------------\n",
      "Iteration Number: 9665\n",
      "Loss: 11.126884510877307\n",
      "l2 norm of gradients: 0.044274869846145086\n",
      "l2 norm of weights: 3.4478645231544487\n",
      "---------------------\n",
      "Iteration Number: 9666\n",
      "Loss: 11.126712624292418\n",
      "l2 norm of gradients: 0.04427009515962192\n",
      "l2 norm of weights: 3.4479253446214635\n",
      "---------------------\n",
      "Iteration Number: 9667\n",
      "Loss: 11.126540797340748\n",
      "l2 norm of gradients: 0.0442653215400198\n",
      "l2 norm of weights: 3.4479861608984645\n",
      "---------------------\n",
      "Iteration Number: 9668\n",
      "Loss: 11.126369030000394\n",
      "l2 norm of gradients: 0.04426054898688892\n",
      "l2 norm of weights: 3.4480469719841396\n",
      "---------------------\n",
      "Iteration Number: 9669\n",
      "Loss: 11.12619732224948\n",
      "l2 norm of gradients: 0.04425577749977988\n",
      "l2 norm of weights: 3.448107777877176\n",
      "---------------------\n",
      "Iteration Number: 9670\n",
      "Loss: 11.12602567406612\n",
      "l2 norm of gradients: 0.04425100707824375\n",
      "l2 norm of weights: 3.448168578576264\n",
      "---------------------\n",
      "Iteration Number: 9671\n",
      "Loss: 11.125854085428452\n",
      "l2 norm of gradients: 0.04424623772183183\n",
      "l2 norm of weights: 3.448229374080093\n",
      "---------------------\n",
      "Iteration Number: 9672\n",
      "Loss: 11.125682556314613\n",
      "l2 norm of gradients: 0.04424146943009599\n",
      "l2 norm of weights: 3.448290164387353\n",
      "---------------------\n",
      "Iteration Number: 9673\n",
      "Loss: 11.125511086702753\n",
      "l2 norm of gradients: 0.044236702202588435\n",
      "l2 norm of weights: 3.4483509494967364\n",
      "---------------------\n",
      "Iteration Number: 9674\n",
      "Loss: 11.125339676571047\n",
      "l2 norm of gradients: 0.04423193603886179\n",
      "l2 norm of weights: 3.448411729406935\n",
      "---------------------\n",
      "Iteration Number: 9675\n",
      "Loss: 11.125168325897661\n",
      "l2 norm of gradients: 0.04422717093846904\n",
      "l2 norm of weights: 3.4484725041166415\n",
      "---------------------\n",
      "Iteration Number: 9676\n",
      "Loss: 11.124997034660783\n",
      "l2 norm of gradients: 0.0442224069009636\n",
      "l2 norm of weights: 3.4485332736245495\n",
      "---------------------\n",
      "Iteration Number: 9677\n",
      "Loss: 11.124825802838602\n",
      "l2 norm of gradients: 0.044217643925899275\n",
      "l2 norm of weights: 3.448594037929354\n",
      "---------------------\n",
      "Iteration Number: 9678\n",
      "Loss: 11.124654630409331\n",
      "l2 norm of gradients: 0.04421288201283029\n",
      "l2 norm of weights: 3.44865479702975\n",
      "---------------------\n",
      "Iteration Number: 9679\n",
      "Loss: 11.124483517351175\n",
      "l2 norm of gradients: 0.04420812116131126\n",
      "l2 norm of weights: 3.4487155509244336\n",
      "---------------------\n",
      "Iteration Number: 9680\n",
      "Loss: 11.124312463642365\n",
      "l2 norm of gradients: 0.04420336137089715\n",
      "l2 norm of weights: 3.448776299612101\n",
      "---------------------\n",
      "Iteration Number: 9681\n",
      "Loss: 11.124141469261131\n",
      "l2 norm of gradients: 0.04419860264114345\n",
      "l2 norm of weights: 3.4488370430914514\n",
      "---------------------\n",
      "Iteration Number: 9682\n",
      "Loss: 11.123970534185727\n",
      "l2 norm of gradients: 0.0441938449716059\n",
      "l2 norm of weights: 3.4488977813611816\n",
      "---------------------\n",
      "Iteration Number: 9683\n",
      "Loss: 11.123799658394404\n",
      "l2 norm of gradients: 0.04418908836184072\n",
      "l2 norm of weights: 3.448958514419992\n",
      "---------------------\n",
      "Iteration Number: 9684\n",
      "Loss: 11.123628841865425\n",
      "l2 norm of gradients: 0.044184332811404534\n",
      "l2 norm of weights: 3.4490192422665817\n",
      "---------------------\n",
      "Iteration Number: 9685\n",
      "Loss: 11.12345808457707\n",
      "l2 norm of gradients: 0.04417957831985432\n",
      "l2 norm of weights: 3.4490799648996515\n",
      "---------------------\n",
      "Iteration Number: 9686\n",
      "Loss: 11.12328738650762\n",
      "l2 norm of gradients: 0.04417482488674749\n",
      "l2 norm of weights: 3.4491406823179034\n",
      "---------------------\n",
      "Iteration Number: 9687\n",
      "Loss: 11.12311674763538\n",
      "l2 norm of gradients: 0.044170072511641845\n",
      "l2 norm of weights: 3.4492013945200393\n",
      "---------------------\n",
      "Iteration Number: 9688\n",
      "Loss: 11.122946167938657\n",
      "l2 norm of gradients: 0.04416532119409555\n",
      "l2 norm of weights: 3.449262101504762\n",
      "---------------------\n",
      "Iteration Number: 9689\n",
      "Loss: 11.122775647395752\n",
      "l2 norm of gradients: 0.04416057093366723\n",
      "l2 norm of weights: 3.449322803270776\n",
      "---------------------\n",
      "Iteration Number: 9690\n",
      "Loss: 11.12260518598501\n",
      "l2 norm of gradients: 0.04415582172991587\n",
      "l2 norm of weights: 3.4493834998167854\n",
      "---------------------\n",
      "Iteration Number: 9691\n",
      "Loss: 11.122434783684753\n",
      "l2 norm of gradients: 0.044151073582400835\n",
      "l2 norm of weights: 3.4494441911414957\n",
      "---------------------\n",
      "Iteration Number: 9692\n",
      "Loss: 11.122264440473339\n",
      "l2 norm of gradients: 0.044146326490681925\n",
      "l2 norm of weights: 3.4495048772436134\n",
      "---------------------\n",
      "Iteration Number: 9693\n",
      "Loss: 11.122094156329116\n",
      "l2 norm of gradients: 0.04414158045431928\n",
      "l2 norm of weights: 3.449565558121845\n",
      "---------------------\n",
      "Iteration Number: 9694\n",
      "Loss: 11.121923931230459\n",
      "l2 norm of gradients: 0.044136835472873504\n",
      "l2 norm of weights: 3.4496262337748984\n",
      "---------------------\n",
      "Iteration Number: 9695\n",
      "Loss: 11.121753765155745\n",
      "l2 norm of gradients: 0.04413209154590555\n",
      "l2 norm of weights: 3.449686904201482\n",
      "---------------------\n",
      "Iteration Number: 9696\n",
      "Loss: 11.121583658083347\n",
      "l2 norm of gradients: 0.04412734867297679\n",
      "l2 norm of weights: 3.4497475694003046\n",
      "---------------------\n",
      "Iteration Number: 9697\n",
      "Loss: 11.121413609991684\n",
      "l2 norm of gradients: 0.04412260685364895\n",
      "l2 norm of weights: 3.449808229370077\n",
      "---------------------\n",
      "Iteration Number: 9698\n",
      "Loss: 11.121243620859149\n",
      "l2 norm of gradients: 0.04411786608748423\n",
      "l2 norm of weights: 3.4498688841095095\n",
      "---------------------\n",
      "Iteration Number: 9699\n",
      "Loss: 11.121073690664158\n",
      "l2 norm of gradients: 0.044113126374045124\n",
      "l2 norm of weights: 3.4499295336173144\n",
      "---------------------\n",
      "Iteration Number: 9700\n",
      "Loss: 11.12090381938515\n",
      "l2 norm of gradients: 0.044108387712894596\n",
      "l2 norm of weights: 3.449990177892203\n",
      "---------------------\n",
      "Iteration Number: 9701\n",
      "Loss: 11.12073400700055\n",
      "l2 norm of gradients: 0.04410365010359596\n",
      "l2 norm of weights: 3.4500508169328885\n",
      "---------------------\n",
      "Iteration Number: 9702\n",
      "Loss: 11.120564253488812\n",
      "l2 norm of gradients: 0.04409891354571296\n",
      "l2 norm of weights: 3.4501114507380852\n",
      "---------------------\n",
      "Iteration Number: 9703\n",
      "Loss: 11.120394558828393\n",
      "l2 norm of gradients: 0.0440941780388097\n",
      "l2 norm of weights: 3.4501720793065083\n",
      "---------------------\n",
      "Iteration Number: 9704\n",
      "Loss: 11.120224922997759\n",
      "l2 norm of gradients: 0.044089443582450705\n",
      "l2 norm of weights: 3.4502327026368724\n",
      "---------------------\n",
      "Iteration Number: 9705\n",
      "Loss: 11.120055345975391\n",
      "l2 norm of gradients: 0.04408471017620085\n",
      "l2 norm of weights: 3.450293320727894\n",
      "---------------------\n",
      "Iteration Number: 9706\n",
      "Loss: 11.119885827739768\n",
      "l2 norm of gradients: 0.04407997781962548\n",
      "l2 norm of weights: 3.450353933578289\n",
      "---------------------\n",
      "Iteration Number: 9707\n",
      "Loss: 11.119716368269389\n",
      "l2 norm of gradients: 0.04407524651229023\n",
      "l2 norm of weights: 3.450414541186777\n",
      "---------------------\n",
      "Iteration Number: 9708\n",
      "Loss: 11.119546967542762\n",
      "l2 norm of gradients: 0.044070516253761176\n",
      "l2 norm of weights: 3.450475143552075\n",
      "---------------------\n",
      "Iteration Number: 9709\n",
      "Loss: 11.119377625538409\n",
      "l2 norm of gradients: 0.04406578704360486\n",
      "l2 norm of weights: 3.4505357406729034\n",
      "---------------------\n",
      "Iteration Number: 9710\n",
      "Loss: 11.119208342234856\n",
      "l2 norm of gradients: 0.04406105888138809\n",
      "l2 norm of weights: 3.4505963325479807\n",
      "---------------------\n",
      "Iteration Number: 9711\n",
      "Loss: 11.119039117610626\n",
      "l2 norm of gradients: 0.044056331766678086\n",
      "l2 norm of weights: 3.4506569191760295\n",
      "---------------------\n",
      "Iteration Number: 9712\n",
      "Loss: 11.118869951644289\n",
      "l2 norm of gradients: 0.04405160569904257\n",
      "l2 norm of weights: 3.45071750055577\n",
      "---------------------\n",
      "Iteration Number: 9713\n",
      "Loss: 11.118700844314384\n",
      "l2 norm of gradients: 0.04404688067804952\n",
      "l2 norm of weights: 3.4507780766859253\n",
      "---------------------\n",
      "Iteration Number: 9714\n",
      "Loss: 11.118531795599479\n",
      "l2 norm of gradients: 0.044042156703267395\n",
      "l2 norm of weights: 3.450838647565218\n",
      "---------------------\n",
      "Iteration Number: 9715\n",
      "Loss: 11.118362805478153\n",
      "l2 norm of gradients: 0.04403743377426495\n",
      "l2 norm of weights: 3.4508992131923724\n",
      "---------------------\n",
      "Iteration Number: 9716\n",
      "Loss: 11.118193873928991\n",
      "l2 norm of gradients: 0.04403271189061146\n",
      "l2 norm of weights: 3.4509597735661126\n",
      "---------------------\n",
      "Iteration Number: 9717\n",
      "Loss: 11.118025000930597\n",
      "l2 norm of gradients: 0.04402799105187645\n",
      "l2 norm of weights: 3.451020328685164\n",
      "---------------------\n",
      "Iteration Number: 9718\n",
      "Loss: 11.117856186461566\n",
      "l2 norm of gradients: 0.044023271257629976\n",
      "l2 norm of weights: 3.451080878548254\n",
      "---------------------\n",
      "Iteration Number: 9719\n",
      "Loss: 11.117687430500517\n",
      "l2 norm of gradients: 0.044018552507442334\n",
      "l2 norm of weights: 3.451141423154107\n",
      "---------------------\n",
      "Iteration Number: 9720\n",
      "Loss: 11.117518733026078\n",
      "l2 norm of gradients: 0.04401383480088432\n",
      "l2 norm of weights: 3.451201962501454\n",
      "---------------------\n",
      "Iteration Number: 9721\n",
      "Loss: 11.117350094016878\n",
      "l2 norm of gradients: 0.044009118137527066\n",
      "l2 norm of weights: 3.4512624965890204\n",
      "---------------------\n",
      "Iteration Number: 9722\n",
      "Loss: 11.117181513451566\n",
      "l2 norm of gradients: 0.044004402516942154\n",
      "l2 norm of weights: 3.451323025415537\n",
      "---------------------\n",
      "Iteration Number: 9723\n",
      "Loss: 11.117012991308794\n",
      "l2 norm of gradients: 0.04399968793870144\n",
      "l2 norm of weights: 3.451383548979734\n",
      "---------------------\n",
      "Iteration Number: 9724\n",
      "Loss: 11.116844527567235\n",
      "l2 norm of gradients: 0.04399497440237727\n",
      "l2 norm of weights: 3.4514440672803413\n",
      "---------------------\n",
      "Iteration Number: 9725\n",
      "Loss: 11.116676122205547\n",
      "l2 norm of gradients: 0.043990261907542315\n",
      "l2 norm of weights: 3.4515045803160906\n",
      "---------------------\n",
      "Iteration Number: 9726\n",
      "Loss: 11.116507775202434\n",
      "l2 norm of gradients: 0.04398555045376969\n",
      "l2 norm of weights: 3.4515650880857143\n",
      "---------------------\n",
      "Iteration Number: 9727\n",
      "Loss: 11.116339486536571\n",
      "l2 norm of gradients: 0.04398084004063285\n",
      "l2 norm of weights: 3.4516255905879456\n",
      "---------------------\n",
      "Iteration Number: 9728\n",
      "Loss: 11.116171256186673\n",
      "l2 norm of gradients: 0.04397613066770567\n",
      "l2 norm of weights: 3.451686087821518\n",
      "---------------------\n",
      "Iteration Number: 9729\n",
      "Loss: 11.116003084131448\n",
      "l2 norm of gradients: 0.043971422334562374\n",
      "l2 norm of weights: 3.451746579785166\n",
      "---------------------\n",
      "Iteration Number: 9730\n",
      "Loss: 11.115834970349622\n",
      "l2 norm of gradients: 0.04396671504077757\n",
      "l2 norm of weights: 3.4518070664776253\n",
      "---------------------\n",
      "Iteration Number: 9731\n",
      "Loss: 11.115666914819919\n",
      "l2 norm of gradients: 0.043962008785926314\n",
      "l2 norm of weights: 3.4518675478976317\n",
      "---------------------\n",
      "Iteration Number: 9732\n",
      "Loss: 11.115498917521093\n",
      "l2 norm of gradients: 0.043957303569583994\n",
      "l2 norm of weights: 3.4519280240439225\n",
      "---------------------\n",
      "Iteration Number: 9733\n",
      "Loss: 11.115330978431889\n",
      "l2 norm of gradients: 0.04395259939132636\n",
      "l2 norm of weights: 3.4519884949152346\n",
      "---------------------\n",
      "Iteration Number: 9734\n",
      "Loss: 11.115163097531074\n",
      "l2 norm of gradients: 0.04394789625072965\n",
      "l2 norm of weights: 3.452048960510307\n",
      "---------------------\n",
      "Iteration Number: 9735\n",
      "Loss: 11.114995274797405\n",
      "l2 norm of gradients: 0.04394319414737036\n",
      "l2 norm of weights: 3.452109420827878\n",
      "---------------------\n",
      "Iteration Number: 9736\n",
      "Loss: 11.114827510209679\n",
      "l2 norm of gradients: 0.04393849308082543\n",
      "l2 norm of weights: 3.452169875866688\n",
      "---------------------\n",
      "Iteration Number: 9737\n",
      "Loss: 11.114659803746676\n",
      "l2 norm of gradients: 0.043933793050672215\n",
      "l2 norm of weights: 3.452230325625478\n",
      "---------------------\n",
      "Iteration Number: 9738\n",
      "Loss: 11.11449215538721\n",
      "l2 norm of gradients: 0.043929094056488385\n",
      "l2 norm of weights: 3.4522907701029895\n",
      "---------------------\n",
      "Iteration Number: 9739\n",
      "Loss: 11.114324565110065\n",
      "l2 norm of gradients: 0.043924396097852064\n",
      "l2 norm of weights: 3.4523512092979636\n",
      "---------------------\n",
      "Iteration Number: 9740\n",
      "Loss: 11.11415703289408\n",
      "l2 norm of gradients: 0.04391969917434171\n",
      "l2 norm of weights: 3.452411643209144\n",
      "---------------------\n",
      "Iteration Number: 9741\n",
      "Loss: 11.113989558718085\n",
      "l2 norm of gradients: 0.04391500328553614\n",
      "l2 norm of weights: 3.452472071835274\n",
      "---------------------\n",
      "Iteration Number: 9742\n",
      "Loss: 11.113822142560904\n",
      "l2 norm of gradients: 0.04391030843101466\n",
      "l2 norm of weights: 3.4525324951750984\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 9743\n",
      "Loss: 11.113654784401398\n",
      "l2 norm of gradients: 0.043905614610356844\n",
      "l2 norm of weights: 3.4525929132273623\n",
      "---------------------\n",
      "Iteration Number: 9744\n",
      "Loss: 11.113487484218417\n",
      "l2 norm of gradients: 0.043900921823142716\n",
      "l2 norm of weights: 3.4526533259908114\n",
      "---------------------\n",
      "Iteration Number: 9745\n",
      "Loss: 11.113320241990829\n",
      "l2 norm of gradients: 0.04389623006895261\n",
      "l2 norm of weights: 3.4527137334641935\n",
      "---------------------\n",
      "Iteration Number: 9746\n",
      "Loss: 11.113153057697517\n",
      "l2 norm of gradients: 0.04389153934736738\n",
      "l2 norm of weights: 3.4527741356462545\n",
      "---------------------\n",
      "Iteration Number: 9747\n",
      "Loss: 11.112985931317352\n",
      "l2 norm of gradients: 0.04388684965796809\n",
      "l2 norm of weights: 3.4528345325357432\n",
      "---------------------\n",
      "Iteration Number: 9748\n",
      "Loss: 11.112818862829249\n",
      "l2 norm of gradients: 0.04388216100033633\n",
      "l2 norm of weights: 3.4528949241314093\n",
      "---------------------\n",
      "Iteration Number: 9749\n",
      "Loss: 11.112651852212093\n",
      "l2 norm of gradients: 0.04387747337405396\n",
      "l2 norm of weights: 3.4529553104320017\n",
      "---------------------\n",
      "Iteration Number: 9750\n",
      "Loss: 11.112484899444814\n",
      "l2 norm of gradients: 0.043872786778703286\n",
      "l2 norm of weights: 3.453015691436271\n",
      "---------------------\n",
      "Iteration Number: 9751\n",
      "Loss: 11.112318004506333\n",
      "l2 norm of gradients: 0.043868101213866986\n",
      "l2 norm of weights: 3.4530760671429688\n",
      "---------------------\n",
      "Iteration Number: 9752\n",
      "Loss: 11.11215116737558\n",
      "l2 norm of gradients: 0.04386341667912809\n",
      "l2 norm of weights: 3.4531364375508473\n",
      "---------------------\n",
      "Iteration Number: 9753\n",
      "Loss: 11.111984388031495\n",
      "l2 norm of gradients: 0.04385873317407004\n",
      "l2 norm of weights: 3.453196802658659\n",
      "---------------------\n",
      "Iteration Number: 9754\n",
      "Loss: 11.111817666453037\n",
      "l2 norm of gradients: 0.04385405069827665\n",
      "l2 norm of weights: 3.4532571624651567\n",
      "---------------------\n",
      "Iteration Number: 9755\n",
      "Loss: 11.111651002619165\n",
      "l2 norm of gradients: 0.04384936925133208\n",
      "l2 norm of weights: 3.453317516969096\n",
      "---------------------\n",
      "Iteration Number: 9756\n",
      "Loss: 11.111484396508839\n",
      "l2 norm of gradients: 0.04384468883282093\n",
      "l2 norm of weights: 3.4533778661692316\n",
      "---------------------\n",
      "Iteration Number: 9757\n",
      "Loss: 11.111317848101066\n",
      "l2 norm of gradients: 0.04384000944232811\n",
      "l2 norm of weights: 3.453438210064318\n",
      "---------------------\n",
      "Iteration Number: 9758\n",
      "Loss: 11.111151357374816\n",
      "l2 norm of gradients: 0.04383533107943897\n",
      "l2 norm of weights: 3.453498548653114\n",
      "---------------------\n",
      "Iteration Number: 9759\n",
      "Loss: 11.11098492430909\n",
      "l2 norm of gradients: 0.0438306537437392\n",
      "l2 norm of weights: 3.453558881934375\n",
      "---------------------\n",
      "Iteration Number: 9760\n",
      "Loss: 11.110818548882904\n",
      "l2 norm of gradients: 0.043825977434814874\n",
      "l2 norm of weights: 3.4536192099068597\n",
      "---------------------\n",
      "Iteration Number: 9761\n",
      "Loss: 11.110652231075264\n",
      "l2 norm of gradients: 0.043821302152252474\n",
      "l2 norm of weights: 3.453679532569327\n",
      "---------------------\n",
      "Iteration Number: 9762\n",
      "Loss: 11.110485970865213\n",
      "l2 norm of gradients: 0.04381662789563881\n",
      "l2 norm of weights: 3.4537398499205363\n",
      "---------------------\n",
      "Iteration Number: 9763\n",
      "Loss: 11.110319768231772\n",
      "l2 norm of gradients: 0.043811954664561094\n",
      "l2 norm of weights: 3.453800161959249\n",
      "---------------------\n",
      "Iteration Number: 9764\n",
      "Loss: 11.110153623154009\n",
      "l2 norm of gradients: 0.04380728245860691\n",
      "l2 norm of weights: 3.453860468684224\n",
      "---------------------\n",
      "Iteration Number: 9765\n",
      "Loss: 11.109987535610959\n",
      "l2 norm of gradients: 0.043802611277364206\n",
      "l2 norm of weights: 3.453920770094225\n",
      "---------------------\n",
      "Iteration Number: 9766\n",
      "Loss: 11.109821505581694\n",
      "l2 norm of gradients: 0.04379794112042137\n",
      "l2 norm of weights: 3.453981066188014\n",
      "---------------------\n",
      "Iteration Number: 9767\n",
      "Loss: 11.10965553304529\n",
      "l2 norm of gradients: 0.04379327198736708\n",
      "l2 norm of weights: 3.454041356964354\n",
      "---------------------\n",
      "Iteration Number: 9768\n",
      "Loss: 11.109489617980826\n",
      "l2 norm of gradients: 0.043788603877790426\n",
      "l2 norm of weights: 3.4541016424220095\n",
      "---------------------\n",
      "Iteration Number: 9769\n",
      "Loss: 11.1093237603674\n",
      "l2 norm of gradients: 0.04378393679128089\n",
      "l2 norm of weights: 3.4541619225597455\n",
      "---------------------\n",
      "Iteration Number: 9770\n",
      "Loss: 11.109157960184113\n",
      "l2 norm of gradients: 0.04377927072742832\n",
      "l2 norm of weights: 3.4542221973763274\n",
      "---------------------\n",
      "Iteration Number: 9771\n",
      "Loss: 11.108992217410076\n",
      "l2 norm of gradients: 0.0437746056858229\n",
      "l2 norm of weights: 3.4542824668705214\n",
      "---------------------\n",
      "Iteration Number: 9772\n",
      "Loss: 11.10882653202441\n",
      "l2 norm of gradients: 0.04376994166605526\n",
      "l2 norm of weights: 3.4543427310410952\n",
      "---------------------\n",
      "Iteration Number: 9773\n",
      "Loss: 11.10866090400624\n",
      "l2 norm of gradients: 0.04376527866771634\n",
      "l2 norm of weights: 3.4544029898868156\n",
      "---------------------\n",
      "Iteration Number: 9774\n",
      "Loss: 11.108495333334712\n",
      "l2 norm of gradients: 0.043760616690397484\n",
      "l2 norm of weights: 3.4544632434064524\n",
      "---------------------\n",
      "Iteration Number: 9775\n",
      "Loss: 11.108329819988972\n",
      "l2 norm of gradients: 0.043755955733690406\n",
      "l2 norm of weights: 3.4545234915987737\n",
      "---------------------\n",
      "Iteration Number: 9776\n",
      "Loss: 11.10816436394818\n",
      "l2 norm of gradients: 0.043751295797187194\n",
      "l2 norm of weights: 3.4545837344625507\n",
      "---------------------\n",
      "Iteration Number: 9777\n",
      "Loss: 11.1079989651915\n",
      "l2 norm of gradients: 0.04374663688048029\n",
      "l2 norm of weights: 3.4546439719965543\n",
      "---------------------\n",
      "Iteration Number: 9778\n",
      "Loss: 11.107833623698106\n",
      "l2 norm of gradients: 0.04374197898316258\n",
      "l2 norm of weights: 3.454704204199555\n",
      "---------------------\n",
      "Iteration Number: 9779\n",
      "Loss: 11.107668339447184\n",
      "l2 norm of gradients: 0.04373732210482721\n",
      "l2 norm of weights: 3.454764431070326\n",
      "---------------------\n",
      "Iteration Number: 9780\n",
      "Loss: 11.107503112417932\n",
      "l2 norm of gradients: 0.04373266624506778\n",
      "l2 norm of weights: 3.4548246526076407\n",
      "---------------------\n",
      "Iteration Number: 9781\n",
      "Loss: 11.10733794258955\n",
      "l2 norm of gradients: 0.043728011403478255\n",
      "l2 norm of weights: 3.4548848688102725\n",
      "---------------------\n",
      "Iteration Number: 9782\n",
      "Loss: 11.107172829941264\n",
      "l2 norm of gradients: 0.04372335757965294\n",
      "l2 norm of weights: 3.454945079676996\n",
      "---------------------\n",
      "Iteration Number: 9783\n",
      "Loss: 11.107007774452272\n",
      "l2 norm of gradients: 0.04371870477318653\n",
      "l2 norm of weights: 3.4550052852065867\n",
      "---------------------\n",
      "Iteration Number: 9784\n",
      "Loss: 11.106842776101821\n",
      "l2 norm of gradients: 0.04371405298367411\n",
      "l2 norm of weights: 3.455065485397821\n",
      "---------------------\n",
      "Iteration Number: 9785\n",
      "Loss: 11.10667783486915\n",
      "l2 norm of gradients: 0.04370940221071109\n",
      "l2 norm of weights: 3.455125680249475\n",
      "---------------------\n",
      "Iteration Number: 9786\n",
      "Loss: 11.106512950733508\n",
      "l2 norm of gradients: 0.043704752453893256\n",
      "l2 norm of weights: 3.4551858697603275\n",
      "---------------------\n",
      "Iteration Number: 9787\n",
      "Loss: 11.10634812367415\n",
      "l2 norm of gradients: 0.04370010371281684\n",
      "l2 norm of weights: 3.455246053929156\n",
      "---------------------\n",
      "Iteration Number: 9788\n",
      "Loss: 11.106183353670346\n",
      "l2 norm of gradients: 0.04369545598707837\n",
      "l2 norm of weights: 3.45530623275474\n",
      "---------------------\n",
      "Iteration Number: 9789\n",
      "Loss: 11.106018640701375\n",
      "l2 norm of gradients: 0.04369080927627474\n",
      "l2 norm of weights: 3.4553664062358593\n",
      "---------------------\n",
      "Iteration Number: 9790\n",
      "Loss: 11.105853984746513\n",
      "l2 norm of gradients: 0.04368616358000326\n",
      "l2 norm of weights: 3.4554265743712946\n",
      "---------------------\n",
      "Iteration Number: 9791\n",
      "Loss: 11.10568938578507\n",
      "l2 norm of gradients: 0.04368151889786158\n",
      "l2 norm of weights: 3.4554867371598266\n",
      "---------------------\n",
      "Iteration Number: 9792\n",
      "Loss: 11.10552484379634\n",
      "l2 norm of gradients: 0.04367687522944773\n",
      "l2 norm of weights: 3.4555468946002383\n",
      "---------------------\n",
      "Iteration Number: 9793\n",
      "Loss: 11.105360358759636\n",
      "l2 norm of gradients: 0.04367223257436011\n",
      "l2 norm of weights: 3.4556070466913127\n",
      "---------------------\n",
      "Iteration Number: 9794\n",
      "Loss: 11.105195930654284\n",
      "l2 norm of gradients: 0.043667590932197464\n",
      "l2 norm of weights: 3.455667193431833\n",
      "---------------------\n",
      "Iteration Number: 9795\n",
      "Loss: 11.105031559459619\n",
      "l2 norm of gradients: 0.043662950302558956\n",
      "l2 norm of weights: 3.4557273348205833\n",
      "---------------------\n",
      "Iteration Number: 9796\n",
      "Loss: 11.10486724515497\n",
      "l2 norm of gradients: 0.04365831068504406\n",
      "l2 norm of weights: 3.4557874708563494\n",
      "---------------------\n",
      "Iteration Number: 9797\n",
      "Loss: 11.104702987719687\n",
      "l2 norm of gradients: 0.043653672079252664\n",
      "l2 norm of weights: 3.4558476015379167\n",
      "---------------------\n",
      "Iteration Number: 9798\n",
      "Loss: 11.104538787133144\n",
      "l2 norm of gradients: 0.043649034484784965\n",
      "l2 norm of weights: 3.455907726864072\n",
      "---------------------\n",
      "Iteration Number: 9799\n",
      "Loss: 11.104374643374697\n",
      "l2 norm of gradients: 0.04364439790124162\n",
      "l2 norm of weights: 3.4559678468336026\n",
      "---------------------\n",
      "Iteration Number: 9800\n",
      "Loss: 11.104210556423714\n",
      "l2 norm of gradients: 0.04363976232822355\n",
      "l2 norm of weights: 3.456027961445297\n",
      "---------------------\n",
      "Iteration Number: 9801\n",
      "Loss: 11.10404652625959\n",
      "l2 norm of gradients: 0.043635127765332106\n",
      "l2 norm of weights: 3.4560880706979438\n",
      "---------------------\n",
      "Iteration Number: 9802\n",
      "Loss: 11.103882552861721\n",
      "l2 norm of gradients: 0.04363049421216901\n",
      "l2 norm of weights: 3.456148174590332\n",
      "---------------------\n",
      "Iteration Number: 9803\n",
      "Loss: 11.103718636209509\n",
      "l2 norm of gradients: 0.043625861668336326\n",
      "l2 norm of weights: 3.4562082731212542\n",
      "---------------------\n",
      "Iteration Number: 9804\n",
      "Loss: 11.103554776282362\n",
      "l2 norm of gradients: 0.04362123013343648\n",
      "l2 norm of weights: 3.456268366289499\n",
      "---------------------\n",
      "Iteration Number: 9805\n",
      "Loss: 11.103390973059701\n",
      "l2 norm of gradients: 0.04361659960707228\n",
      "l2 norm of weights: 3.456328454093859\n",
      "---------------------\n",
      "Iteration Number: 9806\n",
      "Loss: 11.103227226520957\n",
      "l2 norm of gradients: 0.0436119700888469\n",
      "l2 norm of weights: 3.456388536533127\n",
      "---------------------\n",
      "Iteration Number: 9807\n",
      "Loss: 11.103063536645571\n",
      "l2 norm of gradients: 0.04360734157836384\n",
      "l2 norm of weights: 3.456448613606097\n",
      "---------------------\n",
      "Iteration Number: 9808\n",
      "Loss: 11.10289990341299\n",
      "l2 norm of gradients: 0.04360271407522704\n",
      "l2 norm of weights: 3.4565086853115625\n",
      "---------------------\n",
      "Iteration Number: 9809\n",
      "Loss: 11.102736326802672\n",
      "l2 norm of gradients: 0.043598087579040726\n",
      "l2 norm of weights: 3.456568751648318\n",
      "---------------------\n",
      "Iteration Number: 9810\n",
      "Loss: 11.102572806794083\n",
      "l2 norm of gradients: 0.043593462089409535\n",
      "l2 norm of weights: 3.4566288126151603\n",
      "---------------------\n",
      "Iteration Number: 9811\n",
      "Loss: 11.102409343366691\n",
      "l2 norm of gradients: 0.04358883760593848\n",
      "l2 norm of weights: 3.4566888682108843\n",
      "---------------------\n",
      "Iteration Number: 9812\n",
      "Loss: 11.102245936499985\n",
      "l2 norm of gradients: 0.0435842141282329\n",
      "l2 norm of weights: 3.4567489184342883\n",
      "---------------------\n",
      "Iteration Number: 9813\n",
      "Loss: 11.102082586173458\n",
      "l2 norm of gradients: 0.04357959165589849\n",
      "l2 norm of weights: 3.45680896328417\n",
      "---------------------\n",
      "Iteration Number: 9814\n",
      "Loss: 11.101919292366604\n",
      "l2 norm of gradients: 0.04357497018854136\n",
      "l2 norm of weights: 3.456869002759327\n",
      "---------------------\n",
      "Iteration Number: 9815\n",
      "Loss: 11.101756055058939\n",
      "l2 norm of gradients: 0.043570349725767954\n",
      "l2 norm of weights: 3.45692903685856\n",
      "---------------------\n",
      "Iteration Number: 9816\n",
      "Loss: 11.101592874229986\n",
      "l2 norm of gradients: 0.04356573026718504\n",
      "l2 norm of weights: 3.456989065580668\n",
      "---------------------\n",
      "Iteration Number: 9817\n",
      "Loss: 11.10142974985926\n",
      "l2 norm of gradients: 0.04356111181239985\n",
      "l2 norm of weights: 3.4570490889244527\n",
      "---------------------\n",
      "Iteration Number: 9818\n",
      "Loss: 11.10126668192631\n",
      "l2 norm of gradients: 0.04355649436101987\n",
      "l2 norm of weights: 3.4571091068887156\n",
      "---------------------\n",
      "Iteration Number: 9819\n",
      "Loss: 11.101103670410671\n",
      "l2 norm of gradients: 0.043551877912653\n",
      "l2 norm of weights: 3.4571691194722587\n",
      "---------------------\n",
      "Iteration Number: 9820\n",
      "Loss: 11.100940715291904\n",
      "l2 norm of gradients: 0.043547262466907495\n",
      "l2 norm of weights: 3.457229126673885\n",
      "---------------------\n",
      "Iteration Number: 9821\n",
      "Loss: 11.100777816549568\n",
      "l2 norm of gradients: 0.04354264802339197\n",
      "l2 norm of weights: 3.4572891284923988\n",
      "---------------------\n",
      "Iteration Number: 9822\n",
      "Loss: 11.100614974163243\n",
      "l2 norm of gradients: 0.04353803458171543\n",
      "l2 norm of weights: 3.4573491249266044\n",
      "---------------------\n",
      "Iteration Number: 9823\n",
      "Loss: 11.10045218811249\n",
      "l2 norm of gradients: 0.043533422141487164\n",
      "l2 norm of weights: 3.457409115975307\n",
      "---------------------\n",
      "Iteration Number: 9824\n",
      "Loss: 11.10028945837692\n",
      "l2 norm of gradients: 0.043528810702316914\n",
      "l2 norm of weights: 3.4574691016373134\n",
      "---------------------\n",
      "Iteration Number: 9825\n",
      "Loss: 11.100126784936123\n",
      "l2 norm of gradients: 0.043524200263814726\n",
      "l2 norm of weights: 3.4575290819114297\n",
      "---------------------\n",
      "Iteration Number: 9826\n",
      "Loss: 11.099964167769697\n",
      "l2 norm of gradients: 0.04351959082559103\n",
      "l2 norm of weights: 3.457589056796464\n",
      "---------------------\n",
      "Iteration Number: 9827\n",
      "Loss: 11.09980160685727\n",
      "l2 norm of gradients: 0.04351498238725658\n",
      "l2 norm of weights: 3.4576490262912243\n",
      "---------------------\n",
      "Iteration Number: 9828\n",
      "Loss: 11.099639102178452\n",
      "l2 norm of gradients: 0.04351037494842254\n",
      "l2 norm of weights: 3.4577089903945195\n",
      "---------------------\n",
      "Iteration Number: 9829\n",
      "Loss: 11.099476653712887\n",
      "l2 norm of gradients: 0.04350576850870038\n",
      "l2 norm of weights: 3.4577689491051595\n",
      "---------------------\n",
      "Iteration Number: 9830\n",
      "Loss: 11.099314261440215\n",
      "l2 norm of gradients: 0.04350116306770199\n",
      "l2 norm of weights: 3.4578289024219555\n",
      "---------------------\n",
      "Iteration Number: 9831\n",
      "Loss: 11.099151925340088\n",
      "l2 norm of gradients: 0.04349655862503955\n",
      "l2 norm of weights: 3.4578888503437186\n",
      "---------------------\n",
      "Iteration Number: 9832\n",
      "Loss: 11.09898964539216\n",
      "l2 norm of gradients: 0.04349195518032568\n",
      "l2 norm of weights: 3.4579487928692605\n",
      "---------------------\n",
      "Iteration Number: 9833\n",
      "Loss: 11.098827421576097\n",
      "l2 norm of gradients: 0.04348735273317326\n",
      "l2 norm of weights: 3.4580087299973936\n",
      "---------------------\n",
      "Iteration Number: 9834\n",
      "Loss: 11.098665253871587\n",
      "l2 norm of gradients: 0.04348275128319562\n",
      "l2 norm of weights: 3.4580686617269327\n",
      "---------------------\n",
      "Iteration Number: 9835\n",
      "Loss: 11.098503142258295\n",
      "l2 norm of gradients: 0.04347815083000639\n",
      "l2 norm of weights: 3.4581285880566917\n",
      "---------------------\n",
      "Iteration Number: 9836\n",
      "Loss: 11.09834108671593\n",
      "l2 norm of gradients: 0.04347355137321958\n",
      "l2 norm of weights: 3.4581885089854847\n",
      "---------------------\n",
      "Iteration Number: 9837\n",
      "Loss: 11.098179087224192\n",
      "l2 norm of gradients: 0.04346895291244956\n",
      "l2 norm of weights: 3.4582484245121288\n",
      "---------------------\n",
      "Iteration Number: 9838\n",
      "Loss: 11.098017143762785\n",
      "l2 norm of gradients: 0.04346435544731103\n",
      "l2 norm of weights: 3.4583083346354395\n",
      "---------------------\n",
      "Iteration Number: 9839\n",
      "Loss: 11.097855256311437\n",
      "l2 norm of gradients: 0.04345975897741909\n",
      "l2 norm of weights: 3.458368239354235\n",
      "---------------------\n",
      "Iteration Number: 9840\n",
      "Loss: 11.097693424849869\n",
      "l2 norm of gradients: 0.043455163502389156\n",
      "l2 norm of weights: 3.458428138667333\n",
      "---------------------\n",
      "Iteration Number: 9841\n",
      "Loss: 11.09753164935782\n",
      "l2 norm of gradients: 0.04345056902183702\n",
      "l2 norm of weights: 3.4584880325735523\n",
      "---------------------\n",
      "Iteration Number: 9842\n",
      "Loss: 11.097369929815033\n",
      "l2 norm of gradients: 0.04344597553537882\n",
      "l2 norm of weights: 3.458547921071712\n",
      "---------------------\n",
      "Iteration Number: 9843\n",
      "Loss: 11.097208266201271\n",
      "l2 norm of gradients: 0.04344138304263106\n",
      "l2 norm of weights: 3.458607804160633\n",
      "---------------------\n",
      "Iteration Number: 9844\n",
      "Loss: 11.097046658496284\n",
      "l2 norm of gradients: 0.04343679154321061\n",
      "l2 norm of weights: 3.458667681839136\n",
      "---------------------\n",
      "Iteration Number: 9845\n",
      "Loss: 11.096885106679855\n",
      "l2 norm of gradients: 0.04343220103673465\n",
      "l2 norm of weights: 3.458727554106043\n",
      "---------------------\n",
      "Iteration Number: 9846\n",
      "Loss: 11.096723610731752\n",
      "l2 norm of gradients: 0.04342761152282077\n",
      "l2 norm of weights: 3.4587874209601766\n",
      "---------------------\n",
      "Iteration Number: 9847\n",
      "Loss: 11.096562170631767\n",
      "l2 norm of gradients: 0.04342302300108689\n",
      "l2 norm of weights: 3.4588472824003595\n",
      "---------------------\n",
      "Iteration Number: 9848\n",
      "Loss: 11.096400786359698\n",
      "l2 norm of gradients: 0.04341843547115126\n",
      "l2 norm of weights: 3.4589071384254164\n",
      "---------------------\n",
      "Iteration Number: 9849\n",
      "Loss: 11.09623945789535\n",
      "l2 norm of gradients: 0.04341384893263251\n",
      "l2 norm of weights: 3.458966989034172\n",
      "---------------------\n",
      "Iteration Number: 9850\n",
      "Loss: 11.096078185218534\n",
      "l2 norm of gradients: 0.04340926338514964\n",
      "l2 norm of weights: 3.4590268342254515\n",
      "---------------------\n",
      "Iteration Number: 9851\n",
      "Loss: 11.095916968309076\n",
      "l2 norm of gradients: 0.04340467882832198\n",
      "l2 norm of weights: 3.4590866739980815\n",
      "---------------------\n",
      "Iteration Number: 9852\n",
      "Loss: 11.0957558071468\n",
      "l2 norm of gradients: 0.0434000952617692\n",
      "l2 norm of weights: 3.459146508350888\n",
      "---------------------\n",
      "Iteration Number: 9853\n",
      "Loss: 11.095594701711557\n",
      "l2 norm of gradients: 0.04339551268511134\n",
      "l2 norm of weights: 3.459206337282701\n",
      "---------------------\n",
      "Iteration Number: 9854\n",
      "Loss: 11.095433651983186\n",
      "l2 norm of gradients: 0.043390931097968835\n",
      "l2 norm of weights: 3.459266160792347\n",
      "---------------------\n",
      "Iteration Number: 9855\n",
      "Loss: 11.095272657941543\n",
      "l2 norm of gradients: 0.04338635049996237\n",
      "l2 norm of weights: 3.4593259788786552\n",
      "---------------------\n",
      "Iteration Number: 9856\n",
      "Loss: 11.095111719566491\n",
      "l2 norm of gradients: 0.043381770890713066\n",
      "l2 norm of weights: 3.459385791540457\n",
      "---------------------\n",
      "Iteration Number: 9857\n",
      "Loss: 11.094950836837906\n",
      "l2 norm of gradients: 0.04337719226984239\n",
      "l2 norm of weights: 3.459445598776582\n",
      "---------------------\n",
      "Iteration Number: 9858\n",
      "Loss: 11.094790009735673\n",
      "l2 norm of gradients: 0.043372614636972125\n",
      "l2 norm of weights: 3.4595054005858623\n",
      "---------------------\n",
      "Iteration Number: 9859\n",
      "Loss: 11.09462923823967\n",
      "l2 norm of gradients: 0.04336803799172441\n",
      "l2 norm of weights: 3.4595651969671297\n",
      "---------------------\n",
      "Iteration Number: 9860\n",
      "Loss: 11.094468522329809\n",
      "l2 norm of gradients: 0.043363462333721764\n",
      "l2 norm of weights: 3.459624987919218\n",
      "---------------------\n",
      "Iteration Number: 9861\n",
      "Loss: 11.094307861985987\n",
      "l2 norm of gradients: 0.04335888766258702\n",
      "l2 norm of weights: 3.45968477344096\n",
      "---------------------\n",
      "Iteration Number: 9862\n",
      "Loss: 11.094147257188128\n",
      "l2 norm of gradients: 0.04335431397794339\n",
      "l2 norm of weights: 3.4597445535311904\n",
      "---------------------\n",
      "Iteration Number: 9863\n",
      "Loss: 11.093986707916146\n",
      "l2 norm of gradients: 0.04334974127941444\n",
      "l2 norm of weights: 3.4598043281887447\n",
      "---------------------\n",
      "Iteration Number: 9864\n",
      "Loss: 11.093826214149976\n",
      "l2 norm of gradients: 0.04334516956662408\n",
      "l2 norm of weights: 3.459864097412459\n",
      "---------------------\n",
      "Iteration Number: 9865\n",
      "Loss: 11.093665775869553\n",
      "l2 norm of gradients: 0.04334059883919651\n",
      "l2 norm of weights: 3.45992386120117\n",
      "---------------------\n",
      "Iteration Number: 9866\n",
      "Loss: 11.093505393054844\n",
      "l2 norm of gradients: 0.043336029096756386\n",
      "l2 norm of weights: 3.4599836195537144\n",
      "---------------------\n",
      "Iteration Number: 9867\n",
      "Loss: 11.09334506568578\n",
      "l2 norm of gradients: 0.04333146033892865\n",
      "l2 norm of weights: 3.4600433724689315\n",
      "---------------------\n",
      "Iteration Number: 9868\n",
      "Loss: 11.093184793742346\n",
      "l2 norm of gradients: 0.04332689256533857\n",
      "l2 norm of weights: 3.4601031199456593\n",
      "---------------------\n",
      "Iteration Number: 9869\n",
      "Loss: 11.093024577204512\n",
      "l2 norm of gradients: 0.04332232577561183\n",
      "l2 norm of weights: 3.460162861982738\n",
      "---------------------\n",
      "Iteration Number: 9870\n",
      "Loss: 11.092864416052251\n",
      "l2 norm of gradients: 0.043317759969374384\n",
      "l2 norm of weights: 3.4602225985790085\n",
      "---------------------\n",
      "Iteration Number: 9871\n",
      "Loss: 11.09270431026556\n",
      "l2 norm of gradients: 0.043313195146252606\n",
      "l2 norm of weights: 3.460282329733312\n",
      "---------------------\n",
      "Iteration Number: 9872\n",
      "Loss: 11.092544259824438\n",
      "l2 norm of gradients: 0.04330863130587319\n",
      "l2 norm of weights: 3.4603420554444897\n",
      "---------------------\n",
      "Iteration Number: 9873\n",
      "Loss: 11.092384264708896\n",
      "l2 norm of gradients: 0.043304068447863144\n",
      "l2 norm of weights: 3.4604017757113845\n",
      "---------------------\n",
      "Iteration Number: 9874\n",
      "Loss: 11.09222432489894\n",
      "l2 norm of gradients: 0.04329950657184988\n",
      "l2 norm of weights: 3.4604614905328397\n",
      "---------------------\n",
      "Iteration Number: 9875\n",
      "Loss: 11.092064440374596\n",
      "l2 norm of gradients: 0.04329494567746112\n",
      "l2 norm of weights: 3.4605211999077006\n",
      "---------------------\n",
      "Iteration Number: 9876\n",
      "Loss: 11.091904611115893\n",
      "l2 norm of gradients: 0.043290385764324944\n",
      "l2 norm of weights: 3.460580903834811\n",
      "---------------------\n",
      "Iteration Number: 9877\n",
      "Loss: 11.091744837102882\n",
      "l2 norm of gradients: 0.0432858268320698\n",
      "l2 norm of weights: 3.4606406023130174\n",
      "---------------------\n",
      "Iteration Number: 9878\n",
      "Loss: 11.0915851183156\n",
      "l2 norm of gradients: 0.043281268880324415\n",
      "l2 norm of weights: 3.460700295341166\n",
      "---------------------\n",
      "Iteration Number: 9879\n",
      "Loss: 11.091425454734111\n",
      "l2 norm of gradients: 0.04327671190871793\n",
      "l2 norm of weights: 3.4607599829181033\n",
      "---------------------\n",
      "Iteration Number: 9880\n",
      "Loss: 11.09126584633847\n",
      "l2 norm of gradients: 0.04327215591687983\n",
      "l2 norm of weights: 3.460819665042678\n",
      "---------------------\n",
      "Iteration Number: 9881\n",
      "Loss: 11.091106293108762\n",
      "l2 norm of gradients: 0.043267600904439873\n",
      "l2 norm of weights: 3.460879341713739\n",
      "---------------------\n",
      "Iteration Number: 9882\n",
      "Loss: 11.090946795025062\n",
      "l2 norm of gradients: 0.04326304687102828\n",
      "l2 norm of weights: 3.460939012930135\n",
      "---------------------\n",
      "Iteration Number: 9883\n",
      "Loss: 11.090787352067458\n",
      "l2 norm of gradients: 0.04325849381627546\n",
      "l2 norm of weights: 3.4609986786907165\n",
      "---------------------\n",
      "Iteration Number: 9884\n",
      "Loss: 11.090627964216052\n",
      "l2 norm of gradients: 0.04325394173981232\n",
      "l2 norm of weights: 3.461058338994335\n",
      "---------------------\n",
      "Iteration Number: 9885\n",
      "Loss: 11.090468631450943\n",
      "l2 norm of gradients: 0.043249390641270015\n",
      "l2 norm of weights: 3.4611179938398413\n",
      "---------------------\n",
      "Iteration Number: 9886\n",
      "Loss: 11.090309353752252\n",
      "l2 norm of gradients: 0.0432448405202801\n",
      "l2 norm of weights: 3.461177643226088\n",
      "---------------------\n",
      "Iteration Number: 9887\n",
      "Loss: 11.0901501311001\n",
      "l2 norm of gradients: 0.04324029137647443\n",
      "l2 norm of weights: 3.4612372871519295\n",
      "---------------------\n",
      "Iteration Number: 9888\n",
      "Loss: 11.08999096347461\n",
      "l2 norm of gradients: 0.043235743209485214\n",
      "l2 norm of weights: 3.4612969256162183\n",
      "---------------------\n",
      "Iteration Number: 9889\n",
      "Loss: 11.089831850855933\n",
      "l2 norm of gradients: 0.04323119601894503\n",
      "l2 norm of weights: 3.461356558617809\n",
      "---------------------\n",
      "Iteration Number: 9890\n",
      "Loss: 11.089672793224203\n",
      "l2 norm of gradients: 0.043226649804486775\n",
      "l2 norm of weights: 3.4614161861555583\n",
      "---------------------\n",
      "Iteration Number: 9891\n",
      "Loss: 11.089513790559586\n",
      "l2 norm of gradients: 0.043222104565743706\n",
      "l2 norm of weights: 3.4614758082283217\n",
      "---------------------\n",
      "Iteration Number: 9892\n",
      "Loss: 11.089354842842237\n",
      "l2 norm of gradients: 0.04321756030234935\n",
      "l2 norm of weights: 3.461535424834956\n",
      "---------------------\n",
      "Iteration Number: 9893\n",
      "Loss: 11.089195950052332\n",
      "l2 norm of gradients: 0.04321301701393771\n",
      "l2 norm of weights: 3.461595035974319\n",
      "---------------------\n",
      "Iteration Number: 9894\n",
      "Loss: 11.08903711217005\n",
      "l2 norm of gradients: 0.043208474700143024\n",
      "l2 norm of weights: 3.4616546416452696\n",
      "---------------------\n",
      "Iteration Number: 9895\n",
      "Loss: 11.088878329175568\n",
      "l2 norm of gradients: 0.043203933360599904\n",
      "l2 norm of weights: 3.4617142418466664\n",
      "---------------------\n",
      "Iteration Number: 9896\n",
      "Loss: 11.088719601049092\n",
      "l2 norm of gradients: 0.04319939299494329\n",
      "l2 norm of weights: 3.4617738365773696\n",
      "---------------------\n",
      "Iteration Number: 9897\n",
      "Loss: 11.088560927770827\n",
      "l2 norm of gradients: 0.04319485360280849\n",
      "l2 norm of weights: 3.461833425836239\n",
      "---------------------\n",
      "Iteration Number: 9898\n",
      "Loss: 11.088402309320976\n",
      "l2 norm of gradients: 0.04319031518383115\n",
      "l2 norm of weights: 3.461893009622138\n",
      "---------------------\n",
      "Iteration Number: 9899\n",
      "Loss: 11.088243745679767\n",
      "l2 norm of gradients: 0.04318577773764721\n",
      "l2 norm of weights: 3.461952587933927\n",
      "---------------------\n",
      "Iteration Number: 9900\n",
      "Loss: 11.088085236827423\n",
      "l2 norm of gradients: 0.04318124126389303\n",
      "l2 norm of weights: 3.4620121607704695\n",
      "---------------------\n",
      "Iteration Number: 9901\n",
      "Loss: 11.087926782744173\n",
      "l2 norm of gradients: 0.043176705762205225\n",
      "l2 norm of weights: 3.4620717281306295\n",
      "---------------------\n",
      "Iteration Number: 9902\n",
      "Loss: 11.087768383410278\n",
      "l2 norm of gradients: 0.0431721712322208\n",
      "l2 norm of weights: 3.4621312900132715\n",
      "---------------------\n",
      "Iteration Number: 9903\n",
      "Loss: 11.087610038805979\n",
      "l2 norm of gradients: 0.0431676376735771\n",
      "l2 norm of weights: 3.46219084641726\n",
      "---------------------\n",
      "Iteration Number: 9904\n",
      "Loss: 11.08745174891153\n",
      "l2 norm of gradients: 0.04316310508591178\n",
      "l2 norm of weights: 3.4622503973414616\n",
      "---------------------\n",
      "Iteration Number: 9905\n",
      "Loss: 11.087293513707207\n",
      "l2 norm of gradients: 0.043158573468862864\n",
      "l2 norm of weights: 3.462309942784742\n",
      "---------------------\n",
      "Iteration Number: 9906\n",
      "Loss: 11.087135333173288\n",
      "l2 norm of gradients: 0.04315404282206869\n",
      "l2 norm of weights: 3.4623694827459706\n",
      "---------------------\n",
      "Iteration Number: 9907\n",
      "Loss: 11.086977207290055\n",
      "l2 norm of gradients: 0.043149513145167934\n",
      "l2 norm of weights: 3.4624290172240135\n",
      "---------------------\n",
      "Iteration Number: 9908\n",
      "Loss: 11.086819136037793\n",
      "l2 norm of gradients: 0.043144984437799655\n",
      "l2 norm of weights: 3.462488546217741\n",
      "---------------------\n",
      "Iteration Number: 9909\n",
      "Loss: 11.08666111939681\n",
      "l2 norm of gradients: 0.043140456699603175\n",
      "l2 norm of weights: 3.462548069726022\n",
      "---------------------\n",
      "Iteration Number: 9910\n",
      "Loss: 11.086503157347407\n",
      "l2 norm of gradients: 0.043135929930218246\n",
      "l2 norm of weights: 3.462607587747727\n",
      "---------------------\n",
      "Iteration Number: 9911\n",
      "Loss: 11.08634524986991\n",
      "l2 norm of gradients: 0.04313140412928484\n",
      "l2 norm of weights: 3.462667100281728\n",
      "---------------------\n",
      "Iteration Number: 9912\n",
      "Loss: 11.08618739694463\n",
      "l2 norm of gradients: 0.043126879296443384\n",
      "l2 norm of weights: 3.462726607326896\n",
      "---------------------\n",
      "Iteration Number: 9913\n",
      "Loss: 11.086029598551903\n",
      "l2 norm of gradients: 0.043122355431334546\n",
      "l2 norm of weights: 3.4627861088821046\n",
      "---------------------\n",
      "Iteration Number: 9914\n",
      "Loss: 11.085871854672074\n",
      "l2 norm of gradients: 0.043117832533599396\n",
      "l2 norm of weights: 3.462845604946226\n",
      "---------------------\n",
      "Iteration Number: 9915\n",
      "Loss: 11.085714165285486\n",
      "l2 norm of gradients: 0.04311331060287931\n",
      "l2 norm of weights: 3.462905095518136\n",
      "---------------------\n",
      "Iteration Number: 9916\n",
      "Loss: 11.085556530372497\n",
      "l2 norm of gradients: 0.043108789638816\n",
      "l2 norm of weights: 3.4629645805967084\n",
      "---------------------\n",
      "Iteration Number: 9917\n",
      "Loss: 11.085398949913465\n",
      "l2 norm of gradients: 0.04310426964105151\n",
      "l2 norm of weights: 3.4630240601808193\n",
      "---------------------\n",
      "Iteration Number: 9918\n",
      "Loss: 11.085241423888773\n",
      "l2 norm of gradients: 0.04309975060922827\n",
      "l2 norm of weights: 3.4630835342693453\n",
      "---------------------\n",
      "Iteration Number: 9919\n",
      "Loss: 11.085083952278787\n",
      "l2 norm of gradients: 0.04309523254298898\n",
      "l2 norm of weights: 3.4631430028611634\n",
      "---------------------\n",
      "Iteration Number: 9920\n",
      "Loss: 11.084926535063897\n",
      "l2 norm of gradients: 0.04309071544197667\n",
      "l2 norm of weights: 3.4632024659551517\n",
      "---------------------\n",
      "Iteration Number: 9921\n",
      "Loss: 11.084769172224505\n",
      "l2 norm of gradients: 0.04308619930583475\n",
      "l2 norm of weights: 3.4632619235501885\n",
      "---------------------\n",
      "Iteration Number: 9922\n",
      "Loss: 11.084611863741003\n",
      "l2 norm of gradients: 0.04308168413420693\n",
      "l2 norm of weights: 3.463321375645154\n",
      "---------------------\n",
      "Iteration Number: 9923\n",
      "Loss: 11.084454609593813\n",
      "l2 norm of gradients: 0.04307716992673731\n",
      "l2 norm of weights: 3.4633808222389275\n",
      "---------------------\n",
      "Iteration Number: 9924\n",
      "Loss: 11.084297409763346\n",
      "l2 norm of gradients: 0.04307265668307023\n",
      "l2 norm of weights: 3.463440263330391\n",
      "---------------------\n",
      "Iteration Number: 9925\n",
      "Loss: 11.084140264230026\n",
      "l2 norm of gradients: 0.043068144402850464\n",
      "l2 norm of weights: 3.4634996989184264\n",
      "---------------------\n",
      "Iteration Number: 9926\n",
      "Loss: 11.083983172974296\n",
      "l2 norm of gradients: 0.043063633085723044\n",
      "l2 norm of weights: 3.463559129001915\n",
      "---------------------\n",
      "Iteration Number: 9927\n",
      "Loss: 11.083826135976594\n",
      "l2 norm of gradients: 0.043059122731333346\n",
      "l2 norm of weights: 3.4636185535797406\n",
      "---------------------\n",
      "Iteration Number: 9928\n",
      "Loss: 11.083669153217365\n",
      "l2 norm of gradients: 0.04305461333932713\n",
      "l2 norm of weights: 3.4636779726507876\n",
      "---------------------\n",
      "Iteration Number: 9929\n",
      "Loss: 11.083512224677076\n",
      "l2 norm of gradients: 0.04305010490935044\n",
      "l2 norm of weights: 3.4637373862139396\n",
      "---------------------\n",
      "Iteration Number: 9930\n",
      "Loss: 11.083355350336182\n",
      "l2 norm of gradients: 0.043045597441049625\n",
      "l2 norm of weights: 3.4637967942680836\n",
      "---------------------\n",
      "Iteration Number: 9931\n",
      "Loss: 11.083198530175167\n",
      "l2 norm of gradients: 0.04304109093407146\n",
      "l2 norm of weights: 3.463856196812105\n",
      "---------------------\n",
      "Iteration Number: 9932\n",
      "Loss: 11.083041764174506\n",
      "l2 norm of gradients: 0.04303658538806296\n",
      "l2 norm of weights: 3.463915593844891\n",
      "---------------------\n",
      "Iteration Number: 9933\n",
      "Loss: 11.082885052314683\n",
      "l2 norm of gradients: 0.04303208080267151\n",
      "l2 norm of weights: 3.4639749853653297\n",
      "---------------------\n",
      "Iteration Number: 9934\n",
      "Loss: 11.082728394576204\n",
      "l2 norm of gradients: 0.04302757717754484\n",
      "l2 norm of weights: 3.4640343713723087\n",
      "---------------------\n",
      "Iteration Number: 9935\n",
      "Loss: 11.082571790939568\n",
      "l2 norm of gradients: 0.04302307451233095\n",
      "l2 norm of weights: 3.4640937518647186\n",
      "---------------------\n",
      "Iteration Number: 9936\n",
      "Loss: 11.082415241385291\n",
      "l2 norm of gradients: 0.04301857280667826\n",
      "l2 norm of weights: 3.464153126841448\n",
      "---------------------\n",
      "Iteration Number: 9937\n",
      "Loss: 11.08225874589389\n",
      "l2 norm of gradients: 0.043014072060235435\n",
      "l2 norm of weights: 3.464212496301389\n",
      "---------------------\n",
      "Iteration Number: 9938\n",
      "Loss: 11.082102304445886\n",
      "l2 norm of gradients: 0.04300957227265152\n",
      "l2 norm of weights: 3.4642718602434326\n",
      "---------------------\n",
      "Iteration Number: 9939\n",
      "Loss: 11.08194591702183\n",
      "l2 norm of gradients: 0.04300507344357587\n",
      "l2 norm of weights: 3.464331218666471\n",
      "---------------------\n",
      "Iteration Number: 9940\n",
      "Loss: 11.081789583602252\n",
      "l2 norm of gradients: 0.043000575572658184\n",
      "l2 norm of weights: 3.4643905715693974\n",
      "---------------------\n",
      "Iteration Number: 9941\n",
      "Loss: 11.0816333041677\n",
      "l2 norm of gradients: 0.04299607865954848\n",
      "l2 norm of weights: 3.464449918951105\n",
      "---------------------\n",
      "Iteration Number: 9942\n",
      "Loss: 11.081477078698745\n",
      "l2 norm of gradients: 0.042991582703897094\n",
      "l2 norm of weights: 3.4645092608104897\n",
      "---------------------\n",
      "Iteration Number: 9943\n",
      "Loss: 11.081320907175947\n",
      "l2 norm of gradients: 0.04298708770535471\n",
      "l2 norm of weights: 3.464568597146446\n",
      "---------------------\n",
      "Iteration Number: 9944\n",
      "Loss: 11.081164789579878\n",
      "l2 norm of gradients: 0.04298259366357234\n",
      "l2 norm of weights: 3.46462792795787\n",
      "---------------------\n",
      "Iteration Number: 9945\n",
      "Loss: 11.081008725891124\n",
      "l2 norm of gradients: 0.042978100578201284\n",
      "l2 norm of weights: 3.4646872532436586\n",
      "---------------------\n",
      "Iteration Number: 9946\n",
      "Loss: 11.080852716090265\n",
      "l2 norm of gradients: 0.04297360844889323\n",
      "l2 norm of weights: 3.4647465730027096\n",
      "---------------------\n",
      "Iteration Number: 9947\n",
      "Loss: 11.08069676015791\n",
      "l2 norm of gradients: 0.04296911727530014\n",
      "l2 norm of weights: 3.464805887233921\n",
      "---------------------\n",
      "Iteration Number: 9948\n",
      "Loss: 11.080540858074652\n",
      "l2 norm of gradients: 0.04296462705707435\n",
      "l2 norm of weights: 3.4648651959361927\n",
      "---------------------\n",
      "Iteration Number: 9949\n",
      "Loss: 11.080385009821109\n",
      "l2 norm of gradients: 0.04296013779386845\n",
      "l2 norm of weights: 3.464924499108424\n",
      "---------------------\n",
      "Iteration Number: 9950\n",
      "Loss: 11.080229215377898\n",
      "l2 norm of gradients: 0.04295564948533546\n",
      "l2 norm of weights: 3.4649837967495154\n",
      "---------------------\n",
      "Iteration Number: 9951\n",
      "Loss: 11.080073474725658\n",
      "l2 norm of gradients: 0.04295116213112865\n",
      "l2 norm of weights: 3.4650430888583683\n",
      "---------------------\n",
      "Iteration Number: 9952\n",
      "Loss: 11.079917787845007\n",
      "l2 norm of gradients: 0.042946675730901614\n",
      "l2 norm of weights: 3.465102375433885\n",
      "---------------------\n",
      "Iteration Number: 9953\n",
      "Loss: 11.079762154716594\n",
      "l2 norm of gradients: 0.042942190284308325\n",
      "l2 norm of weights: 3.4651616564749688\n",
      "---------------------\n",
      "Iteration Number: 9954\n",
      "Loss: 11.079606575321069\n",
      "l2 norm of gradients: 0.04293770579100306\n",
      "l2 norm of weights: 3.465220931980523\n",
      "---------------------\n",
      "Iteration Number: 9955\n",
      "Loss: 11.07945104963909\n",
      "l2 norm of gradients: 0.04293322225064035\n",
      "l2 norm of weights: 3.4652802019494517\n",
      "---------------------\n",
      "Iteration Number: 9956\n",
      "Loss: 11.079295577651324\n",
      "l2 norm of gradients: 0.042928739662875165\n",
      "l2 norm of weights: 3.465339466380661\n",
      "---------------------\n",
      "Iteration Number: 9957\n",
      "Loss: 11.07914015933844\n",
      "l2 norm of gradients: 0.0429242580273627\n",
      "l2 norm of weights: 3.4653987252730563\n",
      "---------------------\n",
      "Iteration Number: 9958\n",
      "Loss: 11.078984794681121\n",
      "l2 norm of gradients: 0.042919777343758576\n",
      "l2 norm of weights: 3.465457978625544\n",
      "---------------------\n",
      "Iteration Number: 9959\n",
      "Loss: 11.078829483660055\n",
      "l2 norm of gradients: 0.04291529761171863\n",
      "l2 norm of weights: 3.4655172264370324\n",
      "---------------------\n",
      "Iteration Number: 9960\n",
      "Loss: 11.078674226255941\n",
      "l2 norm of gradients: 0.04291081883089912\n",
      "l2 norm of weights: 3.4655764687064288\n",
      "---------------------\n",
      "Iteration Number: 9961\n",
      "Loss: 11.078519022449472\n",
      "l2 norm of gradients: 0.042906341000956534\n",
      "l2 norm of weights: 3.465635705432643\n",
      "---------------------\n",
      "Iteration Number: 9962\n",
      "Loss: 11.07836387222137\n",
      "l2 norm of gradients: 0.042901864121547735\n",
      "l2 norm of weights: 3.4656949366145846\n",
      "---------------------\n",
      "Iteration Number: 9963\n",
      "Loss: 11.07820877555234\n",
      "l2 norm of gradients: 0.04289738819232994\n",
      "l2 norm of weights: 3.465754162251164\n",
      "---------------------\n",
      "Iteration Number: 9964\n",
      "Loss: 11.078053732423122\n",
      "l2 norm of gradients: 0.042892913212960626\n",
      "l2 norm of weights: 3.4658133823412918\n",
      "---------------------\n",
      "Iteration Number: 9965\n",
      "Loss: 11.077898742814437\n",
      "l2 norm of gradients: 0.04288843918309761\n",
      "l2 norm of weights: 3.465872596883881\n",
      "---------------------\n",
      "Iteration Number: 9966\n",
      "Loss: 11.077743806707037\n",
      "l2 norm of gradients: 0.042883966102399064\n",
      "l2 norm of weights: 3.465931805877844\n",
      "---------------------\n",
      "Iteration Number: 9967\n",
      "Loss: 11.07758892408166\n",
      "l2 norm of gradients: 0.042879493970523444\n",
      "l2 norm of weights: 3.4659910093220945\n",
      "---------------------\n",
      "Iteration Number: 9968\n",
      "Loss: 11.077434094919067\n",
      "l2 norm of gradients: 0.04287502278712949\n",
      "l2 norm of weights: 3.466050207215547\n",
      "---------------------\n",
      "Iteration Number: 9969\n",
      "Loss: 11.077279319200018\n",
      "l2 norm of gradients: 0.042870552551876405\n",
      "l2 norm of weights: 3.466109399557116\n",
      "---------------------\n",
      "Iteration Number: 9970\n",
      "Loss: 11.077124596905284\n",
      "l2 norm of gradients: 0.04286608326442354\n",
      "l2 norm of weights: 3.466168586345718\n",
      "---------------------\n",
      "Iteration Number: 9971\n",
      "Loss: 11.076969928015647\n",
      "l2 norm of gradients: 0.04286161492443069\n",
      "l2 norm of weights: 3.466227767580269\n",
      "---------------------\n",
      "Iteration Number: 9972\n",
      "Loss: 11.076815312511886\n",
      "l2 norm of gradients: 0.04285714753155791\n",
      "l2 norm of weights: 3.466286943259687\n",
      "---------------------\n",
      "Iteration Number: 9973\n",
      "Loss: 11.076660750374801\n",
      "l2 norm of gradients: 0.0428526810854656\n",
      "l2 norm of weights: 3.46634611338289\n",
      "---------------------\n",
      "Iteration Number: 9974\n",
      "Loss: 11.07650624158519\n",
      "l2 norm of gradients: 0.04284821558581446\n",
      "l2 norm of weights: 3.4664052779487964\n",
      "---------------------\n",
      "Iteration Number: 9975\n",
      "Loss: 11.07635178612385\n",
      "l2 norm of gradients: 0.04284375103226552\n",
      "l2 norm of weights: 3.4664644369563264\n",
      "---------------------\n",
      "Iteration Number: 9976\n",
      "Loss: 11.076197383971612\n",
      "l2 norm of gradients: 0.04283928742448015\n",
      "l2 norm of weights: 3.4665235904044005\n",
      "---------------------\n",
      "Iteration Number: 9977\n",
      "Loss: 11.07604303510929\n",
      "l2 norm of gradients: 0.042834824762119986\n",
      "l2 norm of weights: 3.466582738291939\n",
      "---------------------\n",
      "Iteration Number: 9978\n",
      "Loss: 11.075888739517712\n",
      "l2 norm of gradients: 0.04283036304484703\n",
      "l2 norm of weights: 3.466641880617865\n",
      "---------------------\n",
      "Iteration Number: 9979\n",
      "Loss: 11.075734497177727\n",
      "l2 norm of gradients: 0.04282590227232358\n",
      "l2 norm of weights: 3.466701017381101\n",
      "---------------------\n",
      "Iteration Number: 9980\n",
      "Loss: 11.075580308070165\n",
      "l2 norm of gradients: 0.04282144244421226\n",
      "l2 norm of weights: 3.46676014858057\n",
      "---------------------\n",
      "Iteration Number: 9981\n",
      "Loss: 11.075426172175886\n",
      "l2 norm of gradients: 0.042816983560176006\n",
      "l2 norm of weights: 3.4668192742151964\n",
      "---------------------\n",
      "Iteration Number: 9982\n",
      "Loss: 11.075272089475751\n",
      "l2 norm of gradients: 0.042812525619878083\n",
      "l2 norm of weights: 3.466878394283906\n",
      "---------------------\n",
      "Iteration Number: 9983\n",
      "Loss: 11.075118059950618\n",
      "l2 norm of gradients: 0.04280806862298208\n",
      "l2 norm of weights: 3.466937508785623\n",
      "---------------------\n",
      "Iteration Number: 9984\n",
      "Loss: 11.074964083581376\n",
      "l2 norm of gradients: 0.04280361256915185\n",
      "l2 norm of weights: 3.4669966177192757\n",
      "---------------------\n",
      "Iteration Number: 9985\n",
      "Loss: 11.074810160348887\n",
      "l2 norm of gradients: 0.04279915745805163\n",
      "l2 norm of weights: 3.46705572108379\n",
      "---------------------\n",
      "Iteration Number: 9986\n",
      "Loss: 11.074656290234062\n",
      "l2 norm of gradients: 0.042794703289345926\n",
      "l2 norm of weights: 3.467114818878095\n",
      "---------------------\n",
      "Iteration Number: 9987\n",
      "Loss: 11.074502473217777\n",
      "l2 norm of gradients: 0.042790250062699604\n",
      "l2 norm of weights: 3.467173911101119\n",
      "---------------------\n",
      "Iteration Number: 9988\n",
      "Loss: 11.07434870928095\n",
      "l2 norm of gradients: 0.04278579777777777\n",
      "l2 norm of weights: 3.467232997751792\n",
      "---------------------\n",
      "Iteration Number: 9989\n",
      "Loss: 11.074194998404478\n",
      "l2 norm of gradients: 0.04278134643424594\n",
      "l2 norm of weights: 3.467292078829044\n",
      "---------------------\n",
      "Iteration Number: 9990\n",
      "Loss: 11.074041340569288\n",
      "l2 norm of gradients: 0.04277689603176985\n",
      "l2 norm of weights: 3.4673511543318067\n",
      "---------------------\n",
      "Iteration Number: 9991\n",
      "Loss: 11.07388773575631\n",
      "l2 norm of gradients: 0.04277244657001568\n",
      "l2 norm of weights: 3.4674102242590115\n",
      "---------------------\n",
      "Iteration Number: 9992\n",
      "Loss: 11.073734183946463\n",
      "l2 norm of gradients: 0.04276799804864978\n",
      "l2 norm of weights: 3.467469288609591\n",
      "---------------------\n",
      "Iteration Number: 9993\n",
      "Loss: 11.073580685120703\n",
      "l2 norm of gradients: 0.04276355046733889\n",
      "l2 norm of weights: 3.467528347382479\n",
      "---------------------\n",
      "Iteration Number: 9994\n",
      "Loss: 11.073427239259962\n",
      "l2 norm of gradients: 0.04275910382575006\n",
      "l2 norm of weights: 3.4675874005766096\n",
      "---------------------\n",
      "Iteration Number: 9995\n",
      "Loss: 11.0732738463452\n",
      "l2 norm of gradients: 0.04275465812355066\n",
      "l2 norm of weights: 3.4676464481909184\n",
      "---------------------\n",
      "Iteration Number: 9996\n",
      "Loss: 11.073120506357382\n",
      "l2 norm of gradients: 0.042750213360408316\n",
      "l2 norm of weights: 3.4677054902243403\n",
      "---------------------\n",
      "Iteration Number: 9997\n",
      "Loss: 11.072967219277482\n",
      "l2 norm of gradients: 0.042745769535991074\n",
      "l2 norm of weights: 3.467764526675812\n",
      "---------------------\n",
      "Iteration Number: 9998\n",
      "Loss: 11.072813985086462\n",
      "l2 norm of gradients: 0.04274132664996719\n",
      "l2 norm of weights: 3.4678235575442717\n",
      "---------------------\n",
      "Iteration Number: 9999\n",
      "Loss: 11.072660803765313\n",
      "l2 norm of gradients: 0.042736884702005276\n",
      "l2 norm of weights: 3.4678825828286564\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "lambda_ = 1.0\n",
    "max_iter = 10000\n",
    "model = fit(xtrain_normal, ytrain, learning_rate, lambda_, max_iter, verbose=1) #keep the verbose on here for your submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: \", accuracy(xtrain_normal, ytrain, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 5 0.9100899100899101\n",
      "0.01 2 0.9250749250749251\n",
      "0.01 1 0.9270729270729271\n",
      "0.01 0.1 0.9200799200799201\n",
      "0.01 0.01 0.9250749250749251\n",
      "0.001 5 0.903096903096903\n",
      "0.001 2 0.9200799200799201\n",
      "0.001 1 0.9210789210789211\n",
      "0.001 0.1 0.9090909090909091\n",
      "0.001 0.01 0.9110889110889111\n",
      "0.0001 5 0.9240759240759241\n",
      "0.0001 2 0.9210789210789211\n",
      "0.0001 1 0.9140859140859141\n",
      "0.0001 0.1 0.9230769230769231\n",
      "0.0001 0.01 0.913086913086913\n",
      "1e-05 5 0.9280719280719281\n",
      "1e-05 2 0.9280719280719281\n",
      "1e-05 1 0.9310689310689311\n",
      "1e-05 0.1 0.9200799200799201\n",
      "1e-05 0.01 0.922077922077922\n"
     ]
    }
   ],
   "source": [
    "#grid search for finding the best hyperparams and model\n",
    "\n",
    "best_model = None\n",
    "best_val = -1\n",
    "for lr in [0.01, 0.001, 0.0001, 0.00001]:\n",
    "    for la in [5, 2, 1, 0.1, 0.01]:\n",
    "        model = fit(xtrain_normal, ytrain, lr, la, 10000, verbose=0)\n",
    "        val_acc = accuracy(xval_normal, yval, model)\n",
    "        print(lr, la, val_acc)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_model = model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.934\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: \", accuracy(xtest_normal, ytest, best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEJCAYAAABc/7oDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuVklEQVR4nO3deZxcVZ3//9fp7mydPd3ZOp2QhIRAQFkCMSA7olFQcOEjjgqjKIMDA46ogOMsOl9R0R8jOgITEQFZPyISEBA0iLKGRdkSlgQSkk5n62xk3/r+/ji3k0qnO13pVHV1db2fj0c9qurUvXU/dR6QT5/lnhOSJEFERCQXygodgIiIdB1KKiIikjNKKiIikjNKKiIikjNKKiIikjMVhQ6gwDT1TUSkfUJLhaWeVKivry90CFmrrq6moaGh0GEUlOpAdQCqAyhsHdTU1LT6mbq/REQkZ5RUREQkZ5RUREQkZ5RUREQkZ5RUREQkZzpk9peZ3QicDixz90PSskHAXcBoYD5g7r4q/ewK4DxgO3Cxuz+clk8CbgJ6AQ8Cl7h7YmY9gFuAScAK4NPuPr8jfpuIiOzUUS2Vm4CpzcouB2a4+3hgRvoeM5sInA0cnJ5zrZmVp+dcB5wPjE8fTd95HrDK3ccB/wP8MG+/REREWtUhScXd/wqsbFZ8BnBz+vpm4MyM8jvdfbO7zwPmApPNbDjQz92fdveE2DI5s4Xvuhs4xcxavDEnF5I5s2m891aS7dvzdQkRkaJUyJsfh7r7YgB3X2xmQ9LyEcAzGcfVpWVb09fNy5vOWZh+1zYzWwNUAbvdGWRm5xNbO7g71dXVex34+ifqWPeAU/3Z8ynrVbnX57dXRUVFu+LtSlQHqgNQHUDnrYPOeEd9Sy2MZA/lezpnN+4+DZjWdEx77kht3LwZgBVLlxD69Nvr89tLdxGrDkB1AKoD0B31LVmadmmRPi9Ly+uAkRnH1QL1aXltC+W7nGNmFUB/du9uy52KbvF529a8XUJEpBgVMqncB5ybvj4XmJ5RfraZ9TCzMcQB+WfTrrK1ZjYlHS85p9k5Td/1KeDRdNwlP7qlSWWrkoqISKaOmlJ8B3AiUG1mdcB/Aj8A3MzOAxYAZwG4+ywzc2A2sA240N2bRsS/ws4pxQ+lD4BfAr82s7nEFsrZef1B3brHZ7VURER2EZKkpFd/T9qzSnHy4jM0/vxKyv79fwij9s9DWC1TP7LqAFQHoDqATjGm0uIMW91R3x4VaUtF3V8iIrtQUmmPpu6vrVsKG4eISCejpNIeGqgXEWmRkkp7qKUiItIiJZX2SFsqiZKKiMgulFTaQy0VEZEWKam0h8ZURERapKTSHhVqqYiItERJpT3U/SUi0iIllfaoSFe30TItIiK7UFJphxBCbK2opSIisgsllfbq1k0D9SIizSiptJdaKiIiu1FSaa8KtVRERJpTUmkvtVRERHajpNJe3bppmRYRkWaUVNqrew+1VEREmlFSaa8ePWHzpkJHISLSqSiptFd3JRURkeaUVNop9OgBWzYXOgwRkU5FSaW9uvdQS0VEpBkllfbq0RM2q6UiIpJJSaW9uveELZtIkqTQkYiIdBpKKu3VowckiaYVi4hkUFJprx4947O6wEREdlBSaa/uPeLzFg3Wi4g0UVJprx0tFSUVEZEmSirtFNT9JSKyGyWV9lL3l4jIbpRU2kvdXyIiu1FSaa80qSTq/hIR2aGi0AGY2b8CXwIS4BXgC0AlcBcwGpgPmLuvSo+/AjgP2A5c7O4Pp+WTgJuAXsCDwCXunr87E9X9JSKym3a1VMzsJDM7fl8vbmYjgIuBI939EKAcOBu4HJjh7uOBGel7zGxi+vnBwFTgWjMrT7/uOuB8YHz6mLqv8e1Rz8r4vGljXi8jIlJMskoqZvYXM3t/+voy4E7gDjP7Vg5iqAB6mVkFsYVSD5wB3Jx+fjNwZvr6DOBOd9/s7vOAucBkMxsO9HP3p9PWyS0Z5+RHr17xeeOGvF5GRKSYZNv9dQjwTPr6y8CJwDrgSeDK9l7c3ReZ2Y+BBcBG4BF3f8TMhrr74vSYxWY2JD1lREYcAHVp2db0dfPy3ZjZ+cQWDe5OdXV1e8Nnaffu9CKh7z58x96oqKjYp3i7AtWB6gBUB9B56yDbpFIGJGa2PxDc/TUAMxu4LxdPzz8DGAOsBn5jZp/bwymhhbJkD+W7cfdpwLSmYxoaGrKOdzc9K9m4soHN+/Ide6G6upp9ircLUB2oDkB1AIWtg5qamlY/y3ZM5Qngf4EfA78DSBPMvv6iDwDz3H25u28F7gGOAZamXVqkz8vS4+uAkRnn1xK7y+rS183L86tXb42piIhkyLal8o/ApcBy4Edp2YHANft4/QXAFDOrJHZ/nQI8D6wHzgV+kD5PT4+/D7jdzK4GaogD8s+6+3YzW2tmU4CZwDnAz/Yxtrb17EWycX3eLyMiUiyySiruvgL4VrOyB/b14u4+08zuBv4GbAP+Tuya6gO4mZ1HTDxnpcfPMjMHZqfHX+ju29Ov+wo7pxQ/lD7yq7K3BupFRDKEbDaZMrPuxNbKYcR/8Hdw93PyEVgHSerr299Ltv2678OSRZR/539zGFLr1I+sOgDVAagOoFOMqbQ0lp1199fNwKHA/cDS3IRV/EKvShK1VEREdsg2qUwFxrj76jzGUnx6VsImJRURkSbZzv5aAPTIZyBFKZ39lTQ2FjoSEZFOIduWyi3AdDO7hmbdX+7+aM6jKha9KuM+9Zs2xkF7EZESl21SuSh9bn73fAKMzV04RaZX0/pfG5RURETIfkrxmHwHUoxCZe942/6GdTBocKHDEREpuKyXvk8XfDyGuKZWHfC0u2/LV2BFoTKdXb1+XWHjEBHpJLJdpfhA4DXgduJS9XcAr5vZQXmMrfPr2y8+r1tb2DhERDqJbGd/XUu8032kux/t7rXA9Wl56eodk0qy7t0CByIi0jlkm1QOA65utpPiT9Ly0tWnb3xWUhERAbJPKvXACc3KjqMjVgLuxEK37nFb4fXq/hIRgewH6r8F3GdmvwfeAfYDTgP2tPdJaejTV2MqIiKprFoq7n4fcATwKtA3fZ7k7tP3eGIp6N2XRC0VERFgL6YUu/ubwP/LYyzFqU8/dX+JiKRaTSpmNs3dz09f/5rWt+ct5qXv91no3ZdkZWkvwS0i0mRPLZV5Ga/n5juQotWnL6zX7C8REdhDUnH372e8/T93X9L8GDMblpeoikmf/rB+Hcn27YTy8kJHIyJSUNlOKX6zlfLZuQqkaPUfEFcqXrum0JGIiBRctkllt20jzawfUPIbiYR+A+OLd1cVNhARkU5gj7O/zGwhcYC+l5ktaPZxFXENsNLWP00qa5RURETamlL8OWIr5UHg8xnlCbDU3d/IV2BFI00qyZpVuzfnRERKzB6Tirv/BcDMqt1dm7G3RC0VEZEdst2ka4OZHUZc76uajDEWd/+P/IRWHEK37nHXRyUVEZGs91M5H3gSOBm4DHgPcCkwLn+hFZF+A0k0UC8ikvXsr28CU93948DG9PlTwNa8RVZM+g9US0VEhOyTyhB3fzx93WhmZe7+EPDRPMVVVEL/QbBqRaHDEBEpuGyTSp2ZjU5fvwmcYWbHAVvyElWxqRoMq1eQNG4vdCQiIgWV7SrFVwEHAfOB7wJ3A92J+9VL1RDYvh1Wr4RBgwsdjYhIwbTZUjGzAPwV+CNA2u01EBjo7tflN7ziEKrSRLJieWEDEREpsDaTSrov/StkLMni7lvcfV0+AysqVUMASFYsK3AgIiKFlW3319+BA4DXcx2AmQ0AbgAOId6p/0XgDeAuYDSxy83cfVV6/BXAecB24GJ3fzgtnwTcBPQirgBwSZoQ86+py0tJRURKXLYD9Y8BfzCz/zKz88zsi02PHMRwDfAHdz8QOBR4DbgcmOHu44EZ6XvMbCJwNnAwMBW41sya1pu/DjgfGJ8+puYgtqyEHj3jDpAr1f0lIqUt25bK+4mbdp3QrDwBbmzvxdOVjo8H/hFitxqwxczOAE5MD7uZmNQuA84A7nT3zcA8M5sLTDaz+UA/d386/d5bgDOBh9ob216rGkLSoJaKiJS2bJdpOSlP1x8LLAd+ZWaHAi8AlwBD3X1xeu3FZjYkPX4E8EzG+XVp2db0dfPy3aSrA5yffjfV1dU5+SGrR+7Htjmv5ez7WlJRUZHX7y8GqgPVAagOoPPWQbYtFcysCvgIMMzdf2RmNUCZu9e1cWpb1z8C+Bd3n2lm15B2dbWipYWAkz2U78bdpwHTmo5paMjN/vKNAweTLHuU5Yvr43pgeVBdXU2u4i1WqgPVAagOoLB1UFNT0+pn2a79dQJx8PyzQNMCkuOJ4xj7og6oc/eZ6fu7iUlmqZkNT689HFiWcfzIjPNrgfq0vLaF8o4zdETcAXLZ4g69rIhIZ5LtQP1PgE+7+1RgW1o2E5i8LxdP971faGYT0qJTiFsU3wecm5adC0xPX98HnG1mPcxsDDGxPZt2la01synpfTXnZJzTIcKwNKctWdSRlxUR6VSyTSqj3X1G+rqpW2kLe9F9tgf/AtxmZi8DhwFXAj8ATjWzOcCp6XvcfRbgxMTzB+BCd29aG+UrxKnJc4G36MhBeoChsTmYLNmX3kARkeKWbVKYbWYfaronJPUB4k2R+8TdXwSObOGjU1o5/nvA91oof554r0tBhJ69YEAVLFVLRURKV7ZJ5VLg92b2AHG/+v8jrlB8Rt4iK0Y1I0kWLSh0FCIiBZNV95e7PwO8F5hFvC9lHjDZ3Z/LY2xFJ4wcA/XvkGzb1vbBIiJdUFYtFTP7urv/mLhacWb519z96rxEVoxGjoVt22DJQqgdU+hoREQ6XLYD9a3tQ//tXAXSFYRRYwFIFswrcCQiIoWxx5aKmZ2cviw3s5PY9SbDscDafAVWlIbWQPfusFBJRURKU1vdX79Mn3uy6xpfCbCEOB1YUqGsHEaMJlkwt9ChiIgUxB6TiruPgbhAo7uf0zEhFbcwdgLJ4w+TbNtGqMjFbTwiIsUj29lfSihZCuMnwpYtsPDtQociItLh2hpTeZxWFmZs4u7H5zSiYrf/QQAkc2YTxhxQ4GBERDpWW/0zN3RIFF1IGDAIqoeSvPUacUsXEZHS0daYys0dFUhXEsZNJJn1N5LGRkJZtrO2RUSKn/7Fy4eJh8HaNZpaLCIlR0klD8LBhwGQzPpbYQMREelgSip5EPoNhJFjlFREpOS0mlTM7JmM1//ZMeF0HeGQI+Ct10k2bih0KCIiHWZPLZUDzKxn+vrSjgimKwnvOQq2byd5WQs5i0jp2NPsr+nAm2Y2n7iHyl9bOkj3qbRi/wOh/yCSvz0F7zuh0NGIiHSIVpOKu3/BzI4FRgNHsXMdMMlCKCsjHDGF5Ik/kWzaGHeGFBHp4tq6T+UJ4Akz6657VvZemHQsyZ8fJHnlecJRxxU6HBGRvMtqxUN3vzFd+v7zwAhgEXCruz+az+CK3viDoP9Akpl/ASUVESkBWU0pNrMvAXcRl7u/B1gM3G5mX85jbEUvlJUTppwErzxPsnplocMREcm7bNdm/yZwqru/1FRgZncBvwV+kY/Auopw7AdIHr6H5Ok/Ez78yUKHIyKSV9ne/FgFzG5W9gYwKLfhdD1hWC2Mm0jy5J9Ikj0u+CwiUvSyTSpPAFebWSWAmfUGfgQ8la/AupJw3KmwdBG8/nKhQxERyatsk8oFwHuBNWa2FFgNHAr8U57i6lLCUcdB3/40/nF6oUMREcmrbGd/LQZOMLNaoAaod/e6vEbWhYRu3QknnUZy3+0kixcSho8sdEgiInmxV5uop4lEyaQdwokfJnnobpI/Tiecc1GhwxERyQutUtxBQt/+hKNPJnn6UZKVywsdjohIXiipdKDwkU9BAskDvyl0KCIiedFm95eZlQEnAk+4+5a8R9SFhaohhOM/SPLXh0mmfoIweFihQxIRyak2Wyru3ghMV0LJjfCRs6CsnOT+OwodiohIzmU7UP9XM5vi7s+0fejeM7Ny4HlgkbufbmaDiMvCjAbmA+buq9JjrwDOA7YDF7v7w2n5JOAmoBfwIHCJu3e6uw3DgCrCyaeRPHIvycmnE0aPL3RIIiI5k+2YyjvAQ2Z2k5n9t5l9t+mRozguAV7LeH85MMPdxwMz0veY2UTgbOBgYCpwbZqQAK4DzgfGp4+pOYot58Jpn473rdz5C91lLyJdSrZJpRdwL5AAtcDIjMc+Se99OQ24IaP4DKBpqf2bgTMzyu90983uPg+YC0w2s+FAP3d/Om2d3JJxTqcTelUSPnFO3G545mOFDkdEJGeyvfnxC3mM4SfEBSv7ZpQNTW+4xN0Xm9mQtHwEkNkFV5eWbWXX+2eayndjZucTWzS4O9XV1Tn4CXsv+aix8ok/0vjbmxl0/Acp69e/zXMqKioKFm9noTpQHYDqADpvHWR986OZHQR8ivgP/kVmNgHo4e7tXtDKzE4Hlrn7C2Z2YhanhBbKkj2U78bdpwHTmo5paGjIJtS8SP7hAhq/9zUarvshZed9rc3jq6urKWS8nYHqQHUAqgMobB3U1NS0+lm2+6mcBfyV+Nf/OWlxX+DqfYzt/cDHzGw+cCdwspndCixNu7RIn5elx9exa5dbLVCflte2UN6phZFjCB8+i+SZx0heeq7Q4YiI7LNsx1S+S9xP5QLirCuAl4iLSrabu1/h7rXuPpo4AP+ou38OuA84Nz3sXKBpJcb7gLPNrIeZjSEOyD+bdpWtNbMpZhaIia8oVm8Mp50FI/aj8dc/J1m7ptDhiIjsk2yTyhBiEoGd3UoJrXQx5cAPgFPNbA5wavoed58FOHFvlz8AF7p7U5L7CnGwfy7wFvBQnmLLqVDRjbIv/iusX0vjjT8haWwsdEgiIu2W7ZjKC8T96W/JKDsbeDZXgbj7Y8Bj6esVwCmtHPc94HstlD8PHJKreDpSGDWWYOeR3H59XHDyQx8vdEgiIu2SbVK5GHjEzM4DepvZw8ABwAfzFlmJCSd+mOT1l0l+dwvJ2AmE8RMLHZKIyF7LqvvL3V8HDgR+Dnwb+BXwHnefk8fYSkoIgbJzL4LqYTReeyVJw9JChyQisteyXqXY3TcATxK7qB5393X5CqpUhco+lF30bWjcTuP//j+SjRsKHZKIyF7JdkrxKDN7nLgO1wPAfDN7wsz2y2dwpSgMG0HZP10GixfSOO1HJNu2FTokEZGsZdtSuZk4WD/A3YcAA4Hn2LmUiuRQmHgY4bNfgVdfILnxf0gat7d9kohIJ5BtUpkEfMPd1wOkXV+XpeWSB2XHf4jwyXNJnnuc5Lb/08KTIlIUsp399QwwmTim0uRI4OmcRyQ7lE39JI0b1pM8dDd060Zy4eWFDklEZI9aTSrNlrV/C3jQzB4AFhKXSvkIcHt+w5Pw8c/D1i0kf7qPtWWB5FNfIJSVt32iiEgB7Kml0nxZ+3vS5yHAZuB3QM98BCU7hRDAzoPuPdj44G8Ia9bAF79KKFdiEZHOp9Wkkufl7mUvhBAIH/88vQZVse7W60k2rKXs/G8SelUWOjQRkV3szdL3lcA4oE9mubs/leugpGW9P3kO60M5yW3X0fjDyyj7l38nVA1p+0QRkQ6SVVIxs3OA/wW2ABszPkqAUXmIS1pRdvyHSKqH0nj9D2i88uuUXfhvhLETCh2WiAiQfUvlKuCT7v7HfAYj2QkTD6Ps8qto/Ol3abzqCsKnv0Q48cNx/EVEpICyvU9lC+kKwtI5hJpRlH37aph4WFzd+IarSTZtbPtEEZE8yjap/DtwtZl1vg2RS1jo04+yi75NOPNzJM89TuOVXydZ8FahwxKREpZt99ebxN0f/9nMmsoCkLi75rYWUCgrI5xmJGMn0Hjj/9B45dcJH/0MYeonNe1YRDpcti2VXxM36DqUuI/KAcStfA/IU1yyl8JBh1L2Xz8jHHEMyb230vjDy0iWLCp0WCJSYrJtqVQB/+HuWoCqEwu9+xLO/waNh72P5LbrafzuJYSPnEX40CcI3boVOjwRKQHZtlR+RdxOWIpA2eTjKfvOzwjvPYpk+m00fvcSkjdeLXRYIlICsm2pTAYuMrN/A3bZktDdj895VLLPwoAqwgWXkbzyPI23XU/jj79FOPokwifOIQyoKnR4ItJFZZtUfpE+pMiE9xxJ2Xd+TvLAnSSPTCd54anYHfahjxN6aOk2EcmtUOL7dCT19fWFjiFr1dXVNDQ0tPv8ZNliGu+5GV54CgYMIpz5udh6KaJVj/e1DroC1YHqAApbBzU1NRBnAO8m22VavtjaZ+5+Y/vCko4Whgyn/ILLSebOptFvJLnppySP3EvZxz4Dhx9NKMt2iE1EpGXZdn81H6QfBuxP3LRLSaXIhHETKbviRyTPP0ly3+00Xv9DqB1N2UfPhsOmKLmISLtllVTc/aTmZWnr5aCcRyQdIoRAOOpYkklHkzz7OMnv76Lxuh/AyDGUnWZw+JSi6hYTkc4h66XvW3AT0AB8IzehSCGEsnLClBNJjjqOZOZfSB64K7ZcBg8jfPBMwjGnELr3KHSYIlIksh1Tad4fUgl8Dlid64CkMEJ5OeGYk0mmnAB/f4bGh39Hctv1JNNvJ5x0Wnz07VfoMEWkk8u2pbKNuHdKpkXAl3MbjhRaKCuHSe+n7IhjYM6smFzuv4PkD78lTD4uJpf9xhU6TBHppLJNKmOavV/v7qU9n6+LCyHAAYdQfsAhJPULSGbcH7vHnpwBYw4gnPhhwpHHqmtMRHah+1RK6D6VfZVsWE/y9J9JHnsQltRB776E93+AcNyphGG1HRJDoeugM1AdqA6gSO9TMbM/s3u3V6bE3U9pb2BmNpK4+vEwoBGY5u7XmNkg4C5gNDAfMHdflZ5zBXAesB242N0fTssnEScP9AIeBC7RApi5FSp7E045neTk0+CNV2h87EGSP00neeR3sP+BhGNOJhx5HKGyd6FDFZECaeuGhFuB21p4PAa8Fzh6H6+/DbjU3Q8CpgAXmtlE4HJghruPB2ak70k/Oxs4GJgKXGtmTfNerwPOJy7JPz79XPIghEA48L2UX3A5ZVf9ivCpL8CG9SS/vpbGr59L4y/+P5LZL5I0NhY6VBHpYHtsqbj7LzPfm1kVcAVxgP4u4sZd7ebui4HF6eu1ZvYaMAI4AzgxPexmYhK7LC2/0903A/PMbC4w2czmA/3c/ek0zluAM4GH9iU+aVvoP5DwoY+TfPBMmD+X5KkZJM/+heTZv8Cg6thymXwcjNo/jtOISJeW7ZTifsT7US4Cfg8c4e453bfWzEYDhwMzgaFpwsHdF5vZkPSwEcAzGafVpWVb09fNy1u6zvnEFg3uTnV18eyQXFFR0bnjHTwYjjqaZMs32PzcE2z880NsefR+kkd+R/nwWnoe+wF6HvsBKkaNbfclOn0ddADVgeoAOm8dtDWm0gv4KnApsbVwrLvPynUQZtYH+C3wVXd/N2PL4uZa+lM32UP5btx9GjCt6ZhiGuwrqsHJCYfChEMpW7+W5G9Ps/25x1l/9y2s/81NMGK/OHPsyGMJw1rM/a0qqjrIE9WB6gA6xUB9i9pqqcwDyoGrgOeBoWY2NPMAd390X4Izs27EhHKbu9+TFi81s+FpK2U4sCwtrwNGZpxeC9Sn5bUtlEuBhd59Ccd9EI77IMm7q0heeIrkucdJpt9GMv02GD6ScNj7CIcfDfvtr3XHRIpcW0llE/Ev/q+08nkCtLsvw8wC8EvgNXe/OuOj+4BzgR+kz9Mzym83s6uBGuKA/LPuvt3M1prZFGL32TnAz9obl+RH6DeQcNJpcNJpJCuXk/x9JsmLz5A8fA/JQ3fDgKo0wbwPDjiEUKEtkEWKTVsD9aPzfP33E1dAfsXMXkzLvkVMJm5m5wELgLPSeGaZmQOziTPHLnT37el5X2HnlOKH0CB9pxYGDSaccjqccjrJ+rUkLz8fE8xTM+J9ML16E94zCd5zJOHgI7REjEiR0M2PuvmxU0m2bIbXXiL5+zMkLz8Ha9dACDB6POGQSQw89hRWD6gu6W6yUvjvoC2qg04xptL+TbpEOkro3gMOnUw4dHK8z+Wdt0heeZ7k1RdIfn8nK++/A/r0Ixx8eGzFTDxcrRiRTkRJRTqtUFYGY8YTxoyHj32GZO279Fkwh7VPP0Yy6+8w8y8kIcR7YA46lHDQe2HcRK1HJlJASipSNELffvQ64UOsP3gSSeP2eLPlqy+QvPYSyR/vJfnDb6GiW1wy5qBDCQe+N3ablWuzMZGOoqQiRSmUlcPYCYSxE+Bj/0CyaQPMmR0TzGsvk9x7a7xRqVdlnEl24HtjkqkZVdLjMSL5pqQiXULoWRnHWN5zJADJ2jUkr78Cr78UE81Lz8YkU9kHxh1EOOBgwriJsN84QoX+NxDJFf3fJF1S6NufcNSxcNSxACQrlpG88SrMnU0yZxbJy8/FJNO9O4w9kDB+ImH8wfF1D43JiLSXkoqUhFA1hHDMyXDMyQAk766COa/FBDNnFsnvnSRphPLyOPA/fiJh7IGw/wTCgKoCRy9SPJRUpCSFfgNh0jGESccAcQMy3no9TTKzSR79Pckj98aDB1UTxkyIEwDGToBRYwnduhcueJFOTElFhLgBGe+ZFO/iB5KtW2Hh2yRvvwFvvxGfX3gydpmVV8TEMnbCzskCVUO0tL8ISioiLQrduu1MGKlk9UqY9yZJmmSSxx+BGffHRNNvQJy+vN/+hP3Gx8UxBwwqVPgiBaOkIpKlMGAQHD6FcPgUAJLt22HR/J2tmXfeInnlhTg2AzBgUJxdtt84wuhxMdH0G1jAXyCSf0oqIu0Umgb1R+0PJ34EgGTzpthtNn8uvDM3JpqXn2PHGnsDq2NySZMNo8YS+ivRSNehpCKSQ6FHz7hUzLiJO8qSTRtgwTySd+bGVQAWzCV5cebOXeT6DYDaMYSRo2HkWELtGBg2QisBSFFSUhHJs9CzEg44mHDAwTvKko0bYMHbJAvfhoXzSOrmkcy4H7Zti8mmolvcIbN2NIwcQxg5BmpHEyr7FOpniGRFSUWkAEKvSphwCGHCITvKkm3bYEkdSd08WDifZOHbcfn/J/+0s1VTNYTV+0+gccgIwohRUDMKhtRoVQDpNPRfokgnESoqYmukdjTEuQBxLGbNyphk6ubBwnlsW7SA5Lknd04IKK+I3WU1McmEmlEwYj8YPDSukSbSgZRURDqxEAIMqIpbLaf30FRXV7O8fhEsWURS/w7ULyBZtIBk3pvw3OM7WzXdusPw2oxksx/UjIz31GhRTckTJRWRIhS694gzx0aN3aU82bQRFtdlJJt34ppnzzy2M9n06AnDRxKG1cYWzvBaGFYLg4fH+3NE9oGSikgXEnr22rmxWYZkwzqoX0hSvyAmm/oFJG+8As/8eWeyCWUweCgMq90t4YQ+2l1TsqOkIlICQtOS/+MO2qU82bQBltaTLFkES+piK2dJHcnsF2Hb1p0Jp0/fjGSTPg8fAVVDNfVZdqGkIlLCQs/KHXf9Z0oat8OK5XE22uI6WLooJpuXnoUn/rgz2ZSXQ9VQGDKcMLQmPg8ZDkNq4tiNEk7JUVIRkd2EsnIYPAwGD9ux8VmTZP3aOElgSR0srYdli0mW1ZPMmQWbN+2ecIbWpIlGCacUKKmIyF4JvfvGbQD2P3CX8iRJ4N3VsTtt+eJdE86bs2Dzxj0nnMHDoHpoTDjdtVFasVJSEZGcCCFA/4HQf+AuqwfAXiYciItxVg8lVKeJZnDGa63+3KkpqYhI3mWVcBqWkixfAg1LoWEJyfKlJG++CjMfgyTZmXQqKmgYUsP2QdWE6qFQPYwweGhMONXD4t44UjBKKiJSULsknGZdagDJtq2wcjks35l0Kt5dxfb6hSRvvwkb1u3ayqnsE8eDqoYQqgbvfB40BKoGQ2UfbaiWR0oqItKphYpucXB/SA1NqWBAdTUNDQ1Aeg9Ow9KYdHa0cpZA/Tskrz4PW7bsmnR69IrJJTPZDKomVA2BqiExuWnFgXZTUhGRohYq+8CoPnFvm2afJUkC696FFctgxXKSFctgZcbz22/A+rXx2KaTyitgUDUMGkwYFJMPVYPTpDMYBlYTunXvyJ9YVJRURKTLCiFA3/7xMXr8bkkH0htAVzTAymUkK5bDyp0JKHntpbigZ+aYDsTvG1gVE8zA6ozXVTEhDagq2RlsSioiUtJCz0oYMQpGjGo56WzbBqsaMlo4DbBqBcmqBlixjGTua7u3diCuQjCgOnatpUlnR+Jpet2j6yUeJRURkT0IFRU7bwRt5Zhk82ZYvQJWNZCsbIhJaPUKklUrdnazrXs3Hpt5Yu++u7Zy0lZP6D8wTp3uXwV9+hbVxIIulVTMbCpwDVAO3ODuPyhwSCJSAkKPHjC0Jt7M2coxyZbNsHplTDyrYmtnZxJaQTJ/DqxdE4/NPLGiAvoPSpPMoB0JZ2PtfiQV3WLiGTAIKnt3iuTTZZKKmZUDPwdOBeqA58zsPnefXdjIRETS7QqaVg9o5Zhk69Y4hrN6JaxZRbJ6JaxZAatXxteLF5K8/hJsWM+7zU/u1j1NPAMJTUkofcT3VXHqdq/KvCafLpNUgMnAXHd/G8DM7gTOAJRURKQohG7d0ps4h8b3rRyXbN7MwLKEVfPeIlmzakfiaUo+yaL5MOtvsGljPD7z5O49YqI547OUTT4+57+hKyWVEcDCjPd1wPuaH2Rm5wPnA7g71dXVHRNdDlRUVBRVvPmgOlAdgOoAYh0MHl67x2MaN66nceUKGlc1sH1VA40r42P7ygZ6jRhJjzzUYVdKKi1O3Ghe4O7TgGlNnzfdQFUMqjNu+CpVqgPVAagOYC/qoEclDBsVHxnWAmvbWYc1NTWtftaVbhutA0ZmvK8F6gsUi4hISepKLZXngPFmNgZYBJwN/ENhQxIRKS1dpqXi7tuAi4CHgddikc8qbFQiIqWlK7VUcPcHgQcLHYeISKnqMi0VEREpPCUVERHJGSUVERHJGSUVERHJmZAku90fWEpK+seLiOyDFleRKfWWSiimh5m9UOgYCv1QHagOVAedpg5aVOpJRUREckhJRUREckZJpbhMa/uQLk91oDoA1QF00joo9YF6ERHJIbVUREQkZ5RUREQkZ7rUgpJdiZmNBG4BhgGNwDR3v8bMBgF3AaOB+YC5+6pCxZlvZlYOPA8scvfTS+33A5jZAOAG4BDivVVfBN6ghOrBzP4V+BLx978CfAGopAvXgZndCJwOLHP3Q9KyVv/7N7MrgPOA7cDF7v5wAcJWS6UT2wZc6u4HAVOAC81sInA5MMPdxwMz0vdd2SXErQyalNrvB7gG+IO7HwgcSqyPkqkHMxsBXAwcmf7jWk7cL6mr18FNwNRmZS3+5vTfhrOBg9Nzrk3/IOtwSiqdlLsvdve/pa/XEv8hGQGcAdycHnYzcGZBAuwAZlYLnEb8K71Jyfx+ADPrBxwP/BLA3be4+2pKrB6IvSq9zKyC2EKpp4vXgbv/FVjZrLi133wGcKe7b3b3ecBcYHJHxNmckkoRMLPRwOHATGCouy+GmHiAIQUMLd9+AnyT2P3XpJR+P8BYYDnwKzP7u5ndYGa9KaF6cPdFwI+BBcBiYI27P0IJ1UGG1n7zCGBhxnF1aVmHU1Lp5MysD/Bb4Kvu/m6h4+koZtbUl/xCoWMpsArgCOA6dz8cWE/X6+bZIzMbSPxLfAxQA/Q2s88VNqpOp6VlUwpyv4iSSidmZt2ICeU2d78nLV5qZsPTz4cDywoVX569H/iYmc0H7gRONrNbKZ3f36QOqHP3men7u4lJppTq4QPAPHdf7u5bgXuAYyitOmjS2m+uA0ZmHFdL7CLscEoqnZSZBWI/+mvufnXGR/cB56avzwWmd3RsHcHdr3D3WncfTRyAfNTdP0eJ/P4m7r4EWGhmE9KiU4DZlFY9LACmmFll+v/FKcQxxlKqgyat/eb7gLPNrIeZjQHGA88WID7dUd9ZmdmxwOPE6ZNNYwrfIo6rODCK+D/bWe7efDCvSzGzE4Gvp1OKqyi9338YcbJCd+Bt4nTaMkqoHszsO8CnibMi/06cXtyHLlwHZnYHcCJQDSwF/hO4l1Z+s5n9G3G6+TZid/lDHR+1koqIiOSQur9ERCRnlFRERCRnlFRERCRnlFRERCRnlFRERCRnlFRERCRntPS9FDUz+wfga8CBwFrgReB77v5EgeKZDwwlLj/e5CZ3vyiLcx8DbnX3G9o6Nt/M7B+BL7n7sYWORYqLkooULTP7GnEdrAuAh4EtxGW/zwB2SypmVuHu2zogtI+6+59y/aUdGL9IuympSFEys/7Ad4EvZKyLBnB/+sDM/ou4sdUm4GPA18zsQeB64FjisuI/dPdfpMdPBq4FDgA2Etdc+5qZ9STe0f5h4l4ec4DT3X3pXsb8j8Q7wZ8hbqa0Gvhnd3/IzL4HHEdcjuQnpK0bM0uAi4CvEv9/HWNmXwYuAwYRk+cF7l6fXiMh7kHzVaAf8Kv02G7EFX5PcPdX0mOHAO8Ao9x9+V78jmOIe7wcALwJXOLuT2X8xv8ABgMNwLfd/TYzG0dcdugwYCtxT5BPZ115UjQ0piLF6migJ/C7No47g7gI4wDgNuAO4uJ7NcCngCvN7JT02GuAa9y9H7A/cTkMiGss9Scu2FdFbBltbGfc7yPu2lgNXAX80syCu/8bcVmei9y9T7PusjPT8yaa2cnA9wEDhhOTwp3NrvFx4EjiwpNnAF90983pcZmr+34G+NNeJpRBwAPAT4l1cTXwgJlVpUvy/xT4sLv3JS76+GJ66n8DjwADiYsd/izba0pxUUtFilUV0JBFd9DT7n4vgJlVE1sop7v7JuBFM7sB+DxxF72twDgzq3b3BmKLgrS8Chjn7i8DbS3Hf6+ZZcb1jabWEPBORsvoZmLLaCiwZA/f9/2M9Z0+C9zYtIFbuoXsKjMb7e7z0+N/mB6/Mm31fIbY0roZuNvMrnD3xvR3X9XGb2nuNGCOu/86fX+HmV0MfBT4DXGdukPMbEG638fi9LitwH5AjbvX0UL3pHQNaqlIsVoBVKc7Ae5J5sZFNcDKdCfNJu+wczOj84hdOq+b2XPpni4AvyaO2dxpZvVmdlW6LUFrznT3ARmPX2R8tiN5uPuG9GWfvfwN72R8xzpiXYxo5fh30nNIl89fD5xgZgcC44ir2+6NXa6fcY0R7r6euOjjBcBiM3sgvQ7EzdYC8KyZzTKzL+7ldaVIKKlIsXqaOFZyZhvHZa6YWg8MMrO+GWWjgEUA7j7H3T9D3E3vh8S/6nu7+1Z3/467TyR26ZwOnJObn9FqrK2V1xP/4gcg7XKqavoNqcx9NUax674aNxO7wD4P3J222PbGLtfPuEZTHT7s7qcSu+ZeB36Rli9x9y+7ew3wT8Q91Mft5bWlCKj7S4qSu68xs/8Afp52NT1C7GL5AHCSu3+zhXMWmtlTwPfN7OvEVsl5pOMM6W6CD7v7cjNbnZ623cxOIg46zwbeTa+zvfn358BS4vbBe3I7scV0O3FPkSuBmRldXwDfMLOZxBbQJcRxjya/Bl4mTr/+fBvXCukkhUwPAj9Lp3I78ElgIvB7MxtKHPuZQRxzWkdaT2Z2FrErsg5YRUyU+ahDKTC1VKRopZuXfQ34NnEf94XEmVL37uG0zwCjiX9x/w74T3f/Y/rZVGCWma0jDtqfnf4lP4w42P8u8R/yvwC37uEa95vZuoxHW5MJmlwDfMrMVpnZT1s6wN1nAP9O3BF0MXFCwdnNDptOHPd5kTio/suM8+uAvxH/UX+8jXiOISaHzMcaYkvtUmK32zeJY1QNxH9PLiXW7UrgBOCf0+86CpiZ1u19xBlj89q4vhQh7aci0oWkU4rHu/vcPRxzI1Dv7t/uuMikVKj7S6SEmNlo4BPA4QUORboodX+JlAgz+2/gVeBH6nqSfFH3l4iI5IxaKiIikjNKKiIikjNKKiIikjNKKiIikjNKKiIikjP/P0Xo35DrYJ3VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_xlabel('Cross Entropy Loss')\n",
    "ax.set_ylabel('Number of Iterations')\n",
    "ax.plot(cost_list,np.arange(max_iter))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
