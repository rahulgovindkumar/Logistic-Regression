{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Rahul Govindkumar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is library for scientific computing in Python. It has efficient implementation of n-dimensional array (tensor) manupulations, which is useful for machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a list into numpy array (tensor)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 4],\n",
       "       [2, 6, 9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [[1, 2, 4], [2, 6, 9]]\n",
    "a = np.array(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the dimensions of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply simple arithmetic operation on all element of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  6, 12],\n",
       "       [ 6, 18, 27]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can transpose a tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 6],\n",
       "       [4, 9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a.T.shape)\n",
    "a.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply aggregate functions on the whole tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or on one dimension of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  8, 13])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 17])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do element-wise arithmetic operation on two tensors (of the same size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6, 20],\n",
       "       [ 2, 12,  9]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = np.array([[1, 2, 4], [2, 6, 9]])\n",
    "c2 = np.array([[2, 3, 5], [1, 2, 1]])\n",
    "c1 * c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to multiply all columns of a tensor by vector (for example if you want to multiply all data features by their lables) you need a trick. This multiplication shows up in calculating the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 4]\n",
      " [2 6 9]]\n",
      "[ 1 -1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 4], [2, 6, 9]])\n",
    "b = np.array([1,-1])\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to multiply the first row of a by 1 and the second row of a by -1. Simply multiplying a by b does not work because a and b do not have the same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this multiplication we first have to assume b has one column and then repeat the column of b with the number of columns in a. We use tile function to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1],\n",
       "       [-1, -1, -1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_repeat = np.tile(b,  (a.shape[1],1)).T\n",
    "print(b_repeat.shape)\n",
    "b_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can multiply each column of a by b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  4],\n",
       "       [-2, -6, -9]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create inital random vector using numpy (using N(0,1)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0 #mean\n",
    "sigma = 1 #standard deviation\n",
    "r = np.random.normal(mu,sigma, 1000) #draws 1000 samples from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply functions on tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of Normal distribution\n",
    "def normal(x, mu, sigma):\n",
    "    return np.exp( -0.5 * ((x-mu)/sigma)**2)/np.sqrt(2.0*np.pi*sigma**2)\n",
    "\n",
    "#probability of samples on the Normal distribution\n",
    "probabilities = normal(r, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has useful APIs for analysis. Here we plot the histogram of samples and also plot the probabilies to see if the samples follow the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x21d39eb53a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bins = np.histogram(r,50,density=True)\n",
    "plt.hist(bins[:-1], bins, weights=counts)\n",
    "plt.scatter(r, probabilities, c='b', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    p = re.compile(',')\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    header = f.readline().strip()\n",
    "    varnames = p.split(header)\n",
    "    namehash = {}\n",
    "    for l in f:\n",
    "        li = p.split(l.strip())\n",
    "        xdata.append([float(x) for x in li[:-1]])\n",
    "        ydata.append(float(li[-1]))\n",
    "    \n",
    "    return np.array(xdata), np.array(ydata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming our data is x is available in numpy we use numpy to implement logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain_whole, ytrain_whole) = read_data('datasets/spambase-train.csv')\n",
    "(xtest, ytest) = read_data('datasets/spambase-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of xtrain: (3601, 54)\n",
      "The shape of ytrain: (3601,)\n",
      "The shape of xtest: (1000, 54)\n",
      "The shape of ytest: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of xtrain:\", xtrain_whole.shape)\n",
    "print(\"The shape of ytrain:\", ytrain_whole.shape)\n",
    "print(\"The shape of xtest:\", xtest.shape)\n",
    "print(\"The shape of ytest:\", ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before training make we normalize the input data (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmean = np.mean(xtrain_whole, axis=0)\n",
    "xstd = np.std(xtrain_whole, axis=0)\n",
    "xtrain_normal_whole = (xtrain_whole-xmean) / xstd\n",
    "xtest_normal = (xtest-xmean) / xstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a validation set. We create an array of indecies and permute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "premute_indicies = np.random.permutation(np.arange(xtrain_whole.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the first 2600 data points as the training data and rest as the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_normal = xtrain_normal_whole[premute_indicies[:2600]]\n",
    "ytrain = ytrain_whole[premute_indicies[:2600]]\n",
    "xval_normal = xtrain_normal_whole[premute_indicies[2600:]]\n",
    "yval = ytrain_whole[premute_indicies[2600:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiallizing the weights and bias with random values from N(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(0, 1, xtrain_normal.shape[1]);\n",
    "bias = np.random.normal(0,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sigmoid function\n",
    "def sigmoid(v):\n",
    "    #return np.exp(-np.logaddexp(0, -v)) #numerically stable implementation of sigmoid function \n",
    "    return 1.0 / (1+np.exp(-v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use dot-product from numpy to calculate the margin and pass it to the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w: weight vector (numpy array of size n)\n",
    "#b: numpy array of size 1\n",
    "#returns p(y=1|x, w, b)\n",
    "def prob(x, w, b):\n",
    "    return sigmoid(np.dot(x,w) + b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate $l_2$ penalty using linalg library of numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.464679606069855"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Cross Entropy Loss} = -\\frac{1}{|D|}[\\sum_{(y^i,\\mathbf{x}^i)\\in\\mathcal{D}} \n",
    " y^i \\log p(y=1|\\mathbf{x}^i;\\mathbf{w},b)  +  (1-y^i) \\log (1 - p(y=1|\\mathbf{x}^i;\\mathbf{w},b))]+\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w: weight vector (numpy array of size n)\n",
    "#y_prob: p(y|x, w, b)\n",
    "#y_true: class variable data\n",
    "#lambda_: l2 penalty coefficient\n",
    "#returns the cross entropy loss\n",
    "def loss(w, y_prob, y_true, lambda_):\n",
    "    \n",
    "    n_sample=len(w)\n",
    "    \n",
    "    cross_entropy_Loss=-(1/abs(n_sample))*np.sum((y_true * np.log(y_prob + 0.0000000000000000001)) + (1-y_true)*np.log(1-y_prob + 0.0000000000000000001)) + lambda_*0.5*np.linalg.norm(w)\n",
    "    \n",
    "    return cross_entropy_Loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x: input variables (data of size m x n with m data point and n features)\n",
    "#w: weight vector (numpy array of size n)\n",
    "#y_prob: p(y|x, w, b)\n",
    "#y_true: class variable data\n",
    "#lambda_: l2 penalty coefficient\n",
    "#returns tuple of gradient w.r.t w and w.r.t to bias\n",
    "\n",
    "def grad_w_b(x, w, y_prob, y_true, lambda_):\n",
    "    \n",
    "    n_samples=len(x)\n",
    "#     grad_w = np.zeros(w.shape)\n",
    "#     grad_b = 0.0\n",
    "    \n",
    "    grad_w = (1/n_samples) * np.dot(x.T, (y_prob - y_true))\n",
    "    grad_b = (1/n_samples) * np.sum(y_prob -y_true)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#lambda_ is the coeffienct of l2 norm penalty\n",
    "#learning_rate is learning rate of gradient descent algorithm\n",
    "#max_iter determines the maximum number of iterations if the gradients descent does not converge.\n",
    "#continue the training while gradient > 0.1 or the number steps is less max_iter\n",
    "\n",
    "#returns model as tuple of (weights,bias)\n",
    "\n",
    "def fit(x, y_true, learning_rate, lambda_, max_iter, verbose=0):\n",
    "    weights = np.random.normal(0, 1, x.shape[1]);\n",
    "    bias = np.random.normal(0,1,1)\n",
    "    \n",
    "    for i in range (max_iter):\n",
    "        \n",
    "        y_prod = prob(x, weights, bias)\n",
    "        dw,db = grad_w_b(x, weights, y_prod, y_true, lambda_)\n",
    "        \n",
    "        weights = weights - learning_rate * dw\n",
    "        bias = bias - learning_rate * db\n",
    "        \n",
    "        cross_loss = loss(weights, y_prod, y_true, lambda_)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration Number: \"+str(i)+\"\\nLoss: \"+ str(cross_loss)+\"\\nl2 norm of gradients: \"+str(np.linalg.norm(dw))+\"\\nl2 norm of weights: \"+str(np.linalg.norm(weights)))\n",
    "            print(\"---------------------\")\n",
    "            cost_list.append(cross_loss)\n",
    "    #change the condition appropriately\n",
    "#     while True:\n",
    "        \n",
    "#         if verbose: #verbose is used for debugging purposes\n",
    "#             #print iteration number, loss, l2 norm of gradients, l2 norm of weights\n",
    "#             pass\n",
    "    return (weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y_true, model):\n",
    "    w, b = model\n",
    "    return np.sum((prob(x, w, b)>0.5).astype(np.double) == y_true)  / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 0\n",
      "Loss: 165.08992278766098\n",
      "l2 norm of gradients: 1.5712841571143472\n",
      "l2 norm of weights: 6.749898914526341\n",
      "---------------------\n",
      "Iteration Number: 1\n",
      "Loss: 164.96958129222804\n",
      "l2 norm of gradients: 1.5706966434456426\n",
      "l2 norm of weights: 6.749433510757873\n",
      "---------------------\n",
      "Iteration Number: 2\n",
      "Loss: 164.8493290889799\n",
      "l2 norm of gradients: 1.5701090226134353\n",
      "l2 norm of weights: 6.74896845133604\n",
      "---------------------\n",
      "Iteration Number: 3\n",
      "Loss: 164.7291661691766\n",
      "l2 norm of gradients: 1.5695212957976998\n",
      "l2 norm of weights: 6.748503736038181\n",
      "---------------------\n",
      "Iteration Number: 4\n",
      "Loss: 164.6090925240257\n",
      "l2 norm of gradients: 1.5689334641718993\n",
      "l2 norm of weights: 6.748039364641625\n",
      "---------------------\n",
      "Iteration Number: 5\n",
      "Loss: 164.48910814463196\n",
      "l2 norm of gradients: 1.5683455289030148\n",
      "l2 norm of weights: 6.747575336923698\n",
      "---------------------\n",
      "Iteration Number: 6\n",
      "Loss: 164.36921302180806\n",
      "l2 norm of gradients: 1.5677574911515728\n",
      "l2 norm of weights: 6.747111652661718\n",
      "---------------------\n",
      "Iteration Number: 7\n",
      "Loss: 164.24940714573742\n",
      "l2 norm of gradients: 1.567169352071679\n",
      "l2 norm of weights: 6.746648311633001\n",
      "---------------------\n",
      "Iteration Number: 8\n",
      "Loss: 164.12969050669403\n",
      "l2 norm of gradients: 1.5665811128110474\n",
      "l2 norm of weights: 6.746185313614855\n",
      "---------------------\n",
      "Iteration Number: 9\n",
      "Loss: 164.01006309459927\n",
      "l2 norm of gradients: 1.565992774511037\n",
      "l2 norm of weights: 6.745722658384585\n",
      "---------------------\n",
      "Iteration Number: 10\n",
      "Loss: 163.8905248989357\n",
      "l2 norm of gradients: 1.5654043383066818\n",
      "l2 norm of weights: 6.745260345719499\n",
      "---------------------\n",
      "Iteration Number: 11\n",
      "Loss: 163.77107590929776\n",
      "l2 norm of gradients: 1.5648158053267311\n",
      "l2 norm of weights: 6.7447983753969\n",
      "---------------------\n",
      "Iteration Number: 12\n",
      "Loss: 163.65171611449006\n",
      "l2 norm of gradients: 1.5642271766936815\n",
      "l2 norm of weights: 6.744336747194092\n",
      "---------------------\n",
      "Iteration Number: 13\n",
      "Loss: 163.53244550359398\n",
      "l2 norm of gradients: 1.563638453523817\n",
      "l2 norm of weights: 6.743875460888381\n",
      "---------------------\n",
      "Iteration Number: 14\n",
      "Loss: 163.41326406514648\n",
      "l2 norm of gradients: 1.563049636927246\n",
      "l2 norm of weights: 6.743414516257074\n",
      "---------------------\n",
      "Iteration Number: 15\n",
      "Loss: 163.29417178735827\n",
      "l2 norm of gradients: 1.5624607280079403\n",
      "l2 norm of weights: 6.742953913077481\n",
      "---------------------\n",
      "Iteration Number: 16\n",
      "Loss: 163.17516865843484\n",
      "l2 norm of gradients: 1.5618717278637757\n",
      "l2 norm of weights: 6.742493651126916\n",
      "---------------------\n",
      "Iteration Number: 17\n",
      "Loss: 163.05625466623042\n",
      "l2 norm of gradients: 1.5612826375865716\n",
      "l2 norm of weights: 6.7420337301826985\n",
      "---------------------\n",
      "Iteration Number: 18\n",
      "Loss: 162.93742979803687\n",
      "l2 norm of gradients: 1.5606934582621332\n",
      "l2 norm of weights: 6.741574150022154\n",
      "---------------------\n",
      "Iteration Number: 19\n",
      "Loss: 162.81869404145257\n",
      "l2 norm of gradients: 1.5601041909702935\n",
      "l2 norm of weights: 6.741114910422614\n",
      "---------------------\n",
      "Iteration Number: 20\n",
      "Loss: 162.7000473834082\n",
      "l2 norm of gradients: 1.5595148367849556\n",
      "l2 norm of weights: 6.740656011161419\n",
      "---------------------\n",
      "Iteration Number: 21\n",
      "Loss: 162.581489810618\n",
      "l2 norm of gradients: 1.5589253967741357\n",
      "l2 norm of weights: 6.740197452015916\n",
      "---------------------\n",
      "Iteration Number: 22\n",
      "Loss: 162.46302130988988\n",
      "l2 norm of gradients: 1.5583358720000084\n",
      "l2 norm of weights: 6.739739232763462\n",
      "---------------------\n",
      "Iteration Number: 23\n",
      "Loss: 162.3446418675239\n",
      "l2 norm of gradients: 1.5577462635189494\n",
      "l2 norm of weights: 6.739281353181427\n",
      "---------------------\n",
      "Iteration Number: 24\n",
      "Loss: 162.226351469332\n",
      "l2 norm of gradients: 1.557156572381581\n",
      "l2 norm of weights: 6.738823813047192\n",
      "---------------------\n",
      "Iteration Number: 25\n",
      "Loss: 162.1081501013587\n",
      "l2 norm of gradients: 1.5565667996328192\n",
      "l2 norm of weights: 6.7383666121381465\n",
      "---------------------\n",
      "Iteration Number: 26\n",
      "Loss: 161.99003774907868\n",
      "l2 norm of gradients: 1.555976946311917\n",
      "l2 norm of weights: 6.7379097502316965\n",
      "---------------------\n",
      "Iteration Number: 27\n",
      "Loss: 161.87201439798503\n",
      "l2 norm of gradients: 1.5553870134525123\n",
      "l2 norm of weights: 6.737453227105262\n",
      "---------------------\n",
      "Iteration Number: 28\n",
      "Loss: 161.7540800330982\n",
      "l2 norm of gradients: 1.5547970020826742\n",
      "l2 norm of weights: 6.7369970425362755\n",
      "---------------------\n",
      "Iteration Number: 29\n",
      "Loss: 161.63623463927613\n",
      "l2 norm of gradients: 1.5542069132249503\n",
      "l2 norm of weights: 6.7365411963021895\n",
      "---------------------\n",
      "Iteration Number: 30\n",
      "Loss: 161.51847820119144\n",
      "l2 norm of gradients: 1.553616747896414\n",
      "l2 norm of weights: 6.736085688180468\n",
      "---------------------\n",
      "Iteration Number: 31\n",
      "Loss: 161.40081070342282\n",
      "l2 norm of gradients: 1.5530265071087113\n",
      "l2 norm of weights: 6.735630517948597\n",
      "---------------------\n",
      "Iteration Number: 32\n",
      "Loss: 161.2832321298334\n",
      "l2 norm of gradients: 1.5524361918681109\n",
      "l2 norm of weights: 6.735175685384079\n",
      "---------------------\n",
      "Iteration Number: 33\n",
      "Loss: 161.16574246470014\n",
      "l2 norm of gradients: 1.5518458031755504\n",
      "l2 norm of weights: 6.734721190264436\n",
      "---------------------\n",
      "Iteration Number: 34\n",
      "Loss: 161.04834169143888\n",
      "l2 norm of gradients: 1.5512553420266861\n",
      "l2 norm of weights: 6.7342670323672085\n",
      "---------------------\n",
      "Iteration Number: 35\n",
      "Loss: 160.93102979359915\n",
      "l2 norm of gradients: 1.550664809411942\n",
      "l2 norm of weights: 6.733813211469962\n",
      "---------------------\n",
      "Iteration Number: 36\n",
      "Loss: 160.81380675469694\n",
      "l2 norm of gradients: 1.5500742063165565\n",
      "l2 norm of weights: 6.733359727350279\n",
      "---------------------\n",
      "Iteration Number: 37\n",
      "Loss: 160.69667255736056\n",
      "l2 norm of gradients: 1.549483533720635\n",
      "l2 norm of weights: 6.732906579785767\n",
      "---------------------\n",
      "Iteration Number: 38\n",
      "Loss: 160.5796271847534\n",
      "l2 norm of gradients: 1.548892792599196\n",
      "l2 norm of weights: 6.732453768554058\n",
      "---------------------\n",
      "Iteration Number: 39\n",
      "Loss: 160.4626706193951\n",
      "l2 norm of gradients: 1.5483019839222238\n",
      "l2 norm of weights: 6.732001293432806\n",
      "---------------------\n",
      "Iteration Number: 40\n",
      "Loss: 160.34580284343997\n",
      "l2 norm of gradients: 1.5477111086547144\n",
      "l2 norm of weights: 6.731549154199692\n",
      "---------------------\n",
      "Iteration Number: 41\n",
      "Loss: 160.22902383932086\n",
      "l2 norm of gradients: 1.5471201677567281\n",
      "l2 norm of weights: 6.731097350632423\n",
      "---------------------\n",
      "Iteration Number: 42\n",
      "Loss: 160.11233358868444\n",
      "l2 norm of gradients: 1.546529162183439\n",
      "l2 norm of weights: 6.730645882508728\n",
      "---------------------\n",
      "Iteration Number: 43\n",
      "Loss: 159.99573207357665\n",
      "l2 norm of gradients: 1.5459380928851834\n",
      "l2 norm of weights: 6.730194749606371\n",
      "---------------------\n",
      "Iteration Number: 44\n",
      "Loss: 159.87921927517792\n",
      "l2 norm of gradients: 1.5453469608075114\n",
      "l2 norm of weights: 6.729743951703138\n",
      "---------------------\n",
      "Iteration Number: 45\n",
      "Loss: 159.7627951749452\n",
      "l2 norm of gradients: 1.5447557668912364\n",
      "l2 norm of weights: 6.729293488576847\n",
      "---------------------\n",
      "Iteration Number: 46\n",
      "Loss: 159.64645975398778\n",
      "l2 norm of gradients: 1.5441645120724852\n",
      "l2 norm of weights: 6.728843360005346\n",
      "---------------------\n",
      "Iteration Number: 47\n",
      "Loss: 159.53021299301133\n",
      "l2 norm of gradients: 1.5435731972827478\n",
      "l2 norm of weights: 6.728393565766512\n",
      "---------------------\n",
      "Iteration Number: 48\n",
      "Loss: 159.41405487270887\n",
      "l2 norm of gradients: 1.542981823448929\n",
      "l2 norm of weights: 6.727944105638256\n",
      "---------------------\n",
      "Iteration Number: 49\n",
      "Loss: 159.29798537362254\n",
      "l2 norm of gradients: 1.542390391493397\n",
      "l2 norm of weights: 6.727494979398519\n",
      "---------------------\n",
      "Iteration Number: 50\n",
      "Loss: 159.18200447603593\n",
      "l2 norm of gradients: 1.5417989023340344\n",
      "l2 norm of weights: 6.727046186825276\n",
      "---------------------\n",
      "Iteration Number: 51\n",
      "Loss: 159.0661121596922\n",
      "l2 norm of gradients: 1.5412073568842892\n",
      "l2 norm of weights: 6.726597727696533\n",
      "---------------------\n",
      "Iteration Number: 52\n",
      "Loss: 158.95030840460035\n",
      "l2 norm of gradients: 1.5406157560532239\n",
      "l2 norm of weights: 6.726149601790336\n",
      "---------------------\n",
      "Iteration Number: 53\n",
      "Loss: 158.83459319045713\n",
      "l2 norm of gradients: 1.5400241007455662\n",
      "l2 norm of weights: 6.725701808884763\n",
      "---------------------\n",
      "Iteration Number: 54\n",
      "Loss: 158.7189664965225\n",
      "l2 norm of gradients: 1.539432391861759\n",
      "l2 norm of weights: 6.725254348757925\n",
      "---------------------\n",
      "Iteration Number: 55\n",
      "Loss: 158.60342830204382\n",
      "l2 norm of gradients: 1.538840630298012\n",
      "l2 norm of weights: 6.7248072211879775\n",
      "---------------------\n",
      "Iteration Number: 56\n",
      "Loss: 158.48797858613568\n",
      "l2 norm of gradients: 1.5382488169463493\n",
      "l2 norm of weights: 6.724360425953105\n",
      "---------------------\n",
      "Iteration Number: 57\n",
      "Loss: 158.3726173274307\n",
      "l2 norm of gradients: 1.5376569526946626\n",
      "l2 norm of weights: 6.723913962831536\n",
      "---------------------\n",
      "Iteration Number: 58\n",
      "Loss: 158.2573445047843\n",
      "l2 norm of gradients: 1.5370650384267583\n",
      "l2 norm of weights: 6.723467831601536\n",
      "---------------------\n",
      "Iteration Number: 59\n",
      "Loss: 158.14216009638704\n",
      "l2 norm of gradients: 1.53647307502241\n",
      "l2 norm of weights: 6.723022032041412\n",
      "---------------------\n",
      "Iteration Number: 60\n",
      "Loss: 158.0270640806605\n",
      "l2 norm of gradients: 1.535881063357407\n",
      "l2 norm of weights: 6.722576563929508\n",
      "---------------------\n",
      "Iteration Number: 61\n",
      "Loss: 157.9120564354215\n",
      "l2 norm of gradients: 1.5352890043036045\n",
      "l2 norm of weights: 6.7221314270442125\n",
      "---------------------\n",
      "Iteration Number: 62\n",
      "Loss: 157.79713713874997\n",
      "l2 norm of gradients: 1.5346968987289742\n",
      "l2 norm of weights: 6.721686621163953\n",
      "---------------------\n",
      "Iteration Number: 63\n",
      "Loss: 157.68230616818164\n",
      "l2 norm of gradients: 1.5341047474976524\n",
      "l2 norm of weights: 6.721242146067203\n",
      "---------------------\n",
      "Iteration Number: 64\n",
      "Loss: 157.5675635012241\n",
      "l2 norm of gradients: 1.533512551469992\n",
      "l2 norm of weights: 6.7207980015324775\n",
      "---------------------\n",
      "Iteration Number: 65\n",
      "Loss: 157.45290911507098\n",
      "l2 norm of gradients: 1.5329203115026098\n",
      "l2 norm of weights: 6.720354187338336\n",
      "---------------------\n",
      "Iteration Number: 66\n",
      "Loss: 157.33834298693617\n",
      "l2 norm of gradients: 1.5323280284484364\n",
      "l2 norm of weights: 6.719910703263382\n",
      "---------------------\n",
      "Iteration Number: 67\n",
      "Loss: 157.2238650935418\n",
      "l2 norm of gradients: 1.5317357031567673\n",
      "l2 norm of weights: 6.719467549086265\n",
      "---------------------\n",
      "Iteration Number: 68\n",
      "Loss: 157.10947541171623\n",
      "l2 norm of gradients: 1.531143336473309\n",
      "l2 norm of weights: 6.719024724585682\n",
      "---------------------\n",
      "Iteration Number: 69\n",
      "Loss: 156.99517391808175\n",
      "l2 norm of gradients: 1.5305509292402308\n",
      "l2 norm of weights: 6.718582229540376\n",
      "---------------------\n",
      "Iteration Number: 70\n",
      "Loss: 156.88096058878114\n",
      "l2 norm of gradients: 1.5299584822962125\n",
      "l2 norm of weights: 6.718140063729136\n",
      "---------------------\n",
      "Iteration Number: 71\n",
      "Loss: 156.76683540005627\n",
      "l2 norm of gradients: 1.5293659964764923\n",
      "l2 norm of weights: 6.717698226930802\n",
      "---------------------\n",
      "Iteration Number: 72\n",
      "Loss: 156.65279832786504\n",
      "l2 norm of gradients: 1.5287734726129176\n",
      "l2 norm of weights: 6.717256718924261\n",
      "---------------------\n",
      "Iteration Number: 73\n",
      "Loss: 156.53884934816153\n",
      "l2 norm of gradients: 1.5281809115339906\n",
      "l2 norm of weights: 6.716815539488453\n",
      "---------------------\n",
      "Iteration Number: 74\n",
      "Loss: 156.42498843648198\n",
      "l2 norm of gradients: 1.5275883140649191\n",
      "l2 norm of weights: 6.716374688402364\n",
      "---------------------\n",
      "Iteration Number: 75\n",
      "Loss: 156.31121556812403\n",
      "l2 norm of gradients: 1.5269956810276615\n",
      "l2 norm of weights: 6.7159341654450335\n",
      "---------------------\n",
      "Iteration Number: 76\n",
      "Loss: 156.19753071864884\n",
      "l2 norm of gradients: 1.5264030132409776\n",
      "l2 norm of weights: 6.715493970395554\n",
      "---------------------\n",
      "Iteration Number: 77\n",
      "Loss: 156.0839338629171\n",
      "l2 norm of gradients: 1.5258103115204742\n",
      "l2 norm of weights: 6.715054103033068\n",
      "---------------------\n",
      "Iteration Number: 78\n",
      "Loss: 155.97042497611227\n",
      "l2 norm of gradients: 1.5252175766786524\n",
      "l2 norm of weights: 6.714614563136772\n",
      "---------------------\n",
      "Iteration Number: 79\n",
      "Loss: 155.85700403260807\n",
      "l2 norm of gradients: 1.5246248095249555\n",
      "l2 norm of weights: 6.714175350485918\n",
      "---------------------\n",
      "Iteration Number: 80\n",
      "Loss: 155.7436710074066\n",
      "l2 norm of gradients: 1.524032010865814\n",
      "l2 norm of weights: 6.713736464859809\n",
      "---------------------\n",
      "Iteration Number: 81\n",
      "Loss: 155.63042587451366\n",
      "l2 norm of gradients: 1.5234391815046946\n",
      "l2 norm of weights: 6.713297906037808\n",
      "---------------------\n",
      "Iteration Number: 82\n",
      "Loss: 155.51726860860754\n",
      "l2 norm of gradients: 1.5228463222421427\n",
      "l2 norm of weights: 6.712859673799331\n",
      "---------------------\n",
      "Iteration Number: 83\n",
      "Loss: 155.40419918328695\n",
      "l2 norm of gradients: 1.5222534338758313\n",
      "l2 norm of weights: 6.712421767923849\n",
      "---------------------\n",
      "Iteration Number: 84\n",
      "Loss: 155.29121757285998\n",
      "l2 norm of gradients: 1.5216605172006052\n",
      "l2 norm of weights: 6.711984188190894\n",
      "---------------------\n",
      "Iteration Number: 85\n",
      "Loss: 155.1783237507417\n",
      "l2 norm of gradients: 1.5210675730085241\n",
      "l2 norm of weights: 6.711546934380054\n",
      "---------------------\n",
      "Iteration Number: 86\n",
      "Loss: 155.06551769075284\n",
      "l2 norm of gradients: 1.5204746020889104\n",
      "l2 norm of weights: 6.711110006270974\n",
      "---------------------\n",
      "Iteration Number: 87\n",
      "Loss: 154.95279936626457\n",
      "l2 norm of gradients: 1.5198816052283903\n",
      "l2 norm of weights: 6.710673403643361\n",
      "---------------------\n",
      "Iteration Number: 88\n",
      "Loss: 154.84016875044284\n",
      "l2 norm of gradients: 1.5192885832109397\n",
      "l2 norm of weights: 6.710237126276981\n",
      "---------------------\n",
      "Iteration Number: 89\n",
      "Loss: 154.72762581630093\n",
      "l2 norm of gradients: 1.5186955368179251\n",
      "l2 norm of weights: 6.709801173951661\n",
      "---------------------\n",
      "Iteration Number: 90\n",
      "Loss: 154.61517053687675\n",
      "l2 norm of gradients: 1.518102466828149\n",
      "l2 norm of weights: 6.709365546447288\n",
      "---------------------\n",
      "Iteration Number: 91\n",
      "Loss: 154.5028028849822\n",
      "l2 norm of gradients: 1.5175093740178895\n",
      "l2 norm of weights: 6.70893024354381\n",
      "---------------------\n",
      "Iteration Number: 92\n",
      "Loss: 154.39052283306305\n",
      "l2 norm of gradients: 1.516916259160943\n",
      "l2 norm of weights: 6.708495265021241\n",
      "---------------------\n",
      "Iteration Number: 93\n",
      "Loss: 154.27833035355707\n",
      "l2 norm of gradients: 1.5163231230286642\n",
      "l2 norm of weights: 6.708060610659655\n",
      "---------------------\n",
      "Iteration Number: 94\n",
      "Loss: 154.16622541892693\n",
      "l2 norm of gradients: 1.5157299663900086\n",
      "l2 norm of weights: 6.707626280239191\n",
      "---------------------\n",
      "Iteration Number: 95\n",
      "Loss: 154.05420800114223\n",
      "l2 norm of gradients: 1.515136790011569\n",
      "l2 norm of weights: 6.707192273540053\n",
      "---------------------\n",
      "Iteration Number: 96\n",
      "Loss: 153.94227807236902\n",
      "l2 norm of gradients: 1.5145435946576176\n",
      "l2 norm of weights: 6.706758590342511\n",
      "---------------------\n",
      "Iteration Number: 97\n",
      "Loss: 153.8304356040997\n",
      "l2 norm of gradients: 1.5139503810901418\n",
      "l2 norm of weights: 6.706325230426897\n",
      "---------------------\n",
      "Iteration Number: 98\n",
      "Loss: 153.71868056830277\n",
      "l2 norm of gradients: 1.5133571500688832\n",
      "l2 norm of weights: 6.705892193573613\n",
      "---------------------\n",
      "Iteration Number: 99\n",
      "Loss: 153.60701293625243\n",
      "l2 norm of gradients: 1.512763902351373\n",
      "l2 norm of weights: 6.705459479563127\n",
      "---------------------\n",
      "Iteration Number: 100\n",
      "Loss: 153.49543267942067\n",
      "l2 norm of gradients: 1.51217063869297\n",
      "l2 norm of weights: 6.7050270881759735\n",
      "---------------------\n",
      "Iteration Number: 101\n",
      "Loss: 153.38393976904862\n",
      "l2 norm of gradients: 1.5115773598468942\n",
      "l2 norm of weights: 6.70459501919276\n",
      "---------------------\n",
      "Iteration Number: 102\n",
      "Loss: 153.27253417613974\n",
      "l2 norm of gradients: 1.5109840665642595\n",
      "l2 norm of weights: 6.704163272394155\n",
      "---------------------\n",
      "Iteration Number: 103\n",
      "Loss: 153.16121587154998\n",
      "l2 norm of gradients: 1.5103907595941104\n",
      "l2 norm of weights: 6.7037318475609045\n",
      "---------------------\n",
      "Iteration Number: 104\n",
      "Loss: 153.04998482615528\n",
      "l2 norm of gradients: 1.509797439683451\n",
      "l2 norm of weights: 6.703300744473821\n",
      "---------------------\n",
      "Iteration Number: 105\n",
      "Loss: 152.93884101052765\n",
      "l2 norm of gradients: 1.5092041075772789\n",
      "l2 norm of weights: 6.702869962913788\n",
      "---------------------\n",
      "Iteration Number: 106\n",
      "Loss: 152.82778439492748\n",
      "l2 norm of gradients: 1.508610764018613\n",
      "l2 norm of weights: 6.702439502661761\n",
      "---------------------\n",
      "Iteration Number: 107\n",
      "Loss: 152.7168149499206\n",
      "l2 norm of gradients: 1.5080174097485237\n",
      "l2 norm of weights: 6.702009363498768\n",
      "---------------------\n",
      "Iteration Number: 108\n",
      "Loss: 152.60593264551278\n",
      "l2 norm of gradients: 1.5074240455061614\n",
      "l2 norm of weights: 6.70157954520591\n",
      "---------------------\n",
      "Iteration Number: 109\n",
      "Loss: 152.49513745181252\n",
      "l2 norm of gradients: 1.506830672028783\n",
      "l2 norm of weights: 6.701150047564361\n",
      "---------------------\n",
      "Iteration Number: 110\n",
      "Loss: 152.3844293388084\n",
      "l2 norm of gradients: 1.5062372900517758\n",
      "l2 norm of weights: 6.70072087035537\n",
      "---------------------\n",
      "Iteration Number: 111\n",
      "Loss: 152.27380827597787\n",
      "l2 norm of gradients: 1.5056439003086843\n",
      "l2 norm of weights: 6.700292013360259\n",
      "---------------------\n",
      "Iteration Number: 112\n",
      "Loss: 152.1632742330538\n",
      "l2 norm of gradients: 1.5050505035312318\n",
      "l2 norm of weights: 6.699863476360427\n",
      "---------------------\n",
      "Iteration Number: 113\n",
      "Loss: 152.05282717948558\n",
      "l2 norm of gradients: 1.504457100449343\n",
      "l2 norm of weights: 6.699435259137348\n",
      "---------------------\n",
      "Iteration Number: 114\n",
      "Loss: 151.9424670844681\n",
      "l2 norm of gradients: 1.5038636917911616\n",
      "l2 norm of weights: 6.699007361472575\n",
      "---------------------\n",
      "Iteration Number: 115\n",
      "Loss: 151.83219391732646\n",
      "l2 norm of gradients: 1.5032702782830738\n",
      "l2 norm of weights: 6.6985797831477365\n",
      "---------------------\n",
      "Iteration Number: 116\n",
      "Loss: 151.72200764712008\n",
      "l2 norm of gradients: 1.502676860649721\n",
      "l2 norm of weights: 6.698152523944537\n",
      "---------------------\n",
      "Iteration Number: 117\n",
      "Loss: 151.6119082425951\n",
      "l2 norm of gradients: 1.5020834396140184\n",
      "l2 norm of weights: 6.697725583644763\n",
      "---------------------\n",
      "Iteration Number: 118\n",
      "Loss: 151.5018956726456\n",
      "l2 norm of gradients: 1.5014900158971685\n",
      "l2 norm of weights: 6.6972989620302785\n",
      "---------------------\n",
      "Iteration Number: 119\n",
      "Loss: 151.3919699057718\n",
      "l2 norm of gradients: 1.500896590218673\n",
      "l2 norm of weights: 6.69687265888303\n",
      "---------------------\n",
      "Iteration Number: 120\n",
      "Loss: 151.28213091047013\n",
      "l2 norm of gradients: 1.5003031632963455\n",
      "l2 norm of weights: 6.69644667398504\n",
      "---------------------\n",
      "Iteration Number: 121\n",
      "Loss: 151.17237865508346\n",
      "l2 norm of gradients: 1.499709735846319\n",
      "l2 norm of weights: 6.696021007118414\n",
      "---------------------\n",
      "Iteration Number: 122\n",
      "Loss: 151.06271310800173\n",
      "l2 norm of gradients: 1.4991163085830541\n",
      "l2 norm of weights: 6.6955956580653435\n",
      "---------------------\n",
      "Iteration Number: 123\n",
      "Loss: 150.95313423698718\n",
      "l2 norm of gradients: 1.4985228822193446\n",
      "l2 norm of weights: 6.695170626608095\n",
      "---------------------\n",
      "Iteration Number: 124\n",
      "Loss: 150.84364201020088\n",
      "l2 norm of gradients: 1.4979294574663218\n",
      "l2 norm of weights: 6.694745912529023\n",
      "---------------------\n",
      "Iteration Number: 125\n",
      "Loss: 150.73423639539737\n",
      "l2 norm of gradients: 1.497336035033456\n",
      "l2 norm of weights: 6.694321515610566\n",
      "---------------------\n",
      "Iteration Number: 126\n",
      "Loss: 150.62491736032356\n",
      "l2 norm of gradients: 1.496742615628556\n",
      "l2 norm of weights: 6.693897435635244\n",
      "---------------------\n",
      "Iteration Number: 127\n",
      "Loss: 150.51568487250216\n",
      "l2 norm of gradients: 1.4961491999577679\n",
      "l2 norm of weights: 6.693473672385664\n",
      "---------------------\n",
      "Iteration Number: 128\n",
      "Loss: 150.40653889923078\n",
      "l2 norm of gradients: 1.4955557887255715\n",
      "l2 norm of weights: 6.693050225644518\n",
      "---------------------\n",
      "Iteration Number: 129\n",
      "Loss: 150.29747940794874\n",
      "l2 norm of gradients: 1.494962382634773\n",
      "l2 norm of weights: 6.692627095194583\n",
      "---------------------\n",
      "Iteration Number: 130\n",
      "Loss: 150.1885063657048\n",
      "l2 norm of gradients: 1.4943689823864983\n",
      "l2 norm of weights: 6.692204280818726\n",
      "---------------------\n",
      "Iteration Number: 131\n",
      "Loss: 150.07961973966354\n",
      "l2 norm of gradients: 1.4937755886801798\n",
      "l2 norm of weights: 6.691781782299898\n",
      "---------------------\n",
      "Iteration Number: 132\n",
      "Loss: 149.97081949661083\n",
      "l2 norm of gradients: 1.4931822022135484\n",
      "l2 norm of weights: 6.691359599421143\n",
      "---------------------\n",
      "Iteration Number: 133\n",
      "Loss: 149.86210560331264\n",
      "l2 norm of gradients: 1.4925888236826144\n",
      "l2 norm of weights: 6.690937731965587\n",
      "---------------------\n",
      "Iteration Number: 134\n",
      "Loss: 149.75347802646283\n",
      "l2 norm of gradients: 1.4919954537816524\n",
      "l2 norm of weights: 6.690516179716453\n",
      "---------------------\n",
      "Iteration Number: 135\n",
      "Loss: 149.644936732731\n",
      "l2 norm of gradients: 1.4914020932031822\n",
      "l2 norm of weights: 6.690094942457049\n",
      "---------------------\n",
      "Iteration Number: 136\n",
      "Loss: 149.5364816882266\n",
      "l2 norm of gradients: 1.4908087426379464\n",
      "l2 norm of weights: 6.689674019970774\n",
      "---------------------\n",
      "Iteration Number: 137\n",
      "Loss: 149.42811285955915\n",
      "l2 norm of gradients: 1.490215402774886\n",
      "l2 norm of weights: 6.689253412041121\n",
      "---------------------\n",
      "Iteration Number: 138\n",
      "Loss: 149.31983021255914\n",
      "l2 norm of gradients: 1.4896220743011146\n",
      "l2 norm of weights: 6.688833118451675\n",
      "---------------------\n",
      "Iteration Number: 139\n",
      "Loss: 149.21163371348624\n",
      "l2 norm of gradients: 1.489028757901889\n",
      "l2 norm of weights: 6.68841313898611\n",
      "---------------------\n",
      "Iteration Number: 140\n",
      "Loss: 149.10352332816416\n",
      "l2 norm of gradients: 1.4884354542605767\n",
      "l2 norm of weights: 6.687993473428198\n",
      "---------------------\n",
      "Iteration Number: 141\n",
      "Loss: 148.9954990223427\n",
      "l2 norm of gradients: 1.4878421640586235\n",
      "l2 norm of weights: 6.687574121561801\n",
      "---------------------\n",
      "Iteration Number: 142\n",
      "Loss: 148.88756076177452\n",
      "l2 norm of gradients: 1.4872488879755157\n",
      "l2 norm of weights: 6.687155083170879\n",
      "---------------------\n",
      "Iteration Number: 143\n",
      "Loss: 148.77970851207994\n",
      "l2 norm of gradients: 1.4866556266887405\n",
      "l2 norm of weights: 6.686736358039485\n",
      "---------------------\n",
      "Iteration Number: 144\n",
      "Loss: 148.67194223853306\n",
      "l2 norm of gradients: 1.4860623808737454\n",
      "l2 norm of weights: 6.68631794595177\n",
      "---------------------\n",
      "Iteration Number: 145\n",
      "Loss: 148.5642619064371\n",
      "l2 norm of gradients: 1.4854691512038924\n",
      "l2 norm of weights: 6.68589984669198\n",
      "---------------------\n",
      "Iteration Number: 146\n",
      "Loss: 148.45666748117134\n",
      "l2 norm of gradients: 1.4848759383504109\n",
      "l2 norm of weights: 6.685482060044458\n",
      "---------------------\n",
      "Iteration Number: 147\n",
      "Loss: 148.34915892766531\n",
      "l2 norm of gradients: 1.4842827429823484\n",
      "l2 norm of weights: 6.685064585793648\n",
      "---------------------\n",
      "Iteration Number: 148\n",
      "Loss: 148.24173621088624\n",
      "l2 norm of gradients: 1.483689565766517\n",
      "l2 norm of weights: 6.684647423724088\n",
      "---------------------\n",
      "Iteration Number: 149\n",
      "Loss: 148.13439929578453\n",
      "l2 norm of gradients: 1.4830964073674382\n",
      "l2 norm of weights: 6.684230573620419\n",
      "---------------------\n",
      "Iteration Number: 150\n",
      "Loss: 148.02714814697754\n",
      "l2 norm of gradients: 1.4825032684472847\n",
      "l2 norm of weights: 6.68381403526738\n",
      "---------------------\n",
      "Iteration Number: 151\n",
      "Loss: 147.91998272910615\n",
      "l2 norm of gradients: 1.4819101496658198\n",
      "l2 norm of weights: 6.683397808449811\n",
      "---------------------\n",
      "Iteration Number: 152\n",
      "Loss: 147.8129030067472\n",
      "l2 norm of gradients: 1.481317051680332\n",
      "l2 norm of weights: 6.682981892952655\n",
      "---------------------\n",
      "Iteration Number: 153\n",
      "Loss: 147.7059089441303\n",
      "l2 norm of gradients: 1.4807239751455696\n",
      "l2 norm of weights: 6.6825662885609525\n",
      "---------------------\n",
      "Iteration Number: 154\n",
      "Loss: 147.5990005057137\n",
      "l2 norm of gradients: 1.480130920713672\n",
      "l2 norm of weights: 6.682150995059851\n",
      "---------------------\n",
      "Iteration Number: 155\n",
      "Loss: 147.4921776556354\n",
      "l2 norm of gradients: 1.4795378890340942\n",
      "l2 norm of weights: 6.6817360122346\n",
      "---------------------\n",
      "Iteration Number: 156\n",
      "Loss: 147.3854403580364\n",
      "l2 norm of gradients: 1.4789448807535353\n",
      "l2 norm of weights: 6.681321339870551\n",
      "---------------------\n",
      "Iteration Number: 157\n",
      "Loss: 147.27878857670012\n",
      "l2 norm of gradients: 1.478351896515857\n",
      "l2 norm of weights: 6.680906977753161\n",
      "---------------------\n",
      "Iteration Number: 158\n",
      "Loss: 147.17222227545776\n",
      "l2 norm of gradients: 1.477758936962006\n",
      "l2 norm of weights: 6.680492925667995\n",
      "---------------------\n",
      "Iteration Number: 159\n",
      "Loss: 147.06574141824862\n",
      "l2 norm of gradients: 1.4771660027299274\n",
      "l2 norm of weights: 6.680079183400719\n",
      "---------------------\n",
      "Iteration Number: 160\n",
      "Loss: 146.9593459686978\n",
      "l2 norm of gradients: 1.4765730944544782\n",
      "l2 norm of weights: 6.679665750737109\n",
      "---------------------\n",
      "Iteration Number: 161\n",
      "Loss: 146.85303589004505\n",
      "l2 norm of gradients: 1.4759802127673396\n",
      "l2 norm of weights: 6.679252627463047\n",
      "---------------------\n",
      "Iteration Number: 162\n",
      "Loss: 146.74681114588756\n",
      "l2 norm of gradients: 1.4753873582969232\n",
      "l2 norm of weights: 6.678839813364523\n",
      "---------------------\n",
      "Iteration Number: 163\n",
      "Loss: 146.64067169967308\n",
      "l2 norm of gradients: 1.4747945316682758\n",
      "l2 norm of weights: 6.6784273082276355\n",
      "---------------------\n",
      "Iteration Number: 164\n",
      "Loss: 146.53461751449572\n",
      "l2 norm of gradients: 1.4742017335029811\n",
      "l2 norm of weights: 6.678015111838592\n",
      "---------------------\n",
      "Iteration Number: 165\n",
      "Loss: 146.4286485533279\n",
      "l2 norm of gradients: 1.4736089644190595\n",
      "l2 norm of weights: 6.677603223983709\n",
      "---------------------\n",
      "Iteration Number: 166\n",
      "Loss: 146.32276477945734\n",
      "l2 norm of gradients: 1.4730162250308632\n",
      "l2 norm of weights: 6.677191644449416\n",
      "---------------------\n",
      "Iteration Number: 167\n",
      "Loss: 146.216966155558\n",
      "l2 norm of gradients: 1.4724235159489703\n",
      "l2 norm of weights: 6.67678037302225\n",
      "---------------------\n",
      "Iteration Number: 168\n",
      "Loss: 146.11125264459\n",
      "l2 norm of gradients: 1.4718308377800755\n",
      "l2 norm of weights: 6.676369409488865\n",
      "---------------------\n",
      "Iteration Number: 169\n",
      "Loss: 146.00562420921938\n",
      "l2 norm of gradients: 1.4712381911268773\n",
      "l2 norm of weights: 6.675958753636021\n",
      "---------------------\n",
      "Iteration Number: 170\n",
      "Loss: 145.90008081213105\n",
      "l2 norm of gradients: 1.470645576587964\n",
      "l2 norm of weights: 6.675548405250596\n",
      "---------------------\n",
      "Iteration Number: 171\n",
      "Loss: 145.79462241565577\n",
      "l2 norm of gradients: 1.4700529947576952\n",
      "l2 norm of weights: 6.6751383641195785\n",
      "---------------------\n",
      "Iteration Number: 172\n",
      "Loss: 145.6892489822811\n",
      "l2 norm of gradients: 1.4694604462260825\n",
      "l2 norm of weights: 6.674728630030074\n",
      "---------------------\n",
      "Iteration Number: 173\n",
      "Loss: 145.58396047444407\n",
      "l2 norm of gradients: 1.4688679315786652\n",
      "l2 norm of weights: 6.674319202769302\n",
      "---------------------\n",
      "Iteration Number: 174\n",
      "Loss: 145.47875685426402\n",
      "l2 norm of gradients: 1.4682754513963858\n",
      "l2 norm of weights: 6.673910082124599\n",
      "---------------------\n",
      "Iteration Number: 175\n",
      "Loss: 145.37363808400286\n",
      "l2 norm of gradients: 1.4676830062554616\n",
      "l2 norm of weights: 6.673501267883416\n",
      "---------------------\n",
      "Iteration Number: 176\n",
      "Loss: 145.26860412559807\n",
      "l2 norm of gradients: 1.4670905967272536\n",
      "l2 norm of weights: 6.67309275983332\n",
      "---------------------\n",
      "Iteration Number: 177\n",
      "Loss: 145.16365494100785\n",
      "l2 norm of gradients: 1.4664982233781345\n",
      "l2 norm of weights: 6.672684557761999\n",
      "---------------------\n",
      "Iteration Number: 178\n",
      "Loss: 145.05879049209904\n",
      "l2 norm of gradients: 1.4659058867693509\n",
      "l2 norm of weights: 6.672276661457255\n",
      "---------------------\n",
      "Iteration Number: 179\n",
      "Loss: 144.95401074069073\n",
      "l2 norm of gradients: 1.465313587456888\n",
      "l2 norm of weights: 6.671869070707015\n",
      "---------------------\n",
      "Iteration Number: 180\n",
      "Loss: 144.84931564844734\n",
      "l2 norm of gradients: 1.4647213259913259\n",
      "l2 norm of weights: 6.671461785299321\n",
      "---------------------\n",
      "Iteration Number: 181\n",
      "Loss: 144.74470517697338\n",
      "l2 norm of gradients: 1.4641291029177002\n",
      "l2 norm of weights: 6.671054805022336\n",
      "---------------------\n",
      "Iteration Number: 182\n",
      "Loss: 144.6401792878341\n",
      "l2 norm of gradients: 1.463536918775354\n",
      "l2 norm of weights: 6.670648129664344\n",
      "---------------------\n",
      "Iteration Number: 183\n",
      "Loss: 144.53573794243377\n",
      "l2 norm of gradients: 1.462944774097793\n",
      "l2 norm of weights: 6.67024175901375\n",
      "---------------------\n",
      "Iteration Number: 184\n",
      "Loss: 144.43138110202025\n",
      "l2 norm of gradients: 1.4623526694125344\n",
      "l2 norm of weights: 6.669835692859084\n",
      "---------------------\n",
      "Iteration Number: 185\n",
      "Loss: 144.327108727971\n",
      "l2 norm of gradients: 1.4617606052409566\n",
      "l2 norm of weights: 6.6694299309889935\n",
      "---------------------\n",
      "Iteration Number: 186\n",
      "Loss: 144.2229207814986\n",
      "l2 norm of gradients: 1.4611685820981446\n",
      "l2 norm of weights: 6.6690244731922546\n",
      "---------------------\n",
      "Iteration Number: 187\n",
      "Loss: 144.11881722357825\n",
      "l2 norm of gradients: 1.4605766004927359\n",
      "l2 norm of weights: 6.668619319257765\n",
      "---------------------\n",
      "Iteration Number: 188\n",
      "Loss: 144.01479801526835\n",
      "l2 norm of gradients: 1.4599846609267613\n",
      "l2 norm of weights: 6.668214468974544\n",
      "---------------------\n",
      "Iteration Number: 189\n",
      "Loss: 143.91086311762618\n",
      "l2 norm of gradients: 1.4593927638954873\n",
      "l2 norm of weights: 6.6678099221317435\n",
      "---------------------\n",
      "Iteration Number: 190\n",
      "Loss: 143.80701249154265\n",
      "l2 norm of gradients: 1.458800909887254\n",
      "l2 norm of weights: 6.667405678518635\n",
      "---------------------\n",
      "Iteration Number: 191\n",
      "Loss: 143.70324609757793\n",
      "l2 norm of gradients: 1.458209099383314\n",
      "l2 norm of weights: 6.6670017379246165\n",
      "---------------------\n",
      "Iteration Number: 192\n",
      "Loss: 143.59956389669532\n",
      "l2 norm of gradients: 1.457617332857665\n",
      "l2 norm of weights: 6.666598100139217\n",
      "---------------------\n",
      "Iteration Number: 193\n",
      "Loss: 143.4959658494459\n",
      "l2 norm of gradients: 1.457025610776888\n",
      "l2 norm of weights: 6.66619476495209\n",
      "---------------------\n",
      "Iteration Number: 194\n",
      "Loss: 143.39245191641578\n",
      "l2 norm of gradients: 1.4564339335999765\n",
      "l2 norm of weights: 6.665791732153017\n",
      "---------------------\n",
      "Iteration Number: 195\n",
      "Loss: 143.2890220582479\n",
      "l2 norm of gradients: 1.4558423017781705\n",
      "l2 norm of weights: 6.665389001531911\n",
      "---------------------\n",
      "Iteration Number: 196\n",
      "Loss: 143.18567623514218\n",
      "l2 norm of gradients: 1.4552507157547832\n",
      "l2 norm of weights: 6.664986572878809\n",
      "---------------------\n",
      "Iteration Number: 197\n",
      "Loss: 143.0824144076084\n",
      "l2 norm of gradients: 1.4546591759650342\n",
      "l2 norm of weights: 6.664584445983885\n",
      "---------------------\n",
      "Iteration Number: 198\n",
      "Loss: 142.97923653596771\n",
      "l2 norm of gradients: 1.454067682835874\n",
      "l2 norm of weights: 6.664182620637437\n",
      "---------------------\n",
      "Iteration Number: 199\n",
      "Loss: 142.87614258032986\n",
      "l2 norm of gradients: 1.4534762367858107\n",
      "l2 norm of weights: 6.663781096629898\n",
      "---------------------\n",
      "Iteration Number: 200\n",
      "Loss: 142.7731325011353\n",
      "l2 norm of gradients: 1.4528848382247375\n",
      "l2 norm of weights: 6.663379873751829\n",
      "---------------------\n",
      "Iteration Number: 201\n",
      "Loss: 142.67020625828843\n",
      "l2 norm of gradients: 1.4522934875537554\n",
      "l2 norm of weights: 6.662978951793927\n",
      "---------------------\n",
      "Iteration Number: 202\n",
      "Loss: 142.5673638119869\n",
      "l2 norm of gradients: 1.451702185164999\n",
      "l2 norm of weights: 6.66257833054702\n",
      "---------------------\n",
      "Iteration Number: 203\n",
      "Loss: 142.46460512208114\n",
      "l2 norm of gradients: 1.4511109314414583\n",
      "l2 norm of weights: 6.662178009802067\n",
      "---------------------\n",
      "Iteration Number: 204\n",
      "Loss: 142.36193014867993\n",
      "l2 norm of gradients: 1.450519726756803\n",
      "l2 norm of weights: 6.661777989350163\n",
      "---------------------\n",
      "Iteration Number: 205\n",
      "Loss: 142.2593388515332\n",
      "l2 norm of gradients: 1.4499285714752035\n",
      "l2 norm of weights: 6.661378268982537\n",
      "---------------------\n",
      "Iteration Number: 206\n",
      "Loss: 142.15683119062058\n",
      "l2 norm of gradients: 1.449337465951153\n",
      "l2 norm of weights: 6.660978848490551\n",
      "---------------------\n",
      "Iteration Number: 207\n",
      "Loss: 142.0544071256231\n",
      "l2 norm of gradients: 1.44874641052929\n",
      "l2 norm of weights: 6.660579727665704\n",
      "---------------------\n",
      "Iteration Number: 208\n",
      "Loss: 141.9520666162557\n",
      "l2 norm of gradients: 1.4481554055442174\n",
      "l2 norm of weights: 6.6601809062996296\n",
      "---------------------\n",
      "Iteration Number: 209\n",
      "Loss: 141.84980962225634\n",
      "l2 norm of gradients: 1.4475644513203265\n",
      "l2 norm of weights: 6.659782384184097\n",
      "---------------------\n",
      "Iteration Number: 210\n",
      "Loss: 141.74763610333343\n",
      "l2 norm of gradients: 1.446973548171616\n",
      "l2 norm of weights: 6.659384161111012\n",
      "---------------------\n",
      "Iteration Number: 211\n",
      "Loss: 141.6455460188794\n",
      "l2 norm of gradients: 1.446382696401515\n",
      "l2 norm of weights: 6.658986236872419\n",
      "---------------------\n",
      "Iteration Number: 212\n",
      "Loss: 141.54353932871294\n",
      "l2 norm of gradients: 1.4457918963027026\n",
      "l2 norm of weights: 6.658588611260498\n",
      "---------------------\n",
      "Iteration Number: 213\n",
      "Loss: 141.44161599218012\n",
      "l2 norm of gradients: 1.4452011481569313\n",
      "l2 norm of weights: 6.658191284067567\n",
      "---------------------\n",
      "Iteration Number: 214\n",
      "Loss: 141.33977596863005\n",
      "l2 norm of gradients: 1.4446104522348489\n",
      "l2 norm of weights: 6.657794255086083\n",
      "---------------------\n",
      "Iteration Number: 215\n",
      "Loss: 141.23801921780333\n",
      "l2 norm of gradients: 1.4440198087958205\n",
      "l2 norm of weights: 6.6573975241086405\n",
      "---------------------\n",
      "Iteration Number: 216\n",
      "Loss: 141.13634569878903\n",
      "l2 norm of gradients: 1.443429218087752\n",
      "l2 norm of weights: 6.6570010909279755\n",
      "---------------------\n",
      "Iteration Number: 217\n",
      "Loss: 141.0347553710924\n",
      "l2 norm of gradients: 1.4428386803469146\n",
      "l2 norm of weights: 6.6566049553369595\n",
      "---------------------\n",
      "Iteration Number: 218\n",
      "Loss: 140.93324819390895\n",
      "l2 norm of gradients: 1.442248195797769\n",
      "l2 norm of weights: 6.656209117128606\n",
      "---------------------\n",
      "Iteration Number: 219\n",
      "Loss: 140.8318241266817\n",
      "l2 norm of gradients: 1.4416577646527913\n",
      "l2 norm of weights: 6.6558135760960715\n",
      "---------------------\n",
      "Iteration Number: 220\n",
      "Loss: 140.73048312865376\n",
      "l2 norm of gradients: 1.4410673871123\n",
      "l2 norm of weights: 6.655418332032647\n",
      "---------------------\n",
      "Iteration Number: 221\n",
      "Loss: 140.62922515907582\n",
      "l2 norm of gradients: 1.440477063364283\n",
      "l2 norm of weights: 6.655023384731769\n",
      "---------------------\n",
      "Iteration Number: 222\n",
      "Loss: 140.52805017689323\n",
      "l2 norm of gradients: 1.4398867935842274\n",
      "l2 norm of weights: 6.654628733987012\n",
      "---------------------\n",
      "Iteration Number: 223\n",
      "Loss: 140.42695814170497\n",
      "l2 norm of gradients: 1.439296577934949\n",
      "l2 norm of weights: 6.654234379592096\n",
      "---------------------\n",
      "Iteration Number: 224\n",
      "Loss: 140.32594901225113\n",
      "l2 norm of gradients: 1.4387064165664254\n",
      "l2 norm of weights: 6.653840321340879\n",
      "---------------------\n",
      "Iteration Number: 225\n",
      "Loss: 140.22502274795244\n",
      "l2 norm of gradients: 1.4381163096156273\n",
      "l2 norm of weights: 6.653446559027364\n",
      "---------------------\n",
      "Iteration Number: 226\n",
      "Loss: 140.12417930780617\n",
      "l2 norm of gradients: 1.4375262572063559\n",
      "l2 norm of weights: 6.653053092445692\n",
      "---------------------\n",
      "Iteration Number: 227\n",
      "Loss: 140.023418650938\n",
      "l2 norm of gradients: 1.4369362594490767\n",
      "l2 norm of weights: 6.652659921390151\n",
      "---------------------\n",
      "Iteration Number: 228\n",
      "Loss: 139.92274073654005\n",
      "l2 norm of gradients: 1.4363463164407593\n",
      "l2 norm of weights: 6.6522670456551705\n",
      "---------------------\n",
      "Iteration Number: 229\n",
      "Loss: 139.82214552345428\n",
      "l2 norm of gradients: 1.4357564282647188\n",
      "l2 norm of weights: 6.651874465035321\n",
      "---------------------\n",
      "Iteration Number: 230\n",
      "Loss: 139.72163297100084\n",
      "l2 norm of gradients: 1.4351665949904557\n",
      "l2 norm of weights: 6.65148217932532\n",
      "---------------------\n",
      "Iteration Number: 231\n",
      "Loss: 139.62120303800913\n",
      "l2 norm of gradients: 1.4345768166735027\n",
      "l2 norm of weights: 6.651090188320025\n",
      "---------------------\n",
      "Iteration Number: 232\n",
      "Loss: 139.52085568362963\n",
      "l2 norm of gradients: 1.433987093355269\n",
      "l2 norm of weights: 6.65069849181444\n",
      "---------------------\n",
      "Iteration Number: 233\n",
      "Loss: 139.4205908670787\n",
      "l2 norm of gradients: 1.4333974250628914\n",
      "l2 norm of weights: 6.65030708960371\n",
      "---------------------\n",
      "Iteration Number: 234\n",
      "Loss: 139.32040854710579\n",
      "l2 norm of gradients: 1.432807811809084\n",
      "l2 norm of weights: 6.649915981483127\n",
      "---------------------\n",
      "Iteration Number: 235\n",
      "Loss: 139.2203086827953\n",
      "l2 norm of gradients: 1.4322182535919932\n",
      "l2 norm of weights: 6.6495251672481235\n",
      "---------------------\n",
      "Iteration Number: 236\n",
      "Loss: 139.12029123343143\n",
      "l2 norm of gradients: 1.431628750395053\n",
      "l2 norm of weights: 6.6491346466942804\n",
      "---------------------\n",
      "Iteration Number: 237\n",
      "Loss: 139.02035615782953\n",
      "l2 norm of gradients: 1.4310393021868444\n",
      "l2 norm of weights: 6.648744419617318\n",
      "---------------------\n",
      "Iteration Number: 238\n",
      "Loss: 138.9205034150339\n",
      "l2 norm of gradients: 1.4304499089209577\n",
      "l2 norm of weights: 6.648354485813106\n",
      "---------------------\n",
      "Iteration Number: 239\n",
      "Loss: 138.82073296423988\n",
      "l2 norm of gradients: 1.4298605705358556\n",
      "l2 norm of weights: 6.647964845077656\n",
      "---------------------\n",
      "Iteration Number: 240\n",
      "Loss: 138.7210447645651\n",
      "l2 norm of gradients: 1.429271286954743\n",
      "l2 norm of weights: 6.647575497207123\n",
      "---------------------\n",
      "Iteration Number: 241\n",
      "Loss: 138.62143877491178\n",
      "l2 norm of gradients: 1.4286820580854354\n",
      "l2 norm of weights: 6.64718644199781\n",
      "---------------------\n",
      "Iteration Number: 242\n",
      "Loss: 138.52191495441542\n",
      "l2 norm of gradients: 1.4280928838202331\n",
      "l2 norm of weights: 6.646797679246161\n",
      "---------------------\n",
      "Iteration Number: 243\n",
      "Loss: 138.42247326233496\n",
      "l2 norm of gradients: 1.4275037640357984\n",
      "l2 norm of weights: 6.646409208748767\n",
      "---------------------\n",
      "Iteration Number: 244\n",
      "Loss: 138.32311365767913\n",
      "l2 norm of gradients: 1.4269146985930352\n",
      "l2 norm of weights: 6.646021030302362\n",
      "---------------------\n",
      "Iteration Number: 245\n",
      "Loss: 138.22383609957265\n",
      "l2 norm of gradients: 1.426325687336972\n",
      "l2 norm of weights: 6.645633143703825\n",
      "---------------------\n",
      "Iteration Number: 246\n",
      "Loss: 138.12464054727056\n",
      "l2 norm of gradients: 1.425736730096649\n",
      "l2 norm of weights: 6.6452455487501805\n",
      "---------------------\n",
      "Iteration Number: 247\n",
      "Loss: 138.02552696004923\n",
      "l2 norm of gradients: 1.4251478266850077\n",
      "l2 norm of weights: 6.644858245238597\n",
      "---------------------\n",
      "Iteration Number: 248\n",
      "Loss: 137.9264952969406\n",
      "l2 norm of gradients: 1.424558976898785\n",
      "l2 norm of weights: 6.644471232966385\n",
      "---------------------\n",
      "Iteration Number: 249\n",
      "Loss: 137.82754551742858\n",
      "l2 norm of gradients: 1.4239701805184102\n",
      "l2 norm of weights: 6.644084511731004\n",
      "---------------------\n",
      "Iteration Number: 250\n",
      "Loss: 137.72867758056375\n",
      "l2 norm of gradients: 1.423381437307905\n",
      "l2 norm of weights: 6.643698081330051\n",
      "---------------------\n",
      "Iteration Number: 251\n",
      "Loss: 137.6298914458573\n",
      "l2 norm of gradients: 1.4227927470147903\n",
      "l2 norm of weights: 6.643311941561272\n",
      "---------------------\n",
      "Iteration Number: 252\n",
      "Loss: 137.53118707262647\n",
      "l2 norm of gradients: 1.4222041093699918\n",
      "l2 norm of weights: 6.642926092222557\n",
      "---------------------\n",
      "Iteration Number: 253\n",
      "Loss: 137.43256442030838\n",
      "l2 norm of gradients: 1.4216155240877537\n",
      "l2 norm of weights: 6.642540533111936\n",
      "---------------------\n",
      "Iteration Number: 254\n",
      "Loss: 137.33402344835406\n",
      "l2 norm of gradients: 1.4210269908655542\n",
      "l2 norm of weights: 6.642155264027585\n",
      "---------------------\n",
      "Iteration Number: 255\n",
      "Loss: 137.23556411614098\n",
      "l2 norm of gradients: 1.4204385093840264\n",
      "l2 norm of weights: 6.641770284767822\n",
      "---------------------\n",
      "Iteration Number: 256\n",
      "Loss: 137.13718638331486\n",
      "l2 norm of gradients: 1.4198500793068807\n",
      "l2 norm of weights: 6.6413855951311085\n",
      "---------------------\n",
      "Iteration Number: 257\n",
      "Loss: 137.0388902093598\n",
      "l2 norm of gradients: 1.4192617002808334\n",
      "l2 norm of weights: 6.641001194916049\n",
      "---------------------\n",
      "Iteration Number: 258\n",
      "Loss: 136.94067555404922\n",
      "l2 norm of gradients: 1.4186733719355402\n",
      "l2 norm of weights: 6.640617083921388\n",
      "---------------------\n",
      "Iteration Number: 259\n",
      "Loss: 136.8425423769394\n",
      "l2 norm of gradients: 1.4180850938835294\n",
      "l2 norm of weights: 6.640233261946015\n",
      "---------------------\n",
      "Iteration Number: 260\n",
      "Loss: 136.74449063790433\n",
      "l2 norm of gradients: 1.4174968657201454\n",
      "l2 norm of weights: 6.6398497287889615\n",
      "---------------------\n",
      "Iteration Number: 261\n",
      "Loss: 136.64652029664344\n",
      "l2 norm of gradients: 1.4169086870234904\n",
      "l2 norm of weights: 6.639466484249396\n",
      "---------------------\n",
      "Iteration Number: 262\n",
      "Loss: 136.54863131314812\n",
      "l2 norm of gradients: 1.4163205573543751\n",
      "l2 norm of weights: 6.6390835281266325\n",
      "---------------------\n",
      "Iteration Number: 263\n",
      "Loss: 136.4508236470777\n",
      "l2 norm of gradients: 1.4157324762562717\n",
      "l2 norm of weights: 6.638700860220124\n",
      "---------------------\n",
      "Iteration Number: 264\n",
      "Loss: 136.3530972586806\n",
      "l2 norm of gradients: 1.4151444432552704\n",
      "l2 norm of weights: 6.638318480329463\n",
      "---------------------\n",
      "Iteration Number: 265\n",
      "Loss: 136.25545210791768\n",
      "l2 norm of gradients: 1.4145564578600407\n",
      "l2 norm of weights: 6.637936388254384\n",
      "---------------------\n",
      "Iteration Number: 266\n",
      "Loss: 136.15788815490257\n",
      "l2 norm of gradients: 1.4139685195617986\n",
      "l2 norm of weights: 6.637554583794758\n",
      "---------------------\n",
      "Iteration Number: 267\n",
      "Loss: 136.06040535995757\n",
      "l2 norm of gradients: 1.4133806278342758\n",
      "l2 norm of weights: 6.637173066750598\n",
      "---------------------\n",
      "Iteration Number: 268\n",
      "Loss: 135.9630036831137\n",
      "l2 norm of gradients: 1.4127927821336956\n",
      "l2 norm of weights: 6.636791836922051\n",
      "---------------------\n",
      "Iteration Number: 269\n",
      "Loss: 135.86568308485977\n",
      "l2 norm of gradients: 1.4122049818987514\n",
      "l2 norm of weights: 6.6364108941094075\n",
      "---------------------\n",
      "Iteration Number: 270\n",
      "Loss: 135.76844352559792\n",
      "l2 norm of gradients: 1.4116172265505902\n",
      "l2 norm of weights: 6.636030238113092\n",
      "---------------------\n",
      "Iteration Number: 271\n",
      "Loss: 135.67128496584587\n",
      "l2 norm of gradients: 1.4110295154928008\n",
      "l2 norm of weights: 6.635649868733665\n",
      "---------------------\n",
      "Iteration Number: 272\n",
      "Loss: 135.57420736609802\n",
      "l2 norm of gradients: 1.4104418481114063\n",
      "l2 norm of weights: 6.635269785771827\n",
      "---------------------\n",
      "Iteration Number: 273\n",
      "Loss: 135.47721068712298\n",
      "l2 norm of gradients: 1.4098542237748604\n",
      "l2 norm of weights: 6.6348899890284105\n",
      "---------------------\n",
      "Iteration Number: 274\n",
      "Loss: 135.38029488970187\n",
      "l2 norm of gradients: 1.4092666418340503\n",
      "l2 norm of weights: 6.634510478304384\n",
      "---------------------\n",
      "Iteration Number: 275\n",
      "Loss: 135.2834599347104\n",
      "l2 norm of gradients: 1.4086791016223004\n",
      "l2 norm of weights: 6.634131253400851\n",
      "---------------------\n",
      "Iteration Number: 276\n",
      "Loss: 135.1867057829449\n",
      "l2 norm of gradients: 1.4080916024553842\n",
      "l2 norm of weights: 6.63375231411905\n",
      "---------------------\n",
      "Iteration Number: 277\n",
      "Loss: 135.0900323955765\n",
      "l2 norm of gradients: 1.4075041436315383\n",
      "l2 norm of weights: 6.63337366026035\n",
      "---------------------\n",
      "Iteration Number: 278\n",
      "Loss: 134.99343973363153\n",
      "l2 norm of gradients: 1.406916724431481\n",
      "l2 norm of weights: 6.632995291626256\n",
      "---------------------\n",
      "Iteration Number: 279\n",
      "Loss: 134.89692775844472\n",
      "l2 norm of gradients: 1.4063293441184372\n",
      "l2 norm of weights: 6.6326172080184\n",
      "---------------------\n",
      "Iteration Number: 280\n",
      "Loss: 134.80049643112986\n",
      "l2 norm of gradients: 1.4057420019381652\n",
      "l2 norm of weights: 6.632239409238549\n",
      "---------------------\n",
      "Iteration Number: 281\n",
      "Loss: 134.70414571351384\n",
      "l2 norm of gradients: 1.4051546971189894\n",
      "l2 norm of weights: 6.6318618950885995\n",
      "---------------------\n",
      "Iteration Number: 282\n",
      "Loss: 134.6078755667221\n",
      "l2 norm of gradients: 1.4045674288718377\n",
      "l2 norm of weights: 6.631484665370576\n",
      "---------------------\n",
      "Iteration Number: 283\n",
      "Loss: 134.5116859527165\n",
      "l2 norm of gradients: 1.4039801963902812\n",
      "l2 norm of weights: 6.631107719886633\n",
      "---------------------\n",
      "Iteration Number: 284\n",
      "Loss: 134.4155768330812\n",
      "l2 norm of gradients: 1.403392998850581\n",
      "l2 norm of weights: 6.6307310584390535\n",
      "---------------------\n",
      "Iteration Number: 285\n",
      "Loss: 134.31954816974653\n",
      "l2 norm of gradients: 1.4028058354117376\n",
      "l2 norm of weights: 6.630354680830244\n",
      "---------------------\n",
      "Iteration Number: 286\n",
      "Loss: 134.2235999245308\n",
      "l2 norm of gradients: 1.4022187052155448\n",
      "l2 norm of weights: 6.629978586862744\n",
      "---------------------\n",
      "Iteration Number: 287\n",
      "Loss: 134.12773205988398\n",
      "l2 norm of gradients: 1.4016316073866482\n",
      "l2 norm of weights: 6.629602776339211\n",
      "---------------------\n",
      "Iteration Number: 288\n",
      "Loss: 134.03194453777962\n",
      "l2 norm of gradients: 1.4010445410326071\n",
      "l2 norm of weights: 6.629227249062432\n",
      "---------------------\n",
      "Iteration Number: 289\n",
      "Loss: 133.936237320611\n",
      "l2 norm of gradients: 1.4004575052439632\n",
      "l2 norm of weights: 6.628852004835316\n",
      "---------------------\n",
      "Iteration Number: 290\n",
      "Loss: 133.84061037086002\n",
      "l2 norm of gradients: 1.3998704990943098\n",
      "l2 norm of weights: 6.628477043460895\n",
      "---------------------\n",
      "Iteration Number: 291\n",
      "Loss: 133.74506365098202\n",
      "l2 norm of gradients: 1.3992835216403667\n",
      "l2 norm of weights: 6.62810236474232\n",
      "---------------------\n",
      "Iteration Number: 292\n",
      "Loss: 133.64959712383114\n",
      "l2 norm of gradients: 1.3986965719220619\n",
      "l2 norm of weights: 6.627727968482869\n",
      "---------------------\n",
      "Iteration Number: 293\n",
      "Loss: 133.55421075227528\n",
      "l2 norm of gradients: 1.3981096489626108\n",
      "l2 norm of weights: 6.627353854485933\n",
      "---------------------\n",
      "Iteration Number: 294\n",
      "Loss: 133.45890449915188\n",
      "l2 norm of gradients: 1.397522751768608\n",
      "l2 norm of weights: 6.626980022555026\n",
      "---------------------\n",
      "Iteration Number: 295\n",
      "Loss: 133.36367832768528\n",
      "l2 norm of gradients: 1.396935879330115\n",
      "l2 norm of weights: 6.626606472493779\n",
      "---------------------\n",
      "Iteration Number: 296\n",
      "Loss: 133.26853220116087\n",
      "l2 norm of gradients: 1.3963490306207575\n",
      "l2 norm of weights: 6.6262332041059375\n",
      "---------------------\n",
      "Iteration Number: 297\n",
      "Loss: 133.17346608280513\n",
      "l2 norm of gradients: 1.3957622045978224\n",
      "l2 norm of weights: 6.625860217195368\n",
      "---------------------\n",
      "Iteration Number: 298\n",
      "Loss: 133.07847993616474\n",
      "l2 norm of gradients: 1.3951754002023637\n",
      "l2 norm of weights: 6.625487511566045\n",
      "---------------------\n",
      "Iteration Number: 299\n",
      "Loss: 132.98357372502173\n",
      "l2 norm of gradients: 1.3945886163593062\n",
      "l2 norm of weights: 6.625115087022062\n",
      "---------------------\n",
      "Iteration Number: 300\n",
      "Loss: 132.88874741306657\n",
      "l2 norm of gradients: 1.3940018519775568\n",
      "l2 norm of weights: 6.624742943367622\n",
      "---------------------\n",
      "Iteration Number: 301\n",
      "Loss: 132.79400096425542\n",
      "l2 norm of gradients: 1.3934151059501194\n",
      "l2 norm of weights: 6.624371080407038\n",
      "---------------------\n",
      "Iteration Number: 302\n",
      "Loss: 132.69933434280557\n",
      "l2 norm of gradients: 1.3928283771542114\n",
      "l2 norm of weights: 6.623999497944739\n",
      "---------------------\n",
      "Iteration Number: 303\n",
      "Loss: 132.60474751271687\n",
      "l2 norm of gradients: 1.3922416644513855\n",
      "l2 norm of weights: 6.623628195785257\n",
      "---------------------\n",
      "Iteration Number: 304\n",
      "Loss: 132.51024043864663\n",
      "l2 norm of gradients: 1.391654966687654\n",
      "l2 norm of weights: 6.6232571737332355\n",
      "---------------------\n",
      "Iteration Number: 305\n",
      "Loss: 132.41581308503686\n",
      "l2 norm of gradients: 1.3910682826936178\n",
      "l2 norm of weights: 6.622886431593423\n",
      "---------------------\n",
      "Iteration Number: 306\n",
      "Loss: 132.32146541656917\n",
      "l2 norm of gradients: 1.390481611284596\n",
      "l2 norm of weights: 6.6225159691706725\n",
      "---------------------\n",
      "Iteration Number: 307\n",
      "Loss: 132.22719739797398\n",
      "l2 norm of gradients: 1.389894951260762\n",
      "l2 norm of weights: 6.622145786269945\n",
      "---------------------\n",
      "Iteration Number: 308\n",
      "Loss: 132.1330089945625\n",
      "l2 norm of gradients: 1.3893083014072825\n",
      "l2 norm of weights: 6.6217758826963005\n",
      "---------------------\n",
      "Iteration Number: 309\n",
      "Loss: 132.0389001712315\n",
      "l2 norm of gradients: 1.388721660494456\n",
      "l2 norm of weights: 6.621406258254903\n",
      "---------------------\n",
      "Iteration Number: 310\n",
      "Loss: 131.94487089345122\n",
      "l2 norm of gradients: 1.3881350272778596\n",
      "l2 norm of weights: 6.621036912751015\n",
      "---------------------\n",
      "Iteration Number: 311\n",
      "Loss: 131.85092112675\n",
      "l2 norm of gradients: 1.387548400498495\n",
      "l2 norm of weights: 6.6206678459900035\n",
      "---------------------\n",
      "Iteration Number: 312\n",
      "Loss: 131.75705083677383\n",
      "l2 norm of gradients: 1.3869617788829394\n",
      "l2 norm of weights: 6.620299057777326\n",
      "---------------------\n",
      "Iteration Number: 313\n",
      "Loss: 131.6632599892206\n",
      "l2 norm of gradients: 1.386375161143498\n",
      "l2 norm of weights: 6.619930547918542\n",
      "---------------------\n",
      "Iteration Number: 314\n",
      "Loss: 131.56954855013763\n",
      "l2 norm of gradients: 1.385788545978361\n",
      "l2 norm of weights: 6.619562316219303\n",
      "---------------------\n",
      "Iteration Number: 315\n",
      "Loss: 131.47591648579598\n",
      "l2 norm of gradients: 1.3852019320717621\n",
      "l2 norm of weights: 6.619194362485358\n",
      "---------------------\n",
      "Iteration Number: 316\n",
      "Loss: 131.3823637625181\n",
      "l2 norm of gradients: 1.3846153180941394\n",
      "l2 norm of weights: 6.618826686522546\n",
      "---------------------\n",
      "Iteration Number: 317\n",
      "Loss: 131.28889034675436\n",
      "l2 norm of gradients: 1.3840287027023\n",
      "l2 norm of weights: 6.618459288136798\n",
      "---------------------\n",
      "Iteration Number: 318\n",
      "Loss: 131.19549620513413\n",
      "l2 norm of gradients: 1.3834420845395874\n",
      "l2 norm of weights: 6.618092167134133\n",
      "---------------------\n",
      "Iteration Number: 319\n",
      "Loss: 131.10218130468473\n",
      "l2 norm of gradients: 1.3828554622360496\n",
      "l2 norm of weights: 6.617725323320662\n",
      "---------------------\n",
      "Iteration Number: 320\n",
      "Loss: 131.00894561218755\n",
      "l2 norm of gradients: 1.3822688344086114\n",
      "l2 norm of weights: 6.617358756502582\n",
      "---------------------\n",
      "Iteration Number: 321\n",
      "Loss: 130.915789095117\n",
      "l2 norm of gradients: 1.381682199661247\n",
      "l2 norm of weights: 6.616992466486172\n",
      "---------------------\n",
      "Iteration Number: 322\n",
      "Loss: 130.82271172076443\n",
      "l2 norm of gradients: 1.3810955565851581\n",
      "l2 norm of weights: 6.616626453077801\n",
      "---------------------\n",
      "Iteration Number: 323\n",
      "Loss: 130.72971345669063\n",
      "l2 norm of gradients: 1.3805089037589513\n",
      "l2 norm of weights: 6.616260716083917\n",
      "---------------------\n",
      "Iteration Number: 324\n",
      "Loss: 130.63679427067987\n",
      "l2 norm of gradients: 1.3799222397488182\n",
      "l2 norm of weights: 6.61589525531105\n",
      "---------------------\n",
      "Iteration Number: 325\n",
      "Loss: 130.54395413064856\n",
      "l2 norm of gradients: 1.3793355631087179\n",
      "l2 norm of weights: 6.61553007056581\n",
      "---------------------\n",
      "Iteration Number: 326\n",
      "Loss: 130.45119300478981\n",
      "l2 norm of gradients: 1.3787488723805625\n",
      "l2 norm of weights: 6.615165161654885\n",
      "---------------------\n",
      "Iteration Number: 327\n",
      "Loss: 130.35851086125376\n",
      "l2 norm of gradients: 1.3781621660944026\n",
      "l2 norm of weights: 6.614800528385042\n",
      "---------------------\n",
      "Iteration Number: 328\n",
      "Loss: 130.26590766882333\n",
      "l2 norm of gradients: 1.377575442768615\n",
      "l2 norm of weights: 6.6144361705631205\n",
      "---------------------\n",
      "Iteration Number: 329\n",
      "Loss: 130.1733833959368\n",
      "l2 norm of gradients: 1.3769887009100936\n",
      "l2 norm of weights: 6.6140720879960355\n",
      "---------------------\n",
      "Iteration Number: 330\n",
      "Loss: 130.08093801161976\n",
      "l2 norm of gradients: 1.3764019390144404\n",
      "l2 norm of weights: 6.6137082804907745\n",
      "---------------------\n",
      "Iteration Number: 331\n",
      "Loss: 129.98857148482097\n",
      "l2 norm of gradients: 1.3758151555661586\n",
      "l2 norm of weights: 6.613344747854394\n",
      "---------------------\n",
      "Iteration Number: 332\n",
      "Loss: 129.89628378494214\n",
      "l2 norm of gradients: 1.3752283490388475\n",
      "l2 norm of weights: 6.612981489894022\n",
      "---------------------\n",
      "Iteration Number: 333\n",
      "Loss: 129.8040748813243\n",
      "l2 norm of gradients: 1.374641517895398\n",
      "l2 norm of weights: 6.612618506416854\n",
      "---------------------\n",
      "Iteration Number: 334\n",
      "Loss: 129.71194474374542\n",
      "l2 norm of gradients: 1.374054660588191\n",
      "l2 norm of weights: 6.6122557972301506\n",
      "---------------------\n",
      "Iteration Number: 335\n",
      "Loss: 129.6198933419641\n",
      "l2 norm of gradients: 1.3734677755592952\n",
      "l2 norm of weights: 6.611893362141237\n",
      "---------------------\n",
      "Iteration Number: 336\n",
      "Loss: 129.52792064601744\n",
      "l2 norm of gradients: 1.3728808612406678\n",
      "l2 norm of weights: 6.611531200957503\n",
      "---------------------\n",
      "Iteration Number: 337\n",
      "Loss: 129.4360266262761\n",
      "l2 norm of gradients: 1.3722939160543555\n",
      "l2 norm of weights: 6.611169313486399\n",
      "---------------------\n",
      "Iteration Number: 338\n",
      "Loss: 129.34421125309575\n",
      "l2 norm of gradients: 1.3717069384126956\n",
      "l2 norm of weights: 6.6108076995354335\n",
      "---------------------\n",
      "Iteration Number: 339\n",
      "Loss: 129.2524744971162\n",
      "l2 norm of gradients: 1.371119926718521\n",
      "l2 norm of weights: 6.610446358912177\n",
      "---------------------\n",
      "Iteration Number: 340\n",
      "Loss: 129.1608163291742\n",
      "l2 norm of gradients: 1.370532879365363\n",
      "l2 norm of weights: 6.610085291424255\n",
      "---------------------\n",
      "Iteration Number: 341\n",
      "Loss: 129.06923672032815\n",
      "l2 norm of gradients: 1.3699457947376572\n",
      "l2 norm of weights: 6.609724496879346\n",
      "---------------------\n",
      "Iteration Number: 342\n",
      "Loss: 128.97773564185465\n",
      "l2 norm of gradients: 1.3693586712109485\n",
      "l2 norm of weights: 6.609363975085184\n",
      "---------------------\n",
      "Iteration Number: 343\n",
      "Loss: 128.8863130650649\n",
      "l2 norm of gradients: 1.368771507152098\n",
      "l2 norm of weights: 6.609003725849557\n",
      "---------------------\n",
      "Iteration Number: 344\n",
      "Loss: 128.7949689618897\n",
      "l2 norm of gradients: 1.3681843009194907\n",
      "l2 norm of weights: 6.608643748980297\n",
      "---------------------\n",
      "Iteration Number: 345\n",
      "Loss: 128.703703304072\n",
      "l2 norm of gradients: 1.367597050863242\n",
      "l2 norm of weights: 6.608284044285291\n",
      "---------------------\n",
      "Iteration Number: 346\n",
      "Loss: 128.61251606353557\n",
      "l2 norm of gradients: 1.3670097553254086\n",
      "l2 norm of weights: 6.60792461157247\n",
      "---------------------\n",
      "Iteration Number: 347\n",
      "Loss: 128.52140721258812\n",
      "l2 norm of gradients: 1.3664224126401938\n",
      "l2 norm of weights: 6.607565450649811\n",
      "---------------------\n",
      "Iteration Number: 348\n",
      "Loss: 128.43037672395437\n",
      "l2 norm of gradients: 1.3658350211341603\n",
      "l2 norm of weights: 6.607206561325333\n",
      "---------------------\n",
      "Iteration Number: 349\n",
      "Loss: 128.3394245700893\n",
      "l2 norm of gradients: 1.3652475791264371\n",
      "l2 norm of weights: 6.606847943407098\n",
      "---------------------\n",
      "Iteration Number: 350\n",
      "Loss: 128.2485507238765\n",
      "l2 norm of gradients: 1.3646600849289303\n",
      "l2 norm of weights: 6.606489596703211\n",
      "---------------------\n",
      "Iteration Number: 351\n",
      "Loss: 128.15775515858024\n",
      "l2 norm of gradients: 1.3640725368465345\n",
      "l2 norm of weights: 6.606131521021811\n",
      "---------------------\n",
      "Iteration Number: 352\n",
      "Loss: 128.067037847361\n",
      "l2 norm of gradients: 1.3634849331773407\n",
      "l2 norm of weights: 6.605773716171076\n",
      "---------------------\n",
      "Iteration Number: 353\n",
      "Loss: 127.97639876377339\n",
      "l2 norm of gradients: 1.3628972722128483\n",
      "l2 norm of weights: 6.605416181959221\n",
      "---------------------\n",
      "Iteration Number: 354\n",
      "Loss: 127.8858378816714\n",
      "l2 norm of gradients: 1.3623095522381747\n",
      "l2 norm of weights: 6.6050589181944925\n",
      "---------------------\n",
      "Iteration Number: 355\n",
      "Loss: 127.79535517475313\n",
      "l2 norm of gradients: 1.361721771532268\n",
      "l2 norm of weights: 6.604701924685169\n",
      "---------------------\n",
      "Iteration Number: 356\n",
      "Loss: 127.70495061742962\n",
      "l2 norm of gradients: 1.3611339283681128\n",
      "l2 norm of weights: 6.6043452012395605\n",
      "---------------------\n",
      "Iteration Number: 357\n",
      "Loss: 127.61462418396215\n",
      "l2 norm of gradients: 1.360546021012945\n",
      "l2 norm of weights: 6.603988747666004\n",
      "---------------------\n",
      "Iteration Number: 358\n",
      "Loss: 127.52437584893111\n",
      "l2 norm of gradients: 1.3599580477284599\n",
      "l2 norm of weights: 6.603632563772865\n",
      "---------------------\n",
      "Iteration Number: 359\n",
      "Loss: 127.4342055870017\n",
      "l2 norm of gradients: 1.359370006771022\n",
      "l2 norm of weights: 6.603276649368534\n",
      "---------------------\n",
      "Iteration Number: 360\n",
      "Loss: 127.34411337344358\n",
      "l2 norm of gradients: 1.358781896391875\n",
      "l2 norm of weights: 6.6029210042614235\n",
      "---------------------\n",
      "Iteration Number: 361\n",
      "Loss: 127.25409918320503\n",
      "l2 norm of gradients: 1.3581937148373513\n",
      "l2 norm of weights: 6.602565628259968\n",
      "---------------------\n",
      "Iteration Number: 362\n",
      "Loss: 127.16416299186832\n",
      "l2 norm of gradients: 1.3576054603490815\n",
      "l2 norm of weights: 6.6022105211726245\n",
      "---------------------\n",
      "Iteration Number: 363\n",
      "Loss: 127.07430477508171\n",
      "l2 norm of gradients: 1.3570171311642003\n",
      "l2 norm of weights: 6.601855682807866\n",
      "---------------------\n",
      "Iteration Number: 364\n",
      "Loss: 126.98452450857347\n",
      "l2 norm of gradients: 1.3564287255155587\n",
      "l2 norm of weights: 6.601501112974183\n",
      "---------------------\n",
      "Iteration Number: 365\n",
      "Loss: 126.89482216854206\n",
      "l2 norm of gradients: 1.3558402416319288\n",
      "l2 norm of weights: 6.60114681148008\n",
      "---------------------\n",
      "Iteration Number: 366\n",
      "Loss: 126.80519773115913\n",
      "l2 norm of gradients: 1.3552516777382115\n",
      "l2 norm of weights: 6.600792778134078\n",
      "---------------------\n",
      "Iteration Number: 367\n",
      "Loss: 126.71565117291671\n",
      "l2 norm of gradients: 1.3546630320556439\n",
      "l2 norm of weights: 6.600439012744706\n",
      "---------------------\n",
      "Iteration Number: 368\n",
      "Loss: 126.62618247066905\n",
      "l2 norm of gradients: 1.3540743028020044\n",
      "l2 norm of weights: 6.6000855151205045\n",
      "---------------------\n",
      "Iteration Number: 369\n",
      "Loss: 126.5367916012025\n",
      "l2 norm of gradients: 1.3534854881918181\n",
      "l2 norm of weights: 6.599732285070021\n",
      "---------------------\n",
      "Iteration Number: 370\n",
      "Loss: 126.44747854181159\n",
      "l2 norm of gradients: 1.3528965864365623\n",
      "l2 norm of weights: 6.599379322401812\n",
      "---------------------\n",
      "Iteration Number: 371\n",
      "Loss: 126.35824326959242\n",
      "l2 norm of gradients: 1.352307595744869\n",
      "l2 norm of weights: 6.599026626924436\n",
      "---------------------\n",
      "Iteration Number: 372\n",
      "Loss: 126.26908576236515\n",
      "l2 norm of gradients: 1.35171851432273\n",
      "l2 norm of weights: 6.5986741984464565\n",
      "---------------------\n",
      "Iteration Number: 373\n",
      "Loss: 126.1800059976557\n",
      "l2 norm of gradients: 1.3511293403736977\n",
      "l2 norm of weights: 6.598322036776437\n",
      "---------------------\n",
      "Iteration Number: 374\n",
      "Loss: 126.09100395374684\n",
      "l2 norm of gradients: 1.3505400720990874\n",
      "l2 norm of weights: 6.597970141722941\n",
      "---------------------\n",
      "Iteration Number: 375\n",
      "Loss: 126.00207960856442\n",
      "l2 norm of gradients: 1.3499507076981787\n",
      "l2 norm of weights: 6.597618513094532\n",
      "---------------------\n",
      "Iteration Number: 376\n",
      "Loss: 125.91323294068079\n",
      "l2 norm of gradients: 1.349361245368414\n",
      "l2 norm of weights: 6.597267150699767\n",
      "---------------------\n",
      "Iteration Number: 377\n",
      "Loss: 125.82446392872734\n",
      "l2 norm of gradients: 1.3487716833055994\n",
      "l2 norm of weights: 6.596916054347198\n",
      "---------------------\n",
      "Iteration Number: 378\n",
      "Loss: 125.73577255144814\n",
      "l2 norm of gradients: 1.348182019704101\n",
      "l2 norm of weights: 6.5965652238453725\n",
      "---------------------\n",
      "Iteration Number: 379\n",
      "Loss: 125.64715878802465\n",
      "l2 norm of gradients: 1.347592252757044\n",
      "l2 norm of weights: 6.596214659002825\n",
      "---------------------\n",
      "Iteration Number: 380\n",
      "Loss: 125.55862261772738\n",
      "l2 norm of gradients: 1.3470023806565081\n",
      "l2 norm of weights: 6.595864359628084\n",
      "---------------------\n",
      "Iteration Number: 381\n",
      "Loss: 125.47016401994641\n",
      "l2 norm of gradients: 1.3464124015937227\n",
      "l2 norm of weights: 6.5955143255296615\n",
      "---------------------\n",
      "Iteration Number: 382\n",
      "Loss: 125.38178297436681\n",
      "l2 norm of gradients: 1.3458223137592615\n",
      "l2 norm of weights: 6.595164556516059\n",
      "---------------------\n",
      "Iteration Number: 383\n",
      "Loss: 125.29347946102297\n",
      "l2 norm of gradients: 1.345232115343237\n",
      "l2 norm of weights: 6.594815052395761\n",
      "---------------------\n",
      "Iteration Number: 384\n",
      "Loss: 125.20525345994066\n",
      "l2 norm of gradients: 1.3446418045354906\n",
      "l2 norm of weights: 6.594465812977234\n",
      "---------------------\n",
      "Iteration Number: 385\n",
      "Loss: 125.1171049514457\n",
      "l2 norm of gradients: 1.3440513795257858\n",
      "l2 norm of weights: 6.594116838068929\n",
      "---------------------\n",
      "Iteration Number: 386\n",
      "Loss: 125.02903391606908\n",
      "l2 norm of gradients: 1.3434608385039977\n",
      "l2 norm of weights: 6.59376812747927\n",
      "---------------------\n",
      "Iteration Number: 387\n",
      "Loss: 124.94104033472581\n",
      "l2 norm of gradients: 1.3428701796603035\n",
      "l2 norm of weights: 6.593419681016665\n",
      "---------------------\n",
      "Iteration Number: 388\n",
      "Loss: 124.8531241881239\n",
      "l2 norm of gradients: 1.3422794011853687\n",
      "l2 norm of weights: 6.593071498489495\n",
      "---------------------\n",
      "Iteration Number: 389\n",
      "Loss: 124.76528545756122\n",
      "l2 norm of gradients: 1.341688501270537\n",
      "l2 norm of weights: 6.592723579706117\n",
      "---------------------\n",
      "Iteration Number: 390\n",
      "Loss: 124.67752412443528\n",
      "l2 norm of gradients: 1.3410974781080125\n",
      "l2 norm of weights: 6.592375924474858\n",
      "---------------------\n",
      "Iteration Number: 391\n",
      "Loss: 124.58984017033141\n",
      "l2 norm of gradients: 1.3405063298910496\n",
      "l2 norm of weights: 6.5920285326040196\n",
      "---------------------\n",
      "Iteration Number: 392\n",
      "Loss: 124.50223357693888\n",
      "l2 norm of gradients: 1.3399150548141323\n",
      "l2 norm of weights: 6.59168140390187\n",
      "---------------------\n",
      "Iteration Number: 393\n",
      "Loss: 124.41470432637489\n",
      "l2 norm of gradients: 1.3393236510731612\n",
      "l2 norm of weights: 6.591334538176646\n",
      "---------------------\n",
      "Iteration Number: 394\n",
      "Loss: 124.32725240076145\n",
      "l2 norm of gradients: 1.3387321168656316\n",
      "l2 norm of weights: 6.590987935236551\n",
      "---------------------\n",
      "Iteration Number: 395\n",
      "Loss: 124.23987778263044\n",
      "l2 norm of gradients: 1.3381404503908176\n",
      "l2 norm of weights: 6.590641594889753\n",
      "---------------------\n",
      "Iteration Number: 396\n",
      "Loss: 124.15258045459666\n",
      "l2 norm of gradients: 1.337548649849949\n",
      "l2 norm of weights: 6.590295516944383\n",
      "---------------------\n",
      "Iteration Number: 397\n",
      "Loss: 124.06536039933563\n",
      "l2 norm of gradients: 1.3369567134463913\n",
      "l2 norm of weights: 6.58994970120853\n",
      "---------------------\n",
      "Iteration Number: 398\n",
      "Loss: 123.97821759999503\n",
      "l2 norm of gradients: 1.3363646393858235\n",
      "l2 norm of weights: 6.589604147490247\n",
      "---------------------\n",
      "Iteration Number: 399\n",
      "Loss: 123.89115203975805\n",
      "l2 norm of gradients: 1.3357724258764136\n",
      "l2 norm of weights: 6.589258855597544\n",
      "---------------------\n",
      "Iteration Number: 400\n",
      "Loss: 123.80416370217903\n",
      "l2 norm of gradients: 1.3351800711289943\n",
      "l2 norm of weights: 6.588913825338383\n",
      "---------------------\n",
      "Iteration Number: 401\n",
      "Loss: 123.7172525706785\n",
      "l2 norm of gradients: 1.3345875733572383\n",
      "l2 norm of weights: 6.588569056520687\n",
      "---------------------\n",
      "Iteration Number: 402\n",
      "Loss: 123.63041862922039\n",
      "l2 norm of gradients: 1.3339949307778292\n",
      "l2 norm of weights: 6.588224548952328\n",
      "---------------------\n",
      "Iteration Number: 403\n",
      "Loss: 123.5436618618362\n",
      "l2 norm of gradients: 1.333402141610637\n",
      "l2 norm of weights: 6.587880302441131\n",
      "---------------------\n",
      "Iteration Number: 404\n",
      "Loss: 123.45698225279791\n",
      "l2 norm of gradients: 1.3328092040788866\n",
      "l2 norm of weights: 6.58753631679487\n",
      "---------------------\n",
      "Iteration Number: 405\n",
      "Loss: 123.37037978653078\n",
      "l2 norm of gradients: 1.3322161164093298\n",
      "l2 norm of weights: 6.5871925918212675\n",
      "---------------------\n",
      "Iteration Number: 406\n",
      "Loss: 123.28385444757768\n",
      "l2 norm of gradients: 1.3316228768324134\n",
      "l2 norm of weights: 6.586849127327993\n",
      "---------------------\n",
      "Iteration Number: 407\n",
      "Loss: 123.19740622086935\n",
      "l2 norm of gradients: 1.3310294835824479\n",
      "l2 norm of weights: 6.586505923122662\n",
      "---------------------\n",
      "Iteration Number: 408\n",
      "Loss: 123.11103509135906\n",
      "l2 norm of gradients: 1.3304359348977757\n",
      "l2 norm of weights: 6.586162979012831\n",
      "---------------------\n",
      "Iteration Number: 409\n",
      "Loss: 123.0247410443746\n",
      "l2 norm of gradients: 1.329842229020935\n",
      "l2 norm of weights: 6.585820294805999\n",
      "---------------------\n",
      "Iteration Number: 410\n",
      "Loss: 122.93852406522649\n",
      "l2 norm of gradients: 1.3292483641988286\n",
      "l2 norm of weights: 6.585477870309608\n",
      "---------------------\n",
      "Iteration Number: 411\n",
      "Loss: 122.8523841395443\n",
      "l2 norm of gradients: 1.328654338682885\n",
      "l2 norm of weights: 6.585135705331037\n",
      "---------------------\n",
      "Iteration Number: 412\n",
      "Loss: 122.76632125323998\n",
      "l2 norm of gradients: 1.3280601507292247\n",
      "l2 norm of weights: 6.584793799677603\n",
      "---------------------\n",
      "Iteration Number: 413\n",
      "Loss: 122.68033539209894\n",
      "l2 norm of gradients: 1.3274657985988205\n",
      "l2 norm of weights: 6.584452153156555\n",
      "---------------------\n",
      "Iteration Number: 414\n",
      "Loss: 122.5944265425463\n",
      "l2 norm of gradients: 1.3268712805576621\n",
      "l2 norm of weights: 6.58411076557508\n",
      "---------------------\n",
      "Iteration Number: 415\n",
      "Loss: 122.50859469084128\n",
      "l2 norm of gradients: 1.326276594876914\n",
      "l2 norm of weights: 6.583769636740298\n",
      "---------------------\n",
      "Iteration Number: 416\n",
      "Loss: 122.42283982342906\n",
      "l2 norm of gradients: 1.325681739833079\n",
      "l2 norm of weights: 6.583428766459257\n",
      "---------------------\n",
      "Iteration Number: 417\n",
      "Loss: 122.33716192733124\n",
      "l2 norm of gradients: 1.325086713708154\n",
      "l2 norm of weights: 6.583088154538937\n",
      "---------------------\n",
      "Iteration Number: 418\n",
      "Loss: 122.25156098922783\n",
      "l2 norm of gradients: 1.324491514789792\n",
      "l2 norm of weights: 6.582747800786245\n",
      "---------------------\n",
      "Iteration Number: 419\n",
      "Loss: 122.16603699638635\n",
      "l2 norm of gradients: 1.323896141371458\n",
      "l2 norm of weights: 6.582407705008014\n",
      "---------------------\n",
      "Iteration Number: 420\n",
      "Loss: 122.08058993598965\n",
      "l2 norm of gradients: 1.3233005917525862\n",
      "l2 norm of weights: 6.582067867011003\n",
      "---------------------\n",
      "Iteration Number: 421\n",
      "Loss: 121.99521979565668\n",
      "l2 norm of gradients: 1.3227048642387371\n",
      "l2 norm of weights: 6.581728286601894\n",
      "---------------------\n",
      "Iteration Number: 422\n",
      "Loss: 121.90992656295067\n",
      "l2 norm of gradients: 1.322108957141753\n",
      "l2 norm of weights: 6.581388963587291\n",
      "---------------------\n",
      "Iteration Number: 423\n",
      "Loss: 121.82471022584559\n",
      "l2 norm of gradients: 1.3215128687799136\n",
      "l2 norm of weights: 6.581049897773718\n",
      "---------------------\n",
      "Iteration Number: 424\n",
      "Loss: 121.73957077226261\n",
      "l2 norm of gradients: 1.3209165974780885\n",
      "l2 norm of weights: 6.580711088967617\n",
      "---------------------\n",
      "Iteration Number: 425\n",
      "Loss: 121.65450819041021\n",
      "l2 norm of gradients: 1.3203201415678942\n",
      "l2 norm of weights: 6.580372536975351\n",
      "---------------------\n",
      "Iteration Number: 426\n",
      "Loss: 121.56952246867442\n",
      "l2 norm of gradients: 1.3197234993878448\n",
      "l2 norm of weights: 6.580034241603196\n",
      "---------------------\n",
      "Iteration Number: 427\n",
      "Loss: 121.4846135956468\n",
      "l2 norm of gradients: 1.3191266692835062\n",
      "l2 norm of weights: 6.579696202657342\n",
      "---------------------\n",
      "Iteration Number: 428\n",
      "Loss: 121.399781560049\n",
      "l2 norm of gradients: 1.3185296496076475\n",
      "l2 norm of weights: 6.579358419943894\n",
      "---------------------\n",
      "Iteration Number: 429\n",
      "Loss: 121.31502635080452\n",
      "l2 norm of gradients: 1.3179324387203935\n",
      "l2 norm of weights: 6.579020893268869\n",
      "---------------------\n",
      "Iteration Number: 430\n",
      "Loss: 121.23034795698332\n",
      "l2 norm of gradients: 1.317335034989375\n",
      "l2 norm of weights: 6.578683622438192\n",
      "---------------------\n",
      "Iteration Number: 431\n",
      "Loss: 121.14574636774124\n",
      "l2 norm of gradients: 1.3167374367898816\n",
      "l2 norm of weights: 6.578346607257698\n",
      "---------------------\n",
      "Iteration Number: 432\n",
      "Loss: 121.06122157262713\n",
      "l2 norm of gradients: 1.3161396425050096\n",
      "l2 norm of weights: 6.578009847533128\n",
      "---------------------\n",
      "Iteration Number: 433\n",
      "Loss: 120.97677356121348\n",
      "l2 norm of gradients: 1.3155416505258137\n",
      "l2 norm of weights: 6.577673343070131\n",
      "---------------------\n",
      "Iteration Number: 434\n",
      "Loss: 120.89240232315659\n",
      "l2 norm of gradients: 1.3149434592514568\n",
      "l2 norm of weights: 6.57733709367426\n",
      "---------------------\n",
      "Iteration Number: 435\n",
      "Loss: 120.8081078483861\n",
      "l2 norm of gradients: 1.314345067089359\n",
      "l2 norm of weights: 6.577001099150969\n",
      "---------------------\n",
      "Iteration Number: 436\n",
      "Loss: 120.7238901270842\n",
      "l2 norm of gradients: 1.3137464724553456\n",
      "l2 norm of weights: 6.576665359305614\n",
      "---------------------\n",
      "Iteration Number: 437\n",
      "Loss: 120.63974914929065\n",
      "l2 norm of gradients: 1.313147673773799\n",
      "l2 norm of weights: 6.576329873943454\n",
      "---------------------\n",
      "Iteration Number: 438\n",
      "Loss: 120.55568490563057\n",
      "l2 norm of gradients: 1.3125486694778037\n",
      "l2 norm of weights: 6.575994642869643\n",
      "---------------------\n",
      "Iteration Number: 439\n",
      "Loss: 120.47169738657341\n",
      "l2 norm of gradients: 1.311949458009298\n",
      "l2 norm of weights: 6.575659665889235\n",
      "---------------------\n",
      "Iteration Number: 440\n",
      "Loss: 120.38778658282875\n",
      "l2 norm of gradients: 1.3113500378192198\n",
      "l2 norm of weights: 6.57532494280718\n",
      "---------------------\n",
      "Iteration Number: 441\n",
      "Loss: 120.30395248529457\n",
      "l2 norm of gradients: 1.3107504073676555\n",
      "l2 norm of weights: 6.57499047342832\n",
      "---------------------\n",
      "Iteration Number: 442\n",
      "Loss: 120.22019508488529\n",
      "l2 norm of gradients: 1.3101505651239889\n",
      "l2 norm of weights: 6.574656257557392\n",
      "---------------------\n",
      "Iteration Number: 443\n",
      "Loss: 120.13651437301571\n",
      "l2 norm of gradients: 1.3095505095670485\n",
      "l2 norm of weights: 6.574322294999028\n",
      "---------------------\n",
      "Iteration Number: 444\n",
      "Loss: 120.05291034069592\n",
      "l2 norm of gradients: 1.3089502391852545\n",
      "l2 norm of weights: 6.573988585557744\n",
      "---------------------\n",
      "Iteration Number: 445\n",
      "Loss: 119.96938297975048\n",
      "l2 norm of gradients: 1.3083497524767682\n",
      "l2 norm of weights: 6.573655129037949\n",
      "---------------------\n",
      "Iteration Number: 446\n",
      "Loss: 119.88593228148028\n",
      "l2 norm of gradients: 1.3077490479496379\n",
      "l2 norm of weights: 6.57332192524394\n",
      "---------------------\n",
      "Iteration Number: 447\n",
      "Loss: 119.80255823802948\n",
      "l2 norm of gradients: 1.3071481241219491\n",
      "l2 norm of weights: 6.572988973979901\n",
      "---------------------\n",
      "Iteration Number: 448\n",
      "Loss: 119.71926084097636\n",
      "l2 norm of gradients: 1.3065469795219706\n",
      "l2 norm of weights: 6.572656275049898\n",
      "---------------------\n",
      "Iteration Number: 449\n",
      "Loss: 119.63604008261629\n",
      "l2 norm of gradients: 1.3059456126883033\n",
      "l2 norm of weights: 6.5723238282578835\n",
      "---------------------\n",
      "Iteration Number: 450\n",
      "Loss: 119.55289595494864\n",
      "l2 norm of gradients: 1.3053440221700274\n",
      "l2 norm of weights: 6.571991633407692\n",
      "---------------------\n",
      "Iteration Number: 451\n",
      "Loss: 119.4698284505677\n",
      "l2 norm of gradients: 1.3047422065268517\n",
      "l2 norm of weights: 6.57165969030304\n",
      "---------------------\n",
      "Iteration Number: 452\n",
      "Loss: 119.38683756173465\n",
      "l2 norm of gradients: 1.3041401643292616\n",
      "l2 norm of weights: 6.571327998747521\n",
      "---------------------\n",
      "Iteration Number: 453\n",
      "Loss: 119.30392328131175\n",
      "l2 norm of gradients: 1.3035378941586673\n",
      "l2 norm of weights: 6.5709965585446115\n",
      "---------------------\n",
      "Iteration Number: 454\n",
      "Loss: 119.22108560182859\n",
      "l2 norm of gradients: 1.3029353946075526\n",
      "l2 norm of weights: 6.57066536949766\n",
      "---------------------\n",
      "Iteration Number: 455\n",
      "Loss: 119.1383245163314\n",
      "l2 norm of gradients: 1.302332664279625\n",
      "l2 norm of weights: 6.570334431409898\n",
      "---------------------\n",
      "Iteration Number: 456\n",
      "Loss: 119.05564001772281\n",
      "l2 norm of gradients: 1.3017297017899623\n",
      "l2 norm of weights: 6.570003744084424\n",
      "---------------------\n",
      "Iteration Number: 457\n",
      "Loss: 118.97303209928371\n",
      "l2 norm of gradients: 1.301126505765166\n",
      "l2 norm of weights: 6.569673307324215\n",
      "---------------------\n",
      "Iteration Number: 458\n",
      "Loss: 118.89050075419553\n",
      "l2 norm of gradients: 1.300523074843507\n",
      "l2 norm of weights: 6.5693431209321185\n",
      "---------------------\n",
      "Iteration Number: 459\n",
      "Loss: 118.80804597598691\n",
      "l2 norm of gradients: 1.2999194076750789\n",
      "l2 norm of weights: 6.569013184710854\n",
      "---------------------\n",
      "Iteration Number: 460\n",
      "Loss: 118.72566775798533\n",
      "l2 norm of gradients: 1.2993155029219465\n",
      "l2 norm of weights: 6.56868349846301\n",
      "---------------------\n",
      "Iteration Number: 461\n",
      "Loss: 118.64336609407093\n",
      "l2 norm of gradients: 1.2987113592582975\n",
      "l2 norm of weights: 6.568354061991044\n",
      "---------------------\n",
      "Iteration Number: 462\n",
      "Loss: 118.56114097793437\n",
      "l2 norm of gradients: 1.2981069753705938\n",
      "l2 norm of weights: 6.568024875097279\n",
      "---------------------\n",
      "Iteration Number: 463\n",
      "Loss: 118.47899240347216\n",
      "l2 norm of gradients: 1.2975023499577232\n",
      "l2 norm of weights: 6.567695937583907\n",
      "---------------------\n",
      "Iteration Number: 464\n",
      "Loss: 118.39692036472587\n",
      "l2 norm of gradients: 1.2968974817311498\n",
      "l2 norm of weights: 6.5673672492529835\n",
      "---------------------\n",
      "Iteration Number: 465\n",
      "Loss: 118.31492485571972\n",
      "l2 norm of gradients: 1.2962923694150692\n",
      "l2 norm of weights: 6.5670388099064265\n",
      "---------------------\n",
      "Iteration Number: 466\n",
      "Loss: 118.23300587080381\n",
      "l2 norm of gradients: 1.2956870117465589\n",
      "l2 norm of weights: 6.5667106193460185\n",
      "---------------------\n",
      "Iteration Number: 467\n",
      "Loss: 118.15116340436671\n",
      "l2 norm of gradients: 1.2950814074757333\n",
      "l2 norm of weights: 6.5663826773734\n",
      "---------------------\n",
      "Iteration Number: 468\n",
      "Loss: 118.06939745075725\n",
      "l2 norm of gradients: 1.2944755553658969\n",
      "l2 norm of weights: 6.5660549837900755\n",
      "---------------------\n",
      "Iteration Number: 469\n",
      "Loss: 117.987708004684\n",
      "l2 norm of gradients: 1.2938694541936988\n",
      "l2 norm of weights: 6.565727538397405\n",
      "---------------------\n",
      "Iteration Number: 470\n",
      "Loss: 117.90609506071182\n",
      "l2 norm of gradients: 1.293263102749287\n",
      "l2 norm of weights: 6.56540034099661\n",
      "---------------------\n",
      "Iteration Number: 471\n",
      "Loss: 117.8245586136544\n",
      "l2 norm of gradients: 1.2926564998364645\n",
      "l2 norm of weights: 6.565073391388764\n",
      "---------------------\n",
      "Iteration Number: 472\n",
      "Loss: 117.74309865840208\n",
      "l2 norm of gradients: 1.2920496442728457\n",
      "l2 norm of weights: 6.5647466893748\n",
      "---------------------\n",
      "Iteration Number: 473\n",
      "Loss: 117.6617151899411\n",
      "l2 norm of gradients: 1.291442534890011\n",
      "l2 norm of weights: 6.564420234755502\n",
      "---------------------\n",
      "Iteration Number: 474\n",
      "Loss: 117.58040820349002\n",
      "l2 norm of gradients: 1.2908351705336656\n",
      "l2 norm of weights: 6.56409402733151\n",
      "---------------------\n",
      "Iteration Number: 475\n",
      "Loss: 117.49917769400768\n",
      "l2 norm of gradients: 1.2902275500637963\n",
      "l2 norm of weights: 6.563768066903313\n",
      "---------------------\n",
      "Iteration Number: 476\n",
      "Loss: 117.41802365689705\n",
      "l2 norm of gradients: 1.2896196723548297\n",
      "l2 norm of weights: 6.563442353271253\n",
      "---------------------\n",
      "Iteration Number: 477\n",
      "Loss: 117.33694608742509\n",
      "l2 norm of gradients: 1.2890115362957906\n",
      "l2 norm of weights: 6.563116886235522\n",
      "---------------------\n",
      "Iteration Number: 478\n",
      "Loss: 117.2559449811625\n",
      "l2 norm of gradients: 1.2884031407904617\n",
      "l2 norm of weights: 6.5627916655961585\n",
      "---------------------\n",
      "Iteration Number: 479\n",
      "Loss: 117.17502033366534\n",
      "l2 norm of gradients: 1.287794484757544\n",
      "l2 norm of weights: 6.562466691153051\n",
      "---------------------\n",
      "Iteration Number: 480\n",
      "Loss: 117.09417214047429\n",
      "l2 norm of gradients: 1.2871855671308159\n",
      "l2 norm of weights: 6.562141962705932\n",
      "---------------------\n",
      "Iteration Number: 481\n",
      "Loss: 117.01340039729979\n",
      "l2 norm of gradients: 1.2865763868592945\n",
      "l2 norm of weights: 6.5618174800543825\n",
      "---------------------\n",
      "Iteration Number: 482\n",
      "Loss: 116.93270510007345\n",
      "l2 norm of gradients: 1.2859669429073983\n",
      "l2 norm of weights: 6.561493242997824\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 483\n",
      "Loss: 116.852086244423\n",
      "l2 norm of gradients: 1.2853572342551092\n",
      "l2 norm of weights: 6.561169251335525\n",
      "---------------------\n",
      "Iteration Number: 484\n",
      "Loss: 116.77154382658189\n",
      "l2 norm of gradients: 1.2847472598981333\n",
      "l2 norm of weights: 6.560845504866594\n",
      "---------------------\n",
      "Iteration Number: 485\n",
      "Loss: 116.69107784232541\n",
      "l2 norm of gradients: 1.284137018848067\n",
      "l2 norm of weights: 6.5605220033899805\n",
      "---------------------\n",
      "Iteration Number: 486\n",
      "Loss: 116.61068828800947\n",
      "l2 norm of gradients: 1.2835265101325593\n",
      "l2 norm of weights: 6.560198746704477\n",
      "---------------------\n",
      "Iteration Number: 487\n",
      "Loss: 116.53037515956541\n",
      "l2 norm of gradients: 1.2829157327954765\n",
      "l2 norm of weights: 6.55987573460871\n",
      "---------------------\n",
      "Iteration Number: 488\n",
      "Loss: 116.45013845327075\n",
      "l2 norm of gradients: 1.2823046858970677\n",
      "l2 norm of weights: 6.559552966901149\n",
      "---------------------\n",
      "Iteration Number: 489\n",
      "Loss: 116.36997816570339\n",
      "l2 norm of gradients: 1.281693368514131\n",
      "l2 norm of weights: 6.559230443380099\n",
      "---------------------\n",
      "Iteration Number: 490\n",
      "Loss: 116.28989429279228\n",
      "l2 norm of gradients: 1.2810817797401775\n",
      "l2 norm of weights: 6.558908163843699\n",
      "---------------------\n",
      "Iteration Number: 491\n",
      "Loss: 116.20988683138046\n",
      "l2 norm of gradients: 1.2804699186856012\n",
      "l2 norm of weights: 6.558586128089928\n",
      "---------------------\n",
      "Iteration Number: 492\n",
      "Loss: 116.12995577761605\n",
      "l2 norm of gradients: 1.2798577844778434\n",
      "l2 norm of weights: 6.558264335916592\n",
      "---------------------\n",
      "Iteration Number: 493\n",
      "Loss: 116.05010112817617\n",
      "l2 norm of gradients: 1.279245376261562\n",
      "l2 norm of weights: 6.557942787121338\n",
      "---------------------\n",
      "Iteration Number: 494\n",
      "Loss: 115.97032287977407\n",
      "l2 norm of gradients: 1.2786326931988001\n",
      "l2 norm of weights: 6.557621481501638\n",
      "---------------------\n",
      "Iteration Number: 495\n",
      "Loss: 115.8906210288589\n",
      "l2 norm of gradients: 1.278019734469154\n",
      "l2 norm of weights: 6.557300418854802\n",
      "---------------------\n",
      "Iteration Number: 496\n",
      "Loss: 115.81099557235964\n",
      "l2 norm of gradients: 1.2774064992699423\n",
      "l2 norm of weights: 6.556979598977967\n",
      "---------------------\n",
      "Iteration Number: 497\n",
      "Loss: 115.73144650682389\n",
      "l2 norm of gradients: 1.276792986816377\n",
      "l2 norm of weights: 6.556659021668096\n",
      "---------------------\n",
      "Iteration Number: 498\n",
      "Loss: 115.65197382915225\n",
      "l2 norm of gradients: 1.2761791963417324\n",
      "l2 norm of weights: 6.556338686721989\n",
      "---------------------\n",
      "Iteration Number: 499\n",
      "Loss: 115.57257753608222\n",
      "l2 norm of gradients: 1.2755651270975163\n",
      "l2 norm of weights: 6.556018593936265\n",
      "---------------------\n",
      "Iteration Number: 500\n",
      "Loss: 115.49325762463089\n",
      "l2 norm of gradients: 1.2749507783536411\n",
      "l2 norm of weights: 6.555698743107374\n",
      "---------------------\n",
      "Iteration Number: 501\n",
      "Loss: 115.41401409170957\n",
      "l2 norm of gradients: 1.274336149398595\n",
      "l2 norm of weights: 6.555379134031594\n",
      "---------------------\n",
      "Iteration Number: 502\n",
      "Loss: 115.33484693405424\n",
      "l2 norm of gradients: 1.2737212395396136\n",
      "l2 norm of weights: 6.555059766505022\n",
      "---------------------\n",
      "Iteration Number: 503\n",
      "Loss: 115.25575614889762\n",
      "l2 norm of gradients: 1.2731060481028529\n",
      "l2 norm of weights: 6.554740640323584\n",
      "---------------------\n",
      "Iteration Number: 504\n",
      "Loss: 115.1767417331283\n",
      "l2 norm of gradients: 1.2724905744335604\n",
      "l2 norm of weights: 6.554421755283028\n",
      "---------------------\n",
      "Iteration Number: 505\n",
      "Loss: 115.09780368381367\n",
      "l2 norm of gradients: 1.2718748178962502\n",
      "l2 norm of weights: 6.5541031111789225\n",
      "---------------------\n",
      "Iteration Number: 506\n",
      "Loss: 115.01894199803742\n",
      "l2 norm of gradients: 1.271258777874873\n",
      "l2 norm of weights: 6.553784707806661\n",
      "---------------------\n",
      "Iteration Number: 507\n",
      "Loss: 114.94015667289958\n",
      "l2 norm of gradients: 1.2706424537729915\n",
      "l2 norm of weights: 6.553466544961454\n",
      "---------------------\n",
      "Iteration Number: 508\n",
      "Loss: 114.86144770538405\n",
      "l2 norm of gradients: 1.2700258450139528\n",
      "l2 norm of weights: 6.553148622438335\n",
      "---------------------\n",
      "Iteration Number: 509\n",
      "Loss: 114.78281509278136\n",
      "l2 norm of gradients: 1.2694089510410629\n",
      "l2 norm of weights: 6.552830940032156\n",
      "---------------------\n",
      "Iteration Number: 510\n",
      "Loss: 114.70425883221341\n",
      "l2 norm of gradients: 1.26879177131776\n",
      "l2 norm of weights: 6.552513497537588\n",
      "---------------------\n",
      "Iteration Number: 511\n",
      "Loss: 114.62577892063668\n",
      "l2 norm of gradients: 1.268174305327788\n",
      "l2 norm of weights: 6.552196294749117\n",
      "---------------------\n",
      "Iteration Number: 512\n",
      "Loss: 114.54737535557291\n",
      "l2 norm of gradients: 1.2675565525753716\n",
      "l2 norm of weights: 6.551879331461049\n",
      "---------------------\n",
      "Iteration Number: 513\n",
      "Loss: 114.46904813385538\n",
      "l2 norm of gradients: 1.2669385125853891\n",
      "l2 norm of weights: 6.551562607467504\n",
      "---------------------\n",
      "Iteration Number: 514\n",
      "Loss: 114.39079725290635\n",
      "l2 norm of gradients: 1.266320184903547\n",
      "l2 norm of weights: 6.55124612256242\n",
      "---------------------\n",
      "Iteration Number: 515\n",
      "Loss: 114.31262270968588\n",
      "l2 norm of gradients: 1.2657015690965543\n",
      "l2 norm of weights: 6.550929876539546\n",
      "---------------------\n",
      "Iteration Number: 516\n",
      "Loss: 114.23452450161028\n",
      "l2 norm of gradients: 1.265082664752295\n",
      "l2 norm of weights: 6.55061386919245\n",
      "---------------------\n",
      "Iteration Number: 517\n",
      "Loss: 114.15650262565214\n",
      "l2 norm of gradients: 1.2644634714800047\n",
      "l2 norm of weights: 6.550298100314508\n",
      "---------------------\n",
      "Iteration Number: 518\n",
      "Loss: 114.07855707909036\n",
      "l2 norm of gradients: 1.263843988910441\n",
      "l2 norm of weights: 6.549982569698914\n",
      "---------------------\n",
      "Iteration Number: 519\n",
      "Loss: 114.00068785912075\n",
      "l2 norm of gradients: 1.2632242166960597\n",
      "l2 norm of weights: 6.549667277138668\n",
      "---------------------\n",
      "Iteration Number: 520\n",
      "Loss: 113.92289496287555\n",
      "l2 norm of gradients: 1.2626041545111861\n",
      "l2 norm of weights: 6.549352222426589\n",
      "---------------------\n",
      "Iteration Number: 521\n",
      "Loss: 113.84517838730237\n",
      "l2 norm of gradients: 1.2619838020521892\n",
      "l2 norm of weights: 6.5490374053553\n",
      "---------------------\n",
      "Iteration Number: 522\n",
      "Loss: 113.76753812974891\n",
      "l2 norm of gradients: 1.261363159037654\n",
      "l2 norm of weights: 6.548722825717238\n",
      "---------------------\n",
      "Iteration Number: 523\n",
      "Loss: 113.68997418731955\n",
      "l2 norm of gradients: 1.2607422252085532\n",
      "l2 norm of weights: 6.548408483304647\n",
      "---------------------\n",
      "Iteration Number: 524\n",
      "Loss: 113.61248655689157\n",
      "l2 norm of gradients: 1.2601210003284202\n",
      "l2 norm of weights: 6.548094377909583\n",
      "---------------------\n",
      "Iteration Number: 525\n",
      "Loss: 113.53507523562783\n",
      "l2 norm of gradients: 1.2594994841835196\n",
      "l2 norm of weights: 6.547780509323908\n",
      "---------------------\n",
      "Iteration Number: 526\n",
      "Loss: 113.45774022068817\n",
      "l2 norm of gradients: 1.2588776765830183\n",
      "l2 norm of weights: 6.547466877339294\n",
      "---------------------\n",
      "Iteration Number: 527\n",
      "Loss: 113.38048150883542\n",
      "l2 norm of gradients: 1.2582555773591562\n",
      "l2 norm of weights: 6.547153481747219\n",
      "---------------------\n",
      "Iteration Number: 528\n",
      "Loss: 113.3032990971678\n",
      "l2 norm of gradients: 1.2576331863674157\n",
      "l2 norm of weights: 6.546840322338966\n",
      "---------------------\n",
      "Iteration Number: 529\n",
      "Loss: 113.22619298262678\n",
      "l2 norm of gradients: 1.2570105034866907\n",
      "l2 norm of weights: 6.546527398905626\n",
      "---------------------\n",
      "Iteration Number: 530\n",
      "Loss: 113.14916316197981\n",
      "l2 norm of gradients: 1.2563875286194548\n",
      "l2 norm of weights: 6.546214711238101\n",
      "---------------------\n",
      "Iteration Number: 531\n",
      "Loss: 113.0722096320828\n",
      "l2 norm of gradients: 1.2557642616919296\n",
      "l2 norm of weights: 6.5459022591270895\n",
      "---------------------\n",
      "Iteration Number: 532\n",
      "Loss: 112.99533238985494\n",
      "l2 norm of gradients: 1.2551407026542505\n",
      "l2 norm of weights: 6.5455900423631\n",
      "---------------------\n",
      "Iteration Number: 533\n",
      "Loss: 112.91853143206261\n",
      "l2 norm of gradients: 1.2545168514806337\n",
      "l2 norm of weights: 6.545278060736445\n",
      "---------------------\n",
      "Iteration Number: 534\n",
      "Loss: 112.84180675519659\n",
      "l2 norm of gradients: 1.2538927081695401\n",
      "l2 norm of weights: 6.544966314037239\n",
      "---------------------\n",
      "Iteration Number: 535\n",
      "Loss: 112.76515835600597\n",
      "l2 norm of gradients: 1.2532682727438398\n",
      "l2 norm of weights: 6.544654802055404\n",
      "---------------------\n",
      "Iteration Number: 536\n",
      "Loss: 112.6885862310945\n",
      "l2 norm of gradients: 1.252643545250975\n",
      "l2 norm of weights: 6.544343524580661\n",
      "---------------------\n",
      "Iteration Number: 537\n",
      "Loss: 112.61209037712435\n",
      "l2 norm of gradients: 1.2520185257631218\n",
      "l2 norm of weights: 6.544032481402537\n",
      "---------------------\n",
      "Iteration Number: 538\n",
      "Loss: 112.53567079021532\n",
      "l2 norm of gradients: 1.2513932143773514\n",
      "l2 norm of weights: 6.543721672310361\n",
      "---------------------\n",
      "Iteration Number: 539\n",
      "Loss: 112.45932746713177\n",
      "l2 norm of gradients: 1.2507676112157882\n",
      "l2 norm of weights: 6.543411097093262\n",
      "---------------------\n",
      "Iteration Number: 540\n",
      "Loss: 112.38306040398247\n",
      "l2 norm of gradients: 1.2501417164257695\n",
      "l2 norm of weights: 6.543100755540174\n",
      "---------------------\n",
      "Iteration Number: 541\n",
      "Loss: 112.30686959703279\n",
      "l2 norm of gradients: 1.249515530180001\n",
      "l2 norm of weights: 6.542790647439831\n",
      "---------------------\n",
      "Iteration Number: 542\n",
      "Loss: 112.23075504258841\n",
      "l2 norm of gradients: 1.2488890526767131\n",
      "l2 norm of weights: 6.542480772580768\n",
      "---------------------\n",
      "Iteration Number: 543\n",
      "Loss: 112.15471673666103\n",
      "l2 norm of gradients: 1.248262284139815\n",
      "l2 norm of weights: 6.542171130751322\n",
      "---------------------\n",
      "Iteration Number: 544\n",
      "Loss: 112.07875467529226\n",
      "l2 norm of gradients: 1.2476352248190459\n",
      "l2 norm of weights: 6.541861721739632\n",
      "---------------------\n",
      "Iteration Number: 545\n",
      "Loss: 112.00286885452972\n",
      "l2 norm of gradients: 1.2470078749901272\n",
      "l2 norm of weights: 6.5415525453336345\n",
      "---------------------\n",
      "Iteration Number: 546\n",
      "Loss: 111.92705927001902\n",
      "l2 norm of gradients: 1.2463802349549107\n",
      "l2 norm of weights: 6.541243601321068\n",
      "---------------------\n",
      "Iteration Number: 547\n",
      "Loss: 111.85132591765569\n",
      "l2 norm of gradients: 1.2457523050415262\n",
      "l2 norm of weights: 6.540934889489473\n",
      "---------------------\n",
      "Iteration Number: 548\n",
      "Loss: 111.77566879300035\n",
      "l2 norm of gradients: 1.2451240856045285\n",
      "l2 norm of weights: 6.540626409626189\n",
      "---------------------\n",
      "Iteration Number: 549\n",
      "Loss: 111.70008789187686\n",
      "l2 norm of gradients: 1.2444955770250383\n",
      "l2 norm of weights: 6.540318161518352\n",
      "---------------------\n",
      "Iteration Number: 550\n",
      "Loss: 111.62458320952027\n",
      "l2 norm of gradients: 1.243866779710887\n",
      "l2 norm of weights: 6.540010144952903\n",
      "---------------------\n",
      "Iteration Number: 551\n",
      "Loss: 111.54915474134347\n",
      "l2 norm of gradients: 1.243237694096755\n",
      "l2 norm of weights: 6.539702359716579\n",
      "---------------------\n",
      "Iteration Number: 552\n",
      "Loss: 111.4738024827204\n",
      "l2 norm of gradients: 1.2426083206443104\n",
      "l2 norm of weights: 6.539394805595921\n",
      "---------------------\n",
      "Iteration Number: 553\n",
      "Loss: 111.39852642861946\n",
      "l2 norm of gradients: 1.2419786598423428\n",
      "l2 norm of weights: 6.539087482377266\n",
      "---------------------\n",
      "Iteration Number: 554\n",
      "Loss: 111.32332657427837\n",
      "l2 norm of gradients: 1.2413487122068998\n",
      "l2 norm of weights: 6.5387803898467505\n",
      "---------------------\n",
      "Iteration Number: 555\n",
      "Loss: 111.2482029145835\n",
      "l2 norm of gradients: 1.240718478281416\n",
      "l2 norm of weights: 6.538473527790311\n",
      "---------------------\n",
      "Iteration Number: 556\n",
      "Loss: 111.17315544432046\n",
      "l2 norm of gradients: 1.2400879586368425\n",
      "l2 norm of weights: 6.538166895993687\n",
      "---------------------\n",
      "Iteration Number: 557\n",
      "Loss: 111.09818415820662\n",
      "l2 norm of gradients: 1.2394571538717736\n",
      "l2 norm of weights: 6.537860494242413\n",
      "---------------------\n",
      "Iteration Number: 558\n",
      "Loss: 111.02328905083522\n",
      "l2 norm of gradients: 1.2388260646125706\n",
      "l2 norm of weights: 6.5375543223218235\n",
      "---------------------\n",
      "Iteration Number: 559\n",
      "Loss: 110.94847011652155\n",
      "l2 norm of gradients: 1.2381946915134838\n",
      "l2 norm of weights: 6.537248380017058\n",
      "---------------------\n",
      "Iteration Number: 560\n",
      "Loss: 110.8737273497148\n",
      "l2 norm of gradients: 1.2375630352567708\n",
      "l2 norm of weights: 6.53694266711305\n",
      "---------------------\n",
      "Iteration Number: 561\n",
      "Loss: 110.79906074449485\n",
      "l2 norm of gradients: 1.2369310965528137\n",
      "l2 norm of weights: 6.536637183394535\n",
      "---------------------\n",
      "Iteration Number: 562\n",
      "Loss: 110.7244702949102\n",
      "l2 norm of gradients: 1.2362988761402312\n",
      "l2 norm of weights: 6.53633192864605\n",
      "---------------------\n",
      "Iteration Number: 563\n",
      "Loss: 110.6499559949771\n",
      "l2 norm of gradients: 1.2356663747859897\n",
      "l2 norm of weights: 6.536026902651931\n",
      "---------------------\n",
      "Iteration Number: 564\n",
      "Loss: 110.5755178383676\n",
      "l2 norm of gradients: 1.2350335932855125\n",
      "l2 norm of weights: 6.535722105196313\n",
      "---------------------\n",
      "Iteration Number: 565\n",
      "Loss: 110.50115581856736\n",
      "l2 norm of gradients: 1.234400532462783\n",
      "l2 norm of weights: 6.535417536063134\n",
      "---------------------\n",
      "Iteration Number: 566\n",
      "Loss: 110.42686992929173\n",
      "l2 norm of gradients: 1.2337671931704464\n",
      "l2 norm of weights: 6.535113195036135\n",
      "---------------------\n",
      "Iteration Number: 567\n",
      "Loss: 110.35266016374801\n",
      "l2 norm of gradients: 1.2331335762899107\n",
      "l2 norm of weights: 6.534809081898852\n",
      "---------------------\n",
      "Iteration Number: 568\n",
      "Loss: 110.27852651503271\n",
      "l2 norm of gradients: 1.2324996827314385\n",
      "l2 norm of weights: 6.534505196434627\n",
      "---------------------\n",
      "Iteration Number: 569\n",
      "Loss: 110.20446897614819\n",
      "l2 norm of gradients: 1.2318655134342429\n",
      "l2 norm of weights: 6.534201538426604\n",
      "---------------------\n",
      "Iteration Number: 570\n",
      "Loss: 110.13048753989322\n",
      "l2 norm of gradients: 1.231231069366575\n",
      "l2 norm of weights: 6.533898107657728\n",
      "---------------------\n",
      "Iteration Number: 571\n",
      "Loss: 110.05658219904853\n",
      "l2 norm of gradients: 1.2305963515258076\n",
      "l2 norm of weights: 6.533594903910746\n",
      "---------------------\n",
      "Iteration Number: 572\n",
      "Loss: 109.98275294600828\n",
      "l2 norm of gradients: 1.229961360938522\n",
      "l2 norm of weights: 6.533291926968209\n",
      "---------------------\n",
      "Iteration Number: 573\n",
      "Loss: 109.90899977321835\n",
      "l2 norm of gradients: 1.2293260986605816\n",
      "l2 norm of weights: 6.532989176612471\n",
      "---------------------\n",
      "Iteration Number: 574\n",
      "Loss: 109.83532267261259\n",
      "l2 norm of gradients: 1.2286905657772105\n",
      "l2 norm of weights: 6.5326866526256895\n",
      "---------------------\n",
      "Iteration Number: 575\n",
      "Loss: 109.76172163639934\n",
      "l2 norm of gradients: 1.2280547634030632\n",
      "l2 norm of weights: 6.5323843547898255\n",
      "---------------------\n",
      "Iteration Number: 576\n",
      "Loss: 109.6881966561542\n",
      "l2 norm of gradients: 1.2274186926822923\n",
      "l2 norm of weights: 6.532082282886648\n",
      "---------------------\n",
      "Iteration Number: 577\n",
      "Loss: 109.6147477237067\n",
      "l2 norm of gradients: 1.2267823547886134\n",
      "l2 norm of weights: 6.531780436697728\n",
      "---------------------\n",
      "Iteration Number: 578\n",
      "Loss: 109.54137483027166\n",
      "l2 norm of gradients: 1.226145750925364\n",
      "l2 norm of weights: 6.5314788160044435\n",
      "---------------------\n",
      "Iteration Number: 579\n",
      "Loss: 109.46807796707017\n",
      "l2 norm of gradients: 1.2255088823255602\n",
      "l2 norm of weights: 6.531177420587977\n",
      "---------------------\n",
      "Iteration Number: 580\n",
      "Loss: 109.39485712537869\n",
      "l2 norm of gradients: 1.224871750251948\n",
      "l2 norm of weights: 6.530876250229323\n",
      "---------------------\n",
      "Iteration Number: 581\n",
      "Loss: 109.3217122956988\n",
      "l2 norm of gradients: 1.2242343559970528\n",
      "l2 norm of weights: 6.530575304709279\n",
      "---------------------\n",
      "Iteration Number: 582\n",
      "Loss: 109.2486434689007\n",
      "l2 norm of gradients: 1.2235967008832214\n",
      "l2 norm of weights: 6.530274583808452\n",
      "---------------------\n",
      "Iteration Number: 583\n",
      "Loss: 109.17565063530483\n",
      "l2 norm of gradients: 1.2229587862626627\n",
      "l2 norm of weights: 6.529974087307256\n",
      "---------------------\n",
      "Iteration Number: 584\n",
      "Loss: 109.10273378524764\n",
      "l2 norm of gradients: 1.2223206135174836\n",
      "l2 norm of weights: 6.52967381498592\n",
      "---------------------\n",
      "Iteration Number: 585\n",
      "Loss: 109.02989290853913\n",
      "l2 norm of gradients: 1.221682184059718\n",
      "l2 norm of weights: 6.529373766624478\n",
      "---------------------\n",
      "Iteration Number: 586\n",
      "Loss: 108.95712799519373\n",
      "l2 norm of gradients: 1.2210434993313564\n",
      "l2 norm of weights: 6.52907394200278\n",
      "---------------------\n",
      "Iteration Number: 587\n",
      "Loss: 108.88443903458561\n",
      "l2 norm of gradients: 1.220404560804365\n",
      "l2 norm of weights: 6.528774340900482\n",
      "---------------------\n",
      "Iteration Number: 588\n",
      "Loss: 108.81182601620313\n",
      "l2 norm of gradients: 1.2197653699807054\n",
      "l2 norm of weights: 6.528474963097057\n",
      "---------------------\n",
      "Iteration Number: 589\n",
      "Loss: 108.73928892907418\n",
      "l2 norm of gradients: 1.2191259283923472\n",
      "l2 norm of weights: 6.528175808371793\n",
      "---------------------\n",
      "Iteration Number: 590\n",
      "Loss: 108.66682776222216\n",
      "l2 norm of gradients: 1.2184862376012764\n",
      "l2 norm of weights: 6.527876876503787\n",
      "---------------------\n",
      "Iteration Number: 591\n",
      "Loss: 108.5944425043575\n",
      "l2 norm of gradients: 1.2178462991994983\n",
      "l2 norm of weights: 6.5275781672719555\n",
      "---------------------\n",
      "Iteration Number: 592\n",
      "Loss: 108.52213314381017\n",
      "l2 norm of gradients: 1.2172061148090374\n",
      "l2 norm of weights: 6.52727968045503\n",
      "---------------------\n",
      "Iteration Number: 593\n",
      "Loss: 108.44989966881117\n",
      "l2 norm of gradients: 1.216565686081932\n",
      "l2 norm of weights: 6.526981415831561\n",
      "---------------------\n",
      "Iteration Number: 594\n",
      "Loss: 108.37774206735276\n",
      "l2 norm of gradients: 1.2159250147002214\n",
      "l2 norm of weights: 6.526683373179914\n",
      "---------------------\n",
      "Iteration Number: 595\n",
      "Loss: 108.30566032715502\n",
      "l2 norm of gradients: 1.2152841023759322\n",
      "l2 norm of weights: 6.526385552278276\n",
      "---------------------\n",
      "Iteration Number: 596\n",
      "Loss: 108.23365443583906\n",
      "l2 norm of gradients: 1.2146429508510557\n",
      "l2 norm of weights: 6.526087952904654\n",
      "---------------------\n",
      "Iteration Number: 597\n",
      "Loss: 108.16172438044616\n",
      "l2 norm of gradients: 1.2140015618975244\n",
      "l2 norm of weights: 6.525790574836877\n",
      "---------------------\n",
      "Iteration Number: 598\n",
      "Loss: 108.08987014807103\n",
      "l2 norm of gradients: 1.2133599373171786\n",
      "l2 norm of weights: 6.525493417852595\n",
      "---------------------\n",
      "Iteration Number: 599\n",
      "Loss: 108.01809172534566\n",
      "l2 norm of gradients: 1.2127180789417324\n",
      "l2 norm of weights: 6.5251964817292825\n",
      "---------------------\n",
      "Iteration Number: 600\n",
      "Loss: 107.94638909898744\n",
      "l2 norm of gradients: 1.2120759886327308\n",
      "l2 norm of weights: 6.524899766244237\n",
      "---------------------\n",
      "Iteration Number: 601\n",
      "Loss: 107.87476225489542\n",
      "l2 norm of gradients: 1.2114336682815037\n",
      "l2 norm of weights: 6.524603271174587\n",
      "---------------------\n",
      "Iteration Number: 602\n",
      "Loss: 107.80321117925449\n",
      "l2 norm of gradients: 1.2107911198091146\n",
      "l2 norm of weights: 6.524306996297281\n",
      "---------------------\n",
      "Iteration Number: 603\n",
      "Loss: 107.73173585774467\n",
      "l2 norm of gradients: 1.2101483451663024\n",
      "l2 norm of weights: 6.524010941389101\n",
      "---------------------\n",
      "Iteration Number: 604\n",
      "Loss: 107.66033627560786\n",
      "l2 norm of gradients: 1.2095053463334198\n",
      "l2 norm of weights: 6.5237151062266605\n",
      "---------------------\n",
      "Iteration Number: 605\n",
      "Loss: 107.5890124181963\n",
      "l2 norm of gradients: 1.208862125320363\n",
      "l2 norm of weights: 6.523419490586396\n",
      "---------------------\n",
      "Iteration Number: 606\n",
      "Loss: 107.51776427019202\n",
      "l2 norm of gradients: 1.2082186841665012\n",
      "l2 norm of weights: 6.523124094244583\n",
      "---------------------\n",
      "Iteration Number: 607\n",
      "Loss: 107.44659181639832\n",
      "l2 norm of gradients: 1.2075750249405943\n",
      "l2 norm of weights: 6.52282891697733\n",
      "---------------------\n",
      "Iteration Number: 608\n",
      "Loss: 107.37549504094635\n",
      "l2 norm of gradients: 1.2069311497407107\n",
      "l2 norm of weights: 6.522533958560577\n",
      "---------------------\n",
      "Iteration Number: 609\n",
      "Loss: 107.30447392796947\n",
      "l2 norm of gradients: 1.2062870606941343\n",
      "l2 norm of weights: 6.522239218770106\n",
      "---------------------\n",
      "Iteration Number: 610\n",
      "Loss: 107.233528461116\n",
      "l2 norm of gradients: 1.2056427599572714\n",
      "l2 norm of weights: 6.521944697381532\n",
      "---------------------\n",
      "Iteration Number: 611\n",
      "Loss: 107.16265862397276\n",
      "l2 norm of gradients: 1.2049982497155463\n",
      "l2 norm of weights: 6.521650394170311\n",
      "---------------------\n",
      "Iteration Number: 612\n",
      "Loss: 107.09186439970422\n",
      "l2 norm of gradients: 1.2043535321832943\n",
      "l2 norm of weights: 6.5213563089117415\n",
      "---------------------\n",
      "Iteration Number: 613\n",
      "Loss: 107.0211457710704\n",
      "l2 norm of gradients: 1.2037086096036504\n",
      "l2 norm of weights: 6.521062441380963\n",
      "---------------------\n",
      "Iteration Number: 614\n",
      "Loss: 106.95050272067544\n",
      "l2 norm of gradients: 1.2030634842484274\n",
      "l2 norm of weights: 6.520768791352959\n",
      "---------------------\n",
      "Iteration Number: 615\n",
      "Loss: 106.87993523078914\n",
      "l2 norm of gradients: 1.2024181584179925\n",
      "l2 norm of weights: 6.520475358602559\n",
      "---------------------\n",
      "Iteration Number: 616\n",
      "Loss: 106.80944328339433\n",
      "l2 norm of gradients: 1.2017726344411364\n",
      "l2 norm of weights: 6.5201821429044395\n",
      "---------------------\n",
      "Iteration Number: 617\n",
      "Loss: 106.7390268602648\n",
      "l2 norm of gradients: 1.2011269146749362\n",
      "l2 norm of weights: 6.519889144033128\n",
      "---------------------\n",
      "Iteration Number: 618\n",
      "Loss: 106.66868594255219\n",
      "l2 norm of gradients: 1.200481001504613\n",
      "l2 norm of weights: 6.519596361762998\n",
      "---------------------\n",
      "Iteration Number: 619\n",
      "Loss: 106.59842051141936\n",
      "l2 norm of gradients: 1.1998348973433837\n",
      "l2 norm of weights: 6.51930379586828\n",
      "---------------------\n",
      "Iteration Number: 620\n",
      "Loss: 106.52823054749344\n",
      "l2 norm of gradients: 1.199188604632305\n",
      "l2 norm of weights: 6.519011446123056\n",
      "---------------------\n",
      "Iteration Number: 621\n",
      "Loss: 106.45811603130919\n",
      "l2 norm of gradients: 1.1985421258401154\n",
      "l2 norm of weights: 6.518719312301266\n",
      "---------------------\n",
      "Iteration Number: 622\n",
      "Loss: 106.3880769429782\n",
      "l2 norm of gradients: 1.1978954634630665\n",
      "l2 norm of weights: 6.518427394176704\n",
      "---------------------\n",
      "Iteration Number: 623\n",
      "Loss: 106.3181132621659\n",
      "l2 norm of gradients: 1.1972486200247519\n",
      "l2 norm of weights: 6.518135691523026\n",
      "---------------------\n",
      "Iteration Number: 624\n",
      "Loss: 106.24822496833718\n",
      "l2 norm of gradients: 1.1966015980759275\n",
      "l2 norm of weights: 6.517844204113751\n",
      "---------------------\n",
      "Iteration Number: 625\n",
      "Loss: 106.17841204062262\n",
      "l2 norm of gradients: 1.1959544001943285\n",
      "l2 norm of weights: 6.517552931722258\n",
      "---------------------\n",
      "Iteration Number: 626\n",
      "Loss: 106.10867445793647\n",
      "l2 norm of gradients: 1.1953070289844774\n",
      "l2 norm of weights: 6.517261874121791\n",
      "---------------------\n",
      "Iteration Number: 627\n",
      "Loss: 106.03901219864068\n",
      "l2 norm of gradients: 1.1946594870774883\n",
      "l2 norm of weights: 6.516971031085465\n",
      "---------------------\n",
      "Iteration Number: 628\n",
      "Loss: 105.96942524076813\n",
      "l2 norm of gradients: 1.1940117771308632\n",
      "l2 norm of weights: 6.5166804023862595\n",
      "---------------------\n",
      "Iteration Number: 629\n",
      "Loss: 105.89991356234977\n",
      "l2 norm of gradients: 1.193363901828285\n",
      "l2 norm of weights: 6.516389987797028\n",
      "---------------------\n",
      "Iteration Number: 630\n",
      "Loss: 105.83047714061502\n",
      "l2 norm of gradients: 1.1927158638794009\n",
      "l2 norm of weights: 6.5160997870904955\n",
      "---------------------\n",
      "Iteration Number: 631\n",
      "Loss: 105.76111595282129\n",
      "l2 norm of gradients: 1.1920676660196032\n",
      "l2 norm of weights: 6.515809800039263\n",
      "---------------------\n",
      "Iteration Number: 632\n",
      "Loss: 105.6918299756233\n",
      "l2 norm of gradients: 1.1914193110098017\n",
      "l2 norm of weights: 6.5155200264158095\n",
      "---------------------\n",
      "Iteration Number: 633\n",
      "Loss: 105.6226191855776\n",
      "l2 norm of gradients: 1.1907708016361895\n",
      "l2 norm of weights: 6.51523046599249\n",
      "---------------------\n",
      "Iteration Number: 634\n",
      "Loss: 105.5534835586536\n",
      "l2 norm of gradients: 1.1901221407100073\n",
      "l2 norm of weights: 6.514941118541545\n",
      "---------------------\n",
      "Iteration Number: 635\n",
      "Loss: 105.48442307071633\n",
      "l2 norm of gradients: 1.1894733310672951\n",
      "l2 norm of weights: 6.514651983835095\n",
      "---------------------\n",
      "Iteration Number: 636\n",
      "Loss: 105.41543769693146\n",
      "l2 norm of gradients: 1.188824375568644\n",
      "l2 norm of weights: 6.514363061645149\n",
      "---------------------\n",
      "Iteration Number: 637\n",
      "Loss: 105.34652741254928\n",
      "l2 norm of gradients: 1.1881752770989376\n",
      "l2 norm of weights: 6.514074351743604\n",
      "---------------------\n",
      "Iteration Number: 638\n",
      "Loss: 105.27769219220075\n",
      "l2 norm of gradients: 1.1875260385670892\n",
      "l2 norm of weights: 6.513785853902245\n",
      "---------------------\n",
      "Iteration Number: 639\n",
      "Loss: 105.20893201010085\n",
      "l2 norm of gradients: 1.186876662905775\n",
      "l2 norm of weights: 6.5134975678927525\n",
      "---------------------\n",
      "Iteration Number: 640\n",
      "Loss: 105.14024684027085\n",
      "l2 norm of gradients: 1.1862271530711566\n",
      "l2 norm of weights: 6.5132094934866975\n",
      "---------------------\n",
      "Iteration Number: 641\n",
      "Loss: 105.07163665619302\n",
      "l2 norm of gradients: 1.1855775120426018\n",
      "l2 norm of weights: 6.512921630455553\n",
      "---------------------\n",
      "Iteration Number: 642\n",
      "Loss: 105.00310143132455\n",
      "l2 norm of gradients: 1.1849277428224\n",
      "l2 norm of weights: 6.5126339785706895\n",
      "---------------------\n",
      "Iteration Number: 643\n",
      "Loss: 104.93464113823421\n",
      "l2 norm of gradients: 1.184277848435466\n",
      "l2 norm of weights: 6.512346537603379\n",
      "---------------------\n",
      "Iteration Number: 644\n",
      "Loss: 104.86625574973569\n",
      "l2 norm of gradients: 1.1836278319290447\n",
      "l2 norm of weights: 6.512059307324797\n",
      "---------------------\n",
      "Iteration Number: 645\n",
      "Loss: 104.79794523764004\n",
      "l2 norm of gradients: 1.1829776963724083\n",
      "l2 norm of weights: 6.511772287506028\n",
      "---------------------\n",
      "Iteration Number: 646\n",
      "Loss: 104.72970957393677\n",
      "l2 norm of gradients: 1.1823274448565435\n",
      "l2 norm of weights: 6.511485477918063\n",
      "---------------------\n",
      "Iteration Number: 647\n",
      "Loss: 104.66154873002853\n",
      "l2 norm of gradients: 1.1816770804938388\n",
      "l2 norm of weights: 6.5111988783318075\n",
      "---------------------\n",
      "Iteration Number: 648\n",
      "Loss: 104.59346267670963\n",
      "l2 norm of gradients: 1.181026606417763\n",
      "l2 norm of weights: 6.510912488518079\n",
      "---------------------\n",
      "Iteration Number: 649\n",
      "Loss: 104.52545138474842\n",
      "l2 norm of gradients: 1.1803760257825395\n",
      "l2 norm of weights: 6.510626308247611\n",
      "---------------------\n",
      "Iteration Number: 650\n",
      "Loss: 104.45751482450414\n",
      "l2 norm of gradients: 1.1797253417628135\n",
      "l2 norm of weights: 6.510340337291058\n",
      "---------------------\n",
      "Iteration Number: 651\n",
      "Loss: 104.38965296566681\n",
      "l2 norm of gradients: 1.1790745575533155\n",
      "l2 norm of weights: 6.510054575418996\n",
      "---------------------\n",
      "Iteration Number: 652\n",
      "Loss: 104.32186577794955\n",
      "l2 norm of gradients: 1.1784236763685174\n",
      "l2 norm of weights: 6.509769022401924\n",
      "---------------------\n",
      "Iteration Number: 653\n",
      "Loss: 104.25415323017786\n",
      "l2 norm of gradients: 1.1777727014422863\n",
      "l2 norm of weights: 6.5094836780102705\n",
      "---------------------\n",
      "Iteration Number: 654\n",
      "Loss: 104.18651529140811\n",
      "l2 norm of gradients: 1.1771216360275294\n",
      "l2 norm of weights: 6.509198542014389\n",
      "---------------------\n",
      "Iteration Number: 655\n",
      "Loss: 104.11895192979222\n",
      "l2 norm of gradients: 1.1764704833958368\n",
      "l2 norm of weights: 6.508913614184571\n",
      "---------------------\n",
      "Iteration Number: 656\n",
      "Loss: 104.0514631134204\n",
      "l2 norm of gradients: 1.1758192468371165\n",
      "l2 norm of weights: 6.508628894291038\n",
      "---------------------\n",
      "Iteration Number: 657\n",
      "Loss: 103.9840488097736\n",
      "l2 norm of gradients: 1.1751679296592257\n",
      "l2 norm of weights: 6.508344382103952\n",
      "---------------------\n",
      "Iteration Number: 658\n",
      "Loss: 103.91670898605565\n",
      "l2 norm of gradients: 1.174516535187599\n",
      "l2 norm of weights: 6.508060077393412\n",
      "---------------------\n",
      "Iteration Number: 659\n",
      "Loss: 103.84944360908251\n",
      "l2 norm of gradients: 1.1738650667648658\n",
      "l2 norm of weights: 6.507775979929465\n",
      "---------------------\n",
      "Iteration Number: 660\n",
      "Loss: 103.7822526453812\n",
      "l2 norm of gradients: 1.1732135277504707\n",
      "l2 norm of weights: 6.507492089482099\n",
      "---------------------\n",
      "Iteration Number: 661\n",
      "Loss: 103.7151360609014\n",
      "l2 norm of gradients: 1.1725619215202823\n",
      "l2 norm of weights: 6.507208405821255\n",
      "---------------------\n",
      "Iteration Number: 662\n",
      "Loss: 103.64809382115163\n",
      "l2 norm of gradients: 1.1719102514662008\n",
      "l2 norm of weights: 6.5069249287168205\n",
      "---------------------\n",
      "Iteration Number: 663\n",
      "Loss: 103.58112589160895\n",
      "l2 norm of gradients: 1.1712585209957604\n",
      "l2 norm of weights: 6.506641657938639\n",
      "---------------------\n",
      "Iteration Number: 664\n",
      "Loss: 103.51423223691862\n",
      "l2 norm of gradients: 1.1706067335317274\n",
      "l2 norm of weights: 6.506358593256514\n",
      "---------------------\n",
      "Iteration Number: 665\n",
      "Loss: 103.44741282174779\n",
      "l2 norm of gradients: 1.169954892511692\n",
      "l2 norm of weights: 6.506075734440204\n",
      "---------------------\n",
      "Iteration Number: 666\n",
      "Loss: 103.38066760984191\n",
      "l2 norm of gradients: 1.169303001387659\n",
      "l2 norm of weights: 6.5057930812594345\n",
      "---------------------\n",
      "Iteration Number: 667\n",
      "Loss: 103.31399656520863\n",
      "l2 norm of gradients: 1.1686510636256306\n",
      "l2 norm of weights: 6.505510633483893\n",
      "---------------------\n",
      "Iteration Number: 668\n",
      "Loss: 103.24739965078948\n",
      "l2 norm of gradients: 1.1679990827051887\n",
      "l2 norm of weights: 6.5052283908832385\n",
      "---------------------\n",
      "Iteration Number: 669\n",
      "Loss: 103.18087682976348\n",
      "l2 norm of gradients: 1.1673470621190696\n",
      "l2 norm of weights: 6.504946353227099\n",
      "---------------------\n",
      "Iteration Number: 670\n",
      "Loss: 103.11442806432608\n",
      "l2 norm of gradients: 1.1666950053727374\n",
      "l2 norm of weights: 6.504664520285077\n",
      "---------------------\n",
      "Iteration Number: 671\n",
      "Loss: 103.04805331674558\n",
      "l2 norm of gradients: 1.166042915983952\n",
      "l2 norm of weights: 6.504382891826755\n",
      "---------------------\n",
      "Iteration Number: 672\n",
      "Loss: 102.98175254860448\n",
      "l2 norm of gradients: 1.165390797482335\n",
      "l2 norm of weights: 6.504101467621692\n",
      "---------------------\n",
      "Iteration Number: 673\n",
      "Loss: 102.91552572117918\n",
      "l2 norm of gradients: 1.164738653408929\n",
      "l2 norm of weights: 6.503820247439433\n",
      "---------------------\n",
      "Iteration Number: 674\n",
      "Loss: 102.84937279535566\n",
      "l2 norm of gradients: 1.1640864873157573\n",
      "l2 norm of weights: 6.503539231049508\n",
      "---------------------\n",
      "Iteration Number: 675\n",
      "Loss: 102.78329373161343\n",
      "l2 norm of gradients: 1.1634343027653762\n",
      "l2 norm of weights: 6.503258418221435\n",
      "---------------------\n",
      "Iteration Number: 676\n",
      "Loss: 102.71728849003748\n",
      "l2 norm of gradients: 1.1627821033304273\n",
      "l2 norm of weights: 6.502977808724727\n",
      "---------------------\n",
      "Iteration Number: 677\n",
      "Loss: 102.65135703020118\n",
      "l2 norm of gradients: 1.1621298925931836\n",
      "l2 norm of weights: 6.50269740232889\n",
      "---------------------\n",
      "Iteration Number: 678\n",
      "Loss: 102.58549931159214\n",
      "l2 norm of gradients: 1.1614776741450963\n",
      "l2 norm of weights: 6.502417198803428\n",
      "---------------------\n",
      "Iteration Number: 679\n",
      "Loss: 102.51971529288537\n",
      "l2 norm of gradients: 1.1608254515863339\n",
      "l2 norm of weights: 6.5021371979178495\n",
      "---------------------\n",
      "Iteration Number: 680\n",
      "Loss: 102.45400493256246\n",
      "l2 norm of gradients: 1.1601732285253235\n",
      "l2 norm of weights: 6.501857399441663\n",
      "---------------------\n",
      "Iteration Number: 681\n",
      "Loss: 102.38836818878696\n",
      "l2 norm of gradients: 1.1595210085782843\n",
      "l2 norm of weights: 6.501577803144386\n",
      "---------------------\n",
      "Iteration Number: 682\n",
      "Loss: 102.32280501915504\n",
      "l2 norm of gradients: 1.1588687953687635\n",
      "l2 norm of weights: 6.501298408795549\n",
      "---------------------\n",
      "Iteration Number: 683\n",
      "Loss: 102.25731538102647\n",
      "l2 norm of gradients: 1.1582165925271652\n",
      "l2 norm of weights: 6.501019216164693\n",
      "---------------------\n",
      "Iteration Number: 684\n",
      "Loss: 102.19189923119069\n",
      "l2 norm of gradients: 1.1575644036902808\n",
      "l2 norm of weights: 6.500740225021377\n",
      "---------------------\n",
      "Iteration Number: 685\n",
      "Loss: 102.12655652610569\n",
      "l2 norm of gradients: 1.1569122325008139\n",
      "l2 norm of weights: 6.500461435135178\n",
      "---------------------\n",
      "Iteration Number: 686\n",
      "Loss: 102.06128722185574\n",
      "l2 norm of gradients: 1.1562600826069058\n",
      "l2 norm of weights: 6.5001828462757\n",
      "---------------------\n",
      "Iteration Number: 687\n",
      "Loss: 101.99609127412249\n",
      "l2 norm of gradients: 1.155607957661658\n",
      "l2 norm of weights: 6.499904458212568\n",
      "---------------------\n",
      "Iteration Number: 688\n",
      "Loss: 101.93096863816803\n",
      "l2 norm of gradients: 1.1549558613226518\n",
      "l2 norm of weights: 6.4996262707154395\n",
      "---------------------\n",
      "Iteration Number: 689\n",
      "Loss: 101.86591926887363\n",
      "l2 norm of gradients: 1.1543037972514676\n",
      "l2 norm of weights: 6.499348283554005\n",
      "---------------------\n",
      "Iteration Number: 690\n",
      "Loss: 101.8009431206235\n",
      "l2 norm of gradients: 1.1536517691132027\n",
      "l2 norm of weights: 6.4990704964979855\n",
      "---------------------\n",
      "Iteration Number: 691\n",
      "Loss: 101.73604014769603\n",
      "l2 norm of gradients: 1.1529997805759875\n",
      "l2 norm of weights: 6.498792909317145\n",
      "---------------------\n",
      "Iteration Number: 692\n",
      "Loss: 101.67121030356587\n",
      "l2 norm of gradients: 1.152347835310499\n",
      "l2 norm of weights: 6.498515521781288\n",
      "---------------------\n",
      "Iteration Number: 693\n",
      "Loss: 101.60645354152607\n",
      "l2 norm of gradients: 1.151695936989476\n",
      "l2 norm of weights: 6.4982383336602645\n",
      "---------------------\n",
      "Iteration Number: 694\n",
      "Loss: 101.54176981456665\n",
      "l2 norm of gradients: 1.15104408928723\n",
      "l2 norm of weights: 6.4979613447239695\n",
      "---------------------\n",
      "Iteration Number: 695\n",
      "Loss: 101.47715907518962\n",
      "l2 norm of gradients: 1.1503922958791584\n",
      "l2 norm of weights: 6.497684554742354\n",
      "---------------------\n",
      "Iteration Number: 696\n",
      "Loss: 101.41262127528789\n",
      "l2 norm of gradients: 1.1497405604412547\n",
      "l2 norm of weights: 6.497407963485419\n",
      "---------------------\n",
      "Iteration Number: 697\n",
      "Loss: 101.34815636686483\n",
      "l2 norm of gradients: 1.1490888866496194\n",
      "l2 norm of weights: 6.497131570723225\n",
      "---------------------\n",
      "Iteration Number: 698\n",
      "Loss: 101.28376430094494\n",
      "l2 norm of gradients: 1.1484372781799694\n",
      "l2 norm of weights: 6.496855376225892\n",
      "---------------------\n",
      "Iteration Number: 699\n",
      "Loss: 101.21944502854603\n",
      "l2 norm of gradients: 1.1477857387071486\n",
      "l2 norm of weights: 6.496579379763606\n",
      "---------------------\n",
      "Iteration Number: 700\n",
      "Loss: 101.15519850036237\n",
      "l2 norm of gradients: 1.147134271904638\n",
      "l2 norm of weights: 6.496303581106616\n",
      "---------------------\n",
      "Iteration Number: 701\n",
      "Loss: 101.0910246662202\n",
      "l2 norm of gradients: 1.1464828814440629\n",
      "l2 norm of weights: 6.496027980025245\n",
      "---------------------\n",
      "Iteration Number: 702\n",
      "Loss: 101.02692347603258\n",
      "l2 norm of gradients: 1.1458315709947058\n",
      "l2 norm of weights: 6.495752576289888\n",
      "---------------------\n",
      "Iteration Number: 703\n",
      "Loss: 100.96289487926597\n",
      "l2 norm of gradients: 1.145180344223014\n",
      "l2 norm of weights: 6.495477369671015\n",
      "---------------------\n",
      "Iteration Number: 704\n",
      "Loss: 100.89893882470099\n",
      "l2 norm of gradients: 1.1445292047921123\n",
      "l2 norm of weights: 6.495202359939177\n",
      "---------------------\n",
      "Iteration Number: 705\n",
      "Loss: 100.83505526094503\n",
      "l2 norm of gradients: 1.1438781563613118\n",
      "l2 norm of weights: 6.494927546865007\n",
      "---------------------\n",
      "Iteration Number: 706\n",
      "Loss: 100.77124413639949\n",
      "l2 norm of gradients: 1.1432272025856247\n",
      "l2 norm of weights: 6.494652930219225\n",
      "---------------------\n",
      "Iteration Number: 707\n",
      "Loss: 100.70750539870434\n",
      "l2 norm of gradients: 1.1425763471152734\n",
      "l2 norm of weights: 6.494378509772636\n",
      "---------------------\n",
      "Iteration Number: 708\n",
      "Loss: 100.64383899529845\n",
      "l2 norm of gradients: 1.141925593595208\n",
      "l2 norm of weights: 6.494104285296143\n",
      "---------------------\n",
      "Iteration Number: 709\n",
      "Loss: 100.58024487336968\n",
      "l2 norm of gradients: 1.1412749456646185\n",
      "l2 norm of weights: 6.4938302565607415\n",
      "---------------------\n",
      "Iteration Number: 710\n",
      "Loss: 100.51672297950378\n",
      "l2 norm of gradients: 1.140624406956452\n",
      "l2 norm of weights: 6.493556423337524\n",
      "---------------------\n",
      "Iteration Number: 711\n",
      "Loss: 100.45327326013192\n",
      "l2 norm of gradients: 1.1399739810969307\n",
      "l2 norm of weights: 6.493282785397685\n",
      "---------------------\n",
      "Iteration Number: 712\n",
      "Loss: 100.38989566098147\n",
      "l2 norm of gradients: 1.1393236717050708\n",
      "l2 norm of weights: 6.4930093425125275\n",
      "---------------------\n",
      "Iteration Number: 713\n",
      "Loss: 100.32659012783593\n",
      "l2 norm of gradients: 1.138673482392204\n",
      "l2 norm of weights: 6.492736094453455\n",
      "---------------------\n",
      "Iteration Number: 714\n",
      "Loss: 100.26335660580293\n",
      "l2 norm of gradients: 1.1380234167614989\n",
      "l2 norm of weights: 6.4924630409919875\n",
      "---------------------\n",
      "Iteration Number: 715\n",
      "Loss: 100.20019503970634\n",
      "l2 norm of gradients: 1.1373734784074883\n",
      "l2 norm of weights: 6.492190181899757\n",
      "---------------------\n",
      "Iteration Number: 716\n",
      "Loss: 100.13710537401144\n",
      "l2 norm of gradients: 1.136723670915594\n",
      "l2 norm of weights: 6.491917516948512\n",
      "---------------------\n",
      "Iteration Number: 717\n",
      "Loss: 100.07408755282357\n",
      "l2 norm of gradients: 1.1360739978616574\n",
      "l2 norm of weights: 6.491645045910123\n",
      "---------------------\n",
      "Iteration Number: 718\n",
      "Loss: 100.01114151987905\n",
      "l2 norm of gradients: 1.1354244628114718\n",
      "l2 norm of weights: 6.491372768556581\n",
      "---------------------\n",
      "Iteration Number: 719\n",
      "Loss: 99.94826721855709\n",
      "l2 norm of gradients: 1.1347750693203147\n",
      "l2 norm of weights: 6.491100684660003\n",
      "---------------------\n",
      "Iteration Number: 720\n",
      "Loss: 99.88546459177023\n",
      "l2 norm of gradients: 1.134125820932487\n",
      "l2 norm of weights: 6.490828793992638\n",
      "---------------------\n",
      "Iteration Number: 721\n",
      "Loss: 99.8227335822771\n",
      "l2 norm of gradients: 1.133476721180852\n",
      "l2 norm of weights: 6.4905570963268655\n",
      "---------------------\n",
      "Iteration Number: 722\n",
      "Loss: 99.76007413228464\n",
      "l2 norm of gradients: 1.1328277735863783\n",
      "l2 norm of weights: 6.4902855914352005\n",
      "---------------------\n",
      "Iteration Number: 723\n",
      "Loss: 99.69748618384617\n",
      "l2 norm of gradients: 1.132178981657686\n",
      "l2 norm of weights: 6.490014279090294\n",
      "---------------------\n",
      "Iteration Number: 724\n",
      "Loss: 99.63496967846685\n",
      "l2 norm of gradients: 1.131530348890595\n",
      "l2 norm of weights: 6.489743159064941\n",
      "---------------------\n",
      "Iteration Number: 725\n",
      "Loss: 99.57252455749284\n",
      "l2 norm of gradients: 1.1308818787676798\n",
      "l2 norm of weights: 6.48947223113208\n",
      "---------------------\n",
      "Iteration Number: 726\n",
      "Loss: 99.51015076172745\n",
      "l2 norm of gradients: 1.1302335747578227\n",
      "l2 norm of weights: 6.489201495064796\n",
      "---------------------\n",
      "Iteration Number: 727\n",
      "Loss: 99.44784823184797\n",
      "l2 norm of gradients: 1.1295854403157761\n",
      "l2 norm of weights: 6.488930950636323\n",
      "---------------------\n",
      "Iteration Number: 728\n",
      "Loss: 99.38561690786892\n",
      "l2 norm of gradients: 1.128937478881724\n",
      "l2 norm of weights: 6.4886605976200515\n",
      "---------------------\n",
      "Iteration Number: 729\n",
      "Loss: 99.32345672987323\n",
      "l2 norm of gradients: 1.1282896938808498\n",
      "l2 norm of weights: 6.488390435789524\n",
      "---------------------\n",
      "Iteration Number: 730\n",
      "Loss: 99.26136763730159\n",
      "l2 norm of gradients: 1.127642088722909\n",
      "l2 norm of weights: 6.488120464918444\n",
      "---------------------\n",
      "Iteration Number: 731\n",
      "Loss: 99.19934956936946\n",
      "l2 norm of gradients: 1.126994666801802\n",
      "l2 norm of weights: 6.487850684780674\n",
      "---------------------\n",
      "Iteration Number: 732\n",
      "Loss: 99.13740246486583\n",
      "l2 norm of gradients: 1.1263474314951565\n",
      "l2 norm of weights: 6.487581095150245\n",
      "---------------------\n",
      "Iteration Number: 733\n",
      "Loss: 99.07552626253516\n",
      "l2 norm of gradients: 1.1257003861639083\n",
      "l2 norm of weights: 6.487311695801352\n",
      "---------------------\n",
      "Iteration Number: 734\n",
      "Loss: 99.01372090051197\n",
      "l2 norm of gradients: 1.1250535341518928\n",
      "l2 norm of weights: 6.487042486508362\n",
      "---------------------\n",
      "Iteration Number: 735\n",
      "Loss: 98.9519863165951\n",
      "l2 norm of gradients: 1.1244068787854353\n",
      "l2 norm of weights: 6.486773467045814\n",
      "---------------------\n",
      "Iteration Number: 736\n",
      "Loss: 98.8903224484569\n",
      "l2 norm of gradients: 1.123760423372951\n",
      "l2 norm of weights: 6.486504637188422\n",
      "---------------------\n",
      "Iteration Number: 737\n",
      "Loss: 98.82872923327511\n",
      "l2 norm of gradients: 1.1231141712045443\n",
      "l2 norm of weights: 6.4862359967110805\n",
      "---------------------\n",
      "Iteration Number: 738\n",
      "Loss: 98.76720660819123\n",
      "l2 norm of gradients: 1.1224681255516193\n",
      "l2 norm of weights: 6.485967545388863\n",
      "---------------------\n",
      "Iteration Number: 739\n",
      "Loss: 98.70575450955215\n",
      "l2 norm of gradients: 1.1218222896664893\n",
      "l2 norm of weights: 6.48569928299703\n",
      "---------------------\n",
      "Iteration Number: 740\n",
      "Loss: 98.64437287396424\n",
      "l2 norm of gradients: 1.1211766667819958\n",
      "l2 norm of weights: 6.485431209311025\n",
      "---------------------\n",
      "Iteration Number: 741\n",
      "Loss: 98.58306163716098\n",
      "l2 norm of gradients: 1.1205312601111295\n",
      "l2 norm of weights: 6.485163324106482\n",
      "---------------------\n",
      "Iteration Number: 742\n",
      "Loss: 98.52182073506106\n",
      "l2 norm of gradients: 1.1198860728466593\n",
      "l2 norm of weights: 6.484895627159229\n",
      "---------------------\n",
      "Iteration Number: 743\n",
      "Loss: 98.46065010307849\n",
      "l2 norm of gradients: 1.1192411081607643\n",
      "l2 norm of weights: 6.484628118245286\n",
      "---------------------\n",
      "Iteration Number: 744\n",
      "Loss: 98.39954967612593\n",
      "l2 norm of gradients: 1.1185963692046725\n",
      "l2 norm of weights: 6.484360797140873\n",
      "---------------------\n",
      "Iteration Number: 745\n",
      "Loss: 98.3385193892296\n",
      "l2 norm of gradients: 1.1179518591083055\n",
      "l2 norm of weights: 6.4840936636224065\n",
      "---------------------\n",
      "Iteration Number: 746\n",
      "Loss: 98.27755917682136\n",
      "l2 norm of gradients: 1.1173075809799262\n",
      "l2 norm of weights: 6.483826717466506\n",
      "---------------------\n",
      "Iteration Number: 747\n",
      "Loss: 98.21666897313993\n",
      "l2 norm of gradients: 1.1166635379057965\n",
      "l2 norm of weights: 6.483559958449999\n",
      "---------------------\n",
      "Iteration Number: 748\n",
      "Loss: 98.15584871214662\n",
      "l2 norm of gradients: 1.1160197329498365\n",
      "l2 norm of weights: 6.483293386349917\n",
      "---------------------\n",
      "Iteration Number: 749\n",
      "Loss: 98.09509832751186\n",
      "l2 norm of gradients: 1.1153761691532922\n",
      "l2 norm of weights: 6.483027000943504\n",
      "---------------------\n",
      "Iteration Number: 750\n",
      "Loss: 98.03441775263825\n",
      "l2 norm of gradients: 1.1147328495344078\n",
      "l2 norm of weights: 6.4827608020082135\n",
      "---------------------\n",
      "Iteration Number: 751\n",
      "Loss: 97.97380692063147\n",
      "l2 norm of gradients: 1.1140897770881053\n",
      "l2 norm of weights: 6.482494789321715\n",
      "---------------------\n",
      "Iteration Number: 752\n",
      "Loss: 97.91326576424959\n",
      "l2 norm of gradients: 1.113446954785668\n",
      "l2 norm of weights: 6.482228962661898\n",
      "---------------------\n",
      "Iteration Number: 753\n",
      "Loss: 97.85279421626309\n",
      "l2 norm of gradients: 1.1128043855744332\n",
      "l2 norm of weights: 6.481963321806867\n",
      "---------------------\n",
      "Iteration Number: 754\n",
      "Loss: 97.79239220868867\n",
      "l2 norm of gradients: 1.1121620723774872\n",
      "l2 norm of weights: 6.481697866534952\n",
      "---------------------\n",
      "Iteration Number: 755\n",
      "Loss: 97.73205967386214\n",
      "l2 norm of gradients: 1.1115200180933709\n",
      "l2 norm of weights: 6.481432596624706\n",
      "---------------------\n",
      "Iteration Number: 756\n",
      "Loss: 97.67179654343063\n",
      "l2 norm of gradients: 1.1108782255957879\n",
      "l2 norm of weights: 6.481167511854907\n",
      "---------------------\n",
      "Iteration Number: 757\n",
      "Loss: 97.6116027489204\n",
      "l2 norm of gradients: 1.110236697733321\n",
      "l2 norm of weights: 6.4809026120045665\n",
      "---------------------\n",
      "Iteration Number: 758\n",
      "Loss: 97.55147822155897\n",
      "l2 norm of gradients: 1.1095954373291548\n",
      "l2 norm of weights: 6.480637896852922\n",
      "---------------------\n",
      "Iteration Number: 759\n",
      "Loss: 97.49142289247398\n",
      "l2 norm of gradients: 1.1089544471808046\n",
      "l2 norm of weights: 6.480373366179448\n",
      "---------------------\n",
      "Iteration Number: 760\n",
      "Loss: 97.43143669234631\n",
      "l2 norm of gradients: 1.1083137300598511\n",
      "l2 norm of weights: 6.480109019763852\n",
      "---------------------\n",
      "Iteration Number: 761\n",
      "Loss: 97.37151955171673\n",
      "l2 norm of gradients: 1.1076732887116834\n",
      "l2 norm of weights: 6.47984485738608\n",
      "---------------------\n",
      "Iteration Number: 762\n",
      "Loss: 97.31167140099231\n",
      "l2 norm of gradients: 1.1070331258552466\n",
      "l2 norm of weights: 6.479580878826321\n",
      "---------------------\n",
      "Iteration Number: 763\n",
      "Loss: 97.25189217011433\n",
      "l2 norm of gradients: 1.106393244182797\n",
      "l2 norm of weights: 6.479317083865001\n",
      "---------------------\n",
      "Iteration Number: 764\n",
      "Loss: 97.19218178893162\n",
      "l2 norm of gradients: 1.105753646359665\n",
      "l2 norm of weights: 6.479053472282794\n",
      "---------------------\n",
      "Iteration Number: 765\n",
      "Loss: 97.13254018704507\n",
      "l2 norm of gradients: 1.1051143350240213\n",
      "l2 norm of weights: 6.478790043860618\n",
      "---------------------\n",
      "Iteration Number: 766\n",
      "Loss: 97.07296729381844\n",
      "l2 norm of gradients: 1.1044753127866542\n",
      "l2 norm of weights: 6.47852679837964\n",
      "---------------------\n",
      "Iteration Number: 767\n",
      "Loss: 97.01346303841603\n",
      "l2 norm of gradients: 1.1038365822307505\n",
      "l2 norm of weights: 6.478263735621276\n",
      "---------------------\n",
      "Iteration Number: 768\n",
      "Loss: 96.9540273498708\n",
      "l2 norm of gradients: 1.1031981459116842\n",
      "l2 norm of weights: 6.4780008553671955\n",
      "---------------------\n",
      "Iteration Number: 769\n",
      "Loss: 96.894660156705\n",
      "l2 norm of gradients: 1.102560006356812\n",
      "l2 norm of weights: 6.4777381573993225\n",
      "---------------------\n",
      "Iteration Number: 770\n",
      "Loss: 96.83536138751629\n",
      "l2 norm of gradients: 1.1019221660652754\n",
      "l2 norm of weights: 6.477475641499835\n",
      "---------------------\n",
      "Iteration Number: 771\n",
      "Loss: 96.77613097051996\n",
      "l2 norm of gradients: 1.1012846275078099\n",
      "l2 norm of weights: 6.47721330745117\n",
      "---------------------\n",
      "Iteration Number: 772\n",
      "Loss: 96.71696883400672\n",
      "l2 norm of gradients: 1.100647393126561\n",
      "l2 norm of weights: 6.476951155036025\n",
      "---------------------\n",
      "Iteration Number: 773\n",
      "Loss: 96.65787490566501\n",
      "l2 norm of gradients: 1.1000104653349068\n",
      "l2 norm of weights: 6.476689184037356\n",
      "---------------------\n",
      "Iteration Number: 774\n",
      "Loss: 96.59884911330366\n",
      "l2 norm of gradients: 1.0993738465172866\n",
      "l2 norm of weights: 6.476427394238388\n",
      "---------------------\n",
      "Iteration Number: 775\n",
      "Loss: 96.53989138442708\n",
      "l2 norm of gradients: 1.0987375390290388\n",
      "l2 norm of weights: 6.476165785422603\n",
      "---------------------\n",
      "Iteration Number: 776\n",
      "Loss: 96.48100164625245\n",
      "l2 norm of gradients: 1.098101545196243\n",
      "l2 norm of weights: 6.475904357373757\n",
      "---------------------\n",
      "Iteration Number: 777\n",
      "Loss: 96.42217982600322\n",
      "l2 norm of gradients: 1.0974658673155706\n",
      "l2 norm of weights: 6.475643109875871\n",
      "---------------------\n",
      "Iteration Number: 778\n",
      "Loss: 96.36342585051734\n",
      "l2 norm of gradients: 1.096830507654142\n",
      "l2 norm of weights: 6.4753820427132345\n",
      "---------------------\n",
      "Iteration Number: 779\n",
      "Loss: 96.30473964658283\n",
      "l2 norm of gradients: 1.0961954684493884\n",
      "l2 norm of weights: 6.475121155670414\n",
      "---------------------\n",
      "Iteration Number: 780\n",
      "Loss: 96.24612114087554\n",
      "l2 norm of gradients: 1.0955607519089252\n",
      "l2 norm of weights: 6.474860448532243\n",
      "---------------------\n",
      "Iteration Number: 781\n",
      "Loss: 96.18757025981856\n",
      "l2 norm of gradients: 1.0949263602104278\n",
      "l2 norm of weights: 6.474599921083834\n",
      "---------------------\n",
      "Iteration Number: 782\n",
      "Loss: 96.12908692946989\n",
      "l2 norm of gradients: 1.094292295501515\n",
      "l2 norm of weights: 6.474339573110575\n",
      "---------------------\n",
      "Iteration Number: 783\n",
      "Loss: 96.07067107611338\n",
      "l2 norm of gradients: 1.093658559899642\n",
      "l2 norm of weights: 6.47407940439813\n",
      "---------------------\n",
      "Iteration Number: 784\n",
      "Loss: 96.01232262556303\n",
      "l2 norm of gradients: 1.0930251554919967\n",
      "l2 norm of weights: 6.4738194147324455\n",
      "---------------------\n",
      "Iteration Number: 785\n",
      "Loss: 95.95404150361094\n",
      "l2 norm of gradients: 1.0923920843354047\n",
      "l2 norm of weights: 6.473559603899747\n",
      "---------------------\n",
      "Iteration Number: 786\n",
      "Loss: 95.89582763595828\n",
      "l2 norm of gradients: 1.0917593484562393\n",
      "l2 norm of weights: 6.473299971686541\n",
      "---------------------\n",
      "Iteration Number: 787\n",
      "Loss: 95.83768094787204\n",
      "l2 norm of gradients: 1.0911269498503418\n",
      "l2 norm of weights: 6.473040517879619\n",
      "---------------------\n",
      "Iteration Number: 788\n",
      "Loss: 95.77960136477819\n",
      "l2 norm of gradients: 1.090494890482944\n",
      "l2 norm of weights: 6.472781242266058\n",
      "---------------------\n",
      "Iteration Number: 789\n",
      "Loss: 95.72158881194206\n",
      "l2 norm of gradients: 1.0898631722885994\n",
      "l2 norm of weights: 6.472522144633219\n",
      "---------------------\n",
      "Iteration Number: 790\n",
      "Loss: 95.66364321421074\n",
      "l2 norm of gradients: 1.0892317971711234\n",
      "l2 norm of weights: 6.472263224768753\n",
      "---------------------\n",
      "Iteration Number: 791\n",
      "Loss: 95.60576449658045\n",
      "l2 norm of gradients: 1.0886007670035345\n",
      "l2 norm of weights: 6.472004482460597\n",
      "---------------------\n",
      "Iteration Number: 792\n",
      "Loss: 95.54795258392573\n",
      "l2 norm of gradients: 1.0879700836280073\n",
      "l2 norm of weights: 6.471745917496979\n",
      "---------------------\n",
      "Iteration Number: 793\n",
      "Loss: 95.49020740058691\n",
      "l2 norm of gradients: 1.08733974885583\n",
      "l2 norm of weights: 6.471487529666419\n",
      "---------------------\n",
      "Iteration Number: 794\n",
      "Loss: 95.43252887138155\n",
      "l2 norm of gradients: 1.0867097644673664\n",
      "l2 norm of weights: 6.471229318757727\n",
      "---------------------\n",
      "Iteration Number: 795\n",
      "Loss: 95.37491692051809\n",
      "l2 norm of gradients: 1.0860801322120284\n",
      "l2 norm of weights: 6.47097128456001\n",
      "---------------------\n",
      "Iteration Number: 796\n",
      "Loss: 95.31737147244951\n",
      "l2 norm of gradients: 1.085450853808252\n",
      "l2 norm of weights: 6.470713426862664\n",
      "---------------------\n",
      "Iteration Number: 797\n",
      "Loss: 95.25989245106564\n",
      "l2 norm of gradients: 1.0848219309434792\n",
      "l2 norm of weights: 6.470455745455385\n",
      "---------------------\n",
      "Iteration Number: 798\n",
      "Loss: 95.20247978061005\n",
      "l2 norm of gradients: 1.0841933652741496\n",
      "l2 norm of weights: 6.470198240128163\n",
      "---------------------\n",
      "Iteration Number: 799\n",
      "Loss: 95.1451333850382\n",
      "l2 norm of gradients: 1.0835651584256936\n",
      "l2 norm of weights: 6.469940910671287\n",
      "---------------------\n",
      "Iteration Number: 800\n",
      "Loss: 95.08785318818839\n",
      "l2 norm of gradients: 1.082937311992536\n",
      "l2 norm of weights: 6.469683756875342\n",
      "---------------------\n",
      "Iteration Number: 801\n",
      "Loss: 95.0306391136206\n",
      "l2 norm of gradients: 1.0823098275381027\n",
      "l2 norm of weights: 6.469426778531215\n",
      "---------------------\n",
      "Iteration Number: 802\n",
      "Loss: 94.97349108525862\n",
      "l2 norm of gradients: 1.0816827065948353\n",
      "l2 norm of weights: 6.469169975430091\n",
      "---------------------\n",
      "Iteration Number: 803\n",
      "Loss: 94.91640902639394\n",
      "l2 norm of gradients: 1.0810559506642108\n",
      "l2 norm of weights: 6.468913347363456\n",
      "---------------------\n",
      "Iteration Number: 804\n",
      "Loss: 94.85939286067449\n",
      "l2 norm of gradients: 1.0804295612167678\n",
      "l2 norm of weights: 6.468656894123101\n",
      "---------------------\n",
      "Iteration Number: 805\n",
      "Loss: 94.80244251139374\n",
      "l2 norm of gradients: 1.0798035396921384\n",
      "l2 norm of weights: 6.468400615501115\n",
      "---------------------\n",
      "Iteration Number: 806\n",
      "Loss: 94.74555790194621\n",
      "l2 norm of gradients: 1.0791778874990852\n",
      "l2 norm of weights: 6.468144511289893\n",
      "---------------------\n",
      "Iteration Number: 807\n",
      "Loss: 94.68873895550766\n",
      "l2 norm of gradients: 1.0785526060155461\n",
      "l2 norm of weights: 6.467888581282134\n",
      "---------------------\n",
      "Iteration Number: 808\n",
      "Loss: 94.63198559512946\n",
      "l2 norm of gradients: 1.0779276965886833\n",
      "l2 norm of weights: 6.46763282527084\n",
      "---------------------\n",
      "Iteration Number: 809\n",
      "Loss: 94.57529774394287\n",
      "l2 norm of gradients: 1.0773031605349366\n",
      "l2 norm of weights: 6.467377243049322\n",
      "---------------------\n",
      "Iteration Number: 810\n",
      "Loss: 94.51867532495211\n",
      "l2 norm of gradients: 1.0766789991400862\n",
      "l2 norm of weights: 6.467121834411193\n",
      "---------------------\n",
      "Iteration Number: 811\n",
      "Loss: 94.46211826110475\n",
      "l2 norm of gradients: 1.0760552136593173\n",
      "l2 norm of weights: 6.466866599150376\n",
      "---------------------\n",
      "Iteration Number: 812\n",
      "Loss: 94.40562647538329\n",
      "l2 norm of gradients: 1.0754318053172924\n",
      "l2 norm of weights: 6.466611537061098\n",
      "---------------------\n",
      "Iteration Number: 813\n",
      "Loss: 94.34919989041383\n",
      "l2 norm of gradients: 1.0748087753082278\n",
      "l2 norm of weights: 6.466356647937897\n",
      "---------------------\n",
      "Iteration Number: 814\n",
      "Loss: 94.29283842906545\n",
      "l2 norm of gradients: 1.074186124795976\n",
      "l2 norm of weights: 6.466101931575617\n",
      "---------------------\n",
      "Iteration Number: 815\n",
      "Loss: 94.23654201405978\n",
      "l2 norm of gradients: 1.0735638549141133\n",
      "l2 norm of weights: 6.465847387769411\n",
      "---------------------\n",
      "Iteration Number: 816\n",
      "Loss: 94.18031056806765\n",
      "l2 norm of gradients: 1.0729419667660323\n",
      "l2 norm of weights: 6.465593016314743\n",
      "---------------------\n",
      "Iteration Number: 817\n",
      "Loss: 94.12414401373275\n",
      "l2 norm of gradients: 1.0723204614250406\n",
      "l2 norm of weights: 6.465338817007385\n",
      "---------------------\n",
      "Iteration Number: 818\n",
      "Loss: 94.068042273619\n",
      "l2 norm of gradients: 1.0716993399344625\n",
      "l2 norm of weights: 6.465084789643419\n",
      "---------------------\n",
      "Iteration Number: 819\n",
      "Loss: 94.01200527019465\n",
      "l2 norm of gradients: 1.0710786033077473\n",
      "l2 norm of weights: 6.464830934019238\n",
      "---------------------\n",
      "Iteration Number: 820\n",
      "Loss: 93.95603292616197\n",
      "l2 norm of gradients: 1.0704582525285835\n",
      "l2 norm of weights: 6.464577249931547\n",
      "---------------------\n",
      "Iteration Number: 821\n",
      "Loss: 93.9001251637137\n",
      "l2 norm of gradients: 1.0698382885510134\n",
      "l2 norm of weights: 6.464323737177358\n",
      "---------------------\n",
      "Iteration Number: 822\n",
      "Loss: 93.84428190559773\n",
      "l2 norm of gradients: 1.0692187122995582\n",
      "l2 norm of weights: 6.464070395553999\n",
      "---------------------\n",
      "Iteration Number: 823\n",
      "Loss: 93.78850307405314\n",
      "l2 norm of gradients: 1.0685995246693447\n",
      "l2 norm of weights: 6.463817224859106\n",
      "---------------------\n",
      "Iteration Number: 824\n",
      "Loss: 93.7327885915764\n",
      "l2 norm of gradients: 1.0679807265262347\n",
      "l2 norm of weights: 6.46356422489063\n",
      "---------------------\n",
      "Iteration Number: 825\n",
      "Loss: 93.67713838047548\n",
      "l2 norm of gradients: 1.0673623187069645\n",
      "l2 norm of weights: 6.463311395446831\n",
      "---------------------\n",
      "Iteration Number: 826\n",
      "Loss: 93.62155236321043\n",
      "l2 norm of gradients: 1.0667443020192826\n",
      "l2 norm of weights: 6.463058736326282\n",
      "---------------------\n",
      "Iteration Number: 827\n",
      "Loss: 93.56603046204646\n",
      "l2 norm of gradients: 1.0661266772420968\n",
      "l2 norm of weights: 6.46280624732787\n",
      "---------------------\n",
      "Iteration Number: 828\n",
      "Loss: 93.51057259944601\n",
      "l2 norm of gradients: 1.0655094451256222\n",
      "l2 norm of weights: 6.46255392825079\n",
      "---------------------\n",
      "Iteration Number: 829\n",
      "Loss: 93.45517869763972\n",
      "l2 norm of gradients: 1.064892606391535\n",
      "l2 norm of weights: 6.462301778894556\n",
      "---------------------\n",
      "Iteration Number: 830\n",
      "Loss: 93.39984867908805\n",
      "l2 norm of gradients: 1.06427616173313\n",
      "l2 norm of weights: 6.462049799058988\n",
      "---------------------\n",
      "Iteration Number: 831\n",
      "Loss: 93.34458246602247\n",
      "l2 norm of gradients: 1.0636601118154834\n",
      "l2 norm of weights: 6.461797988544223\n",
      "---------------------\n",
      "Iteration Number: 832\n",
      "Loss: 93.2893799807899\n",
      "l2 norm of gradients: 1.0630444572756168\n",
      "l2 norm of weights: 6.4615463471507075\n",
      "---------------------\n",
      "Iteration Number: 833\n",
      "Loss: 93.23424114575047\n",
      "l2 norm of gradients: 1.0624291987226673\n",
      "l2 norm of weights: 6.4612948746792025\n",
      "---------------------\n",
      "Iteration Number: 834\n",
      "Loss: 93.17916588336364\n",
      "l2 norm of gradients: 1.061814336738062\n",
      "l2 norm of weights: 6.461043570930782\n",
      "---------------------\n",
      "Iteration Number: 835\n",
      "Loss: 93.1241541158888\n",
      "l2 norm of gradients: 1.0611998718756932\n",
      "l2 norm of weights: 6.4607924357068285\n",
      "---------------------\n",
      "Iteration Number: 836\n",
      "Loss: 93.06920576573027\n",
      "l2 norm of gradients: 1.0605858046621008\n",
      "l2 norm of weights: 6.46054146880904\n",
      "---------------------\n",
      "Iteration Number: 837\n",
      "Loss: 93.01432075528173\n",
      "l2 norm of gradients: 1.0599721355966558\n",
      "l2 norm of weights: 6.460290670039426\n",
      "---------------------\n",
      "Iteration Number: 838\n",
      "Loss: 92.95949900706216\n",
      "l2 norm of gradients: 1.059358865151748\n",
      "l2 norm of weights: 6.46004003920031\n",
      "---------------------\n",
      "Iteration Number: 839\n",
      "Loss: 92.90474044333696\n",
      "l2 norm of gradients: 1.0587459937729777\n",
      "l2 norm of weights: 6.459789576094322\n",
      "---------------------\n",
      "Iteration Number: 840\n",
      "Loss: 92.85004498666413\n",
      "l2 norm of gradients: 1.0581335218793506\n",
      "l2 norm of weights: 6.4595392805244085\n",
      "---------------------\n",
      "Iteration Number: 841\n",
      "Loss: 92.79541255964287\n",
      "l2 norm of gradients: 1.0575214498634746\n",
      "l2 norm of weights: 6.459289152293824\n",
      "---------------------\n",
      "Iteration Number: 842\n",
      "Loss: 92.74084308461462\n",
      "l2 norm of gradients: 1.0569097780917618\n",
      "l2 norm of weights: 6.459039191206136\n",
      "---------------------\n",
      "Iteration Number: 843\n",
      "Loss: 92.68633648433386\n",
      "l2 norm of gradients: 1.0562985069046322\n",
      "l2 norm of weights: 6.458789397065223\n",
      "---------------------\n",
      "Iteration Number: 844\n",
      "Loss: 92.63189268121795\n",
      "l2 norm of gradients: 1.05568763661672\n",
      "l2 norm of weights: 6.458539769675273\n",
      "---------------------\n",
      "Iteration Number: 845\n",
      "Loss: 92.57751159800041\n",
      "l2 norm of gradients: 1.0550771675170858\n",
      "l2 norm of weights: 6.458290308840784\n",
      "---------------------\n",
      "Iteration Number: 846\n",
      "Loss: 92.52319315736395\n",
      "l2 norm of gradients: 1.0544670998694279\n",
      "l2 norm of weights: 6.4580410143665645\n",
      "---------------------\n",
      "Iteration Number: 847\n",
      "Loss: 92.46893728202765\n",
      "l2 norm of gradients: 1.0538574339122986\n",
      "l2 norm of weights: 6.457791886057731\n",
      "---------------------\n",
      "Iteration Number: 848\n",
      "Loss: 92.41474389478009\n",
      "l2 norm of gradients: 1.0532481698593212\n",
      "l2 norm of weights: 6.4575429237197115\n",
      "---------------------\n",
      "Iteration Number: 849\n",
      "Loss: 92.36061291834592\n",
      "l2 norm of gradients: 1.0526393078994138\n",
      "l2 norm of weights: 6.4572941271582405\n",
      "---------------------\n",
      "Iteration Number: 850\n",
      "Loss: 92.3065442756852\n",
      "l2 norm of gradients: 1.052030848197011\n",
      "l2 norm of weights: 6.457045496179362\n",
      "---------------------\n",
      "Iteration Number: 851\n",
      "Loss: 92.25253788974294\n",
      "l2 norm of gradients: 1.0514227908922904\n",
      "l2 norm of weights: 6.456797030589428\n",
      "---------------------\n",
      "Iteration Number: 852\n",
      "Loss: 92.19859368348857\n",
      "l2 norm of gradients: 1.0508151361014002\n",
      "l2 norm of weights: 6.456548730195094\n",
      "---------------------\n",
      "Iteration Number: 853\n",
      "Loss: 92.14471157989104\n",
      "l2 norm of gradients: 1.0502078839166917\n",
      "l2 norm of weights: 6.456300594803329\n",
      "---------------------\n",
      "Iteration Number: 854\n",
      "Loss: 92.09089150218117\n",
      "l2 norm of gradients: 1.0496010344069515\n",
      "l2 norm of weights: 6.456052624221401\n",
      "---------------------\n",
      "Iteration Number: 855\n",
      "Loss: 92.0371333733528\n",
      "l2 norm of gradients: 1.0489945876176363\n",
      "l2 norm of weights: 6.4558048182568895\n",
      "---------------------\n",
      "Iteration Number: 856\n",
      "Loss: 91.98343711677823\n",
      "l2 norm of gradients: 1.0483885435711107\n",
      "l2 norm of weights: 6.455557176717675\n",
      "---------------------\n",
      "Iteration Number: 857\n",
      "Loss: 91.92980265566622\n",
      "l2 norm of gradients: 1.047782902266886\n",
      "l2 norm of weights: 6.455309699411945\n",
      "---------------------\n",
      "Iteration Number: 858\n",
      "Loss: 91.87622991349285\n",
      "l2 norm of gradients: 1.0471776636818622\n",
      "l2 norm of weights: 6.455062386148189\n",
      "---------------------\n",
      "Iteration Number: 859\n",
      "Loss: 91.82271881370147\n",
      "l2 norm of gradients: 1.0465728277705704\n",
      "l2 norm of weights: 6.454815236735202\n",
      "---------------------\n",
      "Iteration Number: 860\n",
      "Loss: 91.76926927972077\n",
      "l2 norm of gradients: 1.0459683944654181\n",
      "l2 norm of weights: 6.4545682509820805\n",
      "---------------------\n",
      "Iteration Number: 861\n",
      "Loss: 91.71588123514854\n",
      "l2 norm of gradients: 1.0453643636769354\n",
      "l2 norm of weights: 6.454321428698223\n",
      "---------------------\n",
      "Iteration Number: 862\n",
      "Loss: 91.66255460367576\n",
      "l2 norm of gradients: 1.0447607352940242\n",
      "l2 norm of weights: 6.454074769693327\n",
      "---------------------\n",
      "Iteration Number: 863\n",
      "Loss: 91.60928930914585\n",
      "l2 norm of gradients: 1.0441575091842068\n",
      "l2 norm of weights: 6.453828273777398\n",
      "---------------------\n",
      "Iteration Number: 864\n",
      "Loss: 91.5560852754092\n",
      "l2 norm of gradients: 1.0435546851938786\n",
      "l2 norm of weights: 6.45358194076073\n",
      "---------------------\n",
      "Iteration Number: 865\n",
      "Loss: 91.50294242619584\n",
      "l2 norm of gradients: 1.0429522631485586\n",
      "l2 norm of weights: 6.453335770453927\n",
      "---------------------\n",
      "Iteration Number: 866\n",
      "Loss: 91.44986068572783\n",
      "l2 norm of gradients: 1.042350242853147\n",
      "l2 norm of weights: 6.4530897626678865\n",
      "---------------------\n",
      "Iteration Number: 867\n",
      "Loss: 91.39683997800881\n",
      "l2 norm of gradients: 1.0417486240921783\n",
      "l2 norm of weights: 6.452843917213802\n",
      "---------------------\n",
      "Iteration Number: 868\n",
      "Loss: 91.34388022722706\n",
      "l2 norm of gradients: 1.0411474066300779\n",
      "l2 norm of weights: 6.452598233903168\n",
      "---------------------\n",
      "Iteration Number: 869\n",
      "Loss: 91.29098135778193\n",
      "l2 norm of gradients: 1.0405465902114208\n",
      "l2 norm of weights: 6.452352712547771\n",
      "---------------------\n",
      "Iteration Number: 870\n",
      "Loss: 91.23814329395736\n",
      "l2 norm of gradients: 1.039946174561191\n",
      "l2 norm of weights: 6.452107352959697\n",
      "---------------------\n",
      "Iteration Number: 871\n",
      "Loss: 91.18536596014009\n",
      "l2 norm of gradients: 1.0393461593850402\n",
      "l2 norm of weights: 6.451862154951323\n",
      "---------------------\n",
      "Iteration Number: 872\n",
      "Loss: 91.1326492811094\n",
      "l2 norm of gradients: 1.0387465443695492\n",
      "l2 norm of weights: 6.451617118335322\n",
      "---------------------\n",
      "Iteration Number: 873\n",
      "Loss: 91.07999318144213\n",
      "l2 norm of gradients: 1.0381473291824903\n",
      "l2 norm of weights: 6.451372242924656\n",
      "---------------------\n",
      "Iteration Number: 874\n",
      "Loss: 91.02739758593242\n",
      "l2 norm of gradients: 1.0375485134730897\n",
      "l2 norm of weights: 6.451127528532583\n",
      "---------------------\n",
      "Iteration Number: 875\n",
      "Loss: 90.97486241944648\n",
      "l2 norm of gradients: 1.0369500968722907\n",
      "l2 norm of weights: 6.45088297497265\n",
      "---------------------\n",
      "Iteration Number: 876\n",
      "Loss: 90.92238760700046\n",
      "l2 norm of gradients: 1.0363520789930176\n",
      "l2 norm of weights: 6.450638582058695\n",
      "---------------------\n",
      "Iteration Number: 877\n",
      "Loss: 90.86997307369153\n",
      "l2 norm of gradients: 1.0357544594304422\n",
      "l2 norm of weights: 6.450394349604845\n",
      "---------------------\n",
      "Iteration Number: 878\n",
      "Loss: 90.8176187447168\n",
      "l2 norm of gradients: 1.0351572377622462\n",
      "l2 norm of weights: 6.450150277425512\n",
      "---------------------\n",
      "Iteration Number: 879\n",
      "Loss: 90.7653245454045\n",
      "l2 norm of gradients: 1.0345604135488906\n",
      "l2 norm of weights: 6.449906365335401\n",
      "---------------------\n",
      "Iteration Number: 880\n",
      "Loss: 90.71309040118801\n",
      "l2 norm of gradients: 1.0339639863338796\n",
      "l2 norm of weights: 6.4496626131495\n",
      "---------------------\n",
      "Iteration Number: 881\n",
      "Loss: 90.6609162376074\n",
      "l2 norm of gradients: 1.0333679556440278\n",
      "l2 norm of weights: 6.449419020683081\n",
      "---------------------\n",
      "Iteration Number: 882\n",
      "Loss: 90.60880198023595\n",
      "l2 norm of gradients: 1.0327723209897282\n",
      "l2 norm of weights: 6.449175587751702\n",
      "---------------------\n",
      "Iteration Number: 883\n",
      "Loss: 90.55674755502244\n",
      "l2 norm of gradients: 1.0321770818652196\n",
      "l2 norm of weights: 6.4489323141712065\n",
      "---------------------\n",
      "Iteration Number: 884\n",
      "Loss: 90.5047528876963\n",
      "l2 norm of gradients: 1.0315822377488528\n",
      "l2 norm of weights: 6.448689199757717\n",
      "---------------------\n",
      "Iteration Number: 885\n",
      "Loss: 90.45281790434606\n",
      "l2 norm of gradients: 1.0309877881033602\n",
      "l2 norm of weights: 6.448446244327638\n",
      "---------------------\n",
      "Iteration Number: 886\n",
      "Loss: 90.40094253102414\n",
      "l2 norm of gradients: 1.0303937323761239\n",
      "l2 norm of weights: 6.448203447697655\n",
      "---------------------\n",
      "Iteration Number: 887\n",
      "Loss: 90.34912669416735\n",
      "l2 norm of gradients: 1.0298000699994407\n",
      "l2 norm of weights: 6.447960809684734\n",
      "---------------------\n",
      "Iteration Number: 888\n",
      "Loss: 90.29737031997291\n",
      "l2 norm of gradients: 1.0292068003907953\n",
      "l2 norm of weights: 6.447718330106116\n",
      "---------------------\n",
      "Iteration Number: 889\n",
      "Loss: 90.245673335035\n",
      "l2 norm of gradients: 1.0286139229531226\n",
      "l2 norm of weights: 6.447476008779323\n",
      "---------------------\n",
      "Iteration Number: 890\n",
      "Loss: 90.19403566589843\n",
      "l2 norm of gradients: 1.0280214370750806\n",
      "l2 norm of weights: 6.447233845522148\n",
      "---------------------\n",
      "Iteration Number: 891\n",
      "Loss: 90.14245723940502\n",
      "l2 norm of gradients: 1.0274293421313148\n",
      "l2 norm of weights: 6.446991840152665\n",
      "---------------------\n",
      "Iteration Number: 892\n",
      "Loss: 90.09093798244493\n",
      "l2 norm of gradients: 1.0268376374827282\n",
      "l2 norm of weights: 6.446749992489218\n",
      "---------------------\n",
      "Iteration Number: 893\n",
      "Loss: 90.0394778219386\n",
      "l2 norm of gradients: 1.0262463224767464\n",
      "l2 norm of weights: 6.446508302350425\n",
      "---------------------\n",
      "Iteration Number: 894\n",
      "Loss: 89.98807668512052\n",
      "l2 norm of gradients: 1.025655396447587\n",
      "l2 norm of weights: 6.446266769555176\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 895\n",
      "Loss: 89.93673449918235\n",
      "l2 norm of gradients: 1.025064858716524\n",
      "l2 norm of weights: 6.44602539392263\n",
      "---------------------\n",
      "Iteration Number: 896\n",
      "Loss: 89.88545119162143\n",
      "l2 norm of gradients: 1.0244747085921566\n",
      "l2 norm of weights: 6.445784175272216\n",
      "---------------------\n",
      "Iteration Number: 897\n",
      "Loss: 89.83422668998291\n",
      "l2 norm of gradients: 1.0238849453706727\n",
      "l2 norm of weights: 6.445543113423634\n",
      "---------------------\n",
      "Iteration Number: 898\n",
      "Loss: 89.78306092181194\n",
      "l2 norm of gradients: 1.0232955683361167\n",
      "l2 norm of weights: 6.445302208196849\n",
      "---------------------\n",
      "Iteration Number: 899\n",
      "Loss: 89.73195381510197\n",
      "l2 norm of gradients: 1.0227065767606516\n",
      "l2 norm of weights: 6.44506145941209\n",
      "---------------------\n",
      "Iteration Number: 900\n",
      "Loss: 89.68090529756849\n",
      "l2 norm of gradients: 1.0221179699048268\n",
      "l2 norm of weights: 6.444820866889855\n",
      "---------------------\n",
      "Iteration Number: 901\n",
      "Loss: 89.62991529757848\n",
      "l2 norm of gradients: 1.021529747017839\n",
      "l2 norm of weights: 6.444580430450903\n",
      "---------------------\n",
      "Iteration Number: 902\n",
      "Loss: 89.57898374310159\n",
      "l2 norm of gradients: 1.0209419073377968\n",
      "l2 norm of weights: 6.444340149916257\n",
      "---------------------\n",
      "Iteration Number: 903\n",
      "Loss: 89.52811056265764\n",
      "l2 norm of gradients: 1.0203544500919821\n",
      "l2 norm of weights: 6.4441000251072\n",
      "---------------------\n",
      "Iteration Number: 904\n",
      "Loss: 89.47729568468544\n",
      "l2 norm of gradients: 1.0197673744971127\n",
      "l2 norm of weights: 6.443860055845276\n",
      "---------------------\n",
      "Iteration Number: 905\n",
      "Loss: 89.42653903791027\n",
      "l2 norm of gradients: 1.019180679759603\n",
      "l2 norm of weights: 6.443620241952288\n",
      "---------------------\n",
      "Iteration Number: 906\n",
      "Loss: 89.3758405509401\n",
      "l2 norm of gradients: 1.0185943650758231\n",
      "l2 norm of weights: 6.443380583250297\n",
      "---------------------\n",
      "Iteration Number: 907\n",
      "Loss: 89.32520015290235\n",
      "l2 norm of gradients: 1.01800842963236\n",
      "l2 norm of weights: 6.443141079561621\n",
      "---------------------\n",
      "Iteration Number: 908\n",
      "Loss: 89.2746177728592\n",
      "l2 norm of gradients: 1.0174228726062728\n",
      "l2 norm of weights: 6.442901730708833\n",
      "---------------------\n",
      "Iteration Number: 909\n",
      "Loss: 89.22409333984753\n",
      "l2 norm of gradients: 1.016837693165354\n",
      "l2 norm of weights: 6.442662536514757\n",
      "---------------------\n",
      "Iteration Number: 910\n",
      "Loss: 89.17362678341745\n",
      "l2 norm of gradients: 1.0162528904683825\n",
      "l2 norm of weights: 6.442423496802477\n",
      "---------------------\n",
      "Iteration Number: 911\n",
      "Loss: 89.1232180329952\n",
      "l2 norm of gradients: 1.0156684636653808\n",
      "l2 norm of weights: 6.442184611395324\n",
      "---------------------\n",
      "Iteration Number: 912\n",
      "Loss: 89.07286701821592\n",
      "l2 norm of gradients: 1.015084411897869\n",
      "l2 norm of weights: 6.441945880116878\n",
      "---------------------\n",
      "Iteration Number: 913\n",
      "Loss: 89.02257366890368\n",
      "l2 norm of gradients: 1.0145007342991177\n",
      "l2 norm of weights: 6.441707302790973\n",
      "---------------------\n",
      "Iteration Number: 914\n",
      "Loss: 88.97233791508502\n",
      "l2 norm of gradients: 1.0139174299944012\n",
      "l2 norm of weights: 6.441468879241688\n",
      "---------------------\n",
      "Iteration Number: 915\n",
      "Loss: 88.9221596866778\n",
      "l2 norm of gradients: 1.0133344981012467\n",
      "l2 norm of weights: 6.441230609293349\n",
      "---------------------\n",
      "Iteration Number: 916\n",
      "Loss: 88.87203891410748\n",
      "l2 norm of gradients: 1.0127519377296863\n",
      "l2 norm of weights: 6.44099249277053\n",
      "---------------------\n",
      "Iteration Number: 917\n",
      "Loss: 88.82197552758328\n",
      "l2 norm of gradients: 1.0121697479825045\n",
      "l2 norm of weights: 6.440754529498048\n",
      "---------------------\n",
      "Iteration Number: 918\n",
      "Loss: 88.7719694578372\n",
      "l2 norm of gradients: 1.011587927955485\n",
      "l2 norm of weights: 6.44051671930096\n",
      "---------------------\n",
      "Iteration Number: 919\n",
      "Loss: 88.72202063546419\n",
      "l2 norm of gradients: 1.011006476737658\n",
      "l2 norm of weights: 6.440279062004571\n",
      "---------------------\n",
      "Iteration Number: 920\n",
      "Loss: 88.67212899121546\n",
      "l2 norm of gradients: 1.010425393411545\n",
      "l2 norm of weights: 6.440041557434421\n",
      "---------------------\n",
      "Iteration Number: 921\n",
      "Loss: 88.62229445626848\n",
      "l2 norm of gradients: 1.0098446770534015\n",
      "l2 norm of weights: 6.439804205416294\n",
      "---------------------\n",
      "Iteration Number: 922\n",
      "Loss: 88.57251696167317\n",
      "l2 norm of gradients: 1.0092643267334591\n",
      "l2 norm of weights: 6.43956700577621\n",
      "---------------------\n",
      "Iteration Number: 923\n",
      "Loss: 88.52279643862906\n",
      "l2 norm of gradients: 1.008684341516168\n",
      "l2 norm of weights: 6.439329958340424\n",
      "---------------------\n",
      "Iteration Number: 924\n",
      "Loss: 88.47313281876546\n",
      "l2 norm of gradients: 1.0081047204604332\n",
      "l2 norm of weights: 6.439093062935432\n",
      "---------------------\n",
      "Iteration Number: 925\n",
      "Loss: 88.42352603349742\n",
      "l2 norm of gradients: 1.007525462619856\n",
      "l2 norm of weights: 6.438856319387957\n",
      "---------------------\n",
      "Iteration Number: 926\n",
      "Loss: 88.37397601467116\n",
      "l2 norm of gradients: 1.0069465670429678\n",
      "l2 norm of weights: 6.438619727524963\n",
      "---------------------\n",
      "Iteration Number: 927\n",
      "Loss: 88.324482694181\n",
      "l2 norm of gradients: 1.0063680327734663\n",
      "l2 norm of weights: 6.43838328717364\n",
      "---------------------\n",
      "Iteration Number: 928\n",
      "Loss: 88.27504600407781\n",
      "l2 norm of gradients: 1.0057898588504477\n",
      "l2 norm of weights: 6.438146998161413\n",
      "---------------------\n",
      "Iteration Number: 929\n",
      "Loss: 88.2256658765721\n",
      "l2 norm of gradients: 1.00521204430864\n",
      "l2 norm of weights: 6.437910860315932\n",
      "---------------------\n",
      "Iteration Number: 930\n",
      "Loss: 88.17634224402197\n",
      "l2 norm of gradients: 1.0046345881786336\n",
      "l2 norm of weights: 6.437674873465078\n",
      "---------------------\n",
      "Iteration Number: 931\n",
      "Loss: 88.12707503884604\n",
      "l2 norm of gradients: 1.0040574894871075\n",
      "l2 norm of weights: 6.43743903743696\n",
      "---------------------\n",
      "Iteration Number: 932\n",
      "Loss: 88.0778641938028\n",
      "l2 norm of gradients: 1.0034807472570588\n",
      "l2 norm of weights: 6.437203352059909\n",
      "---------------------\n",
      "Iteration Number: 933\n",
      "Loss: 88.02870964170017\n",
      "l2 norm of gradients: 1.0029043605080274\n",
      "l2 norm of weights: 6.436967817162484\n",
      "---------------------\n",
      "Iteration Number: 934\n",
      "Loss: 87.97961131552022\n",
      "l2 norm of gradients: 1.0023283282563205\n",
      "l2 norm of weights: 6.436732432573463\n",
      "---------------------\n",
      "Iteration Number: 935\n",
      "Loss: 87.93056914829505\n",
      "l2 norm of gradients: 1.0017526495152327\n",
      "l2 norm of weights: 6.436497198121852\n",
      "---------------------\n",
      "Iteration Number: 936\n",
      "Loss: 87.88158307338233\n",
      "l2 norm of gradients: 1.001177323295269\n",
      "l2 norm of weights: 6.436262113636872\n",
      "---------------------\n",
      "Iteration Number: 937\n",
      "Loss: 87.83265302422292\n",
      "l2 norm of gradients: 1.0006023486043618\n",
      "l2 norm of weights: 6.436027178947966\n",
      "---------------------\n",
      "Iteration Number: 938\n",
      "Loss: 87.78377893429976\n",
      "l2 norm of gradients: 1.0000277244480886\n",
      "l2 norm of weights: 6.435792393884793\n",
      "---------------------\n",
      "Iteration Number: 939\n",
      "Loss: 87.73496073749402\n",
      "l2 norm of gradients: 0.9994534498298876\n",
      "l2 norm of weights: 6.435557758277233\n",
      "---------------------\n",
      "Iteration Number: 940\n",
      "Loss: 87.68619836760394\n",
      "l2 norm of gradients: 0.9988795237512696\n",
      "l2 norm of weights: 6.435323271955378\n",
      "---------------------\n",
      "Iteration Number: 941\n",
      "Loss: 87.63749175862927\n",
      "l2 norm of gradients: 0.9983059452120318\n",
      "l2 norm of weights: 6.435088934749535\n",
      "---------------------\n",
      "Iteration Number: 942\n",
      "Loss: 87.58884084484588\n",
      "l2 norm of gradients: 0.9977327132104655\n",
      "l2 norm of weights: 6.434854746490226\n",
      "---------------------\n",
      "Iteration Number: 943\n",
      "Loss: 87.54024556052002\n",
      "l2 norm of gradients: 0.9971598267435668\n",
      "l2 norm of weights: 6.434620707008183\n",
      "---------------------\n",
      "Iteration Number: 944\n",
      "Loss: 87.49170584024782\n",
      "l2 norm of gradients: 0.9965872848072398\n",
      "l2 norm of weights: 6.434386816134349\n",
      "---------------------\n",
      "Iteration Number: 945\n",
      "Loss: 87.44322161867682\n",
      "l2 norm of gradients: 0.9960150863965034\n",
      "l2 norm of weights: 6.434153073699878\n",
      "---------------------\n",
      "Iteration Number: 946\n",
      "Loss: 87.39479283052808\n",
      "l2 norm of gradients: 0.9954432305056922\n",
      "l2 norm of weights: 6.433919479536131\n",
      "---------------------\n",
      "Iteration Number: 947\n",
      "Loss: 87.34641941075931\n",
      "l2 norm of gradients: 0.9948717161286582\n",
      "l2 norm of weights: 6.433686033474676\n",
      "---------------------\n",
      "Iteration Number: 948\n",
      "Loss: 87.29810129456753\n",
      "l2 norm of gradients: 0.994300542258969\n",
      "l2 norm of weights: 6.433452735347286\n",
      "---------------------\n",
      "Iteration Number: 949\n",
      "Loss: 87.24983841721834\n",
      "l2 norm of gradients: 0.9937297078901043\n",
      "l2 norm of weights: 6.433219584985942\n",
      "---------------------\n",
      "Iteration Number: 950\n",
      "Loss: 87.20163071404349\n",
      "l2 norm of gradients: 0.9931592120156514\n",
      "l2 norm of weights: 6.432986582222823\n",
      "---------------------\n",
      "Iteration Number: 951\n",
      "Loss: 87.15347812059846\n",
      "l2 norm of gradients: 0.9925890536294976\n",
      "l2 norm of weights: 6.432753726890316\n",
      "---------------------\n",
      "Iteration Number: 952\n",
      "Loss: 87.10538057270082\n",
      "l2 norm of gradients: 0.9920192317260214\n",
      "l2 norm of weights: 6.432521018821004\n",
      "---------------------\n",
      "Iteration Number: 953\n",
      "Loss: 87.05733800611942\n",
      "l2 norm of gradients: 0.9914497453002822\n",
      "l2 norm of weights: 6.432288457847673\n",
      "---------------------\n",
      "Iteration Number: 954\n",
      "Loss: 87.00935035687681\n",
      "l2 norm of gradients: 0.9908805933482058\n",
      "l2 norm of weights: 6.432056043803307\n",
      "---------------------\n",
      "Iteration Number: 955\n",
      "Loss: 86.96141756122648\n",
      "l2 norm of gradients: 0.9903117748667708\n",
      "l2 norm of weights: 6.431823776521086\n",
      "---------------------\n",
      "Iteration Number: 956\n",
      "Loss: 86.91353955540666\n",
      "l2 norm of gradients: 0.989743288854191\n",
      "l2 norm of weights: 6.431591655834389\n",
      "---------------------\n",
      "Iteration Number: 957\n",
      "Loss: 86.86571627596749\n",
      "l2 norm of gradients: 0.9891751343100975\n",
      "l2 norm of weights: 6.431359681576785\n",
      "---------------------\n",
      "Iteration Number: 958\n",
      "Loss: 86.817947659344\n",
      "l2 norm of gradients: 0.9886073102357159\n",
      "l2 norm of weights: 6.431127853582042\n",
      "---------------------\n",
      "Iteration Number: 959\n",
      "Loss: 86.77023364249115\n",
      "l2 norm of gradients: 0.9880398156340453\n",
      "l2 norm of weights: 6.430896171684119\n",
      "---------------------\n",
      "Iteration Number: 960\n",
      "Loss: 86.72257416230798\n",
      "l2 norm of gradients: 0.9874726495100316\n",
      "l2 norm of weights: 6.430664635717165\n",
      "---------------------\n",
      "Iteration Number: 961\n",
      "Loss: 86.67496915569437\n",
      "l2 norm of gradients: 0.9869058108707421\n",
      "l2 norm of weights: 6.430433245515522\n",
      "---------------------\n",
      "Iteration Number: 962\n",
      "Loss: 86.62741856001877\n",
      "l2 norm of gradients: 0.9863392987255356\n",
      "l2 norm of weights: 6.430202000913719\n",
      "---------------------\n",
      "Iteration Number: 963\n",
      "Loss: 86.57992231256063\n",
      "l2 norm of gradients: 0.9857731120862306\n",
      "l2 norm of weights: 6.429970901746474\n",
      "---------------------\n",
      "Iteration Number: 964\n",
      "Loss: 86.5324803509059\n",
      "l2 norm of gradients: 0.9852072499672737\n",
      "l2 norm of weights: 6.429739947848692\n",
      "---------------------\n",
      "Iteration Number: 965\n",
      "Loss: 86.48509261253155\n",
      "l2 norm of gradients: 0.9846417113859032\n",
      "l2 norm of weights: 6.429509139055462\n",
      "---------------------\n",
      "Iteration Number: 966\n",
      "Loss: 86.43775903539645\n",
      "l2 norm of gradients: 0.984076495362312\n",
      "l2 norm of weights: 6.429278475202061\n",
      "---------------------\n",
      "Iteration Number: 967\n",
      "Loss: 86.39047955736584\n",
      "l2 norm of gradients: 0.9835116009198084\n",
      "l2 norm of weights: 6.429047956123946\n",
      "---------------------\n",
      "Iteration Number: 968\n",
      "Loss: 86.34325411649934\n",
      "l2 norm of gradients: 0.9829470270849748\n",
      "l2 norm of weights: 6.4288175816567605\n",
      "---------------------\n",
      "Iteration Number: 969\n",
      "Loss: 86.29608265105165\n",
      "l2 norm of gradients: 0.9823827728878233\n",
      "l2 norm of weights: 6.428587351636324\n",
      "---------------------\n",
      "Iteration Number: 970\n",
      "Loss: 86.24896509936916\n",
      "l2 norm of gradients: 0.9818188373619513\n",
      "l2 norm of weights: 6.428357265898642\n",
      "---------------------\n",
      "Iteration Number: 971\n",
      "Loss: 86.20190139997595\n",
      "l2 norm of gradients: 0.9812552195446921\n",
      "l2 norm of weights: 6.428127324279895\n",
      "---------------------\n",
      "Iteration Number: 972\n",
      "Loss: 86.15489149152252\n",
      "l2 norm of gradients: 0.9806919184772676\n",
      "l2 norm of weights: 6.427897526616442\n",
      "---------------------\n",
      "Iteration Number: 973\n",
      "Loss: 86.10793531289573\n",
      "l2 norm of gradients: 0.9801289332049332\n",
      "l2 norm of weights: 6.427667872744821\n",
      "---------------------\n",
      "Iteration Number: 974\n",
      "Loss: 86.06103280285828\n",
      "l2 norm of gradients: 0.9795662627771262\n",
      "l2 norm of weights: 6.427438362501742\n",
      "---------------------\n",
      "Iteration Number: 975\n",
      "Loss: 86.01418390065517\n",
      "l2 norm of gradients: 0.9790039062476085\n",
      "l2 norm of weights: 6.4272089957240945\n",
      "---------------------\n",
      "Iteration Number: 976\n",
      "Loss: 85.96738854533554\n",
      "l2 norm of gradients: 0.9784418626746084\n",
      "l2 norm of weights: 6.426979772248939\n",
      "---------------------\n",
      "Iteration Number: 977\n",
      "Loss: 85.92064667647222\n",
      "l2 norm of gradients: 0.9778801311209608\n",
      "l2 norm of weights: 6.42675069191351\n",
      "---------------------\n",
      "Iteration Number: 978\n",
      "Loss: 85.87395823329379\n",
      "l2 norm of gradients: 0.9773187106542436\n",
      "l2 norm of weights: 6.4265217545552105\n",
      "---------------------\n",
      "Iteration Number: 979\n",
      "Loss: 85.82732315565319\n",
      "l2 norm of gradients: 0.9767576003469146\n",
      "l2 norm of weights: 6.426292960011617\n",
      "---------------------\n",
      "Iteration Number: 980\n",
      "Loss: 85.7807413831445\n",
      "l2 norm of gradients: 0.9761967992764433\n",
      "l2 norm of weights: 6.426064308120476\n",
      "---------------------\n",
      "Iteration Number: 981\n",
      "Loss: 85.73421285585584\n",
      "l2 norm of gradients: 0.9756363065254428\n",
      "l2 norm of weights: 6.425835798719701\n",
      "---------------------\n",
      "Iteration Number: 982\n",
      "Loss: 85.68773751363901\n",
      "l2 norm of gradients: 0.9750761211817979\n",
      "l2 norm of weights: 6.425607431647372\n",
      "---------------------\n",
      "Iteration Number: 983\n",
      "Loss: 85.64131529676531\n",
      "l2 norm of gradients: 0.9745162423387943\n",
      "l2 norm of weights: 6.425379206741739\n",
      "---------------------\n",
      "Iteration Number: 984\n",
      "Loss: 85.59494614554843\n",
      "l2 norm of gradients: 0.9739566690952405\n",
      "l2 norm of weights: 6.425151123841213\n",
      "---------------------\n",
      "Iteration Number: 985\n",
      "Loss: 85.54863000041819\n",
      "l2 norm of gradients: 0.9733974005555921\n",
      "l2 norm of weights: 6.424923182784374\n",
      "---------------------\n",
      "Iteration Number: 986\n",
      "Loss: 85.50236680196362\n",
      "l2 norm of gradients: 0.9728384358300718\n",
      "l2 norm of weights: 6.424695383409963\n",
      "---------------------\n",
      "Iteration Number: 987\n",
      "Loss: 85.45615649088474\n",
      "l2 norm of gradients: 0.9722797740347889\n",
      "l2 norm of weights: 6.424467725556883\n",
      "---------------------\n",
      "Iteration Number: 988\n",
      "Loss: 85.40999900794024\n",
      "l2 norm of gradients: 0.9717214142918537\n",
      "l2 norm of weights: 6.424240209064199\n",
      "---------------------\n",
      "Iteration Number: 989\n",
      "Loss: 85.36389429427304\n",
      "l2 norm of gradients: 0.9711633557294929\n",
      "l2 norm of weights: 6.4240128337711395\n",
      "---------------------\n",
      "Iteration Number: 990\n",
      "Loss: 85.31784229080255\n",
      "l2 norm of gradients: 0.9706055974821619\n",
      "l2 norm of weights: 6.423785599517088\n",
      "---------------------\n",
      "Iteration Number: 991\n",
      "Loss: 85.27184293885027\n",
      "l2 norm of gradients: 0.9700481386906543\n",
      "l2 norm of weights: 6.423558506141591\n",
      "---------------------\n",
      "Iteration Number: 992\n",
      "Loss: 85.22589617969375\n",
      "l2 norm of gradients: 0.9694909785022092\n",
      "l2 norm of weights: 6.423331553484349\n",
      "---------------------\n",
      "Iteration Number: 993\n",
      "Loss: 85.18000195496994\n",
      "l2 norm of gradients: 0.9689341160706182\n",
      "l2 norm of weights: 6.423104741385221\n",
      "---------------------\n",
      "Iteration Number: 994\n",
      "Loss: 85.13416020612861\n",
      "l2 norm of gradients: 0.9683775505563278\n",
      "l2 norm of weights: 6.422878069684225\n",
      "---------------------\n",
      "Iteration Number: 995\n",
      "Loss: 85.08837087498641\n",
      "l2 norm of gradients: 0.9678212811265424\n",
      "l2 norm of weights: 6.422651538221528\n",
      "---------------------\n",
      "Iteration Number: 996\n",
      "Loss: 85.04263390332754\n",
      "l2 norm of gradients: 0.9672653069553222\n",
      "l2 norm of weights: 6.4224251468374565\n",
      "---------------------\n",
      "Iteration Number: 997\n",
      "Loss: 84.99694923330213\n",
      "l2 norm of gradients: 0.9667096272236815\n",
      "l2 norm of weights: 6.422198895372488\n",
      "---------------------\n",
      "Iteration Number: 998\n",
      "Loss: 84.95131680684145\n",
      "l2 norm of gradients: 0.9661542411196835\n",
      "l2 norm of weights: 6.421972783667251\n",
      "---------------------\n",
      "Iteration Number: 999\n",
      "Loss: 84.90573656619013\n",
      "l2 norm of gradients: 0.9655991478385345\n",
      "l2 norm of weights: 6.42174681156253\n",
      "---------------------\n",
      "Iteration Number: 1000\n",
      "Loss: 84.86020845385194\n",
      "l2 norm of gradients: 0.9650443465826732\n",
      "l2 norm of weights: 6.421520978899253\n",
      "---------------------\n",
      "Iteration Number: 1001\n",
      "Loss: 84.81473241205777\n",
      "l2 norm of gradients: 0.9644898365618617\n",
      "l2 norm of weights: 6.421295285518506\n",
      "---------------------\n",
      "Iteration Number: 1002\n",
      "Loss: 84.76930838364794\n",
      "l2 norm of gradients: 0.9639356169932709\n",
      "l2 norm of weights: 6.421069731261518\n",
      "---------------------\n",
      "Iteration Number: 1003\n",
      "Loss: 84.72393631116971\n",
      "l2 norm of gradients: 0.9633816871015664\n",
      "l2 norm of weights: 6.420844315969669\n",
      "---------------------\n",
      "Iteration Number: 1004\n",
      "Loss: 84.6786161375317\n",
      "l2 norm of gradients: 0.9628280461189911\n",
      "l2 norm of weights: 6.420619039484485\n",
      "---------------------\n",
      "Iteration Number: 1005\n",
      "Loss: 84.63334780559556\n",
      "l2 norm of gradients: 0.9622746932854455\n",
      "l2 norm of weights: 6.42039390164764\n",
      "---------------------\n",
      "Iteration Number: 1006\n",
      "Loss: 84.58813125850831\n",
      "l2 norm of gradients: 0.9617216278485672\n",
      "l2 norm of weights: 6.420168902300951\n",
      "---------------------\n",
      "Iteration Number: 1007\n",
      "Loss: 84.54296643936281\n",
      "l2 norm of gradients: 0.9611688490638072\n",
      "l2 norm of weights: 6.419944041286382\n",
      "---------------------\n",
      "Iteration Number: 1008\n",
      "Loss: 84.49785329153897\n",
      "l2 norm of gradients: 0.9606163561945044\n",
      "l2 norm of weights: 6.419719318446043\n",
      "---------------------\n",
      "Iteration Number: 1009\n",
      "Loss: 84.45279175844078\n",
      "l2 norm of gradients: 0.9600641485119591\n",
      "l2 norm of weights: 6.419494733622181\n",
      "---------------------\n",
      "Iteration Number: 1010\n",
      "Loss: 84.40778178349768\n",
      "l2 norm of gradients: 0.959512225295502\n",
      "l2 norm of weights: 6.419270286657193\n",
      "---------------------\n",
      "Iteration Number: 1011\n",
      "Loss: 84.3628233103463\n",
      "l2 norm of gradients: 0.9589605858325647\n",
      "l2 norm of weights: 6.419045977393614\n",
      "---------------------\n",
      "Iteration Number: 1012\n",
      "Loss: 84.31791628280979\n",
      "l2 norm of gradients: 0.958409229418745\n",
      "l2 norm of weights: 6.41882180567412\n",
      "---------------------\n",
      "Iteration Number: 1013\n",
      "Loss: 84.27306064477578\n",
      "l2 norm of gradients: 0.957858155357872\n",
      "l2 norm of weights: 6.418597771341527\n",
      "---------------------\n",
      "Iteration Number: 1014\n",
      "Loss: 84.22825633996284\n",
      "l2 norm of gradients: 0.9573073629620685\n",
      "l2 norm of weights: 6.418373874238793\n",
      "---------------------\n",
      "Iteration Number: 1015\n",
      "Loss: 84.18350331272708\n",
      "l2 norm of gradients: 0.9567568515518121\n",
      "l2 norm of weights: 6.418150114209013\n",
      "---------------------\n",
      "Iteration Number: 1016\n",
      "Loss: 84.13880150699029\n",
      "l2 norm of gradients: 0.9562066204559933\n",
      "l2 norm of weights: 6.417926491095419\n",
      "---------------------\n",
      "Iteration Number: 1017\n",
      "Loss: 84.09415086719738\n",
      "l2 norm of gradients: 0.9556566690119723\n",
      "l2 norm of weights: 6.417703004741384\n",
      "---------------------\n",
      "Iteration Number: 1018\n",
      "Loss: 84.0495513377477\n",
      "l2 norm of gradients: 0.955106996565634\n",
      "l2 norm of weights: 6.417479654990415\n",
      "---------------------\n",
      "Iteration Number: 1019\n",
      "Loss: 84.00500286303384\n",
      "l2 norm of gradients: 0.9545576024714407\n",
      "l2 norm of weights: 6.417256441686156\n",
      "---------------------\n",
      "Iteration Number: 1020\n",
      "Loss: 83.9605053876662\n",
      "l2 norm of gradients: 0.9540084860924823\n",
      "l2 norm of weights: 6.4170333646723865\n",
      "---------------------\n",
      "Iteration Number: 1021\n",
      "Loss: 83.91605885632757\n",
      "l2 norm of gradients: 0.9534596468005269\n",
      "l2 norm of weights: 6.416810423793022\n",
      "---------------------\n",
      "Iteration Number: 1022\n",
      "Loss: 83.87166321390566\n",
      "l2 norm of gradients: 0.9529110839760649\n",
      "l2 norm of weights: 6.416587618892108\n",
      "---------------------\n",
      "Iteration Number: 1023\n",
      "Loss: 83.82731840521977\n",
      "l2 norm of gradients: 0.9523627970083578\n",
      "l2 norm of weights: 6.416364949813829\n",
      "---------------------\n",
      "Iteration Number: 1024\n",
      "Loss: 83.78302437535095\n",
      "l2 norm of gradients: 0.9518147852954779\n",
      "l2 norm of weights: 6.4161424164025\n",
      "---------------------\n",
      "Iteration Number: 1025\n",
      "Loss: 83.73878106922412\n",
      "l2 norm of gradients: 0.9512670482443514\n",
      "l2 norm of weights: 6.415920018502565\n",
      "---------------------\n",
      "Iteration Number: 1026\n",
      "Loss: 83.69458843219839\n",
      "l2 norm of gradients: 0.9507195852707977\n",
      "l2 norm of weights: 6.415697755958606\n",
      "---------------------\n",
      "Iteration Number: 1027\n",
      "Loss: 83.65044640949726\n",
      "l2 norm of gradients: 0.9501723957995664\n",
      "l2 norm of weights: 6.415475628615329\n",
      "---------------------\n",
      "Iteration Number: 1028\n",
      "Loss: 83.60635494658074\n",
      "l2 norm of gradients: 0.9496254792643734\n",
      "l2 norm of weights: 6.415253636317579\n",
      "---------------------\n",
      "Iteration Number: 1029\n",
      "Loss: 83.56231398885677\n",
      "l2 norm of gradients: 0.949078835107934\n",
      "l2 norm of weights: 6.415031778910322\n",
      "---------------------\n",
      "Iteration Number: 1030\n",
      "Loss: 83.51832348180366\n",
      "l2 norm of gradients: 0.9485324627819961\n",
      "l2 norm of weights: 6.414810056238659\n",
      "---------------------\n",
      "Iteration Number: 1031\n",
      "Loss: 83.4743833713588\n",
      "l2 norm of gradients: 0.9479863617473686\n",
      "l2 norm of weights: 6.414588468147818\n",
      "---------------------\n",
      "Iteration Number: 1032\n",
      "Loss: 83.4304936030289\n",
      "l2 norm of gradients: 0.9474405314739512\n",
      "l2 norm of weights: 6.414367014483156\n",
      "---------------------\n",
      "Iteration Number: 1033\n",
      "Loss: 83.38665412274689\n",
      "l2 norm of gradients: 0.9468949714407605\n",
      "l2 norm of weights: 6.4141456950901565\n",
      "---------------------\n",
      "Iteration Number: 1034\n",
      "Loss: 83.34286487653786\n",
      "l2 norm of gradients: 0.9463496811359536\n",
      "l2 norm of weights: 6.4139245098144295\n",
      "---------------------\n",
      "Iteration Number: 1035\n",
      "Loss: 83.29912581027588\n",
      "l2 norm of gradients: 0.9458046600568529\n",
      "l2 norm of weights: 6.413703458501713\n",
      "---------------------\n",
      "Iteration Number: 1036\n",
      "Loss: 83.25543687023746\n",
      "l2 norm of gradients: 0.9452599077099656\n",
      "l2 norm of weights: 6.413482540997872\n",
      "---------------------\n",
      "Iteration Number: 1037\n",
      "Loss: 83.21179800255739\n",
      "l2 norm of gradients: 0.9447154236110039\n",
      "l2 norm of weights: 6.413261757148897\n",
      "---------------------\n",
      "Iteration Number: 1038\n",
      "Loss: 83.16820915352835\n",
      "l2 norm of gradients: 0.9441712072849022\n",
      "l2 norm of weights: 6.4130411068008994\n",
      "---------------------\n",
      "Iteration Number: 1039\n",
      "Loss: 83.12467026952537\n",
      "l2 norm of gradients: 0.9436272582658332\n",
      "l2 norm of weights: 6.412820589800121\n",
      "---------------------\n",
      "Iteration Number: 1040\n",
      "Loss: 83.08118129702136\n",
      "l2 norm of gradients: 0.9430835760972218\n",
      "l2 norm of weights: 6.412600205992923\n",
      "---------------------\n",
      "Iteration Number: 1041\n",
      "Loss: 83.0377421826404\n",
      "l2 norm of gradients: 0.9425401603317579\n",
      "l2 norm of weights: 6.412379955225794\n",
      "---------------------\n",
      "Iteration Number: 1042\n",
      "Loss: 82.99435287284793\n",
      "l2 norm of gradients: 0.9419970105314066\n",
      "l2 norm of weights: 6.412159837345344\n",
      "---------------------\n",
      "Iteration Number: 1043\n",
      "Loss: 82.95101331444678\n",
      "l2 norm of gradients: 0.9414541262674172\n",
      "l2 norm of weights: 6.411939852198305\n",
      "---------------------\n",
      "Iteration Number: 1044\n",
      "Loss: 82.9077234542213\n",
      "l2 norm of gradients: 0.9409115071203316\n",
      "l2 norm of weights: 6.411719999631535\n",
      "---------------------\n",
      "Iteration Number: 1045\n",
      "Loss: 82.86448323905039\n",
      "l2 norm of gradients: 0.9403691526799884\n",
      "l2 norm of weights: 6.411500279492008\n",
      "---------------------\n",
      "Iteration Number: 1046\n",
      "Loss: 82.82129261596002\n",
      "l2 norm of gradients: 0.9398270625455278\n",
      "l2 norm of weights: 6.411280691626826\n",
      "---------------------\n",
      "Iteration Number: 1047\n",
      "Loss: 82.77815153186745\n",
      "l2 norm of gradients: 0.9392852363253938\n",
      "l2 norm of weights: 6.411061235883208\n",
      "---------------------\n",
      "Iteration Number: 1048\n",
      "Loss: 82.73505993386912\n",
      "l2 norm of gradients: 0.9387436736373355\n",
      "l2 norm of weights: 6.410841912108494\n",
      "---------------------\n",
      "Iteration Number: 1049\n",
      "Loss: 82.69201776919786\n",
      "l2 norm of gradients: 0.9382023741084047\n",
      "l2 norm of weights: 6.410622720150146\n",
      "---------------------\n",
      "Iteration Number: 1050\n",
      "Loss: 82.6490249851053\n",
      "l2 norm of gradients: 0.9376613373749547\n",
      "l2 norm of weights: 6.410403659855745\n",
      "---------------------\n",
      "Iteration Number: 1051\n",
      "Loss: 82.60608152885219\n",
      "l2 norm of gradients: 0.9371205630826367\n",
      "l2 norm of weights: 6.41018473107299\n",
      "---------------------\n",
      "Iteration Number: 1052\n",
      "Loss: 82.56318734798894\n",
      "l2 norm of gradients: 0.9365800508863923\n",
      "l2 norm of weights: 6.409965933649705\n",
      "---------------------\n",
      "Iteration Number: 1053\n",
      "Loss: 82.5203423899318\n",
      "l2 norm of gradients: 0.9360398004504477\n",
      "l2 norm of weights: 6.409747267433826\n",
      "---------------------\n",
      "Iteration Number: 1054\n",
      "Loss: 82.47754660212979\n",
      "l2 norm of gradients: 0.9354998114483044\n",
      "l2 norm of weights: 6.409528732273411\n",
      "---------------------\n",
      "Iteration Number: 1055\n",
      "Loss: 82.43479993230356\n",
      "l2 norm of gradients: 0.9349600835627299\n",
      "l2 norm of weights: 6.409310328016635\n",
      "---------------------\n",
      "Iteration Number: 1056\n",
      "Loss: 82.39210232812694\n",
      "l2 norm of gradients: 0.9344206164857437\n",
      "l2 norm of weights: 6.4090920545117935\n",
      "---------------------\n",
      "Iteration Number: 1057\n",
      "Loss: 82.34945373736367\n",
      "l2 norm of gradients: 0.9338814099186067\n",
      "l2 norm of weights: 6.408873911607296\n",
      "---------------------\n",
      "Iteration Number: 1058\n",
      "Loss: 82.30685410784406\n",
      "l2 norm of gradients: 0.9333424635718041\n",
      "l2 norm of weights: 6.408655899151673\n",
      "---------------------\n",
      "Iteration Number: 1059\n",
      "Loss: 82.2643033873758\n",
      "l2 norm of gradients: 0.9328037771650304\n",
      "l2 norm of weights: 6.408438016993569\n",
      "---------------------\n",
      "Iteration Number: 1060\n",
      "Loss: 82.22180152399743\n",
      "l2 norm of gradients: 0.9322653504271708\n",
      "l2 norm of weights: 6.408220264981746\n",
      "---------------------\n",
      "Iteration Number: 1061\n",
      "Loss: 82.17934846573458\n",
      "l2 norm of gradients: 0.9317271830962833\n",
      "l2 norm of weights: 6.408002642965083\n",
      "---------------------\n",
      "Iteration Number: 1062\n",
      "Loss: 82.13694416067611\n",
      "l2 norm of gradients: 0.9311892749195765\n",
      "l2 norm of weights: 6.407785150792576\n",
      "---------------------\n",
      "Iteration Number: 1063\n",
      "Loss: 82.09458855689607\n",
      "l2 norm of gradients: 0.9306516256533884\n",
      "l2 norm of weights: 6.407567788313335\n",
      "---------------------\n",
      "Iteration Number: 1064\n",
      "Loss: 82.05228160268588\n",
      "l2 norm of gradients: 0.9301142350631627\n",
      "l2 norm of weights: 6.407350555376588\n",
      "---------------------\n",
      "Iteration Number: 1065\n",
      "Loss: 82.01002324632628\n",
      "l2 norm of gradients: 0.9295771029234244\n",
      "l2 norm of weights: 6.407133451831676\n",
      "---------------------\n",
      "Iteration Number: 1066\n",
      "Loss: 81.96781343606928\n",
      "l2 norm of gradients: 0.9290402290177525\n",
      "l2 norm of weights: 6.406916477528057\n",
      "---------------------\n",
      "Iteration Number: 1067\n",
      "Loss: 81.92565212038667\n",
      "l2 norm of gradients: 0.9285036131387536\n",
      "l2 norm of weights: 6.406699632315302\n",
      "---------------------\n",
      "Iteration Number: 1068\n",
      "Loss: 81.8835392477495\n",
      "l2 norm of gradients: 0.9279672550880322\n",
      "l2 norm of weights: 6.406482916043101\n",
      "---------------------\n",
      "Iteration Number: 1069\n",
      "Loss: 81.84147476657482\n",
      "l2 norm of gradients: 0.9274311546761607\n",
      "l2 norm of weights: 6.406266328561255\n",
      "---------------------\n",
      "Iteration Number: 1070\n",
      "Loss: 81.79945862553042\n",
      "l2 norm of gradients: 0.9268953117226472\n",
      "l2 norm of weights: 6.406049869719679\n",
      "---------------------\n",
      "Iteration Number: 1071\n",
      "Loss: 81.75749077314936\n",
      "l2 norm of gradients: 0.9263597260559038\n",
      "l2 norm of weights: 6.4058335393684045\n",
      "---------------------\n",
      "Iteration Number: 1072\n",
      "Loss: 81.71557115820472\n",
      "l2 norm of gradients: 0.9258243975132117\n",
      "l2 norm of weights: 6.405617337357576\n",
      "---------------------\n",
      "Iteration Number: 1073\n",
      "Loss: 81.67369972943233\n",
      "l2 norm of gradients: 0.9252893259406852\n",
      "l2 norm of weights: 6.405401263537453\n",
      "---------------------\n",
      "Iteration Number: 1074\n",
      "Loss: 81.63187643554576\n",
      "l2 norm of gradients: 0.9247545111932362\n",
      "l2 norm of weights: 6.405185317758406\n",
      "---------------------\n",
      "Iteration Number: 1075\n",
      "Loss: 81.59010122538552\n",
      "l2 norm of gradients: 0.9242199531345352\n",
      "l2 norm of weights: 6.404969499870921\n",
      "---------------------\n",
      "Iteration Number: 1076\n",
      "Loss: 81.5483740479338\n",
      "l2 norm of gradients: 0.9236856516369728\n",
      "l2 norm of weights: 6.4047538097256\n",
      "---------------------\n",
      "Iteration Number: 1077\n",
      "Loss: 81.50669485214522\n",
      "l2 norm of gradients: 0.9231516065816185\n",
      "l2 norm of weights: 6.404538247173151\n",
      "---------------------\n",
      "Iteration Number: 1078\n",
      "Loss: 81.46506358693036\n",
      "l2 norm of gradients: 0.9226178178581808\n",
      "l2 norm of weights: 6.404322812064404\n",
      "---------------------\n",
      "Iteration Number: 1079\n",
      "Loss: 81.42348020135069\n",
      "l2 norm of gradients: 0.9220842853649621\n",
      "l2 norm of weights: 6.4041075042502955\n",
      "---------------------\n",
      "Iteration Number: 1080\n",
      "Loss: 81.38194464462013\n",
      "l2 norm of gradients: 0.9215510090088168\n",
      "l2 norm of weights: 6.403892323581878\n",
      "---------------------\n",
      "Iteration Number: 1081\n",
      "Loss: 81.3404568657985\n",
      "l2 norm of gradients: 0.9210179887051054\n",
      "l2 norm of weights: 6.4036772699103155\n",
      "---------------------\n",
      "Iteration Number: 1082\n",
      "Loss: 81.29901681405097\n",
      "l2 norm of gradients: 0.9204852243776479\n",
      "l2 norm of weights: 6.403462343086885\n",
      "---------------------\n",
      "Iteration Number: 1083\n",
      "Loss: 81.25762443862192\n",
      "l2 norm of gradients: 0.9199527159586778\n",
      "l2 norm of weights: 6.403247542962978\n",
      "---------------------\n",
      "Iteration Number: 1084\n",
      "Loss: 81.21627968879322\n",
      "l2 norm of gradients: 0.9194204633887916\n",
      "l2 norm of weights: 6.403032869390096\n",
      "---------------------\n",
      "Iteration Number: 1085\n",
      "Loss: 81.17498251398841\n",
      "l2 norm of gradients: 0.9188884666169024\n",
      "l2 norm of weights: 6.402818322219855\n",
      "---------------------\n",
      "Iteration Number: 1086\n",
      "Loss: 81.13373286349326\n",
      "l2 norm of gradients: 0.9183567256001858\n",
      "l2 norm of weights: 6.402603901303983\n",
      "---------------------\n",
      "Iteration Number: 1087\n",
      "Loss: 81.09253068681356\n",
      "l2 norm of gradients: 0.9178252403040315\n",
      "l2 norm of weights: 6.402389606494317\n",
      "---------------------\n",
      "Iteration Number: 1088\n",
      "Loss: 81.05137593334389\n",
      "l2 norm of gradients: 0.9172940107019882\n",
      "l2 norm of weights: 6.4021754376428115\n",
      "---------------------\n",
      "Iteration Number: 1089\n",
      "Loss: 81.01026855257894\n",
      "l2 norm of gradients: 0.9167630367757117\n",
      "l2 norm of weights: 6.401961394601531\n",
      "---------------------\n",
      "Iteration Number: 1090\n",
      "Loss: 80.969208494083\n",
      "l2 norm of gradients: 0.9162323185149096\n",
      "l2 norm of weights: 6.401747477222652\n",
      "---------------------\n",
      "Iteration Number: 1091\n",
      "Loss: 80.92819570751044\n",
      "l2 norm of gradients: 0.9157018559172855\n",
      "l2 norm of weights: 6.401533685358463\n",
      "---------------------\n",
      "Iteration Number: 1092\n",
      "Loss: 80.88723014233682\n",
      "l2 norm of gradients: 0.9151716489884824\n",
      "l2 norm of weights: 6.401320018861367\n",
      "---------------------\n",
      "Iteration Number: 1093\n",
      "Loss: 80.84631174847848\n",
      "l2 norm of gradients: 0.9146416977420257\n",
      "l2 norm of weights: 6.401106477583875\n",
      "---------------------\n",
      "Iteration Number: 1094\n",
      "Loss: 80.80544047539678\n",
      "l2 norm of gradients: 0.9141120021992645\n",
      "l2 norm of weights: 6.400893061378615\n",
      "---------------------\n",
      "Iteration Number: 1095\n",
      "Loss: 80.76461627292213\n",
      "l2 norm of gradients: 0.9135825623893121\n",
      "l2 norm of weights: 6.400679770098324\n",
      "---------------------\n",
      "Iteration Number: 1096\n",
      "Loss: 80.72383909091812\n",
      "l2 norm of gradients: 0.9130533783489856\n",
      "l2 norm of weights: 6.400466603595851\n",
      "---------------------\n",
      "Iteration Number: 1097\n",
      "Loss: 80.6831088791208\n",
      "l2 norm of gradients: 0.9125244501227451\n",
      "l2 norm of weights: 6.40025356172416\n",
      "---------------------\n",
      "Iteration Number: 1098\n",
      "Loss: 80.64242558739645\n",
      "l2 norm of gradients: 0.9119957777626317\n",
      "l2 norm of weights: 6.4000406443363245\n",
      "---------------------\n",
      "Iteration Number: 1099\n",
      "Loss: 80.60178916562288\n",
      "l2 norm of gradients: 0.9114673613282038\n",
      "l2 norm of weights: 6.399827851285531\n",
      "---------------------\n",
      "Iteration Number: 1100\n",
      "Loss: 80.5611995637418\n",
      "l2 norm of gradients: 0.9109392008864747\n",
      "l2 norm of weights: 6.399615182425079\n",
      "---------------------\n",
      "Iteration Number: 1101\n",
      "Loss: 80.52065673170804\n",
      "l2 norm of gradients: 0.9104112965118472\n",
      "l2 norm of weights: 6.3994026376083815\n",
      "---------------------\n",
      "Iteration Number: 1102\n",
      "Loss: 80.48016061951606\n",
      "l2 norm of gradients: 0.9098836482860483\n",
      "l2 norm of weights: 6.39919021668896\n",
      "---------------------\n",
      "Iteration Number: 1103\n",
      "Loss: 80.43971117719053\n",
      "l2 norm of gradients: 0.9093562562980637\n",
      "l2 norm of weights: 6.398977919520451\n",
      "---------------------\n",
      "Iteration Number: 1104\n",
      "Loss: 80.39930835487607\n",
      "l2 norm of gradients: 0.9088291206440704\n",
      "l2 norm of weights: 6.398765745956605\n",
      "---------------------\n",
      "Iteration Number: 1105\n",
      "Loss: 80.35895210249676\n",
      "l2 norm of gradients: 0.9083022414273689\n",
      "l2 norm of weights: 6.398553695851282\n",
      "---------------------\n",
      "Iteration Number: 1106\n",
      "Loss: 80.31864237026033\n",
      "l2 norm of gradients: 0.9077756187583159\n",
      "l2 norm of weights: 6.398341769058455\n",
      "---------------------\n",
      "Iteration Number: 1107\n",
      "Loss: 80.27837910831259\n",
      "l2 norm of gradients: 0.9072492527542542\n",
      "l2 norm of weights: 6.398129965432212\n",
      "---------------------\n",
      "Iteration Number: 1108\n",
      "Loss: 80.23816226692412\n",
      "l2 norm of gradients: 0.9067231435394427\n",
      "l2 norm of weights: 6.397918284826752\n",
      "---------------------\n",
      "Iteration Number: 1109\n",
      "Loss: 80.19799179614192\n",
      "l2 norm of gradients: 0.906197291244988\n",
      "l2 norm of weights: 6.397706727096388\n",
      "---------------------\n",
      "Iteration Number: 1110\n",
      "Loss: 80.15786764628872\n",
      "l2 norm of gradients: 0.9056716960087705\n",
      "l2 norm of weights: 6.397495292095544\n",
      "---------------------\n",
      "Iteration Number: 1111\n",
      "Loss: 80.11778976765913\n",
      "l2 norm of gradients: 0.9051463579753752\n",
      "l2 norm of weights: 6.39728397967876\n",
      "---------------------\n",
      "Iteration Number: 1112\n",
      "Loss: 80.07775811048333\n",
      "l2 norm of gradients: 0.9046212772960175\n",
      "l2 norm of weights: 6.397072789700687\n",
      "---------------------\n",
      "Iteration Number: 1113\n",
      "Loss: 80.0377726250336\n",
      "l2 norm of gradients: 0.9040964541284712\n",
      "l2 norm of weights: 6.396861722016088\n",
      "---------------------\n",
      "Iteration Number: 1114\n",
      "Loss: 79.99783326182522\n",
      "l2 norm of gradients: 0.9035718886369947\n",
      "l2 norm of weights: 6.396650776479843\n",
      "---------------------\n",
      "Iteration Number: 1115\n",
      "Loss: 79.95793997106671\n",
      "l2 norm of gradients: 0.9030475809922569\n",
      "l2 norm of weights: 6.396439952946943\n",
      "---------------------\n",
      "Iteration Number: 1116\n",
      "Loss: 79.9180927032396\n",
      "l2 norm of gradients: 0.9025235313712624\n",
      "l2 norm of weights: 6.396229251272494\n",
      "---------------------\n",
      "Iteration Number: 1117\n",
      "Loss: 79.87829140876495\n",
      "l2 norm of gradients: 0.9019997399572763\n",
      "l2 norm of weights: 6.396018671311714\n",
      "---------------------\n",
      "Iteration Number: 1118\n",
      "Loss: 79.83853603809075\n",
      "l2 norm of gradients: 0.9014762069397491\n",
      "l2 norm of weights: 6.395808212919938\n",
      "---------------------\n",
      "Iteration Number: 1119\n",
      "Loss: 79.79882654161952\n",
      "l2 norm of gradients: 0.9009529325142387\n",
      "l2 norm of weights: 6.395597875952612\n",
      "---------------------\n",
      "Iteration Number: 1120\n",
      "Loss: 79.75916286990831\n",
      "l2 norm of gradients: 0.9004299168823353\n",
      "l2 norm of weights: 6.395387660265297\n",
      "---------------------\n",
      "Iteration Number: 1121\n",
      "Loss: 79.71954497349022\n",
      "l2 norm of gradients: 0.899907160251584\n",
      "l2 norm of weights: 6.39517756571367\n",
      "---------------------\n",
      "Iteration Number: 1122\n",
      "Loss: 79.67997280289505\n",
      "l2 norm of gradients: 0.8993846628354063\n",
      "l2 norm of weights: 6.394967592153519\n",
      "---------------------\n",
      "Iteration Number: 1123\n",
      "Loss: 79.64044630861109\n",
      "l2 norm of gradients: 0.8988624248530223\n",
      "l2 norm of weights: 6.3947577394407515\n",
      "---------------------\n",
      "Iteration Number: 1124\n",
      "Loss: 79.60096544129642\n",
      "l2 norm of gradients: 0.8983404465293725\n",
      "l2 norm of weights: 6.394548007431385\n",
      "---------------------\n",
      "Iteration Number: 1125\n",
      "Loss: 79.56153015148223\n",
      "l2 norm of gradients: 0.8978187280950389\n",
      "l2 norm of weights: 6.3943383959815545\n",
      "---------------------\n",
      "Iteration Number: 1126\n",
      "Loss: 79.52214038986196\n",
      "l2 norm of gradients: 0.8972972697861646\n",
      "l2 norm of weights: 6.394128904947512\n",
      "---------------------\n",
      "Iteration Number: 1127\n",
      "Loss: 79.48279610707723\n",
      "l2 norm of gradients: 0.8967760718443757\n",
      "l2 norm of weights: 6.39391953418562\n",
      "---------------------\n",
      "Iteration Number: 1128\n",
      "Loss: 79.44349725373145\n",
      "l2 norm of gradients: 0.8962551345167\n",
      "l2 norm of weights: 6.39371028355236\n",
      "---------------------\n",
      "Iteration Number: 1129\n",
      "Loss: 79.40424378056335\n",
      "l2 norm of gradients: 0.895734458055487\n",
      "l2 norm of weights: 6.393501152904328\n",
      "---------------------\n",
      "Iteration Number: 1130\n",
      "Loss: 79.36503563820806\n",
      "l2 norm of gradients: 0.8952140427183273\n",
      "l2 norm of weights: 6.39329214209824\n",
      "---------------------\n",
      "Iteration Number: 1131\n",
      "Loss: 79.3258727774627\n",
      "l2 norm of gradients: 0.8946938887679714\n",
      "l2 norm of weights: 6.393083250990919\n",
      "---------------------\n",
      "Iteration Number: 1132\n",
      "Loss: 79.28675514898187\n",
      "l2 norm of gradients: 0.8941739964722488\n",
      "l2 norm of weights: 6.392874479439313\n",
      "---------------------\n",
      "Iteration Number: 1133\n",
      "Loss: 79.24768270360347\n",
      "l2 norm of gradients: 0.8936543661039857\n",
      "l2 norm of weights: 6.392665827300483\n",
      "---------------------\n",
      "Iteration Number: 1134\n",
      "Loss: 79.20865539200892\n",
      "l2 norm of gradients: 0.8931349979409241\n",
      "l2 norm of weights: 6.3924572944316065\n",
      "---------------------\n",
      "Iteration Number: 1135\n",
      "Loss: 79.16967316499516\n",
      "l2 norm of gradients: 0.8926158922656386\n",
      "l2 norm of weights: 6.392248880689977\n",
      "---------------------\n",
      "Iteration Number: 1136\n",
      "Loss: 79.1307359734497\n",
      "l2 norm of gradients: 0.8920970493654556\n",
      "l2 norm of weights: 6.392040585933009\n",
      "---------------------\n",
      "Iteration Number: 1137\n",
      "Loss: 79.09184376817986\n",
      "l2 norm of gradients: 0.8915784695323691\n",
      "l2 norm of weights: 6.39183241001823\n",
      "---------------------\n",
      "Iteration Number: 1138\n",
      "Loss: 79.0529964998724\n",
      "l2 norm of gradients: 0.8910601530629596\n",
      "l2 norm of weights: 6.391624352803287\n",
      "---------------------\n",
      "Iteration Number: 1139\n",
      "Loss: 79.01419411953067\n",
      "l2 norm of gradients: 0.8905421002583108\n",
      "l2 norm of weights: 6.391416414145945\n",
      "---------------------\n",
      "Iteration Number: 1140\n",
      "Loss: 78.97543657806202\n",
      "l2 norm of gradients: 0.8900243114239258\n",
      "l2 norm of weights: 6.391208593904086\n",
      "---------------------\n",
      "Iteration Number: 1141\n",
      "Loss: 78.93672382611804\n",
      "l2 norm of gradients: 0.8895067868696452\n",
      "l2 norm of weights: 6.39100089193571\n",
      "---------------------\n",
      "Iteration Number: 1142\n",
      "Loss: 78.89805581477167\n",
      "l2 norm of gradients: 0.8889895269095633\n",
      "l2 norm of weights: 6.390793308098938\n",
      "---------------------\n",
      "Iteration Number: 1143\n",
      "Loss: 78.8594324948732\n",
      "l2 norm of gradients: 0.8884725318619449\n",
      "l2 norm of weights: 6.390585842252006\n",
      "---------------------\n",
      "Iteration Number: 1144\n",
      "Loss: 78.82085381740838\n",
      "l2 norm of gradients: 0.8879558020491414\n",
      "l2 norm of weights: 6.390378494253272\n",
      "---------------------\n",
      "Iteration Number: 1145\n",
      "Loss: 78.78231973315948\n",
      "l2 norm of gradients: 0.8874393377975082\n",
      "l2 norm of weights: 6.390171263961211\n",
      "---------------------\n",
      "Iteration Number: 1146\n",
      "Loss: 78.74383019322946\n",
      "l2 norm of gradients: 0.8869231394373209\n",
      "l2 norm of weights: 6.389964151234418\n",
      "---------------------\n",
      "Iteration Number: 1147\n",
      "Loss: 78.70538514850706\n",
      "l2 norm of gradients: 0.8864072073026914\n",
      "l2 norm of weights: 6.389757155931608\n",
      "---------------------\n",
      "Iteration Number: 1148\n",
      "Loss: 78.66698454995988\n",
      "l2 norm of gradients: 0.8858915417314842\n",
      "l2 norm of weights: 6.389550277911617\n",
      "---------------------\n",
      "Iteration Number: 1149\n",
      "Loss: 78.62862834856827\n",
      "l2 norm of gradients: 0.8853761430652342\n",
      "l2 norm of weights: 6.389343517033398\n",
      "---------------------\n",
      "Iteration Number: 1150\n",
      "Loss: 78.59031649533637\n",
      "l2 norm of gradients: 0.8848610116490611\n",
      "l2 norm of weights: 6.389136873156025\n",
      "---------------------\n",
      "Iteration Number: 1151\n",
      "Loss: 78.55204894135565\n",
      "l2 norm of gradients: 0.8843461478315873\n",
      "l2 norm of weights: 6.3889303461386975\n",
      "---------------------\n",
      "Iteration Number: 1152\n",
      "Loss: 78.51382563748035\n",
      "l2 norm of gradients: 0.8838315519648537\n",
      "l2 norm of weights: 6.388723935840728\n",
      "---------------------\n",
      "Iteration Number: 1153\n",
      "Loss: 78.47564653490099\n",
      "l2 norm of gradients: 0.8833172244042378\n",
      "l2 norm of weights: 6.388517642121557\n",
      "---------------------\n",
      "Iteration Number: 1154\n",
      "Loss: 78.43751158450262\n",
      "l2 norm of gradients: 0.8828031655083673\n",
      "l2 norm of weights: 6.388311464840741\n",
      "---------------------\n",
      "Iteration Number: 1155\n",
      "Loss: 78.399420737422\n",
      "l2 norm of gradients: 0.8822893756390406\n",
      "l2 norm of weights: 6.3881054038579625\n",
      "---------------------\n",
      "Iteration Number: 1156\n",
      "Loss: 78.36137394479076\n",
      "l2 norm of gradients: 0.8817758551611408\n",
      "l2 norm of weights: 6.387899459033022\n",
      "---------------------\n",
      "Iteration Number: 1157\n",
      "Loss: 78.3233711575328\n",
      "l2 norm of gradients: 0.8812626044425536\n",
      "l2 norm of weights: 6.387693630225846\n",
      "---------------------\n",
      "Iteration Number: 1158\n",
      "Loss: 78.2854123268101\n",
      "l2 norm of gradients: 0.880749623854085\n",
      "l2 norm of weights: 6.387487917296482\n",
      "---------------------\n",
      "Iteration Number: 1159\n",
      "Loss: 78.2474974037156\n",
      "l2 norm of gradients: 0.880236913769378\n",
      "l2 norm of weights: 6.387282320105095\n",
      "---------------------\n",
      "Iteration Number: 1160\n",
      "Loss: 78.20962633934698\n",
      "l2 norm of gradients: 0.8797244745648303\n",
      "l2 norm of weights: 6.387076838511982\n",
      "---------------------\n",
      "Iteration Number: 1161\n",
      "Loss: 78.17179908482251\n",
      "l2 norm of gradients: 0.8792123066195117\n",
      "l2 norm of weights: 6.386871472377556\n",
      "---------------------\n",
      "Iteration Number: 1162\n",
      "Loss: 78.13401559125924\n",
      "l2 norm of gradients: 0.8787004103150818\n",
      "l2 norm of weights: 6.386666221562358\n",
      "---------------------\n",
      "Iteration Number: 1163\n",
      "Loss: 78.09627580972342\n",
      "l2 norm of gradients: 0.8781887860357096\n",
      "l2 norm of weights: 6.386461085927049\n",
      "---------------------\n",
      "Iteration Number: 1164\n",
      "Loss: 78.05857969141934\n",
      "l2 norm of gradients: 0.8776774341679893\n",
      "l2 norm of weights: 6.386256065332417\n",
      "---------------------\n",
      "Iteration Number: 1165\n",
      "Loss: 78.02092718749626\n",
      "l2 norm of gradients: 0.8771663551008605\n",
      "l2 norm of weights: 6.386051159639372\n",
      "---------------------\n",
      "Iteration Number: 1166\n",
      "Loss: 77.98331824911736\n",
      "l2 norm of gradients: 0.8766555492255271\n",
      "l2 norm of weights: 6.38584636870895\n",
      "---------------------\n",
      "Iteration Number: 1167\n",
      "Loss: 77.94575282737594\n",
      "l2 norm of gradients: 0.8761450169353759\n",
      "l2 norm of weights: 6.385641692402311\n",
      "---------------------\n",
      "Iteration Number: 1168\n",
      "Loss: 77.90823087352149\n",
      "l2 norm of gradients: 0.8756347586258955\n",
      "l2 norm of weights: 6.3854371305807405\n",
      "---------------------\n",
      "Iteration Number: 1169\n",
      "Loss: 77.87075233876682\n",
      "l2 norm of gradients: 0.875124774694597\n",
      "l2 norm of weights: 6.385232683105649\n",
      "---------------------\n",
      "Iteration Number: 1170\n",
      "Loss: 77.83331717415746\n",
      "l2 norm of gradients: 0.8746150655409335\n",
      "l2 norm of weights: 6.385028349838571\n",
      "---------------------\n",
      "Iteration Number: 1171\n",
      "Loss: 77.7959253310086\n",
      "l2 norm of gradients: 0.8741056315662206\n",
      "l2 norm of weights: 6.384824130641171\n",
      "---------------------\n",
      "Iteration Number: 1172\n",
      "Loss: 77.75857676044778\n",
      "l2 norm of gradients: 0.8735964731735567\n",
      "l2 norm of weights: 6.384620025375235\n",
      "---------------------\n",
      "Iteration Number: 1173\n",
      "Loss: 77.72127141377425\n",
      "l2 norm of gradients: 0.8730875907677446\n",
      "l2 norm of weights: 6.384416033902679\n",
      "---------------------\n",
      "Iteration Number: 1174\n",
      "Loss: 77.68400924220074\n",
      "l2 norm of gradients: 0.8725789847552125\n",
      "l2 norm of weights: 6.3842121560855425\n",
      "---------------------\n",
      "Iteration Number: 1175\n",
      "Loss: 77.6467901968944\n",
      "l2 norm of gradients: 0.8720706555439361\n",
      "l2 norm of weights: 6.384008391785996\n",
      "---------------------\n",
      "Iteration Number: 1176\n",
      "Loss: 77.60961422908687\n",
      "l2 norm of gradients: 0.871562603543361\n",
      "l2 norm of weights: 6.383804740866332\n",
      "---------------------\n",
      "Iteration Number: 1177\n",
      "Loss: 77.57248129010567\n",
      "l2 norm of gradients: 0.8710548291643242\n",
      "l2 norm of weights: 6.3836012031889755\n",
      "---------------------\n",
      "Iteration Number: 1178\n",
      "Loss: 77.53539133112703\n",
      "l2 norm of gradients: 0.8705473328189791\n",
      "l2 norm of weights: 6.383397778616476\n",
      "---------------------\n",
      "Iteration Number: 1179\n",
      "Loss: 77.49834430348436\n",
      "l2 norm of gradients: 0.8700401149207173\n",
      "l2 norm of weights: 6.383194467011512\n",
      "---------------------\n",
      "Iteration Number: 1180\n",
      "Loss: 77.46134015837569\n",
      "l2 norm of gradients: 0.869533175884093\n",
      "l2 norm of weights: 6.38299126823689\n",
      "---------------------\n",
      "Iteration Number: 1181\n",
      "Loss: 77.42437884707805\n",
      "l2 norm of gradients: 0.8690265161247482\n",
      "l2 norm of weights: 6.382788182155546\n",
      "---------------------\n",
      "Iteration Number: 1182\n",
      "Loss: 77.38746032095312\n",
      "l2 norm of gradients: 0.8685201360593366\n",
      "l2 norm of weights: 6.382585208630545\n",
      "---------------------\n",
      "Iteration Number: 1183\n",
      "Loss: 77.35058453121727\n",
      "l2 norm of gradients: 0.86801403610545\n",
      "l2 norm of weights: 6.382382347525078\n",
      "---------------------\n",
      "Iteration Number: 1184\n",
      "Loss: 77.3137514292413\n",
      "l2 norm of gradients: 0.8675082166815428\n",
      "l2 norm of weights: 6.382179598702469\n",
      "---------------------\n",
      "Iteration Number: 1185\n",
      "Loss: 77.27696096618743\n",
      "l2 norm of gradients: 0.86700267820686\n",
      "l2 norm of weights: 6.38197696202617\n",
      "---------------------\n",
      "Iteration Number: 1186\n",
      "Loss: 77.24021309351271\n",
      "l2 norm of gradients: 0.866497421101363\n",
      "l2 norm of weights: 6.381774437359763\n",
      "---------------------\n",
      "Iteration Number: 1187\n",
      "Loss: 77.2035077624671\n",
      "l2 norm of gradients: 0.8659924457856576\n",
      "l2 norm of weights: 6.38157202456696\n",
      "---------------------\n",
      "Iteration Number: 1188\n",
      "Loss: 77.16684492438496\n",
      "l2 norm of gradients: 0.8654877526809224\n",
      "l2 norm of weights: 6.381369723511606\n",
      "---------------------\n",
      "Iteration Number: 1189\n",
      "Loss: 77.13022453058912\n",
      "l2 norm of gradients: 0.8649833422088362\n",
      "l2 norm of weights: 6.381167534057672\n",
      "---------------------\n",
      "Iteration Number: 1190\n",
      "Loss: 77.093646532442\n",
      "l2 norm of gradients: 0.8644792147915085\n",
      "l2 norm of weights: 6.380965456069262\n",
      "---------------------\n",
      "Iteration Number: 1191\n",
      "Loss: 77.05711088135241\n",
      "l2 norm of gradients: 0.8639753708514087\n",
      "l2 norm of weights: 6.380763489410613\n",
      "---------------------\n",
      "Iteration Number: 1192\n",
      "Loss: 77.02061752853916\n",
      "l2 norm of gradients: 0.8634718108112965\n",
      "l2 norm of weights: 6.380561633946092\n",
      "---------------------\n",
      "Iteration Number: 1193\n",
      "Loss: 76.98416642551899\n",
      "l2 norm of gradients: 0.862968535094153\n",
      "l2 norm of weights: 6.380359889540197\n",
      "---------------------\n",
      "Iteration Number: 1194\n",
      "Loss: 76.94775752350333\n",
      "l2 norm of gradients: 0.8624655441231115\n",
      "l2 norm of weights: 6.38015825605756\n",
      "---------------------\n",
      "Iteration Number: 1195\n",
      "Loss: 76.91139077403894\n",
      "l2 norm of gradients: 0.8619628383213911\n",
      "l2 norm of weights: 6.379956733362942\n",
      "---------------------\n",
      "Iteration Number: 1196\n",
      "Loss: 76.87506612839748\n",
      "l2 norm of gradients: 0.8614604181122287\n",
      "l2 norm of weights: 6.379755321321239\n",
      "---------------------\n",
      "Iteration Number: 1197\n",
      "Loss: 76.83878353797192\n",
      "l2 norm of gradients: 0.8609582839188128\n",
      "l2 norm of weights: 6.379554019797482\n",
      "---------------------\n",
      "Iteration Number: 1198\n",
      "Loss: 76.80254295420772\n",
      "l2 norm of gradients: 0.8604564361642172\n",
      "l2 norm of weights: 6.379352828656828\n",
      "---------------------\n",
      "Iteration Number: 1199\n",
      "Loss: 76.76634432851664\n",
      "l2 norm of gradients: 0.8599548752713362\n",
      "l2 norm of weights: 6.379151747764575\n",
      "---------------------\n",
      "Iteration Number: 1200\n",
      "Loss: 76.73018761231714\n",
      "l2 norm of gradients: 0.8594536016628203\n",
      "l2 norm of weights: 6.378950776986148\n",
      "---------------------\n",
      "Iteration Number: 1201\n",
      "Loss: 76.69407275703914\n",
      "l2 norm of gradients: 0.8589526157610121\n",
      "l2 norm of weights: 6.378749916187111\n",
      "---------------------\n",
      "Iteration Number: 1202\n",
      "Loss: 76.65799971404446\n",
      "l2 norm of gradients: 0.8584519179878833\n",
      "l2 norm of weights: 6.378549165233158\n",
      "---------------------\n",
      "Iteration Number: 1203\n",
      "Loss: 76.62196843485286\n",
      "l2 norm of gradients: 0.8579515087649726\n",
      "l2 norm of weights: 6.3783485239901205\n",
      "---------------------\n",
      "Iteration Number: 1204\n",
      "Loss: 76.585978870915\n",
      "l2 norm of gradients: 0.8574513885133226\n",
      "l2 norm of weights: 6.378147992323962\n",
      "---------------------\n",
      "Iteration Number: 1205\n",
      "Loss: 76.55003097369645\n",
      "l2 norm of gradients: 0.8569515576534212\n",
      "l2 norm of weights: 6.377947570100782\n",
      "---------------------\n",
      "Iteration Number: 1206\n",
      "Loss: 76.51412469459434\n",
      "l2 norm of gradients: 0.8564520166051394\n",
      "l2 norm of weights: 6.3777472571868135\n",
      "---------------------\n",
      "Iteration Number: 1207\n",
      "Loss: 76.47825998507244\n",
      "l2 norm of gradients: 0.8559527657876727\n",
      "l2 norm of weights: 6.377547053448428\n",
      "---------------------\n",
      "Iteration Number: 1208\n",
      "Loss: 76.44243679677943\n",
      "l2 norm of gradients: 0.8554538056194815\n",
      "l2 norm of weights: 6.377346958752126\n",
      "---------------------\n",
      "Iteration Number: 1209\n",
      "Loss: 76.4066550810602\n",
      "l2 norm of gradients: 0.8549551365182347\n",
      "l2 norm of weights: 6.377146972964552\n",
      "---------------------\n",
      "Iteration Number: 1210\n",
      "Loss: 76.37091478942058\n",
      "l2 norm of gradients: 0.8544567589007509\n",
      "l2 norm of weights: 6.37694709595248\n",
      "---------------------\n",
      "Iteration Number: 1211\n",
      "Loss: 76.33521587343851\n",
      "l2 norm of gradients: 0.8539586731829423\n",
      "l2 norm of weights: 6.376747327582823\n",
      "---------------------\n",
      "Iteration Number: 1212\n",
      "Loss: 76.299558284652\n",
      "l2 norm of gradients: 0.8534608797797597\n",
      "l2 norm of weights: 6.37654766772263\n",
      "---------------------\n",
      "Iteration Number: 1213\n",
      "Loss: 76.26394197450807\n",
      "l2 norm of gradients: 0.8529633791051369\n",
      "l2 norm of weights: 6.376348116239084\n",
      "---------------------\n",
      "Iteration Number: 1214\n",
      "Loss: 76.22836689455316\n",
      "l2 norm of gradients: 0.8524661715719359\n",
      "l2 norm of weights: 6.376148672999508\n",
      "---------------------\n",
      "Iteration Number: 1215\n",
      "Loss: 76.19283299640847\n",
      "l2 norm of gradients: 0.8519692575918958\n",
      "l2 norm of weights: 6.375949337871362\n",
      "---------------------\n",
      "Iteration Number: 1216\n",
      "Loss: 76.15734023156955\n",
      "l2 norm of gradients: 0.8514726375755773\n",
      "l2 norm of weights: 6.37575011072224\n",
      "---------------------\n",
      "Iteration Number: 1217\n",
      "Loss: 76.12188855159509\n",
      "l2 norm of gradients: 0.8509763119323135\n",
      "l2 norm of weights: 6.3755509914198765\n",
      "---------------------\n",
      "Iteration Number: 1218\n",
      "Loss: 76.08647790814616\n",
      "l2 norm of gradients: 0.8504802810701578\n",
      "l2 norm of weights: 6.375351979832143\n",
      "---------------------\n",
      "Iteration Number: 1219\n",
      "Loss: 76.05110825273898\n",
      "l2 norm of gradients: 0.8499845453958337\n",
      "l2 norm of weights: 6.375153075827046\n",
      "---------------------\n",
      "Iteration Number: 1220\n",
      "Loss: 76.01577953697138\n",
      "l2 norm of gradients: 0.8494891053146851\n",
      "l2 norm of weights: 6.374954279272735\n",
      "---------------------\n",
      "Iteration Number: 1221\n",
      "Loss: 75.98049171246385\n",
      "l2 norm of gradients: 0.8489939612306296\n",
      "l2 norm of weights: 6.374755590037494\n",
      "---------------------\n",
      "Iteration Number: 1222\n",
      "Loss: 75.9452447309418\n",
      "l2 norm of gradients: 0.8484991135461091\n",
      "l2 norm of weights: 6.374557007989747\n",
      "---------------------\n",
      "Iteration Number: 1223\n",
      "Loss: 75.91003854382214\n",
      "l2 norm of gradients: 0.8480045626620432\n",
      "l2 norm of weights: 6.374358532998054\n",
      "---------------------\n",
      "Iteration Number: 1224\n",
      "Loss: 75.87487310292782\n",
      "l2 norm of gradients: 0.8475103089777841\n",
      "l2 norm of weights: 6.374160164931117\n",
      "---------------------\n",
      "Iteration Number: 1225\n",
      "Loss: 75.83974835984633\n",
      "l2 norm of gradients: 0.8470163528910709\n",
      "l2 norm of weights: 6.3739619036577775\n",
      "---------------------\n",
      "Iteration Number: 1226\n",
      "Loss: 75.804664266241\n",
      "l2 norm of gradients: 0.8465226947979855\n",
      "l2 norm of weights: 6.373763749047012\n",
      "---------------------\n",
      "Iteration Number: 1227\n",
      "Loss: 75.76962077379504\n",
      "l2 norm of gradients: 0.8460293350929082\n",
      "l2 norm of weights: 6.37356570096794\n",
      "---------------------\n",
      "Iteration Number: 1228\n",
      "Loss: 75.73461783418537\n",
      "l2 norm of gradients: 0.8455362741684777\n",
      "l2 norm of weights: 6.37336775928982\n",
      "---------------------\n",
      "Iteration Number: 1229\n",
      "Loss: 75.69965539905888\n",
      "l2 norm of gradients: 0.8450435124155453\n",
      "l2 norm of weights: 6.373169923882048\n",
      "---------------------\n",
      "Iteration Number: 1230\n",
      "Loss: 75.66473342026566\n",
      "l2 norm of gradients: 0.8445510502231383\n",
      "l2 norm of weights: 6.372972194614164\n",
      "---------------------\n",
      "Iteration Number: 1231\n",
      "Loss: 75.62985184946054\n",
      "l2 norm of gradients: 0.8440588879784168\n",
      "l2 norm of weights: 6.372774571355842\n",
      "---------------------\n",
      "Iteration Number: 1232\n",
      "Loss: 75.59501063830642\n",
      "l2 norm of gradients: 0.843567026066637\n",
      "l2 norm of weights: 6.372577053976903\n",
      "---------------------\n",
      "Iteration Number: 1233\n",
      "Loss: 75.56020973870743\n",
      "l2 norm of gradients: 0.8430754648711104\n",
      "l2 norm of weights: 6.372379642347305\n",
      "---------------------\n",
      "Iteration Number: 1234\n",
      "Loss: 75.52544910227965\n",
      "l2 norm of gradients: 0.8425842047731686\n",
      "l2 norm of weights: 6.3721823363371435\n",
      "---------------------\n",
      "Iteration Number: 1235\n",
      "Loss: 75.49072868079878\n",
      "l2 norm of gradients: 0.8420932461521256\n",
      "l2 norm of weights: 6.3719851358166615\n",
      "---------------------\n",
      "Iteration Number: 1236\n",
      "Loss: 75.45604842620199\n",
      "l2 norm of gradients: 0.8416025893852425\n",
      "l2 norm of weights: 6.371788040656238\n",
      "---------------------\n",
      "Iteration Number: 1237\n",
      "Loss: 75.42140829014875\n",
      "l2 norm of gradients: 0.8411122348476918\n",
      "l2 norm of weights: 6.371591050726395\n",
      "---------------------\n",
      "Iteration Number: 1238\n",
      "Loss: 75.3868082245348\n",
      "l2 norm of gradients: 0.8406221829125254\n",
      "l2 norm of weights: 6.371394165897794\n",
      "---------------------\n",
      "Iteration Number: 1239\n",
      "Loss: 75.35224818118468\n",
      "l2 norm of gradients: 0.8401324339506399\n",
      "l2 norm of weights: 6.371197386041239\n",
      "---------------------\n",
      "Iteration Number: 1240\n",
      "Loss: 75.31772811187378\n",
      "l2 norm of gradients: 0.8396429883307444\n",
      "l2 norm of weights: 6.371000711027676\n",
      "---------------------\n",
      "Iteration Number: 1241\n",
      "Loss: 75.28324796846756\n",
      "l2 norm of gradients: 0.8391538464193301\n",
      "l2 norm of weights: 6.370804140728192\n",
      "---------------------\n",
      "Iteration Number: 1242\n",
      "Loss: 75.24880770298113\n",
      "l2 norm of gradients: 0.8386650085806397\n",
      "l2 norm of weights: 6.370607675014016\n",
      "---------------------\n",
      "Iteration Number: 1243\n",
      "Loss: 75.21440726709827\n",
      "l2 norm of gradients: 0.8381764751766375\n",
      "l2 norm of weights: 6.370411313756517\n",
      "---------------------\n",
      "Iteration Number: 1244\n",
      "Loss: 75.18004661285738\n",
      "l2 norm of gradients: 0.8376882465669803\n",
      "l2 norm of weights: 6.370215056827209\n",
      "---------------------\n",
      "Iteration Number: 1245\n",
      "Loss: 75.14572569217789\n",
      "l2 norm of gradients: 0.8372003231089901\n",
      "l2 norm of weights: 6.370018904097745\n",
      "---------------------\n",
      "Iteration Number: 1246\n",
      "Loss: 75.11144445692972\n",
      "l2 norm of gradients: 0.8367127051576276\n",
      "l2 norm of weights: 6.369822855439926\n",
      "---------------------\n",
      "Iteration Number: 1247\n",
      "Loss: 75.07720285909765\n",
      "l2 norm of gradients: 0.8362253930654653\n",
      "l2 norm of weights: 6.369626910725687\n",
      "---------------------\n",
      "Iteration Number: 1248\n",
      "Loss: 75.04300085065098\n",
      "l2 norm of gradients: 0.8357383871826628\n",
      "l2 norm of weights: 6.369431069827112\n",
      "---------------------\n",
      "Iteration Number: 1249\n",
      "Loss: 75.00883838353556\n",
      "l2 norm of gradients: 0.8352516878569419\n",
      "l2 norm of weights: 6.369235332616425\n",
      "---------------------\n",
      "Iteration Number: 1250\n",
      "Loss: 74.97471540975843\n",
      "l2 norm of gradients: 0.8347652954335636\n",
      "l2 norm of weights: 6.3690396989659925\n",
      "---------------------\n",
      "Iteration Number: 1251\n",
      "Loss: 74.94063188133161\n",
      "l2 norm of gradients: 0.8342792102553053\n",
      "l2 norm of weights: 6.368844168748327\n",
      "---------------------\n",
      "Iteration Number: 1252\n",
      "Loss: 74.90658775028548\n",
      "l2 norm of gradients: 0.8337934326624395\n",
      "l2 norm of weights: 6.368648741836077\n",
      "---------------------\n",
      "Iteration Number: 1253\n",
      "Loss: 74.8725829687463\n",
      "l2 norm of gradients: 0.8333079629927123\n",
      "l2 norm of weights: 6.3684534181020425\n",
      "---------------------\n",
      "Iteration Number: 1254\n",
      "Loss: 74.83861748870889\n",
      "l2 norm of gradients: 0.832822801581323\n",
      "l2 norm of weights: 6.368258197419162\n",
      "---------------------\n",
      "Iteration Number: 1255\n",
      "Loss: 74.80469126226171\n",
      "l2 norm of gradients: 0.8323379487609065\n",
      "l2 norm of weights: 6.368063079660517\n",
      "---------------------\n",
      "Iteration Number: 1256\n",
      "Loss: 74.77080424144113\n",
      "l2 norm of gradients: 0.831853404861513\n",
      "l2 norm of weights: 6.367868064699333\n",
      "---------------------\n",
      "Iteration Number: 1257\n",
      "Loss: 74.73695637849852\n",
      "l2 norm of gradients: 0.8313691702105922\n",
      "l2 norm of weights: 6.367673152408979\n",
      "---------------------\n",
      "Iteration Number: 1258\n",
      "Loss: 74.70314762551187\n",
      "l2 norm of gradients: 0.8308852451329755\n",
      "l2 norm of weights: 6.367478342662968\n",
      "---------------------\n",
      "Iteration Number: 1259\n",
      "Loss: 74.6693779346408\n",
      "l2 norm of gradients: 0.8304016299508603\n",
      "l2 norm of weights: 6.367283635334956\n",
      "---------------------\n",
      "Iteration Number: 1260\n",
      "Loss: 74.63564725805952\n",
      "l2 norm of gradients: 0.8299183249837957\n",
      "l2 norm of weights: 6.367089030298744\n",
      "---------------------\n",
      "Iteration Number: 1261\n",
      "Loss: 74.60195554789219\n",
      "l2 norm of gradients: 0.8294353305486682\n",
      "l2 norm of weights: 6.366894527428273\n",
      "---------------------\n",
      "Iteration Number: 1262\n",
      "Loss: 74.56830275642557\n",
      "l2 norm of gradients: 0.8289526469596876\n",
      "l2 norm of weights: 6.366700126597633\n",
      "---------------------\n",
      "Iteration Number: 1263\n",
      "Loss: 74.53468883590031\n",
      "l2 norm of gradients: 0.8284702745283757\n",
      "l2 norm of weights: 6.366505827681052\n",
      "---------------------\n",
      "Iteration Number: 1264\n",
      "Loss: 74.50111373856389\n",
      "l2 norm of gradients: 0.8279882135635531\n",
      "l2 norm of weights: 6.366311630552907\n",
      "---------------------\n",
      "Iteration Number: 1265\n",
      "Loss: 74.46757741669522\n",
      "l2 norm of gradients: 0.8275064643713301\n",
      "l2 norm of weights: 6.366117535087718\n",
      "---------------------\n",
      "Iteration Number: 1266\n",
      "Loss: 74.43407982251267\n",
      "l2 norm of gradients: 0.8270250272550954\n",
      "l2 norm of weights: 6.3659235411601465\n",
      "---------------------\n",
      "Iteration Number: 1267\n",
      "Loss: 74.40062090841126\n",
      "l2 norm of gradients: 0.8265439025155074\n",
      "l2 norm of weights: 6.365729648645\n",
      "---------------------\n",
      "Iteration Number: 1268\n",
      "Loss: 74.36720062665023\n",
      "l2 norm of gradients: 0.8260630904504851\n",
      "l2 norm of weights: 6.365535857417229\n",
      "---------------------\n",
      "Iteration Number: 1269\n",
      "Loss: 74.33381892973291\n",
      "l2 norm of gradients: 0.8255825913552014\n",
      "l2 norm of weights: 6.36534216735193\n",
      "---------------------\n",
      "Iteration Number: 1270\n",
      "Loss: 74.30047576982193\n",
      "l2 norm of gradients: 0.8251024055220759\n",
      "l2 norm of weights: 6.365148578324343\n",
      "---------------------\n",
      "Iteration Number: 1271\n",
      "Loss: 74.26717109949641\n",
      "l2 norm of gradients: 0.8246225332407682\n",
      "l2 norm of weights: 6.36495509020985\n",
      "---------------------\n",
      "Iteration Number: 1272\n",
      "Loss: 74.23390487109931\n",
      "l2 norm of gradients: 0.8241429747981732\n",
      "l2 norm of weights: 6.364761702883982\n",
      "---------------------\n",
      "Iteration Number: 1273\n",
      "Loss: 74.20067703704832\n",
      "l2 norm of gradients: 0.8236637304784167\n",
      "l2 norm of weights: 6.36456841622241\n",
      "---------------------\n",
      "Iteration Number: 1274\n",
      "Loss: 74.16748754980004\n",
      "l2 norm of gradients: 0.8231848005628508\n",
      "l2 norm of weights: 6.364375230100951\n",
      "---------------------\n",
      "Iteration Number: 1275\n",
      "Loss: 74.13433636191984\n",
      "l2 norm of gradients: 0.8227061853300518\n",
      "l2 norm of weights: 6.364182144395566\n",
      "---------------------\n",
      "Iteration Number: 1276\n",
      "Loss: 74.10122342584677\n",
      "l2 norm of gradients: 0.822227885055817\n",
      "l2 norm of weights: 6.3639891589823625\n",
      "---------------------\n",
      "Iteration Number: 1277\n",
      "Loss: 74.06814869410636\n",
      "l2 norm of gradients: 0.8217499000131644\n",
      "l2 norm of weights: 6.363796273737589\n",
      "---------------------\n",
      "Iteration Number: 1278\n",
      "Loss: 74.035112119268\n",
      "l2 norm of gradients: 0.8212722304723301\n",
      "l2 norm of weights: 6.363603488537641\n",
      "---------------------\n",
      "Iteration Number: 1279\n",
      "Loss: 74.00211365396979\n",
      "l2 norm of gradients: 0.82079487670077\n",
      "l2 norm of weights: 6.3634108032590575\n",
      "---------------------\n",
      "Iteration Number: 1280\n",
      "Loss: 73.96915325075189\n",
      "l2 norm of gradients: 0.8203178389631598\n",
      "l2 norm of weights: 6.363218217778522\n",
      "---------------------\n",
      "Iteration Number: 1281\n",
      "Loss: 73.93623086223573\n",
      "l2 norm of gradients: 0.8198411175213954\n",
      "l2 norm of weights: 6.363025731972863\n",
      "---------------------\n",
      "Iteration Number: 1282\n",
      "Loss: 73.9033464410809\n",
      "l2 norm of gradients: 0.8193647126345968\n",
      "l2 norm of weights: 6.362833345719053\n",
      "---------------------\n",
      "Iteration Number: 1283\n",
      "Loss: 73.87049993995113\n",
      "l2 norm of gradients: 0.8188886245591087\n",
      "l2 norm of weights: 6.3626410588942095\n",
      "---------------------\n",
      "Iteration Number: 1284\n",
      "Loss: 73.83769131155351\n",
      "l2 norm of gradients: 0.8184128535485062\n",
      "l2 norm of weights: 6.362448871375594\n",
      "---------------------\n",
      "Iteration Number: 1285\n",
      "Loss: 73.8049205086113\n",
      "l2 norm of gradients: 0.8179373998535968\n",
      "l2 norm of weights: 6.362256783040613\n",
      "---------------------\n",
      "Iteration Number: 1286\n",
      "Loss: 73.77218748387533\n",
      "l2 norm of gradients: 0.8174622637224267\n",
      "l2 norm of weights: 6.362064793766815\n",
      "---------------------\n",
      "Iteration Number: 1287\n",
      "Loss: 73.73949219005054\n",
      "l2 norm of gradients: 0.8169874454002856\n",
      "l2 norm of weights: 6.361872903431899\n",
      "---------------------\n",
      "Iteration Number: 1288\n",
      "Loss: 73.70683458007149\n",
      "l2 norm of gradients: 0.8165129451297128\n",
      "l2 norm of weights: 6.3616811119137004\n",
      "---------------------\n",
      "Iteration Number: 1289\n",
      "Loss: 73.67421460662905\n",
      "l2 norm of gradients: 0.8160387631505039\n",
      "l2 norm of weights: 6.361489419090207\n",
      "---------------------\n",
      "Iteration Number: 1290\n",
      "Loss: 73.64163222271826\n",
      "l2 norm of gradients: 0.8155648996997189\n",
      "l2 norm of weights: 6.361297824839545\n",
      "---------------------\n",
      "Iteration Number: 1291\n",
      "Loss: 73.60908738099592\n",
      "l2 norm of gradients: 0.8150913550116892\n",
      "l2 norm of weights: 6.361106329039988\n",
      "---------------------\n",
      "Iteration Number: 1292\n",
      "Loss: 73.57658003458567\n",
      "l2 norm of gradients: 0.8146181293180266\n",
      "l2 norm of weights: 6.360914931569953\n",
      "---------------------\n",
      "Iteration Number: 1293\n",
      "Loss: 73.5441101362733\n",
      "l2 norm of gradients: 0.8141452228476329\n",
      "l2 norm of weights: 6.3607236323080025\n",
      "---------------------\n",
      "Iteration Number: 1294\n",
      "Loss: 73.5116776390866\n",
      "l2 norm of gradients: 0.8136726358267097\n",
      "l2 norm of weights: 6.3605324311328415\n",
      "---------------------\n",
      "Iteration Number: 1295\n",
      "Loss: 73.47928249598274\n",
      "l2 norm of gradients: 0.8132003684787675\n",
      "l2 norm of weights: 6.36034132792332\n",
      "---------------------\n",
      "Iteration Number: 1296\n",
      "Loss: 73.44692465996172\n",
      "l2 norm of gradients: 0.8127284210246396\n",
      "l2 norm of weights: 6.360150322558433\n",
      "---------------------\n",
      "Iteration Number: 1297\n",
      "Loss: 73.41460408410133\n",
      "l2 norm of gradients: 0.8122567936824905\n",
      "l2 norm of weights: 6.359959414917321\n",
      "---------------------\n",
      "Iteration Number: 1298\n",
      "Loss: 73.38232072139962\n",
      "l2 norm of gradients: 0.8117854866678305\n",
      "l2 norm of weights: 6.359768604879266\n",
      "---------------------\n",
      "Iteration Number: 1299\n",
      "Loss: 73.35007452495168\n",
      "l2 norm of gradients: 0.8113145001935261\n",
      "l2 norm of weights: 6.359577892323692\n",
      "---------------------\n",
      "Iteration Number: 1300\n",
      "Loss: 73.31786544795307\n",
      "l2 norm of gradients: 0.810843834469816\n",
      "l2 norm of weights: 6.359387277130174\n",
      "---------------------\n",
      "Iteration Number: 1301\n",
      "Loss: 73.28569344347939\n",
      "l2 norm of gradients: 0.8103734897043225\n",
      "l2 norm of weights: 6.359196759178426\n",
      "---------------------\n",
      "Iteration Number: 1302\n",
      "Loss: 73.25355846470956\n",
      "l2 norm of gradients: 0.8099034661020676\n",
      "l2 norm of weights: 6.359006338348307\n",
      "---------------------\n",
      "Iteration Number: 1303\n",
      "Loss: 73.22146046485402\n",
      "l2 norm of gradients: 0.8094337638654863\n",
      "l2 norm of weights: 6.358816014519819\n",
      "---------------------\n",
      "Iteration Number: 1304\n",
      "Loss: 73.189399397219\n",
      "l2 norm of gradients: 0.8089643831944429\n",
      "l2 norm of weights: 6.3586257875731125\n",
      "---------------------\n",
      "Iteration Number: 1305\n",
      "Loss: 73.15737521500036\n",
      "l2 norm of gradients: 0.8084953242862463\n",
      "l2 norm of weights: 6.358435657388474\n",
      "---------------------\n",
      "Iteration Number: 1306\n",
      "Loss: 73.12538787149522\n",
      "l2 norm of gradients: 0.8080265873356673\n",
      "l2 norm of weights: 6.358245623846342\n",
      "---------------------\n",
      "Iteration Number: 1307\n",
      "Loss: 73.09343732002898\n",
      "l2 norm of gradients: 0.8075581725349541\n",
      "l2 norm of weights: 6.358055686827291\n",
      "---------------------\n",
      "Iteration Number: 1308\n",
      "Loss: 73.06152351394881\n",
      "l2 norm of gradients: 0.8070900800738504\n",
      "l2 norm of weights: 6.357865846212047\n",
      "---------------------\n",
      "Iteration Number: 1309\n",
      "Loss: 73.02964640664598\n",
      "l2 norm of gradients: 0.8066223101396126\n",
      "l2 norm of weights: 6.357676101881473\n",
      "---------------------\n",
      "Iteration Number: 1310\n",
      "Loss: 72.99780595153659\n",
      "l2 norm of gradients: 0.8061548629170285\n",
      "l2 norm of weights: 6.357486453716577\n",
      "---------------------\n",
      "Iteration Number: 1311\n",
      "Loss: 72.96600210205709\n",
      "l2 norm of gradients: 0.8056877385884356\n",
      "l2 norm of weights: 6.357296901598514\n",
      "---------------------\n",
      "Iteration Number: 1312\n",
      "Loss: 72.93423481161862\n",
      "l2 norm of gradients: 0.8052209373337398\n",
      "l2 norm of weights: 6.3571074454085785\n",
      "---------------------\n",
      "Iteration Number: 1313\n",
      "Loss: 72.90250403386656\n",
      "l2 norm of gradients: 0.804754459330435\n",
      "l2 norm of weights: 6.356918085028209\n",
      "---------------------\n",
      "Iteration Number: 1314\n",
      "Loss: 72.87080972219717\n",
      "l2 norm of gradients: 0.8042883047536241\n",
      "l2 norm of weights: 6.356728820338988\n",
      "---------------------\n",
      "Iteration Number: 1315\n",
      "Loss: 72.8391518303144\n",
      "l2 norm of gradients: 0.8038224737760368\n",
      "l2 norm of weights: 6.356539651222641\n",
      "---------------------\n",
      "Iteration Number: 1316\n",
      "Loss: 72.80753031161191\n",
      "l2 norm of gradients: 0.8033569665680524\n",
      "l2 norm of weights: 6.356350577561035\n",
      "---------------------\n",
      "Iteration Number: 1317\n",
      "Loss: 72.77594511996398\n",
      "l2 norm of gradients: 0.8028917832977197\n",
      "l2 norm of weights: 6.356161599236184\n",
      "---------------------\n",
      "Iteration Number: 1318\n",
      "Loss: 72.74439620881992\n",
      "l2 norm of gradients: 0.8024269241307783\n",
      "l2 norm of weights: 6.355972716130241\n",
      "---------------------\n",
      "Iteration Number: 1319\n",
      "Loss: 72.71288353197599\n",
      "l2 norm of gradients: 0.801962389230681\n",
      "l2 norm of weights: 6.3557839281255\n",
      "---------------------\n",
      "Iteration Number: 1320\n",
      "Loss: 72.68140704309872\n",
      "l2 norm of gradients: 0.801498178758615\n",
      "l2 norm of weights: 6.355595235104404\n",
      "---------------------\n",
      "Iteration Number: 1321\n",
      "Loss: 72.64996669600752\n",
      "l2 norm of gradients: 0.8010342928735245\n",
      "l2 norm of weights: 6.355406636949533\n",
      "---------------------\n",
      "Iteration Number: 1322\n",
      "Loss: 72.61856244441599\n",
      "l2 norm of gradients: 0.800570731732134\n",
      "l2 norm of weights: 6.355218133543612\n",
      "---------------------\n",
      "Iteration Number: 1323\n",
      "Loss: 72.58719424215705\n",
      "l2 norm of gradients: 0.8001074954889702\n",
      "l2 norm of weights: 6.3550297247695084\n",
      "---------------------\n",
      "Iteration Number: 1324\n",
      "Loss: 72.55586204315098\n",
      "l2 norm of gradients: 0.7996445842963857\n",
      "l2 norm of weights: 6.354841410510231\n",
      "---------------------\n",
      "Iteration Number: 1325\n",
      "Loss: 72.52456580121671\n",
      "l2 norm of gradients: 0.7991819983045831\n",
      "l2 norm of weights: 6.354653190648931\n",
      "---------------------\n",
      "Iteration Number: 1326\n",
      "Loss: 72.49330547034283\n",
      "l2 norm of gradients: 0.7987197376616377\n",
      "l2 norm of weights: 6.354465065068902\n",
      "---------------------\n",
      "Iteration Number: 1327\n",
      "Loss: 72.46208100440427\n",
      "l2 norm of gradients: 0.7982578025135229\n",
      "l2 norm of weights: 6.354277033653578\n",
      "---------------------\n",
      "Iteration Number: 1328\n",
      "Loss: 72.43089235740406\n",
      "l2 norm of gradients: 0.7977961930041331\n",
      "l2 norm of weights: 6.354089096286538\n",
      "---------------------\n",
      "Iteration Number: 1329\n",
      "Loss: 72.39973948335266\n",
      "l2 norm of gradients: 0.7973349092753099\n",
      "l2 norm of weights: 6.3539012528515\n",
      "---------------------\n",
      "Iteration Number: 1330\n",
      "Loss: 72.36862233630742\n",
      "l2 norm of gradients: 0.7968739514668656\n",
      "l2 norm of weights: 6.353713503232325\n",
      "---------------------\n",
      "Iteration Number: 1331\n",
      "Loss: 72.33754087034728\n",
      "l2 norm of gradients: 0.7964133197166087\n",
      "l2 norm of weights: 6.353525847313015\n",
      "---------------------\n",
      "Iteration Number: 1332\n",
      "Loss: 72.30649503959603\n",
      "l2 norm of gradients: 0.795953014160369\n",
      "l2 norm of weights: 6.3533382849777125\n",
      "---------------------\n",
      "Iteration Number: 1333\n",
      "Loss: 72.27548479821245\n",
      "l2 norm of gradients: 0.7954930349320238\n",
      "l2 norm of weights: 6.353150816110705\n",
      "---------------------\n",
      "Iteration Number: 1334\n",
      "Loss: 72.24451010045034\n",
      "l2 norm of gradients: 0.7950333821635223\n",
      "l2 norm of weights: 6.352963440596415\n",
      "---------------------\n",
      "Iteration Number: 1335\n",
      "Loss: 72.21357090039365\n",
      "l2 norm of gradients: 0.7945740559849126\n",
      "l2 norm of weights: 6.352776158319411\n",
      "---------------------\n",
      "Iteration Number: 1336\n",
      "Loss: 72.18266715230851\n",
      "l2 norm of gradients: 0.794115056524367\n",
      "l2 norm of weights: 6.352588969164403\n",
      "---------------------\n",
      "Iteration Number: 1337\n",
      "Loss: 72.15179881060789\n",
      "l2 norm of gradients: 0.7936563839082079\n",
      "l2 norm of weights: 6.3524018730162375\n",
      "---------------------\n",
      "Iteration Number: 1338\n",
      "Loss: 72.12096582949572\n",
      "l2 norm of gradients: 0.7931980382609354\n",
      "l2 norm of weights: 6.352214869759905\n",
      "---------------------\n",
      "Iteration Number: 1339\n",
      "Loss: 72.09016816342236\n",
      "l2 norm of gradients: 0.7927400197052525\n",
      "l2 norm of weights: 6.352027959280535\n",
      "---------------------\n",
      "Iteration Number: 1340\n",
      "Loss: 72.05940576672798\n",
      "l2 norm of gradients: 0.7922823283620917\n",
      "l2 norm of weights: 6.351841141463399\n",
      "---------------------\n",
      "Iteration Number: 1341\n",
      "Loss: 72.02867859380233\n",
      "l2 norm of gradients: 0.7918249643506424\n",
      "l2 norm of weights: 6.351654416193907\n",
      "---------------------\n",
      "Iteration Number: 1342\n",
      "Loss: 71.99798659917991\n",
      "l2 norm of gradients: 0.7913679277883775\n",
      "l2 norm of weights: 6.35146778335761\n",
      "---------------------\n",
      "Iteration Number: 1343\n",
      "Loss: 71.9673297373579\n",
      "l2 norm of gradients: 0.7909112187910801\n",
      "l2 norm of weights: 6.3512812428402015\n",
      "---------------------\n",
      "Iteration Number: 1344\n",
      "Loss: 71.93670796281292\n",
      "l2 norm of gradients: 0.7904548374728705\n",
      "l2 norm of weights: 6.35109479452751\n",
      "---------------------\n",
      "Iteration Number: 1345\n",
      "Loss: 71.90612123010897\n",
      "l2 norm of gradients: 0.7899987839462332\n",
      "l2 norm of weights: 6.350908438305509\n",
      "---------------------\n",
      "Iteration Number: 1346\n",
      "Loss: 71.87556949392123\n",
      "l2 norm of gradients: 0.7895430583220446\n",
      "l2 norm of weights: 6.350722174060309\n",
      "---------------------\n",
      "Iteration Number: 1347\n",
      "Loss: 71.84505270883363\n",
      "l2 norm of gradients: 0.7890876607095996\n",
      "l2 norm of weights: 6.350536001678157\n",
      "---------------------\n",
      "Iteration Number: 1348\n",
      "Loss: 71.8145708295175\n",
      "l2 norm of gradients: 0.7886325912166389\n",
      "l2 norm of weights: 6.3503499210454475\n",
      "---------------------\n",
      "Iteration Number: 1349\n",
      "Loss: 71.78412381075084\n",
      "l2 norm of gradients: 0.7881778499493768\n",
      "l2 norm of weights: 6.350163932048707\n",
      "---------------------\n",
      "Iteration Number: 1350\n",
      "Loss: 71.75371160722673\n",
      "l2 norm of gradients: 0.7877234370125279\n",
      "l2 norm of weights: 6.349978034574605\n",
      "---------------------\n",
      "Iteration Number: 1351\n",
      "Loss: 71.72333417365982\n",
      "l2 norm of gradients: 0.787269352509335\n",
      "l2 norm of weights: 6.349792228509948\n",
      "---------------------\n",
      "Iteration Number: 1352\n",
      "Loss: 71.6929914650658\n",
      "l2 norm of gradients: 0.7868155965415959\n",
      "l2 norm of weights: 6.349606513741683\n",
      "---------------------\n",
      "Iteration Number: 1353\n",
      "Loss: 71.66268343611827\n",
      "l2 norm of gradients: 0.7863621692096916\n",
      "l2 norm of weights: 6.349420890156894\n",
      "---------------------\n",
      "Iteration Number: 1354\n",
      "Loss: 71.63241004176291\n",
      "l2 norm of gradients: 0.7859090706126127\n",
      "l2 norm of weights: 6.349235357642807\n",
      "---------------------\n",
      "Iteration Number: 1355\n",
      "Loss: 71.60217123694444\n",
      "l2 norm of gradients: 0.7854563008479878\n",
      "l2 norm of weights: 6.349049916086782\n",
      "---------------------\n",
      "Iteration Number: 1356\n",
      "Loss: 71.57196697662219\n",
      "l2 norm of gradients: 0.7850038600121095\n",
      "l2 norm of weights: 6.348864565376322\n",
      "---------------------\n",
      "Iteration Number: 1357\n",
      "Loss: 71.5417972158095\n",
      "l2 norm of gradients: 0.784551748199963\n",
      "l2 norm of weights: 6.348679305399063\n",
      "---------------------\n",
      "Iteration Number: 1358\n",
      "Loss: 71.51166190953712\n",
      "l2 norm of gradients: 0.7840999655052527\n",
      "l2 norm of weights: 6.348494136042785\n",
      "---------------------\n",
      "Iteration Number: 1359\n",
      "Loss: 71.48156101282252\n",
      "l2 norm of gradients: 0.7836485120204298\n",
      "l2 norm of weights: 6.348309057195401\n",
      "---------------------\n",
      "Iteration Number: 1360\n",
      "Loss: 71.45149448094806\n",
      "l2 norm of gradients: 0.783197387836719\n",
      "l2 norm of weights: 6.348124068744965\n",
      "---------------------\n",
      "Iteration Number: 1361\n",
      "Loss: 71.42146226885859\n",
      "l2 norm of gradients: 0.7827465930441453\n",
      "l2 norm of weights: 6.347939170579666\n",
      "---------------------\n",
      "Iteration Number: 1362\n",
      "Loss: 71.39146433178603\n",
      "l2 norm of gradients: 0.7822961277315622\n",
      "l2 norm of weights: 6.3477543625878345\n",
      "---------------------\n",
      "Iteration Number: 1363\n",
      "Loss: 71.36150062508399\n",
      "l2 norm of gradients: 0.781845991986678\n",
      "l2 norm of weights: 6.347569644657934\n",
      "---------------------\n",
      "Iteration Number: 1364\n",
      "Loss: 71.33157110381076\n",
      "l2 norm of gradients: 0.7813961858960823\n",
      "l2 norm of weights: 6.347385016678568\n",
      "---------------------\n",
      "Iteration Number: 1365\n",
      "Loss: 71.30167572347291\n",
      "l2 norm of gradients: 0.7809467095452729\n",
      "l2 norm of weights: 6.347200478538476\n",
      "---------------------\n",
      "Iteration Number: 1366\n",
      "Loss: 71.27181443927601\n",
      "l2 norm of gradients: 0.7804975630186829\n",
      "l2 norm of weights: 6.347016030126538\n",
      "---------------------\n",
      "Iteration Number: 1367\n",
      "Loss: 71.24198720658949\n",
      "l2 norm of gradients: 0.780048746399707\n",
      "l2 norm of weights: 6.3468316713317625\n",
      "---------------------\n",
      "Iteration Number: 1368\n",
      "Loss: 71.2121939808301\n",
      "l2 norm of gradients: 0.7796002597707272\n",
      "l2 norm of weights: 6.346647402043304\n",
      "---------------------\n",
      "Iteration Number: 1369\n",
      "Loss: 71.18243471750755\n",
      "l2 norm of gradients: 0.7791521032131405\n",
      "l2 norm of weights: 6.346463222150448\n",
      "---------------------\n",
      "Iteration Number: 1370\n",
      "Loss: 71.15270937204646\n",
      "l2 norm of gradients: 0.778704276807384\n",
      "l2 norm of weights: 6.346279131542619\n",
      "---------------------\n",
      "Iteration Number: 1371\n",
      "Loss: 71.12301790003659\n",
      "l2 norm of gradients: 0.7782567806329609\n",
      "l2 norm of weights: 6.346095130109377\n",
      "---------------------\n",
      "Iteration Number: 1372\n",
      "Loss: 71.09336025690814\n",
      "l2 norm of gradients: 0.7778096147684667\n",
      "l2 norm of weights: 6.345911217740416\n",
      "---------------------\n",
      "Iteration Number: 1373\n",
      "Loss: 71.063736398391\n",
      "l2 norm of gradients: 0.7773627792916148\n",
      "l2 norm of weights: 6.345727394325571\n",
      "---------------------\n",
      "Iteration Number: 1374\n",
      "Loss: 71.03414628005706\n",
      "l2 norm of gradients: 0.7769162742792625\n",
      "l2 norm of weights: 6.345543659754808\n",
      "---------------------\n",
      "Iteration Number: 1375\n",
      "Loss: 71.00458985758611\n",
      "l2 norm of gradients: 0.7764700998074348\n",
      "l2 norm of weights: 6.345360013918232\n",
      "---------------------\n",
      "Iteration Number: 1376\n",
      "Loss: 70.97506708668189\n",
      "l2 norm of gradients: 0.7760242559513517\n",
      "l2 norm of weights: 6.345176456706081\n",
      "---------------------\n",
      "Iteration Number: 1377\n",
      "Loss: 70.94557792303368\n",
      "l2 norm of gradients: 0.7755787427854515\n",
      "l2 norm of weights: 6.344992988008731\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 1378\n",
      "Loss: 70.91612232256279\n",
      "l2 norm of gradients: 0.7751335603834165\n",
      "l2 norm of weights: 6.344809607716692\n",
      "---------------------\n",
      "Iteration Number: 1379\n",
      "Loss: 70.8867002409596\n",
      "l2 norm of gradients: 0.7746887088181973\n",
      "l2 norm of weights: 6.344626315720609\n",
      "---------------------\n",
      "Iteration Number: 1380\n",
      "Loss: 70.85731163422072\n",
      "l2 norm of gradients: 0.7742441881620368\n",
      "l2 norm of weights: 6.3444431119112625\n",
      "---------------------\n",
      "Iteration Number: 1381\n",
      "Loss: 70.8279564581529\n",
      "l2 norm of gradients: 0.7737999984864958\n",
      "l2 norm of weights: 6.344259996179568\n",
      "---------------------\n",
      "Iteration Number: 1382\n",
      "Loss: 70.79863466862678\n",
      "l2 norm of gradients: 0.7733561398624748\n",
      "l2 norm of weights: 6.344076968416576\n",
      "---------------------\n",
      "Iteration Number: 1383\n",
      "Loss: 70.76934622179176\n",
      "l2 norm of gradients: 0.7729126123602398\n",
      "l2 norm of weights: 6.34389402851347\n",
      "---------------------\n",
      "Iteration Number: 1384\n",
      "Loss: 70.74009107352218\n",
      "l2 norm of gradients: 0.7724694160494446\n",
      "l2 norm of weights: 6.343711176361571\n",
      "---------------------\n",
      "Iteration Number: 1385\n",
      "Loss: 70.71086917987313\n",
      "l2 norm of gradients: 0.7720265509991543\n",
      "l2 norm of weights: 6.343528411852332\n",
      "---------------------\n",
      "Iteration Number: 1386\n",
      "Loss: 70.68168049701046\n",
      "l2 norm of gradients: 0.7715840172778691\n",
      "l2 norm of weights: 6.3433457348773405\n",
      "---------------------\n",
      "Iteration Number: 1387\n",
      "Loss: 70.65252498105906\n",
      "l2 norm of gradients: 0.7711418149535455\n",
      "l2 norm of weights: 6.343163145328319\n",
      "---------------------\n",
      "Iteration Number: 1388\n",
      "Loss: 70.62340258805283\n",
      "l2 norm of gradients: 0.7706999440936207\n",
      "l2 norm of weights: 6.342980643097124\n",
      "---------------------\n",
      "Iteration Number: 1389\n",
      "Loss: 70.59431327440065\n",
      "l2 norm of gradients: 0.7702584047650338\n",
      "l2 norm of weights: 6.342798228075743\n",
      "---------------------\n",
      "Iteration Number: 1390\n",
      "Loss: 70.56525699615051\n",
      "l2 norm of gradients: 0.769817197034248\n",
      "l2 norm of weights: 6.342615900156303\n",
      "---------------------\n",
      "Iteration Number: 1391\n",
      "Loss: 70.53623370970806\n",
      "l2 norm of gradients: 0.7693763209672726\n",
      "l2 norm of weights: 6.342433659231058\n",
      "---------------------\n",
      "Iteration Number: 1392\n",
      "Loss: 70.50724337133639\n",
      "l2 norm of gradients: 0.768935776629684\n",
      "l2 norm of weights: 6.342251505192401\n",
      "---------------------\n",
      "Iteration Number: 1393\n",
      "Loss: 70.47828593738961\n",
      "l2 norm of gradients: 0.7684955640866479\n",
      "l2 norm of weights: 6.342069437932855\n",
      "---------------------\n",
      "Iteration Number: 1394\n",
      "Loss: 70.449361364266\n",
      "l2 norm of gradients: 0.7680556834029392\n",
      "l2 norm of weights: 6.341887457345076\n",
      "---------------------\n",
      "Iteration Number: 1395\n",
      "Loss: 70.4204696084718\n",
      "l2 norm of gradients: 0.7676161346429636\n",
      "l2 norm of weights: 6.341705563321856\n",
      "---------------------\n",
      "Iteration Number: 1396\n",
      "Loss: 70.39161062634211\n",
      "l2 norm of gradients: 0.7671769178707772\n",
      "l2 norm of weights: 6.341523755756118\n",
      "---------------------\n",
      "Iteration Number: 1397\n",
      "Loss: 70.36278437446148\n",
      "l2 norm of gradients: 0.7667380331501068\n",
      "l2 norm of weights: 6.341342034540915\n",
      "---------------------\n",
      "Iteration Number: 1398\n",
      "Loss: 70.3339908093667\n",
      "l2 norm of gradients: 0.7662994805443705\n",
      "l2 norm of weights: 6.341160399569438\n",
      "---------------------\n",
      "Iteration Number: 1399\n",
      "Loss: 70.30522988765752\n",
      "l2 norm of gradients: 0.765861260116696\n",
      "l2 norm of weights: 6.340978850735008\n",
      "---------------------\n",
      "Iteration Number: 1400\n",
      "Loss: 70.27650156596147\n",
      "l2 norm of gradients: 0.7654233719299406\n",
      "l2 norm of weights: 6.340797387931077\n",
      "---------------------\n",
      "Iteration Number: 1401\n",
      "Loss: 70.24780580096491\n",
      "l2 norm of gradients: 0.7649858160467096\n",
      "l2 norm of weights: 6.340616011051233\n",
      "---------------------\n",
      "Iteration Number: 1402\n",
      "Loss: 70.21914254924533\n",
      "l2 norm of gradients: 0.7645485925293756\n",
      "l2 norm of weights: 6.340434719989191\n",
      "---------------------\n",
      "Iteration Number: 1403\n",
      "Loss: 70.19051176764818\n",
      "l2 norm of gradients: 0.7641117014400964\n",
      "l2 norm of weights: 6.340253514638802\n",
      "---------------------\n",
      "Iteration Number: 1404\n",
      "Loss: 70.16191341287214\n",
      "l2 norm of gradients: 0.7636751428408324\n",
      "l2 norm of weights: 6.34007239489405\n",
      "---------------------\n",
      "Iteration Number: 1405\n",
      "Loss: 70.13334744181681\n",
      "l2 norm of gradients: 0.7632389167933652\n",
      "l2 norm of weights: 6.3398913606490455\n",
      "---------------------\n",
      "Iteration Number: 1406\n",
      "Loss: 70.10481381132054\n",
      "l2 norm of gradients: 0.7628030233593145\n",
      "l2 norm of weights: 6.339710411798036\n",
      "---------------------\n",
      "Iteration Number: 1407\n",
      "Loss: 70.07631247822628\n",
      "l2 norm of gradients: 0.7623674626001549\n",
      "l2 norm of weights: 6.339529548235396\n",
      "---------------------\n",
      "Iteration Number: 1408\n",
      "Loss: 70.04784339945432\n",
      "l2 norm of gradients: 0.7619322345772331\n",
      "l2 norm of weights: 6.339348769855636\n",
      "---------------------\n",
      "Iteration Number: 1409\n",
      "Loss: 70.01940653204569\n",
      "l2 norm of gradients: 0.7614973393517839\n",
      "l2 norm of weights: 6.3391680765533955\n",
      "---------------------\n",
      "Iteration Number: 1410\n",
      "Loss: 69.99100183294428\n",
      "l2 norm of gradients: 0.7610627769849463\n",
      "l2 norm of weights: 6.338987468223443\n",
      "---------------------\n",
      "Iteration Number: 1411\n",
      "Loss: 69.96262925925197\n",
      "l2 norm of gradients: 0.7606285475377791\n",
      "l2 norm of weights: 6.338806944760682\n",
      "---------------------\n",
      "Iteration Number: 1412\n",
      "Loss: 69.93428876793129\n",
      "l2 norm of gradients: 0.7601946510712765\n",
      "l2 norm of weights: 6.338626506060146\n",
      "---------------------\n",
      "Iteration Number: 1413\n",
      "Loss: 69.9059803162328\n",
      "l2 norm of gradients: 0.759761087646383\n",
      "l2 norm of weights: 6.338446152016998\n",
      "---------------------\n",
      "Iteration Number: 1414\n",
      "Loss: 69.87770386125231\n",
      "l2 norm of gradients: 0.7593278573240076\n",
      "l2 norm of weights: 6.338265882526532\n",
      "---------------------\n",
      "Iteration Number: 1415\n",
      "Loss: 69.8494593601822\n",
      "l2 norm of gradients: 0.7588949601650395\n",
      "l2 norm of weights: 6.338085697484173\n",
      "---------------------\n",
      "Iteration Number: 1416\n",
      "Loss: 69.82124677019421\n",
      "l2 norm of gradients: 0.7584623962303605\n",
      "l2 norm of weights: 6.337905596785477\n",
      "---------------------\n",
      "Iteration Number: 1417\n",
      "Loss: 69.7930660487002\n",
      "l2 norm of gradients: 0.7580301655808598\n",
      "l2 norm of weights: 6.33772558032613\n",
      "---------------------\n",
      "Iteration Number: 1418\n",
      "Loss: 69.76491715293434\n",
      "l2 norm of gradients: 0.7575982682774465\n",
      "l2 norm of weights: 6.337545648001948\n",
      "---------------------\n",
      "Iteration Number: 1419\n",
      "Loss: 69.73680004019948\n",
      "l2 norm of gradients: 0.7571667043810631\n",
      "l2 norm of weights: 6.337365799708877\n",
      "---------------------\n",
      "Iteration Number: 1420\n",
      "Loss: 69.70871466793406\n",
      "l2 norm of gradients: 0.7567354739526987\n",
      "l2 norm of weights: 6.337186035342994\n",
      "---------------------\n",
      "Iteration Number: 1421\n",
      "Loss: 69.68066099359612\n",
      "l2 norm of gradients: 0.7563045770533995\n",
      "l2 norm of weights: 6.337006354800507\n",
      "---------------------\n",
      "Iteration Number: 1422\n",
      "Loss: 69.65263897458959\n",
      "l2 norm of gradients: 0.7558740137442822\n",
      "l2 norm of weights: 6.336826757977748\n",
      "---------------------\n",
      "Iteration Number: 1423\n",
      "Loss: 69.62464856838479\n",
      "l2 norm of gradients: 0.7554437840865454\n",
      "l2 norm of weights: 6.3366472447711875\n",
      "---------------------\n",
      "Iteration Number: 1424\n",
      "Loss: 69.59668973257804\n",
      "l2 norm of gradients: 0.7550138881414806\n",
      "l2 norm of weights: 6.3364678150774205\n",
      "---------------------\n",
      "Iteration Number: 1425\n",
      "Loss: 69.56876242475161\n",
      "l2 norm of gradients: 0.7545843259704829\n",
      "l2 norm of weights: 6.336288468793171\n",
      "---------------------\n",
      "Iteration Number: 1426\n",
      "Loss: 69.54086660245642\n",
      "l2 norm of gradients: 0.7541550976350617\n",
      "l2 norm of weights: 6.336109205815295\n",
      "---------------------\n",
      "Iteration Number: 1427\n",
      "Loss: 69.51300222333938\n",
      "l2 norm of gradients: 0.7537262031968509\n",
      "l2 norm of weights: 6.335930026040775\n",
      "---------------------\n",
      "Iteration Number: 1428\n",
      "Loss: 69.48516924515953\n",
      "l2 norm of gradients: 0.7532976427176189\n",
      "l2 norm of weights: 6.335750929366726\n",
      "---------------------\n",
      "Iteration Number: 1429\n",
      "Loss: 69.45736762558222\n",
      "l2 norm of gradients: 0.7528694162592778\n",
      "l2 norm of weights: 6.335571915690391\n",
      "---------------------\n",
      "Iteration Number: 1430\n",
      "Loss: 69.42959732236298\n",
      "l2 norm of gradients: 0.7524415238838925\n",
      "l2 norm of weights: 6.335392984909141\n",
      "---------------------\n",
      "Iteration Number: 1431\n",
      "Loss: 69.40185829331428\n",
      "l2 norm of gradients: 0.7520139656536899\n",
      "l2 norm of weights: 6.335214136920476\n",
      "---------------------\n",
      "Iteration Number: 1432\n",
      "Loss: 69.37415049633694\n",
      "l2 norm of gradients: 0.7515867416310666\n",
      "l2 norm of weights: 6.335035371622027\n",
      "---------------------\n",
      "Iteration Number: 1433\n",
      "Loss: 69.34647388924239\n",
      "l2 norm of gradients: 0.7511598518785977\n",
      "l2 norm of weights: 6.334856688911553\n",
      "---------------------\n",
      "Iteration Number: 1434\n",
      "Loss: 69.31882842988534\n",
      "l2 norm of gradients: 0.750733296459044\n",
      "l2 norm of weights: 6.334678088686939\n",
      "---------------------\n",
      "Iteration Number: 1435\n",
      "Loss: 69.29121407634334\n",
      "l2 norm of gradients: 0.7503070754353593\n",
      "l2 norm of weights: 6.3344995708462015\n",
      "---------------------\n",
      "Iteration Number: 1436\n",
      "Loss: 69.26363078654215\n",
      "l2 norm of gradients: 0.7498811888706983\n",
      "l2 norm of weights: 6.334321135287485\n",
      "---------------------\n",
      "Iteration Number: 1437\n",
      "Loss: 69.23607851845533\n",
      "l2 norm of gradients: 0.7494556368284225\n",
      "l2 norm of weights: 6.334142781909063\n",
      "---------------------\n",
      "Iteration Number: 1438\n",
      "Loss: 69.20855723025443\n",
      "l2 norm of gradients: 0.7490304193721063\n",
      "l2 norm of weights: 6.333964510609335\n",
      "---------------------\n",
      "Iteration Number: 1439\n",
      "Loss: 69.18106687993097\n",
      "l2 norm of gradients: 0.7486055365655433\n",
      "l2 norm of weights: 6.333786321286833\n",
      "---------------------\n",
      "Iteration Number: 1440\n",
      "Loss: 69.15360742571974\n",
      "l2 norm of gradients: 0.7481809884727526\n",
      "l2 norm of weights: 6.333608213840212\n",
      "---------------------\n",
      "Iteration Number: 1441\n",
      "Loss: 69.12617882566461\n",
      "l2 norm of gradients: 0.747756775157983\n",
      "l2 norm of weights: 6.333430188168259\n",
      "---------------------\n",
      "Iteration Number: 1442\n",
      "Loss: 69.09878103813247\n",
      "l2 norm of gradients: 0.7473328966857185\n",
      "l2 norm of weights: 6.333252244169888\n",
      "---------------------\n",
      "Iteration Number: 1443\n",
      "Loss: 69.07141402125734\n",
      "l2 norm of gradients: 0.7469093531206827\n",
      "l2 norm of weights: 6.33307438174414\n",
      "---------------------\n",
      "Iteration Number: 1444\n",
      "Loss: 69.04407773332109\n",
      "l2 norm of gradients: 0.7464861445278436\n",
      "l2 norm of weights: 6.3328966007901855\n",
      "---------------------\n",
      "Iteration Number: 1445\n",
      "Loss: 69.01677213271367\n",
      "l2 norm of gradients: 0.7460632709724163\n",
      "l2 norm of weights: 6.332718901207321\n",
      "---------------------\n",
      "Iteration Number: 1446\n",
      "Loss: 68.98949717773837\n",
      "l2 norm of gradients: 0.7456407325198681\n",
      "l2 norm of weights: 6.332541282894972\n",
      "---------------------\n",
      "Iteration Number: 1447\n",
      "Loss: 68.96225282678115\n",
      "l2 norm of gradients: 0.7452185292359208\n",
      "l2 norm of weights: 6.332363745752693\n",
      "---------------------\n",
      "Iteration Number: 1448\n",
      "Loss: 68.93503903835405\n",
      "l2 norm of gradients: 0.7447966611865536\n",
      "l2 norm of weights: 6.332186289680163\n",
      "---------------------\n",
      "Iteration Number: 1449\n",
      "Loss: 68.90785577085973\n",
      "l2 norm of gradients: 0.7443751284380062\n",
      "l2 norm of weights: 6.33200891457719\n",
      "---------------------\n",
      "Iteration Number: 1450\n",
      "Loss: 68.8807029828106\n",
      "l2 norm of gradients: 0.7439539310567808\n",
      "l2 norm of weights: 6.331831620343709\n",
      "---------------------\n",
      "Iteration Number: 1451\n",
      "Loss: 68.85358063281228\n",
      "l2 norm of gradients: 0.7435330691096431\n",
      "l2 norm of weights: 6.331654406879784\n",
      "---------------------\n",
      "Iteration Number: 1452\n",
      "Loss: 68.82648867932944\n",
      "l2 norm of gradients: 0.7431125426636266\n",
      "l2 norm of weights: 6.331477274085605\n",
      "---------------------\n",
      "Iteration Number: 1453\n",
      "Loss: 68.79942708109549\n",
      "l2 norm of gradients: 0.74269235178603\n",
      "l2 norm of weights: 6.331300221861489\n",
      "---------------------\n",
      "Iteration Number: 1454\n",
      "Loss: 68.77239579665347\n",
      "l2 norm of gradients: 0.7422724965444216\n",
      "l2 norm of weights: 6.33112325010788\n",
      "---------------------\n",
      "Iteration Number: 1455\n",
      "Loss: 68.74539478481476\n",
      "l2 norm of gradients: 0.7418529770066379\n",
      "l2 norm of weights: 6.330946358725352\n",
      "---------------------\n",
      "Iteration Number: 1456\n",
      "Loss: 68.71842400418885\n",
      "l2 norm of gradients: 0.7414337932407847\n",
      "l2 norm of weights: 6.3307695476146\n",
      "---------------------\n",
      "Iteration Number: 1457\n",
      "Loss: 68.69148341359958\n",
      "l2 norm of gradients: 0.7410149453152367\n",
      "l2 norm of weights: 6.330592816676455\n",
      "---------------------\n",
      "Iteration Number: 1458\n",
      "Loss: 68.66457297188157\n",
      "l2 norm of gradients: 0.7405964332986378\n",
      "l2 norm of weights: 6.330416165811864\n",
      "---------------------\n",
      "Iteration Number: 1459\n",
      "Loss: 68.63769263775056\n",
      "l2 norm of gradients: 0.7401782572598995\n",
      "l2 norm of weights: 6.3302395949219115\n",
      "---------------------\n",
      "Iteration Number: 1460\n",
      "Loss: 68.61084237017548\n",
      "l2 norm of gradients: 0.7397604172682012\n",
      "l2 norm of weights: 6.330063103907802\n",
      "---------------------\n",
      "Iteration Number: 1461\n",
      "Loss: 68.58402212806902\n",
      "l2 norm of gradients: 0.7393429133929877\n",
      "l2 norm of weights: 6.3298866926708675\n",
      "---------------------\n",
      "Iteration Number: 1462\n",
      "Loss: 68.55723187031334\n",
      "l2 norm of gradients: 0.7389257457039685\n",
      "l2 norm of weights: 6.329710361112571\n",
      "---------------------\n",
      "Iteration Number: 1463\n",
      "Loss: 68.5304715559663\n",
      "l2 norm of gradients: 0.7385089142711161\n",
      "l2 norm of weights: 6.329534109134496\n",
      "---------------------\n",
      "Iteration Number: 1464\n",
      "Loss: 68.50374114397364\n",
      "l2 norm of gradients: 0.7380924191646624\n",
      "l2 norm of weights: 6.329357936638359\n",
      "---------------------\n",
      "Iteration Number: 1465\n",
      "Loss: 68.47704059346285\n",
      "l2 norm of gradients: 0.7376762604550982\n",
      "l2 norm of weights: 6.329181843525997\n",
      "---------------------\n",
      "Iteration Number: 1466\n",
      "Loss: 68.45036986346601\n",
      "l2 norm of gradients: 0.7372604382131694\n",
      "l2 norm of weights: 6.329005829699377\n",
      "---------------------\n",
      "Iteration Number: 1467\n",
      "Loss: 68.42372891310558\n",
      "l2 norm of gradients: 0.7368449525098741\n",
      "l2 norm of weights: 6.3288298950605935\n",
      "---------------------\n",
      "Iteration Number: 1468\n",
      "Loss: 68.39711770155367\n",
      "l2 norm of gradients: 0.7364298034164592\n",
      "l2 norm of weights: 6.328654039511864\n",
      "---------------------\n",
      "Iteration Number: 1469\n",
      "Loss: 68.37053618800842\n",
      "l2 norm of gradients: 0.7360149910044175\n",
      "l2 norm of weights: 6.328478262955535\n",
      "---------------------\n",
      "Iteration Number: 1470\n",
      "Loss: 68.34398433177199\n",
      "l2 norm of gradients: 0.7356005153454828\n",
      "l2 norm of weights: 6.328302565294079\n",
      "---------------------\n",
      "Iteration Number: 1471\n",
      "Loss: 68.31746209199677\n",
      "l2 norm of gradients: 0.7351863765116264\n",
      "l2 norm of weights: 6.328126946430092\n",
      "---------------------\n",
      "Iteration Number: 1472\n",
      "Loss: 68.29096942810952\n",
      "l2 norm of gradients: 0.734772574575053\n",
      "l2 norm of weights: 6.327951406266302\n",
      "---------------------\n",
      "Iteration Number: 1473\n",
      "Loss: 68.26450629933655\n",
      "l2 norm of gradients: 0.7343591096081951\n",
      "l2 norm of weights: 6.327775944705556\n",
      "---------------------\n",
      "Iteration Number: 1474\n",
      "Loss: 68.23807266512472\n",
      "l2 norm of gradients: 0.733945981683709\n",
      "l2 norm of weights: 6.327600561650834\n",
      "---------------------\n",
      "Iteration Number: 1475\n",
      "Loss: 68.21166848488114\n",
      "l2 norm of gradients: 0.7335331908744688\n",
      "l2 norm of weights: 6.327425257005237\n",
      "---------------------\n",
      "Iteration Number: 1476\n",
      "Loss: 68.18529371810398\n",
      "l2 norm of gradients: 0.7331207372535614\n",
      "l2 norm of weights: 6.327250030671995\n",
      "---------------------\n",
      "Iteration Number: 1477\n",
      "Loss: 68.15894832420908\n",
      "l2 norm of gradients: 0.732708620894281\n",
      "l2 norm of weights: 6.327074882554464\n",
      "---------------------\n",
      "Iteration Number: 1478\n",
      "Loss: 68.1326322626945\n",
      "l2 norm of gradients: 0.7322968418701222\n",
      "l2 norm of weights: 6.326899812556123\n",
      "---------------------\n",
      "Iteration Number: 1479\n",
      "Loss: 68.10634549318377\n",
      "l2 norm of gradients: 0.731885400254775\n",
      "l2 norm of weights: 6.326724820580581\n",
      "---------------------\n",
      "Iteration Number: 1480\n",
      "Loss: 68.08008797520797\n",
      "l2 norm of gradients: 0.7314742961221172\n",
      "l2 norm of weights: 6.326549906531571\n",
      "---------------------\n",
      "Iteration Number: 1481\n",
      "Loss: 68.05385966840439\n",
      "l2 norm of gradients: 0.7310635295462087\n",
      "l2 norm of weights: 6.326375070312951\n",
      "---------------------\n",
      "Iteration Number: 1482\n",
      "Loss: 68.0276605324959\n",
      "l2 norm of gradients: 0.7306531006012836\n",
      "l2 norm of weights: 6.326200311828706\n",
      "---------------------\n",
      "Iteration Number: 1483\n",
      "Loss: 68.00149052719537\n",
      "l2 norm of gradients: 0.7302430093617441\n",
      "l2 norm of weights: 6.326025630982948\n",
      "---------------------\n",
      "Iteration Number: 1484\n",
      "Loss: 67.97534961217154\n",
      "l2 norm of gradients: 0.729833255902152\n",
      "l2 norm of weights: 6.325851027679912\n",
      "---------------------\n",
      "Iteration Number: 1485\n",
      "Loss: 67.9492377471952\n",
      "l2 norm of gradients: 0.7294238402972216\n",
      "l2 norm of weights: 6.3256765018239625\n",
      "---------------------\n",
      "Iteration Number: 1486\n",
      "Loss: 67.92315489208977\n",
      "l2 norm of gradients: 0.7290147626218116\n",
      "l2 norm of weights: 6.325502053319585\n",
      "---------------------\n",
      "Iteration Number: 1487\n",
      "Loss: 67.89710100668542\n",
      "l2 norm of gradients: 0.7286060229509181\n",
      "l2 norm of weights: 6.325327682071397\n",
      "---------------------\n",
      "Iteration Number: 1488\n",
      "Loss: 67.87107605087289\n",
      "l2 norm of gradients: 0.7281976213596645\n",
      "l2 norm of weights: 6.325153387984134\n",
      "---------------------\n",
      "Iteration Number: 1489\n",
      "Loss: 67.84507998456354\n",
      "l2 norm of gradients: 0.7277895579232943\n",
      "l2 norm of weights: 6.324979170962665\n",
      "---------------------\n",
      "Iteration Number: 1490\n",
      "Loss: 67.8191127677674\n",
      "l2 norm of gradients: 0.7273818327171616\n",
      "l2 norm of weights: 6.324805030911979\n",
      "---------------------\n",
      "Iteration Number: 1491\n",
      "Loss: 67.7931743603573\n",
      "l2 norm of gradients: 0.7269744458167233\n",
      "l2 norm of weights: 6.3246309677371935\n",
      "---------------------\n",
      "Iteration Number: 1492\n",
      "Loss: 67.76726472240888\n",
      "l2 norm of gradients: 0.7265673972975288\n",
      "l2 norm of weights: 6.32445698134355\n",
      "---------------------\n",
      "Iteration Number: 1493\n",
      "Loss: 67.74138381392036\n",
      "l2 norm of gradients: 0.7261606872352113\n",
      "l2 norm of weights: 6.324283071636417\n",
      "---------------------\n",
      "Iteration Number: 1494\n",
      "Loss: 67.715531595135\n",
      "l2 norm of gradients: 0.7257543157054782\n",
      "l2 norm of weights: 6.324109238521288\n",
      "---------------------\n",
      "Iteration Number: 1495\n",
      "Loss: 67.68970802599894\n",
      "l2 norm of gradients: 0.725348282784101\n",
      "l2 norm of weights: 6.323935481903781\n",
      "---------------------\n",
      "Iteration Number: 1496\n",
      "Loss: 67.66391306677119\n",
      "l2 norm of gradients: 0.7249425885469061\n",
      "l2 norm of weights: 6.323761801689643\n",
      "---------------------\n",
      "Iteration Number: 1497\n",
      "Loss: 67.63814667757771\n",
      "l2 norm of gradients: 0.7245372330697645\n",
      "l2 norm of weights: 6.3235881977847415\n",
      "---------------------\n",
      "Iteration Number: 1498\n",
      "Loss: 67.61240881865905\n",
      "l2 norm of gradients: 0.7241322164285812\n",
      "l2 norm of weights: 6.323414670095074\n",
      "---------------------\n",
      "Iteration Number: 1499\n",
      "Loss: 67.58669945040593\n",
      "l2 norm of gradients: 0.723727538699285\n",
      "l2 norm of weights: 6.323241218526761\n",
      "---------------------\n",
      "Iteration Number: 1500\n",
      "Loss: 67.56101853293768\n",
      "l2 norm of gradients: 0.7233231999578185\n",
      "l2 norm of weights: 6.323067842986049\n",
      "---------------------\n",
      "Iteration Number: 1501\n",
      "Loss: 67.53536602671375\n",
      "l2 norm of gradients: 0.7229192002801262\n",
      "l2 norm of weights: 6.322894543379311\n",
      "---------------------\n",
      "Iteration Number: 1502\n",
      "Loss: 67.50974189204466\n",
      "l2 norm of gradients: 0.7225155397421449\n",
      "l2 norm of weights: 6.322721319613044\n",
      "---------------------\n",
      "Iteration Number: 1503\n",
      "Loss: 67.48414608934252\n",
      "l2 norm of gradients: 0.7221122184197916\n",
      "l2 norm of weights: 6.32254817159387\n",
      "---------------------\n",
      "Iteration Number: 1504\n",
      "Loss: 67.45857857904788\n",
      "l2 norm of gradients: 0.7217092363889521\n",
      "l2 norm of weights: 6.32237509922854\n",
      "---------------------\n",
      "Iteration Number: 1505\n",
      "Loss: 67.43303932170014\n",
      "l2 norm of gradients: 0.7213065937254715\n",
      "l2 norm of weights: 6.322202102423926\n",
      "---------------------\n",
      "Iteration Number: 1506\n",
      "Loss: 67.40752827769933\n",
      "l2 norm of gradients: 0.7209042905051402\n",
      "l2 norm of weights: 6.322029181087029\n",
      "---------------------\n",
      "Iteration Number: 1507\n",
      "Loss: 67.38204540764394\n",
      "l2 norm of gradients: 0.7205023268036836\n",
      "l2 norm of weights: 6.321856335124971\n",
      "---------------------\n",
      "Iteration Number: 1508\n",
      "Loss: 67.35659067218212\n",
      "l2 norm of gradients: 0.7201007026967507\n",
      "l2 norm of weights: 6.321683564445004\n",
      "---------------------\n",
      "Iteration Number: 1509\n",
      "Loss: 67.33116403180706\n",
      "l2 norm of gradients: 0.7196994182599011\n",
      "l2 norm of weights: 6.321510868954504\n",
      "---------------------\n",
      "Iteration Number: 1510\n",
      "Loss: 67.30576544718187\n",
      "l2 norm of gradients: 0.7192984735685937\n",
      "l2 norm of weights: 6.321338248560971\n",
      "---------------------\n",
      "Iteration Number: 1511\n",
      "Loss: 67.28039487913114\n",
      "l2 norm of gradients: 0.7188978686981742\n",
      "l2 norm of weights: 6.321165703172031\n",
      "---------------------\n",
      "Iteration Number: 1512\n",
      "Loss: 67.25505228821434\n",
      "l2 norm of gradients: 0.7184976037238624\n",
      "l2 norm of weights: 6.320993232695438\n",
      "---------------------\n",
      "Iteration Number: 1513\n",
      "Loss: 67.22973763526447\n",
      "l2 norm of gradients: 0.718097678720741\n",
      "l2 norm of weights: 6.320820837039065\n",
      "---------------------\n",
      "Iteration Number: 1514\n",
      "Loss: 67.20445088102272\n",
      "l2 norm of gradients: 0.7176980937637418\n",
      "l2 norm of weights: 6.320648516110918\n",
      "---------------------\n",
      "Iteration Number: 1515\n",
      "Loss: 67.17919198637328\n",
      "l2 norm of gradients: 0.7172988489276336\n",
      "l2 norm of weights: 6.320476269819125\n",
      "---------------------\n",
      "Iteration Number: 1516\n",
      "Loss: 67.15396091211517\n",
      "l2 norm of gradients: 0.7168999442870096\n",
      "l2 norm of weights: 6.320304098071936\n",
      "---------------------\n",
      "Iteration Number: 1517\n",
      "Loss: 67.12875761921649\n",
      "l2 norm of gradients: 0.7165013799162742\n",
      "l2 norm of weights: 6.320132000777731\n",
      "---------------------\n",
      "Iteration Number: 1518\n",
      "Loss: 67.10358206847738\n",
      "l2 norm of gradients: 0.7161031558896299\n",
      "l2 norm of weights: 6.319959977845013\n",
      "---------------------\n",
      "Iteration Number: 1519\n",
      "Loss: 67.07843422099302\n",
      "l2 norm of gradients: 0.7157052722810651\n",
      "l2 norm of weights: 6.319788029182414\n",
      "---------------------\n",
      "Iteration Number: 1520\n",
      "Loss: 67.0533140377571\n",
      "l2 norm of gradients: 0.7153077291643397\n",
      "l2 norm of weights: 6.319616154698686\n",
      "---------------------\n",
      "Iteration Number: 1521\n",
      "Loss: 67.02822147969071\n",
      "l2 norm of gradients: 0.7149105266129735\n",
      "l2 norm of weights: 6.319444354302709\n",
      "---------------------\n",
      "Iteration Number: 1522\n",
      "Loss: 67.00315650792611\n",
      "l2 norm of gradients: 0.7145136647002308\n",
      "l2 norm of weights: 6.319272627903488\n",
      "---------------------\n",
      "Iteration Number: 1523\n",
      "Loss: 66.9781190836283\n",
      "l2 norm of gradients: 0.7141171434991094\n",
      "l2 norm of weights: 6.319100975410156\n",
      "---------------------\n",
      "Iteration Number: 1524\n",
      "Loss: 66.95310916781679\n",
      "l2 norm of gradients: 0.7137209630823245\n",
      "l2 norm of weights: 6.318929396731966\n",
      "---------------------\n",
      "Iteration Number: 1525\n",
      "Loss: 66.92812672172643\n",
      "l2 norm of gradients: 0.7133251235222976\n",
      "l2 norm of weights: 6.318757891778301\n",
      "---------------------\n",
      "Iteration Number: 1526\n",
      "Loss: 66.90317170656742\n",
      "l2 norm of gradients: 0.7129296248911415\n",
      "l2 norm of weights: 6.318586460458666\n",
      "---------------------\n",
      "Iteration Number: 1527\n",
      "Loss: 66.87824408358793\n",
      "l2 norm of gradients: 0.712534467260647\n",
      "l2 norm of weights: 6.318415102682694\n",
      "---------------------\n",
      "Iteration Number: 1528\n",
      "Loss: 66.85334381403557\n",
      "l2 norm of gradients: 0.7121396507022686\n",
      "l2 norm of weights: 6.318243818360142\n",
      "---------------------\n",
      "Iteration Number: 1529\n",
      "Loss: 66.82847085922275\n",
      "l2 norm of gradients: 0.711745175287112\n",
      "l2 norm of weights: 6.318072607400893\n",
      "---------------------\n",
      "Iteration Number: 1530\n",
      "Loss: 66.8036251805319\n",
      "l2 norm of gradients: 0.7113510410859194\n",
      "l2 norm of weights: 6.317901469714954\n",
      "---------------------\n",
      "Iteration Number: 1531\n",
      "Loss: 66.77880673929067\n",
      "l2 norm of gradients: 0.7109572481690548\n",
      "l2 norm of weights: 6.31773040521246\n",
      "---------------------\n",
      "Iteration Number: 1532\n",
      "Loss: 66.75401549690928\n",
      "l2 norm of gradients: 0.7105637966064917\n",
      "l2 norm of weights: 6.317559413803667\n",
      "---------------------\n",
      "Iteration Number: 1533\n",
      "Loss: 66.72925141490623\n",
      "l2 norm of gradients: 0.7101706864677982\n",
      "l2 norm of weights: 6.31738849539896\n",
      "---------------------\n",
      "Iteration Number: 1534\n",
      "Loss: 66.70451445470908\n",
      "l2 norm of gradients: 0.7097779178221231\n",
      "l2 norm of weights: 6.317217649908851\n",
      "---------------------\n",
      "Iteration Number: 1535\n",
      "Loss: 66.67980457790173\n",
      "l2 norm of gradients: 0.7093854907381816\n",
      "l2 norm of weights: 6.31704687724397\n",
      "---------------------\n",
      "Iteration Number: 1536\n",
      "Loss: 66.65512174592098\n",
      "l2 norm of gradients: 0.7089934052842416\n",
      "l2 norm of weights: 6.316876177315081\n",
      "---------------------\n",
      "Iteration Number: 1537\n",
      "Loss: 66.63046592048109\n",
      "l2 norm of gradients: 0.7086016615281093\n",
      "l2 norm of weights: 6.316705550033069\n",
      "---------------------\n",
      "Iteration Number: 1538\n",
      "Loss: 66.60583706314625\n",
      "l2 norm of gradients: 0.7082102595371154\n",
      "l2 norm of weights: 6.316534995308943\n",
      "---------------------\n",
      "Iteration Number: 1539\n",
      "Loss: 66.58123513558023\n",
      "l2 norm of gradients: 0.7078191993781002\n",
      "l2 norm of weights: 6.31636451305384\n",
      "---------------------\n",
      "Iteration Number: 1540\n",
      "Loss: 66.55666009948527\n",
      "l2 norm of gradients: 0.7074284811174005\n",
      "l2 norm of weights: 6.316194103179023\n",
      "---------------------\n",
      "Iteration Number: 1541\n",
      "Loss: 66.53211191653212\n",
      "l2 norm of gradients: 0.7070381048208342\n",
      "l2 norm of weights: 6.316023765595877\n",
      "---------------------\n",
      "Iteration Number: 1542\n",
      "Loss: 66.50759054859803\n",
      "l2 norm of gradients: 0.7066480705536866\n",
      "l2 norm of weights: 6.315853500215915\n",
      "---------------------\n",
      "Iteration Number: 1543\n",
      "Loss: 66.48309595738085\n",
      "l2 norm of gradients: 0.7062583783806967\n",
      "l2 norm of weights: 6.315683306950776\n",
      "---------------------\n",
      "Iteration Number: 1544\n",
      "Loss: 66.45862810478954\n",
      "l2 norm of gradients: 0.7058690283660417\n",
      "l2 norm of weights: 6.315513185712222\n",
      "---------------------\n",
      "Iteration Number: 1545\n",
      "Loss: 66.43418695258974\n",
      "l2 norm of gradients: 0.7054800205733244\n",
      "l2 norm of weights: 6.31534313641214\n",
      "---------------------\n",
      "Iteration Number: 1546\n",
      "Loss: 66.40977246282972\n",
      "l2 norm of gradients: 0.705091355065557\n",
      "l2 norm of weights: 6.315173158962548\n",
      "---------------------\n",
      "Iteration Number: 1547\n",
      "Loss: 66.38538459727557\n",
      "l2 norm of gradients: 0.7047030319051486\n",
      "l2 norm of weights: 6.315003253275583\n",
      "---------------------\n",
      "Iteration Number: 1548\n",
      "Loss: 66.36102331802273\n",
      "l2 norm of gradients: 0.7043150511538901\n",
      "l2 norm of weights: 6.31483341926351\n",
      "---------------------\n",
      "Iteration Number: 1549\n",
      "Loss: 66.33668858708032\n",
      "l2 norm of gradients: 0.7039274128729399\n",
      "l2 norm of weights: 6.31466365683872\n",
      "---------------------\n",
      "Iteration Number: 1550\n",
      "Loss: 66.3123803663752\n",
      "l2 norm of gradients: 0.7035401171228104\n",
      "l2 norm of weights: 6.314493965913728\n",
      "---------------------\n",
      "Iteration Number: 1551\n",
      "Loss: 66.28809861811024\n",
      "l2 norm of gradients: 0.7031531639633527\n",
      "l2 norm of weights: 6.3143243464011745\n",
      "---------------------\n",
      "Iteration Number: 1552\n",
      "Loss: 66.26384330439572\n",
      "l2 norm of gradients: 0.7027665534537433\n",
      "l2 norm of weights: 6.314154798213829\n",
      "---------------------\n",
      "Iteration Number: 1553\n",
      "Loss: 66.23961438727352\n",
      "l2 norm of gradients: 0.7023802856524696\n",
      "l2 norm of weights: 6.31398532126458\n",
      "---------------------\n",
      "Iteration Number: 1554\n",
      "Loss: 66.21541182898193\n",
      "l2 norm of gradients: 0.7019943606173161\n",
      "l2 norm of weights: 6.313815915466448\n",
      "---------------------\n",
      "Iteration Number: 1555\n",
      "Loss: 66.19123559180285\n",
      "l2 norm of gradients: 0.7016087784053494\n",
      "l2 norm of weights: 6.3136465807325735\n",
      "---------------------\n",
      "Iteration Number: 1556\n",
      "Loss: 66.16708563787162\n",
      "l2 norm of gradients: 0.7012235390729052\n",
      "l2 norm of weights: 6.313477316976227\n",
      "---------------------\n",
      "Iteration Number: 1557\n",
      "Loss: 66.14296192954664\n",
      "l2 norm of gradients: 0.7008386426755737\n",
      "l2 norm of weights: 6.313308124110801\n",
      "---------------------\n",
      "Iteration Number: 1558\n",
      "Loss: 66.11886442914773\n",
      "l2 norm of gradients: 0.7004540892681854\n",
      "l2 norm of weights: 6.313139002049816\n",
      "---------------------\n",
      "Iteration Number: 1559\n",
      "Loss: 66.0947930990363\n",
      "l2 norm of gradients: 0.7000698789047977\n",
      "l2 norm of weights: 6.312969950706917\n",
      "---------------------\n",
      "Iteration Number: 1560\n",
      "Loss: 66.07074790164403\n",
      "l2 norm of gradients: 0.6996860116386802\n",
      "l2 norm of weights: 6.312800969995872\n",
      "---------------------\n",
      "Iteration Number: 1561\n",
      "Loss: 66.04672879928242\n",
      "l2 norm of gradients: 0.6993024875223017\n",
      "l2 norm of weights: 6.31263205983058\n",
      "---------------------\n",
      "Iteration Number: 1562\n",
      "Loss: 66.02273575444522\n",
      "l2 norm of gradients: 0.6989193066073156\n",
      "l2 norm of weights: 6.312463220125061\n",
      "---------------------\n",
      "Iteration Number: 1563\n",
      "Loss: 65.99876872969715\n",
      "l2 norm of gradients: 0.6985364689445465\n",
      "l2 norm of weights: 6.312294450793461\n",
      "---------------------\n",
      "Iteration Number: 1564\n",
      "Loss: 65.97482768757858\n",
      "l2 norm of gradients: 0.698153974583976\n",
      "l2 norm of weights: 6.312125751750053\n",
      "---------------------\n",
      "Iteration Number: 1565\n",
      "Loss: 65.95091259059946\n",
      "l2 norm of gradients: 0.6977718235747301\n",
      "l2 norm of weights: 6.311957122909235\n",
      "---------------------\n",
      "Iteration Number: 1566\n",
      "Loss: 65.92702340136852\n",
      "l2 norm of gradients: 0.6973900159650641\n",
      "l2 norm of weights: 6.311788564185531\n",
      "---------------------\n",
      "Iteration Number: 1567\n",
      "Loss: 65.90316008258823\n",
      "l2 norm of gradients: 0.6970085518023498\n",
      "l2 norm of weights: 6.311620075493589\n",
      "---------------------\n",
      "Iteration Number: 1568\n",
      "Loss: 65.87932259682488\n",
      "l2 norm of gradients: 0.6966274311330622\n",
      "l2 norm of weights: 6.311451656748183\n",
      "---------------------\n",
      "Iteration Number: 1569\n",
      "Loss: 65.85551090690906\n",
      "l2 norm of gradients: 0.6962466540027656\n",
      "l2 norm of weights: 6.311283307864213\n",
      "---------------------\n",
      "Iteration Number: 1570\n",
      "Loss: 65.83172497552476\n",
      "l2 norm of gradients: 0.6958662204561002\n",
      "l2 norm of weights: 6.3111150287567055\n",
      "---------------------\n",
      "Iteration Number: 1571\n",
      "Loss: 65.80796476541259\n",
      "l2 norm of gradients: 0.6954861305367691\n",
      "l2 norm of weights: 6.31094681934081\n",
      "---------------------\n",
      "Iteration Number: 1572\n",
      "Loss: 65.78423023949867\n",
      "l2 norm of gradients: 0.6951063842875247\n",
      "l2 norm of weights: 6.310778679531803\n",
      "---------------------\n",
      "Iteration Number: 1573\n",
      "Loss: 65.76052136053008\n",
      "l2 norm of gradients: 0.6947269817501556\n",
      "l2 norm of weights: 6.310610609245088\n",
      "---------------------\n",
      "Iteration Number: 1574\n",
      "Loss: 65.73683809151244\n",
      "l2 norm of gradients: 0.6943479229654738\n",
      "l2 norm of weights: 6.31044260839619\n",
      "---------------------\n",
      "Iteration Number: 1575\n",
      "Loss: 65.71318039526258\n",
      "l2 norm of gradients: 0.6939692079733015\n",
      "l2 norm of weights: 6.3102746769007645\n",
      "---------------------\n",
      "Iteration Number: 1576\n",
      "Loss: 65.68954823476757\n",
      "l2 norm of gradients: 0.6935908368124576\n",
      "l2 norm of weights: 6.310106814674588\n",
      "---------------------\n",
      "Iteration Number: 1577\n",
      "Loss: 65.66594157303284\n",
      "l2 norm of gradients: 0.6932128095207457\n",
      "l2 norm of weights: 6.309939021633565\n",
      "---------------------\n",
      "Iteration Number: 1578\n",
      "Loss: 65.64236037311463\n",
      "l2 norm of gradients: 0.6928351261349406\n",
      "l2 norm of weights: 6.309771297693725\n",
      "---------------------\n",
      "Iteration Number: 1579\n",
      "Loss: 65.61880459803305\n",
      "l2 norm of gradients: 0.6924577866907761\n",
      "l2 norm of weights: 6.309603642771224\n",
      "---------------------\n",
      "Iteration Number: 1580\n",
      "Loss: 65.59527421095137\n",
      "l2 norm of gradients: 0.6920807912229322\n",
      "l2 norm of weights: 6.309436056782341\n",
      "---------------------\n",
      "Iteration Number: 1581\n",
      "Loss: 65.57176917494843\n",
      "l2 norm of gradients: 0.6917041397650221\n",
      "l2 norm of weights: 6.309268539643482\n",
      "---------------------\n",
      "Iteration Number: 1582\n",
      "Loss: 65.54828945321654\n",
      "l2 norm of gradients: 0.6913278323495807\n",
      "l2 norm of weights: 6.309101091271181\n",
      "---------------------\n",
      "Iteration Number: 1583\n",
      "Loss: 65.52483500895417\n",
      "l2 norm of gradients: 0.6909518690080516\n",
      "l2 norm of weights: 6.308933711582092\n",
      "---------------------\n",
      "Iteration Number: 1584\n",
      "Loss: 65.50140580548273\n",
      "l2 norm of gradients: 0.6905762497707745\n",
      "l2 norm of weights: 6.308766400492998\n",
      "---------------------\n",
      "Iteration Number: 1585\n",
      "Loss: 65.47800180596931\n",
      "l2 norm of gradients: 0.6902009746669738\n",
      "l2 norm of weights: 6.308599157920809\n",
      "---------------------\n",
      "Iteration Number: 1586\n",
      "Loss: 65.45462297385065\n",
      "l2 norm of gradients: 0.6898260437247465\n",
      "l2 norm of weights: 6.3084319837825555\n",
      "---------------------\n",
      "Iteration Number: 1587\n",
      "Loss: 65.43126927236872\n",
      "l2 norm of gradients: 0.6894514569710495\n",
      "l2 norm of weights: 6.308264877995399\n",
      "---------------------\n",
      "Iteration Number: 1588\n",
      "Loss: 65.40794066498549\n",
      "l2 norm of gradients: 0.6890772144316882\n",
      "l2 norm of weights: 6.3080978404766235\n",
      "---------------------\n",
      "Iteration Number: 1589\n",
      "Loss: 65.38463711512283\n",
      "l2 norm of gradients: 0.6887033161313053\n",
      "l2 norm of weights: 6.307930871143638\n",
      "---------------------\n",
      "Iteration Number: 1590\n",
      "Loss: 65.36135858624716\n",
      "l2 norm of gradients: 0.6883297620933673\n",
      "l2 norm of weights: 6.307763969913977\n",
      "---------------------\n",
      "Iteration Number: 1591\n",
      "Loss: 65.33810504190076\n",
      "l2 norm of gradients: 0.6879565523401554\n",
      "l2 norm of weights: 6.307597136705304\n",
      "---------------------\n",
      "Iteration Number: 1592\n",
      "Loss: 65.31487644550118\n",
      "l2 norm of gradients: 0.6875836868927522\n",
      "l2 norm of weights: 6.307430371435403\n",
      "---------------------\n",
      "Iteration Number: 1593\n",
      "Loss: 65.29167276065807\n",
      "l2 norm of gradients: 0.6872111657710308\n",
      "l2 norm of weights: 6.307263674022187\n",
      "---------------------\n",
      "Iteration Number: 1594\n",
      "Loss: 65.26849395104128\n",
      "l2 norm of gradients: 0.6868389889936443\n",
      "l2 norm of weights: 6.307097044383692\n",
      "---------------------\n",
      "Iteration Number: 1595\n",
      "Loss: 65.24533998029972\n",
      "l2 norm of gradients: 0.6864671565780136\n",
      "l2 norm of weights: 6.306930482438082\n",
      "---------------------\n",
      "Iteration Number: 1596\n",
      "Loss: 65.22221081205952\n",
      "l2 norm of gradients: 0.6860956685403169\n",
      "l2 norm of weights: 6.3067639881036435\n",
      "---------------------\n",
      "Iteration Number: 1597\n",
      "Loss: 65.19910641005077\n",
      "l2 norm of gradients: 0.685724524895479\n",
      "l2 norm of weights: 6.30659756129879\n",
      "---------------------\n",
      "Iteration Number: 1598\n",
      "Loss: 65.17602673802584\n",
      "l2 norm of gradients: 0.6853537256571599\n",
      "l2 norm of weights: 6.30643120194206\n",
      "---------------------\n",
      "Iteration Number: 1599\n",
      "Loss: 65.1529717597875\n",
      "l2 norm of gradients: 0.6849832708377447\n",
      "l2 norm of weights: 6.3062649099521195\n",
      "---------------------\n",
      "Iteration Number: 1600\n",
      "Loss: 65.12994143921627\n",
      "l2 norm of gradients: 0.6846131604483331\n",
      "l2 norm of weights: 6.306098685247755\n",
      "---------------------\n",
      "Iteration Number: 1601\n",
      "Loss: 65.1069357400815\n",
      "l2 norm of gradients: 0.6842433944987281\n",
      "l2 norm of weights: 6.305932527747882\n",
      "---------------------\n",
      "Iteration Number: 1602\n",
      "Loss: 65.0839546263354\n",
      "l2 norm of gradients: 0.6838739729974265\n",
      "l2 norm of weights: 6.305766437371544\n",
      "---------------------\n",
      "Iteration Number: 1603\n",
      "Loss: 65.06099806198323\n",
      "l2 norm of gradients: 0.6835048959516082\n",
      "l2 norm of weights: 6.305600414037901\n",
      "---------------------\n",
      "Iteration Number: 1604\n",
      "Loss: 65.03806601083333\n",
      "l2 norm of gradients: 0.6831361633671272\n",
      "l2 norm of weights: 6.305434457666247\n",
      "---------------------\n",
      "Iteration Number: 1605\n",
      "Loss: 65.0151584370739\n",
      "l2 norm of gradients: 0.6827677752484997\n",
      "l2 norm of weights: 6.305268568175997\n",
      "---------------------\n",
      "Iteration Number: 1606\n",
      "Loss: 64.99227530467292\n",
      "l2 norm of gradients: 0.6823997315988959\n",
      "l2 norm of weights: 6.305102745486693\n",
      "---------------------\n",
      "Iteration Number: 1607\n",
      "Loss: 64.96941657772925\n",
      "l2 norm of gradients: 0.6820320324201298\n",
      "l2 norm of weights: 6.3049369895180005\n",
      "---------------------\n",
      "Iteration Number: 1608\n",
      "Loss: 64.94658222038822\n",
      "l2 norm of gradients: 0.6816646777126493\n",
      "l2 norm of weights: 6.304771300189711\n",
      "---------------------\n",
      "Iteration Number: 1609\n",
      "Loss: 64.92377219676976\n",
      "l2 norm of gradients: 0.6812976674755271\n",
      "l2 norm of weights: 6.304605677421741\n",
      "---------------------\n",
      "Iteration Number: 1610\n",
      "Loss: 64.90098647107163\n",
      "l2 norm of gradients: 0.6809310017064512\n",
      "l2 norm of weights: 6.304440121134135\n",
      "---------------------\n",
      "Iteration Number: 1611\n",
      "Loss: 64.87822500759715\n",
      "l2 norm of gradients: 0.6805646804017156\n",
      "l2 norm of weights: 6.304274631247059\n",
      "---------------------\n",
      "Iteration Number: 1612\n",
      "Loss: 64.85548777049976\n",
      "l2 norm of gradients: 0.6801987035562111\n",
      "l2 norm of weights: 6.304109207680803\n",
      "---------------------\n",
      "Iteration Number: 1613\n",
      "Loss: 64.83277472422256\n",
      "l2 norm of gradients: 0.6798330711634176\n",
      "l2 norm of weights: 6.303943850355786\n",
      "---------------------\n",
      "Iteration Number: 1614\n",
      "Loss: 64.81008583304914\n",
      "l2 norm of gradients: 0.679467783215393\n",
      "l2 norm of weights: 6.303778559192551\n",
      "---------------------\n",
      "Iteration Number: 1615\n",
      "Loss: 64.78742106143227\n",
      "l2 norm of gradients: 0.6791028397027661\n",
      "l2 norm of weights: 6.303613334111765\n",
      "---------------------\n",
      "Iteration Number: 1616\n",
      "Loss: 64.76478037368956\n",
      "l2 norm of gradients: 0.6787382406147284\n",
      "l2 norm of weights: 6.303448175034221\n",
      "---------------------\n",
      "Iteration Number: 1617\n",
      "Loss: 64.74216373433994\n",
      "l2 norm of gradients: 0.6783739859390236\n",
      "l2 norm of weights: 6.303283081880837\n",
      "---------------------\n",
      "Iteration Number: 1618\n",
      "Loss: 64.7195711078331\n",
      "l2 norm of gradients: 0.6780100756619417\n",
      "l2 norm of weights: 6.303118054572654\n",
      "---------------------\n",
      "Iteration Number: 1619\n",
      "Loss: 64.69700245882079\n",
      "l2 norm of gradients: 0.6776465097683093\n",
      "l2 norm of weights: 6.30295309303084\n",
      "---------------------\n",
      "Iteration Number: 1620\n",
      "Loss: 64.67445775176944\n",
      "l2 norm of gradients: 0.6772832882414821\n",
      "l2 norm of weights: 6.302788197176689\n",
      "---------------------\n",
      "Iteration Number: 1621\n",
      "Loss: 64.65193695138036\n",
      "l2 norm of gradients: 0.6769204110633368\n",
      "l2 norm of weights: 6.302623366931616\n",
      "---------------------\n",
      "Iteration Number: 1622\n",
      "Loss: 64.62944002225184\n",
      "l2 norm of gradients: 0.6765578782142632\n",
      "l2 norm of weights: 6.3024586022171665\n",
      "---------------------\n",
      "Iteration Number: 1623\n",
      "Loss: 64.60696692905056\n",
      "l2 norm of gradients: 0.6761956896731574\n",
      "l2 norm of weights: 6.302293902955004\n",
      "---------------------\n",
      "Iteration Number: 1624\n",
      "Loss: 64.58451763651318\n",
      "l2 norm of gradients: 0.6758338454174128\n",
      "l2 norm of weights: 6.3021292690669215\n",
      "---------------------\n",
      "Iteration Number: 1625\n",
      "Loss: 64.56209210945353\n",
      "l2 norm of gradients: 0.6754723454229143\n",
      "l2 norm of weights: 6.301964700474836\n",
      "---------------------\n",
      "Iteration Number: 1626\n",
      "Loss: 64.53969031263264\n",
      "l2 norm of gradients: 0.6751111896640302\n",
      "l2 norm of weights: 6.301800197100788\n",
      "---------------------\n",
      "Iteration Number: 1627\n",
      "Loss: 64.51731221088816\n",
      "l2 norm of gradients: 0.6747503781136049\n",
      "l2 norm of weights: 6.301635758866946\n",
      "---------------------\n",
      "Iteration Number: 1628\n",
      "Loss: 64.49495776911681\n",
      "l2 norm of gradients: 0.6743899107429527\n",
      "l2 norm of weights: 6.301471385695598\n",
      "---------------------\n",
      "Iteration Number: 1629\n",
      "Loss: 64.47262695228054\n",
      "l2 norm of gradients: 0.67402978752185\n",
      "l2 norm of weights: 6.30130707750916\n",
      "---------------------\n",
      "Iteration Number: 1630\n",
      "Loss: 64.45031972523698\n",
      "l2 norm of gradients: 0.6736700084185299\n",
      "l2 norm of weights: 6.301142834230173\n",
      "---------------------\n",
      "Iteration Number: 1631\n",
      "Loss: 64.42803605309939\n",
      "l2 norm of gradients: 0.6733105733996739\n",
      "l2 norm of weights: 6.3009786557813\n",
      "---------------------\n",
      "Iteration Number: 1632\n",
      "Loss: 64.40577590079033\n",
      "l2 norm of gradients: 0.6729514824304074\n",
      "l2 norm of weights: 6.300814542085331\n",
      "---------------------\n",
      "Iteration Number: 1633\n",
      "Loss: 64.38353923344116\n",
      "l2 norm of gradients: 0.672592735474292\n",
      "l2 norm of weights: 6.300650493065178\n",
      "---------------------\n",
      "Iteration Number: 1634\n",
      "Loss: 64.36132601617219\n",
      "l2 norm of gradients: 0.6722343324933198\n",
      "l2 norm of weights: 6.300486508643881\n",
      "---------------------\n",
      "Iteration Number: 1635\n",
      "Loss: 64.3391362141048\n",
      "l2 norm of gradients: 0.6718762734479073\n",
      "l2 norm of weights: 6.3003225887446\n",
      "---------------------\n",
      "Iteration Number: 1636\n",
      "Loss: 64.31696979242928\n",
      "l2 norm of gradients: 0.6715185582968904\n",
      "l2 norm of weights: 6.3001587332906235\n",
      "---------------------\n",
      "Iteration Number: 1637\n",
      "Loss: 64.2948267163526\n",
      "l2 norm of gradients: 0.6711611869975168\n",
      "l2 norm of weights: 6.299994942205361\n",
      "---------------------\n",
      "Iteration Number: 1638\n",
      "Loss: 64.27270695120548\n",
      "l2 norm of gradients: 0.6708041595054421\n",
      "l2 norm of weights: 6.299831215412348\n",
      "---------------------\n",
      "Iteration Number: 1639\n",
      "Loss: 64.25061046229204\n",
      "l2 norm of gradients: 0.6704474757747239\n",
      "l2 norm of weights: 6.299667552835244\n",
      "---------------------\n",
      "Iteration Number: 1640\n",
      "Loss: 64.22853721485124\n",
      "l2 norm of gradients: 0.6700911357578154\n",
      "l2 norm of weights: 6.299503954397832\n",
      "---------------------\n",
      "Iteration Number: 1641\n",
      "Loss: 64.20648717437426\n",
      "l2 norm of gradients: 0.6697351394055623\n",
      "l2 norm of weights: 6.299340420024022\n",
      "---------------------\n",
      "Iteration Number: 1642\n",
      "Loss: 64.18446030624207\n",
      "l2 norm of gradients: 0.6693794866671956\n",
      "l2 norm of weights: 6.299176949637841\n",
      "---------------------\n",
      "Iteration Number: 1643\n",
      "Loss: 64.1624565759211\n",
      "l2 norm of gradients: 0.6690241774903278\n",
      "l2 norm of weights: 6.299013543163449\n",
      "---------------------\n",
      "Iteration Number: 1644\n",
      "Loss: 64.14047594890954\n",
      "l2 norm of gradients: 0.6686692118209483\n",
      "l2 norm of weights: 6.2988502005251235\n",
      "---------------------\n",
      "Iteration Number: 1645\n",
      "Loss: 64.11851839075321\n",
      "l2 norm of gradients: 0.6683145896034182\n",
      "l2 norm of weights: 6.298686921647269\n",
      "---------------------\n",
      "Iteration Number: 1646\n",
      "Loss: 64.09658386703994\n",
      "l2 norm of gradients: 0.6679603107804659\n",
      "l2 norm of weights: 6.298523706454412\n",
      "---------------------\n",
      "Iteration Number: 1647\n",
      "Loss: 64.07467234333323\n",
      "l2 norm of gradients: 0.6676063752931825\n",
      "l2 norm of weights: 6.298360554871204\n",
      "---------------------\n",
      "Iteration Number: 1648\n",
      "Loss: 64.05278378541237\n",
      "l2 norm of gradients: 0.6672527830810183\n",
      "l2 norm of weights: 6.29819746682242\n",
      "---------------------\n",
      "Iteration Number: 1649\n",
      "Loss: 64.03091815888745\n",
      "l2 norm of gradients: 0.6668995340817783\n",
      "l2 norm of weights: 6.298034442232959\n",
      "---------------------\n",
      "Iteration Number: 1650\n",
      "Loss: 64.00907542951309\n",
      "l2 norm of gradients: 0.6665466282316179\n",
      "l2 norm of weights: 6.297871481027844\n",
      "---------------------\n",
      "Iteration Number: 1651\n",
      "Loss: 63.98725556303459\n",
      "l2 norm of gradients: 0.6661940654650395\n",
      "l2 norm of weights: 6.297708583132219\n",
      "---------------------\n",
      "Iteration Number: 1652\n",
      "Loss: 63.96545852527502\n",
      "l2 norm of gradients: 0.6658418457148882\n",
      "l2 norm of weights: 6.297545748471355\n",
      "---------------------\n",
      "Iteration Number: 1653\n",
      "Loss: 63.943684282151985\n",
      "l2 norm of gradients: 0.6654899689123497\n",
      "l2 norm of weights: 6.297382976970643\n",
      "---------------------\n",
      "Iteration Number: 1654\n",
      "Loss: 63.92193279956818\n",
      "l2 norm of gradients: 0.6651384349869446\n",
      "l2 norm of weights: 6.297220268555601\n",
      "---------------------\n",
      "Iteration Number: 1655\n",
      "Loss: 63.900204043351835\n",
      "l2 norm of gradients: 0.6647872438665272\n",
      "l2 norm of weights: 6.297057623151868\n",
      "---------------------\n",
      "Iteration Number: 1656\n",
      "Loss: 63.878497979595046\n",
      "l2 norm of gradients: 0.6644363954772815\n",
      "l2 norm of weights: 6.2968950406852064\n",
      "---------------------\n",
      "Iteration Number: 1657\n",
      "Loss: 63.856814574265\n",
      "l2 norm of gradients: 0.6640858897437174\n",
      "l2 norm of weights: 6.296732521081503\n",
      "---------------------\n",
      "Iteration Number: 1658\n",
      "Loss: 63.835153793372356\n",
      "l2 norm of gradients: 0.6637357265886693\n",
      "l2 norm of weights: 6.296570064266767\n",
      "---------------------\n",
      "Iteration Number: 1659\n",
      "Loss: 63.81351560311817\n",
      "l2 norm of gradients: 0.6633859059332922\n",
      "l2 norm of weights: 6.29640767016713\n",
      "---------------------\n",
      "Iteration Number: 1660\n",
      "Loss: 63.79189996959875\n",
      "l2 norm of gradients: 0.6630364276970597\n",
      "l2 norm of weights: 6.2962453387088475\n",
      "---------------------\n",
      "Iteration Number: 1661\n",
      "Loss: 63.77030685893895\n",
      "l2 norm of gradients: 0.6626872917977608\n",
      "l2 norm of weights: 6.296083069818298\n",
      "---------------------\n",
      "Iteration Number: 1662\n",
      "Loss: 63.74873623740774\n",
      "l2 norm of gradients: 0.6623384981514984\n",
      "l2 norm of weights: 6.295920863421984\n",
      "---------------------\n",
      "Iteration Number: 1663\n",
      "Loss: 63.727188071312476\n",
      "l2 norm of gradients: 0.6619900466726866\n",
      "l2 norm of weights: 6.2957587194465265\n",
      "---------------------\n",
      "Iteration Number: 1664\n",
      "Loss: 63.70566232683559\n",
      "l2 norm of gradients: 0.6616419372740483\n",
      "l2 norm of weights: 6.295596637818676\n",
      "---------------------\n",
      "Iteration Number: 1665\n",
      "Loss: 63.68415897040132\n",
      "l2 norm of gradients: 0.6612941698666136\n",
      "l2 norm of weights: 6.295434618465298\n",
      "---------------------\n",
      "Iteration Number: 1666\n",
      "Loss: 63.662677968350245\n",
      "l2 norm of gradients: 0.6609467443597179\n",
      "l2 norm of weights: 6.295272661313387\n",
      "---------------------\n",
      "Iteration Number: 1667\n",
      "Loss: 63.64121928715295\n",
      "l2 norm of gradients: 0.6605996606610003\n",
      "l2 norm of weights: 6.295110766290056\n",
      "---------------------\n",
      "Iteration Number: 1668\n",
      "Loss: 63.619782893226365\n",
      "l2 norm of gradients: 0.6602529186764015\n",
      "l2 norm of weights: 6.294948933322543\n",
      "---------------------\n",
      "Iteration Number: 1669\n",
      "Loss: 63.5983687530715\n",
      "l2 norm of gradients: 0.659906518310163\n",
      "l2 norm of weights: 6.294787162338207\n",
      "---------------------\n",
      "Iteration Number: 1670\n",
      "Loss: 63.5769768332437\n",
      "l2 norm of gradients: 0.6595604594648251\n",
      "l2 norm of weights: 6.29462545326453\n",
      "---------------------\n",
      "Iteration Number: 1671\n",
      "Loss: 63.555607100381444\n",
      "l2 norm of gradients: 0.6592147420412258\n",
      "l2 norm of weights: 6.294463806029115\n",
      "---------------------\n",
      "Iteration Number: 1672\n",
      "Loss: 63.5342595210079\n",
      "l2 norm of gradients: 0.6588693659385005\n",
      "l2 norm of weights: 6.294302220559688\n",
      "---------------------\n",
      "Iteration Number: 1673\n",
      "Loss: 63.51293406184585\n",
      "l2 norm of gradients: 0.6585243310540797\n",
      "l2 norm of weights: 6.2941406967840985\n",
      "---------------------\n",
      "Iteration Number: 1674\n",
      "Loss: 63.49163068960786\n",
      "l2 norm of gradients: 0.6581796372836894\n",
      "l2 norm of weights: 6.293979234630315\n",
      "---------------------\n",
      "Iteration Number: 1675\n",
      "Loss: 63.47034937104616\n",
      "l2 norm of gradients: 0.6578352845213493\n",
      "l2 norm of weights: 6.29381783402643\n",
      "---------------------\n",
      "Iteration Number: 1676\n",
      "Loss: 63.44909007295769\n",
      "l2 norm of gradients: 0.6574912726593735\n",
      "l2 norm of weights: 6.293656494900658\n",
      "---------------------\n",
      "Iteration Number: 1677\n",
      "Loss: 63.42785276214701\n",
      "l2 norm of gradients: 0.6571476015883679\n",
      "l2 norm of weights: 6.2934952171813325\n",
      "---------------------\n",
      "Iteration Number: 1678\n",
      "Loss: 63.406637405495076\n",
      "l2 norm of gradients: 0.6568042711972325\n",
      "l2 norm of weights: 6.293334000796913\n",
      "---------------------\n",
      "Iteration Number: 1679\n",
      "Loss: 63.38544396989463\n",
      "l2 norm of gradients: 0.656461281373159\n",
      "l2 norm of weights: 6.293172845675977\n",
      "---------------------\n",
      "Iteration Number: 1680\n",
      "Loss: 63.36427242234911\n",
      "l2 norm of gradients: 0.656118632001632\n",
      "l2 norm of weights: 6.293011751747224\n",
      "---------------------\n",
      "Iteration Number: 1681\n",
      "Loss: 63.34312272981483\n",
      "l2 norm of gradients: 0.6557763229664278\n",
      "l2 norm of weights: 6.292850718939477\n",
      "---------------------\n",
      "Iteration Number: 1682\n",
      "Loss: 63.321994859329806\n",
      "l2 norm of gradients: 0.6554343541496158\n",
      "l2 norm of weights: 6.2926897471816785\n",
      "---------------------\n",
      "Iteration Number: 1683\n",
      "Loss: 63.30088877803683\n",
      "l2 norm of gradients: 0.6550927254315582\n",
      "l2 norm of weights: 6.292528836402892\n",
      "---------------------\n",
      "Iteration Number: 1684\n",
      "Loss: 63.27980445295129\n",
      "l2 norm of gradients: 0.6547514366909094\n",
      "l2 norm of weights: 6.2923679865323034\n",
      "---------------------\n",
      "Iteration Number: 1685\n",
      "Loss: 63.25874185134569\n",
      "l2 norm of gradients: 0.6544104878046184\n",
      "l2 norm of weights: 6.292207197499219\n",
      "---------------------\n",
      "Iteration Number: 1686\n",
      "Loss: 63.237700940384016\n",
      "l2 norm of gradients: 0.6540698786479275\n",
      "l2 norm of weights: 6.292046469233064\n",
      "---------------------\n",
      "Iteration Number: 1687\n",
      "Loss: 63.2166816872527\n",
      "l2 norm of gradients: 0.6537296090943744\n",
      "l2 norm of weights: 6.2918858016633905\n",
      "---------------------\n",
      "Iteration Number: 1688\n",
      "Loss: 63.19568405931035\n",
      "l2 norm of gradients: 0.6533896790157919\n",
      "l2 norm of weights: 6.291725194719863\n",
      "---------------------\n",
      "Iteration Number: 1689\n",
      "Loss: 63.174708023881436\n",
      "l2 norm of gradients: 0.6530500882823095\n",
      "l2 norm of weights: 6.291564648332273\n",
      "---------------------\n",
      "Iteration Number: 1690\n",
      "Loss: 63.15375354837836\n",
      "l2 norm of gradients: 0.6527108367623543\n",
      "l2 norm of weights: 6.291404162430532\n",
      "---------------------\n",
      "Iteration Number: 1691\n",
      "Loss: 63.13282060010359\n",
      "l2 norm of gradients: 0.6523719243226521\n",
      "l2 norm of weights: 6.2912437369446685\n",
      "---------------------\n",
      "Iteration Number: 1692\n",
      "Loss: 63.1119091466105\n",
      "l2 norm of gradients: 0.6520333508282284\n",
      "l2 norm of weights: 6.291083371804834\n",
      "---------------------\n",
      "Iteration Number: 1693\n",
      "Loss: 63.09101915539185\n",
      "l2 norm of gradients: 0.6516951161424103\n",
      "l2 norm of weights: 6.290923066941299\n",
      "---------------------\n",
      "Iteration Number: 1694\n",
      "Loss: 63.07015059389481\n",
      "l2 norm of gradients: 0.6513572201268273\n",
      "l2 norm of weights: 6.290762822284455\n",
      "---------------------\n",
      "Iteration Number: 1695\n",
      "Loss: 63.049303429810564\n",
      "l2 norm of gradients: 0.6510196626414135\n",
      "l2 norm of weights: 6.290602637764814\n",
      "---------------------\n",
      "Iteration Number: 1696\n",
      "Loss: 63.028477630711215\n",
      "l2 norm of gradients: 0.6506824435444093\n",
      "l2 norm of weights: 6.290442513313007\n",
      "---------------------\n",
      "Iteration Number: 1697\n",
      "Loss: 63.007673164331166\n",
      "l2 norm of gradients: 0.6503455626923619\n",
      "l2 norm of weights: 6.290282448859785\n",
      "---------------------\n",
      "Iteration Number: 1698\n",
      "Loss: 62.986889998275124\n",
      "l2 norm of gradients: 0.6500090199401289\n",
      "l2 norm of weights: 6.290122444336021\n",
      "---------------------\n",
      "Iteration Number: 1699\n",
      "Loss: 62.96612810035366\n",
      "l2 norm of gradients: 0.6496728151408798\n",
      "l2 norm of weights: 6.2899624996727015\n",
      "---------------------\n",
      "Iteration Number: 1700\n",
      "Loss: 62.945387438419246\n",
      "l2 norm of gradients: 0.6493369481460974\n",
      "l2 norm of weights: 6.28980261480094\n",
      "---------------------\n",
      "Iteration Number: 1701\n",
      "Loss: 62.924667980209634\n",
      "l2 norm of gradients: 0.6490014188055805\n",
      "l2 norm of weights: 6.289642789651965\n",
      "---------------------\n",
      "Iteration Number: 1702\n",
      "Loss: 62.90396969366194\n",
      "l2 norm of gradients: 0.6486662269674461\n",
      "l2 norm of weights: 6.289483024157125\n",
      "---------------------\n",
      "Iteration Number: 1703\n",
      "Loss: 62.88329254667841\n",
      "l2 norm of gradients: 0.6483313724781324\n",
      "l2 norm of weights: 6.28932331824789\n",
      "---------------------\n",
      "Iteration Number: 1704\n",
      "Loss: 62.862636507288684\n",
      "l2 norm of gradients: 0.6479968551824006\n",
      "l2 norm of weights: 6.289163671855845\n",
      "---------------------\n",
      "Iteration Number: 1705\n",
      "Loss: 62.84200154336184\n",
      "l2 norm of gradients: 0.647662674923337\n",
      "l2 norm of weights: 6.2890040849126985\n",
      "---------------------\n",
      "Iteration Number: 1706\n",
      "Loss: 62.821387623122526\n",
      "l2 norm of gradients: 0.647328831542357\n",
      "l2 norm of weights: 6.2888445573502745\n",
      "---------------------\n",
      "Iteration Number: 1707\n",
      "Loss: 62.80079471450641\n",
      "l2 norm of gradients: 0.6469953248792075\n",
      "l2 norm of weights: 6.288685089100516\n",
      "---------------------\n",
      "Iteration Number: 1708\n",
      "Loss: 62.780222785764316\n",
      "l2 norm of gradients: 0.6466621547719689\n",
      "l2 norm of weights: 6.288525680095488\n",
      "---------------------\n",
      "Iteration Number: 1709\n",
      "Loss: 62.75967180502904\n",
      "l2 norm of gradients: 0.646329321057059\n",
      "l2 norm of weights: 6.28836633026737\n",
      "---------------------\n",
      "Iteration Number: 1710\n",
      "Loss: 62.73914174057476\n",
      "l2 norm of gradients: 0.6459968235692362\n",
      "l2 norm of weights: 6.288207039548462\n",
      "---------------------\n",
      "Iteration Number: 1711\n",
      "Loss: 62.71863256057514\n",
      "l2 norm of gradients: 0.6456646621416016\n",
      "l2 norm of weights: 6.288047807871182\n",
      "---------------------\n",
      "Iteration Number: 1712\n",
      "Loss: 62.698144233449014\n",
      "l2 norm of gradients: 0.6453328366056033\n",
      "l2 norm of weights: 6.287888635168068\n",
      "---------------------\n",
      "Iteration Number: 1713\n",
      "Loss: 62.677676727442744\n",
      "l2 norm of gradients: 0.6450013467910397\n",
      "l2 norm of weights: 6.28772952137177\n",
      "---------------------\n",
      "Iteration Number: 1714\n",
      "Loss: 62.65723001101844\n",
      "l2 norm of gradients: 0.6446701925260621\n",
      "l2 norm of weights: 6.2875704664150645\n",
      "---------------------\n",
      "Iteration Number: 1715\n",
      "Loss: 62.636804052610515\n",
      "l2 norm of gradients: 0.6443393736371793\n",
      "l2 norm of weights: 6.287411470230841\n",
      "---------------------\n",
      "Iteration Number: 1716\n",
      "Loss: 62.616398820738496\n",
      "l2 norm of gradients: 0.6440088899492604\n",
      "l2 norm of weights: 6.287252532752104\n",
      "---------------------\n",
      "Iteration Number: 1717\n",
      "Loss: 62.5960142838794\n",
      "l2 norm of gradients: 0.643678741285539\n",
      "l2 norm of weights: 6.287093653911983\n",
      "---------------------\n",
      "Iteration Number: 1718\n",
      "Loss: 62.57565041053179\n",
      "l2 norm of gradients: 0.6433489274676166\n",
      "l2 norm of weights: 6.28693483364372\n",
      "---------------------\n",
      "Iteration Number: 1719\n",
      "Loss: 62.55530716946004\n",
      "l2 norm of gradients: 0.6430194483154668\n",
      "l2 norm of weights: 6.286776071880673\n",
      "---------------------\n",
      "Iteration Number: 1720\n",
      "Loss: 62.534984529229895\n",
      "l2 norm of gradients: 0.6426903036474393\n",
      "l2 norm of weights: 6.286617368556322\n",
      "---------------------\n",
      "Iteration Number: 1721\n",
      "Loss: 62.51468245854918\n",
      "l2 norm of gradients: 0.6423614932802637\n",
      "l2 norm of weights: 6.2864587236042615\n",
      "---------------------\n",
      "Iteration Number: 1722\n",
      "Loss: 62.49440092615412\n",
      "l2 norm of gradients: 0.6420330170290537\n",
      "l2 norm of weights: 6.286300136958203\n",
      "---------------------\n",
      "Iteration Number: 1723\n",
      "Loss: 62.47413990089916\n",
      "l2 norm of gradients: 0.6417048747073115\n",
      "l2 norm of weights: 6.286141608551974\n",
      "---------------------\n",
      "Iteration Number: 1724\n",
      "Loss: 62.45389935156028\n",
      "l2 norm of gradients: 0.6413770661269319\n",
      "l2 norm of weights: 6.28598313831952\n",
      "---------------------\n",
      "Iteration Number: 1725\n",
      "Loss: 62.43367924696693\n",
      "l2 norm of gradients: 0.6410495910982066\n",
      "l2 norm of weights: 6.285824726194905\n",
      "---------------------\n",
      "Iteration Number: 1726\n",
      "Loss: 62.41347955614757\n",
      "l2 norm of gradients: 0.6407224494298293\n",
      "l2 norm of weights: 6.2856663721123045\n",
      "---------------------\n",
      "Iteration Number: 1727\n",
      "Loss: 62.39330024796141\n",
      "l2 norm of gradients: 0.6403956409288988\n",
      "l2 norm of weights: 6.285508076006015\n",
      "---------------------\n",
      "Iteration Number: 1728\n",
      "Loss: 62.37314129152395\n",
      "l2 norm of gradients: 0.6400691654009254\n",
      "l2 norm of weights: 6.285349837810448\n",
      "---------------------\n",
      "Iteration Number: 1729\n",
      "Loss: 62.35300265581112\n",
      "l2 norm of gradients: 0.6397430226498342\n",
      "l2 norm of weights: 6.285191657460129\n",
      "---------------------\n",
      "Iteration Number: 1730\n",
      "Loss: 62.332884309872455\n",
      "l2 norm of gradients: 0.6394172124779708\n",
      "l2 norm of weights: 6.285033534889702\n",
      "---------------------\n",
      "Iteration Number: 1731\n",
      "Loss: 62.3127862229262\n",
      "l2 norm of gradients: 0.6390917346861051\n",
      "l2 norm of weights: 6.284875470033925\n",
      "---------------------\n",
      "Iteration Number: 1732\n",
      "Loss: 62.292708364154585\n",
      "l2 norm of gradients: 0.6387665890734376\n",
      "l2 norm of weights: 6.2847174628276745\n",
      "---------------------\n",
      "Iteration Number: 1733\n",
      "Loss: 62.27265070273722\n",
      "l2 norm of gradients: 0.6384417754376028\n",
      "l2 norm of weights: 6.284559513205939\n",
      "---------------------\n",
      "Iteration Number: 1734\n",
      "Loss: 62.25261320800818\n",
      "l2 norm of gradients: 0.6381172935746755\n",
      "l2 norm of weights: 6.284401621103823\n",
      "---------------------\n",
      "Iteration Number: 1735\n",
      "Loss: 62.23259584916477\n",
      "l2 norm of gradients: 0.6377931432791756\n",
      "l2 norm of weights: 6.284243786456551\n",
      "---------------------\n",
      "Iteration Number: 1736\n",
      "Loss: 62.21259859569317\n",
      "l2 norm of gradients: 0.6374693243440722\n",
      "l2 norm of weights: 6.284086009199455\n",
      "---------------------\n",
      "Iteration Number: 1737\n",
      "Loss: 62.19262141692931\n",
      "l2 norm of gradients: 0.637145836560791\n",
      "l2 norm of weights: 6.283928289267988\n",
      "---------------------\n",
      "Iteration Number: 1738\n",
      "Loss: 62.172664282336534\n",
      "l2 norm of gradients: 0.6368226797192181\n",
      "l2 norm of weights: 6.283770626597715\n",
      "---------------------\n",
      "Iteration Number: 1739\n",
      "Loss: 62.15272716140784\n",
      "l2 norm of gradients: 0.6364998536077053\n",
      "l2 norm of weights: 6.283613021124317\n",
      "---------------------\n",
      "Iteration Number: 1740\n",
      "Loss: 62.13281002362274\n",
      "l2 norm of gradients: 0.6361773580130761\n",
      "l2 norm of weights: 6.28345547278359\n",
      "---------------------\n",
      "Iteration Number: 1741\n",
      "Loss: 62.11291283869987\n",
      "l2 norm of gradients: 0.6358551927206313\n",
      "l2 norm of weights: 6.283297981511444\n",
      "---------------------\n",
      "Iteration Number: 1742\n",
      "Loss: 62.09303557611239\n",
      "l2 norm of gradients: 0.6355333575141542\n",
      "l2 norm of weights: 6.2831405472439\n",
      "---------------------\n",
      "Iteration Number: 1743\n",
      "Loss: 62.07317820560934\n",
      "l2 norm of gradients: 0.6352118521759165\n",
      "l2 norm of weights: 6.2829831699171\n",
      "---------------------\n",
      "Iteration Number: 1744\n",
      "Loss: 62.05334069685492\n",
      "l2 norm of gradients: 0.6348906764866838\n",
      "l2 norm of weights: 6.282825849467293\n",
      "---------------------\n",
      "Iteration Number: 1745\n",
      "Loss: 62.03352301965713\n",
      "l2 norm of gradients: 0.634569830225722\n",
      "l2 norm of weights: 6.282668585830846\n",
      "---------------------\n",
      "Iteration Number: 1746\n",
      "Loss: 62.01372514377594\n",
      "l2 norm of gradients: 0.6342493131708019\n",
      "l2 norm of weights: 6.282511378944241\n",
      "---------------------\n",
      "Iteration Number: 1747\n",
      "Loss: 61.99394703911147\n",
      "l2 norm of gradients: 0.6339291250982063\n",
      "l2 norm of weights: 6.282354228744068\n",
      "---------------------\n",
      "Iteration Number: 1748\n",
      "Loss: 61.974188675503\n",
      "l2 norm of gradients: 0.6336092657827352\n",
      "l2 norm of weights: 6.282197135167036\n",
      "---------------------\n",
      "Iteration Number: 1749\n",
      "Loss: 61.95445002289221\n",
      "l2 norm of gradients: 0.6332897349977124\n",
      "l2 norm of weights: 6.282040098149964\n",
      "---------------------\n",
      "Iteration Number: 1750\n",
      "Loss: 61.93473105126038\n",
      "l2 norm of gradients: 0.6329705325149907\n",
      "l2 norm of weights: 6.281883117629786\n",
      "---------------------\n",
      "Iteration Number: 1751\n",
      "Loss: 61.91503173063883\n",
      "l2 norm of gradients: 0.6326516581049588\n",
      "l2 norm of weights: 6.281726193543547\n",
      "---------------------\n",
      "Iteration Number: 1752\n",
      "Loss: 61.8953520310939\n",
      "l2 norm of gradients: 0.6323331115365475\n",
      "l2 norm of weights: 6.281569325828408\n",
      "---------------------\n",
      "Iteration Number: 1753\n",
      "Loss: 61.87569192270038\n",
      "l2 norm of gradients: 0.6320148925772351\n",
      "l2 norm of weights: 6.28141251442164\n",
      "---------------------\n",
      "Iteration Number: 1754\n",
      "Loss: 61.856051375740314\n",
      "l2 norm of gradients: 0.6316970009930545\n",
      "l2 norm of weights: 6.281255759260628\n",
      "---------------------\n",
      "Iteration Number: 1755\n",
      "Loss: 61.836430360268665\n",
      "l2 norm of gradients: 0.631379436548599\n",
      "l2 norm of weights: 6.281099060282868\n",
      "---------------------\n",
      "Iteration Number: 1756\n",
      "Loss: 61.81682884661765\n",
      "l2 norm of gradients: 0.6310621990070289\n",
      "l2 norm of weights: 6.280942417425969\n",
      "---------------------\n",
      "Iteration Number: 1757\n",
      "Loss: 61.797246805024685\n",
      "l2 norm of gradients: 0.6307452881300785\n",
      "l2 norm of weights: 6.280785830627653\n",
      "---------------------\n",
      "Iteration Number: 1758\n",
      "Loss: 61.77768420589285\n",
      "l2 norm of gradients: 0.6304287036780609\n",
      "l2 norm of weights: 6.280629299825751\n",
      "---------------------\n",
      "Iteration Number: 1759\n",
      "Loss: 61.75814101954483\n",
      "l2 norm of gradients: 0.6301124454098765\n",
      "l2 norm of weights: 6.280472824958211\n",
      "---------------------\n",
      "Iteration Number: 1760\n",
      "Loss: 61.73861721646698\n",
      "l2 norm of gradients: 0.6297965130830185\n",
      "l2 norm of weights: 6.280316405963087\n",
      "---------------------\n",
      "Iteration Number: 1761\n",
      "Loss: 61.719112767032044\n",
      "l2 norm of gradients: 0.6294809064535793\n",
      "l2 norm of weights: 6.280160042778549\n",
      "---------------------\n",
      "Iteration Number: 1762\n",
      "Loss: 61.69962764186496\n",
      "l2 norm of gradients: 0.629165625276258\n",
      "l2 norm of weights: 6.280003735342875\n",
      "---------------------\n",
      "Iteration Number: 1763\n",
      "Loss: 61.68016181147847\n",
      "l2 norm of gradients: 0.6288506693043661\n",
      "l2 norm of weights: 6.279847483594456\n",
      "---------------------\n",
      "Iteration Number: 1764\n",
      "Loss: 61.66071524647588\n",
      "l2 norm of gradients: 0.6285360382898356\n",
      "l2 norm of weights: 6.2796912874717945\n",
      "---------------------\n",
      "Iteration Number: 1765\n",
      "Loss: 61.6412879174728\n",
      "l2 norm of gradients: 0.628221731983224\n",
      "l2 norm of weights: 6.2795351469135\n",
      "---------------------\n",
      "Iteration Number: 1766\n",
      "Loss: 61.621879795289146\n",
      "l2 norm of gradients: 0.627907750133723\n",
      "l2 norm of weights: 6.2793790618582985\n",
      "---------------------\n",
      "Iteration Number: 1767\n",
      "Loss: 61.60249085051607\n",
      "l2 norm of gradients: 0.6275940924891639\n",
      "l2 norm of weights: 6.279223032245023\n",
      "---------------------\n",
      "Iteration Number: 1768\n",
      "Loss: 61.58312105402644\n",
      "l2 norm of gradients: 0.6272807587960255\n",
      "l2 norm of weights: 6.279067058012616\n",
      "---------------------\n",
      "Iteration Number: 1769\n",
      "Loss: 61.563770376597695\n",
      "l2 norm of gradients: 0.6269677487994402\n",
      "l2 norm of weights: 6.278911139100134\n",
      "---------------------\n",
      "Iteration Number: 1770\n",
      "Loss: 61.54443878915805\n",
      "l2 norm of gradients: 0.6266550622432018\n",
      "l2 norm of weights: 6.27875527544674\n",
      "---------------------\n",
      "Iteration Number: 1771\n",
      "Loss: 61.52512626264303\n",
      "l2 norm of gradients: 0.6263426988697721\n",
      "l2 norm of weights: 6.278599466991709\n",
      "---------------------\n",
      "Iteration Number: 1772\n",
      "Loss: 61.50583276796351\n",
      "l2 norm of gradients: 0.6260306584202878\n",
      "l2 norm of weights: 6.278443713674424\n",
      "---------------------\n",
      "Iteration Number: 1773\n",
      "Loss: 61.48655827613581\n",
      "l2 norm of gradients: 0.625718940634568\n",
      "l2 norm of weights: 6.278288015434378\n",
      "---------------------\n",
      "Iteration Number: 1774\n",
      "Loss: 61.46730275822213\n",
      "l2 norm of gradients: 0.625407545251121\n",
      "l2 norm of weights: 6.278132372211177\n",
      "---------------------\n",
      "Iteration Number: 1775\n",
      "Loss: 61.44806618532907\n",
      "l2 norm of gradients: 0.625096472007152\n",
      "l2 norm of weights: 6.2779767839445295\n",
      "---------------------\n",
      "Iteration Number: 1776\n",
      "Loss: 61.42884852860147\n",
      "l2 norm of gradients: 0.6247857206385694\n",
      "l2 norm of weights: 6.277821250574259\n",
      "---------------------\n",
      "Iteration Number: 1777\n",
      "Loss: 61.40964975924517\n",
      "l2 norm of gradients: 0.6244752908799928\n",
      "l2 norm of weights: 6.277665772040295\n",
      "---------------------\n",
      "Iteration Number: 1778\n",
      "Loss: 61.3904698485127\n",
      "l2 norm of gradients: 0.6241651824647602\n",
      "l2 norm of weights: 6.277510348282676\n",
      "---------------------\n",
      "Iteration Number: 1779\n",
      "Loss: 61.37130876764503\n",
      "l2 norm of gradients: 0.6238553951249345\n",
      "l2 norm of weights: 6.277354979241551\n",
      "---------------------\n",
      "Iteration Number: 1780\n",
      "Loss: 61.352166488003554\n",
      "l2 norm of gradients: 0.6235459285913123\n",
      "l2 norm of weights: 6.277199664857174\n",
      "---------------------\n",
      "Iteration Number: 1781\n",
      "Loss: 61.33304298090965\n",
      "l2 norm of gradients: 0.62323678259343\n",
      "l2 norm of weights: 6.277044405069911\n",
      "---------------------\n",
      "Iteration Number: 1782\n",
      "Loss: 61.31393821778433\n",
      "l2 norm of gradients: 0.6229279568595709\n",
      "l2 norm of weights: 6.276889199820234\n",
      "---------------------\n",
      "Iteration Number: 1783\n",
      "Loss: 61.29485217019427\n",
      "l2 norm of gradients: 0.6226194511167743\n",
      "l2 norm of weights: 6.276734049048722\n",
      "---------------------\n",
      "Iteration Number: 1784\n",
      "Loss: 61.27578480950495\n",
      "l2 norm of gradients: 0.6223112650908416\n",
      "l2 norm of weights: 6.276578952696064\n",
      "---------------------\n",
      "Iteration Number: 1785\n",
      "Loss: 61.256736107370564\n",
      "l2 norm of gradients: 0.6220033985063436\n",
      "l2 norm of weights: 6.276423910703056\n",
      "---------------------\n",
      "Iteration Number: 1786\n",
      "Loss: 61.23770603533973\n",
      "l2 norm of gradients: 0.6216958510866291\n",
      "l2 norm of weights: 6.2762689230106\n",
      "---------------------\n",
      "Iteration Number: 1787\n",
      "Loss: 61.218694565069534\n",
      "l2 norm of gradients: 0.6213886225538312\n",
      "l2 norm of weights: 6.276113989559707\n",
      "---------------------\n",
      "Iteration Number: 1788\n",
      "Loss: 61.19970166824653\n",
      "l2 norm of gradients: 0.6210817126288761\n",
      "l2 norm of weights: 6.2759591102914944\n",
      "---------------------\n",
      "Iteration Number: 1789\n",
      "Loss: 61.18072731663273\n",
      "l2 norm of gradients: 0.6207751210314898\n",
      "l2 norm of weights: 6.275804285147185\n",
      "---------------------\n",
      "Iteration Number: 1790\n",
      "Loss: 61.161771481967094\n",
      "l2 norm of gradients: 0.620468847480205\n",
      "l2 norm of weights: 6.275649514068111\n",
      "---------------------\n",
      "Iteration Number: 1791\n",
      "Loss: 61.1428341361004\n",
      "l2 norm of gradients: 0.6201628916923706\n",
      "l2 norm of weights: 6.27549479699571\n",
      "---------------------\n",
      "Iteration Number: 1792\n",
      "Loss: 61.12391525090306\n",
      "l2 norm of gradients: 0.6198572533841579\n",
      "l2 norm of weights: 6.275340133871525\n",
      "---------------------\n",
      "Iteration Number: 1793\n",
      "Loss: 61.105014798253755\n",
      "l2 norm of gradients: 0.6195519322705687\n",
      "l2 norm of weights: 6.275185524637208\n",
      "---------------------\n",
      "Iteration Number: 1794\n",
      "Loss: 61.086132750156956\n",
      "l2 norm of gradients: 0.6192469280654427\n",
      "l2 norm of weights: 6.275030969234512\n",
      "---------------------\n",
      "Iteration Number: 1795\n",
      "Loss: 61.06726907858011\n",
      "l2 norm of gradients: 0.6189422404814654\n",
      "l2 norm of weights: 6.274876467605303\n",
      "---------------------\n",
      "Iteration Number: 1796\n",
      "Loss: 61.04842375557596\n",
      "l2 norm of gradients: 0.6186378692301762\n",
      "l2 norm of weights: 6.274722019691546\n",
      "---------------------\n",
      "Iteration Number: 1797\n",
      "Loss: 61.029596753294534\n",
      "l2 norm of gradients: 0.6183338140219747\n",
      "l2 norm of weights: 6.274567625435314\n",
      "---------------------\n",
      "Iteration Number: 1798\n",
      "Loss: 61.0107880438814\n",
      "l2 norm of gradients: 0.6180300745661309\n",
      "l2 norm of weights: 6.274413284778787\n",
      "---------------------\n",
      "Iteration Number: 1799\n",
      "Loss: 60.99199759942688\n",
      "l2 norm of gradients: 0.6177266505707899\n",
      "l2 norm of weights: 6.274258997664249\n",
      "---------------------\n",
      "Iteration Number: 1800\n",
      "Loss: 60.97322539222604\n",
      "l2 norm of gradients: 0.6174235417429826\n",
      "l2 norm of weights: 6.274104764034087\n",
      "---------------------\n",
      "Iteration Number: 1801\n",
      "Loss: 60.95447139462386\n",
      "l2 norm of gradients: 0.6171207477886311\n",
      "l2 norm of weights: 6.273950583830795\n",
      "---------------------\n",
      "Iteration Number: 1802\n",
      "Loss: 60.935735578804724\n",
      "l2 norm of gradients: 0.616818268412558\n",
      "l2 norm of weights: 6.2737964569969735\n",
      "---------------------\n",
      "Iteration Number: 1803\n",
      "Loss: 60.9170179172155\n",
      "l2 norm of gradients: 0.6165161033184936\n",
      "l2 norm of weights: 6.273642383475323\n",
      "---------------------\n",
      "Iteration Number: 1804\n",
      "Loss: 60.89831838227276\n",
      "l2 norm of gradients: 0.6162142522090838\n",
      "l2 norm of weights: 6.27348836320865\n",
      "---------------------\n",
      "Iteration Number: 1805\n",
      "Loss: 60.87963694644031\n",
      "l2 norm of gradients: 0.6159127147858982\n",
      "l2 norm of weights: 6.273334396139869\n",
      "---------------------\n",
      "Iteration Number: 1806\n",
      "Loss: 60.86097358217974\n",
      "l2 norm of gradients: 0.615611490749437\n",
      "l2 norm of weights: 6.27318048221199\n",
      "---------------------\n",
      "Iteration Number: 1807\n",
      "Loss: 60.84232826208623\n",
      "l2 norm of gradients: 0.6153105797991406\n",
      "l2 norm of weights: 6.273026621368136\n",
      "---------------------\n",
      "Iteration Number: 1808\n",
      "Loss: 60.82370095872278\n",
      "l2 norm of gradients: 0.6150099816333955\n",
      "l2 norm of weights: 6.272872813551528\n",
      "---------------------\n",
      "Iteration Number: 1809\n",
      "Loss: 60.805091644725564\n",
      "l2 norm of gradients: 0.6147096959495434\n",
      "l2 norm of weights: 6.272719058705491\n",
      "---------------------\n",
      "Iteration Number: 1810\n",
      "Loss: 60.78650029283172\n",
      "l2 norm of gradients: 0.6144097224438887\n",
      "l2 norm of weights: 6.2725653567734545\n",
      "---------------------\n",
      "Iteration Number: 1811\n",
      "Loss: 60.76792687573057\n",
      "l2 norm of gradients: 0.6141100608117068\n",
      "l2 norm of weights: 6.272411707698951\n",
      "---------------------\n",
      "Iteration Number: 1812\n",
      "Loss: 60.74937136622825\n",
      "l2 norm of gradients: 0.6138107107472512\n",
      "l2 norm of weights: 6.2722581114256135\n",
      "---------------------\n",
      "Iteration Number: 1813\n",
      "Loss: 60.730833737102024\n",
      "l2 norm of gradients: 0.613511671943762\n",
      "l2 norm of weights: 6.272104567897182\n",
      "---------------------\n",
      "Iteration Number: 1814\n",
      "Loss: 60.71231396124743\n",
      "l2 norm of gradients: 0.6132129440934735\n",
      "l2 norm of weights: 6.271951077057496\n",
      "---------------------\n",
      "Iteration Number: 1815\n",
      "Loss: 60.69381201157603\n",
      "l2 norm of gradients: 0.6129145268876226\n",
      "l2 norm of weights: 6.2717976388504955\n",
      "---------------------\n",
      "Iteration Number: 1816\n",
      "Loss: 60.67532786100406\n",
      "l2 norm of gradients: 0.6126164200164557\n",
      "l2 norm of weights: 6.271644253220228\n",
      "---------------------\n",
      "Iteration Number: 1817\n",
      "Loss: 60.656861482586734\n",
      "l2 norm of gradients: 0.612318623169238\n",
      "l2 norm of weights: 6.271490920110838\n",
      "---------------------\n",
      "Iteration Number: 1818\n",
      "Loss: 60.638412849383926\n",
      "l2 norm of gradients: 0.6120211360342604\n",
      "l2 norm of weights: 6.271337639466575\n",
      "---------------------\n",
      "Iteration Number: 1819\n",
      "Loss: 60.61998193442998\n",
      "l2 norm of gradients: 0.6117239582988477\n",
      "l2 norm of weights: 6.271184411231787\n",
      "---------------------\n",
      "Iteration Number: 1820\n",
      "Loss: 60.60156871092995\n",
      "l2 norm of gradients: 0.6114270896493665\n",
      "l2 norm of weights: 6.271031235350928\n",
      "---------------------\n",
      "Iteration Number: 1821\n",
      "Loss: 60.58317315201482\n",
      "l2 norm of gradients: 0.6111305297712335\n",
      "l2 norm of weights: 6.270878111768548\n",
      "---------------------\n",
      "Iteration Number: 1822\n",
      "Loss: 60.564795230965665\n",
      "l2 norm of gradients: 0.6108342783489229\n",
      "l2 norm of weights: 6.2707250404293005\n",
      "---------------------\n",
      "Iteration Number: 1823\n",
      "Loss: 60.546434920996944\n",
      "l2 norm of gradients: 0.6105383350659743\n",
      "l2 norm of weights: 6.270572021277941\n",
      "---------------------\n",
      "Iteration Number: 1824\n",
      "Loss: 60.52809219547316\n",
      "l2 norm of gradients: 0.6102426996050015\n",
      "l2 norm of weights: 6.270419054259323\n",
      "---------------------\n",
      "Iteration Number: 1825\n",
      "Loss: 60.509767027741574\n",
      "l2 norm of gradients: 0.6099473716476992\n",
      "l2 norm of weights: 6.270266139318403\n",
      "---------------------\n",
      "Iteration Number: 1826\n",
      "Loss: 60.491459391217035\n",
      "l2 norm of gradients: 0.6096523508748521\n",
      "l2 norm of weights: 6.270113276400237\n",
      "---------------------\n",
      "Iteration Number: 1827\n",
      "Loss: 60.47316925930846\n",
      "l2 norm of gradients: 0.609357636966342\n",
      "l2 norm of weights: 6.26996046544998\n",
      "---------------------\n",
      "Iteration Number: 1828\n",
      "Loss: 60.454896605571044\n",
      "l2 norm of gradients: 0.6090632296011559\n",
      "l2 norm of weights: 6.269807706412888\n",
      "---------------------\n",
      "Iteration Number: 1829\n",
      "Loss: 60.43664140355328\n",
      "l2 norm of gradients: 0.6087691284573942\n",
      "l2 norm of weights: 6.269654999234317\n",
      "---------------------\n",
      "Iteration Number: 1830\n",
      "Loss: 60.418403626852076\n",
      "l2 norm of gradients: 0.6084753332122784\n",
      "l2 norm of weights: 6.269502343859721\n",
      "---------------------\n",
      "Iteration Number: 1831\n",
      "Loss: 60.40018324905685\n",
      "l2 norm of gradients: 0.6081818435421592\n",
      "l2 norm of weights: 6.269349740234656\n",
      "---------------------\n",
      "Iteration Number: 1832\n",
      "Loss: 60.3819802439027\n",
      "l2 norm of gradients: 0.6078886591225243\n",
      "l2 norm of weights: 6.269197188304774\n",
      "---------------------\n",
      "Iteration Number: 1833\n",
      "Loss: 60.36379458506898\n",
      "l2 norm of gradients: 0.607595779628006\n",
      "l2 norm of weights: 6.269044688015828\n",
      "---------------------\n",
      "Iteration Number: 1834\n",
      "Loss: 60.345626246356034\n",
      "l2 norm of gradients: 0.6073032047323899\n",
      "l2 norm of weights: 6.26889223931367\n",
      "---------------------\n",
      "Iteration Number: 1835\n",
      "Loss: 60.327475201538505\n",
      "l2 norm of gradients: 0.6070109341086216\n",
      "l2 norm of weights: 6.268739842144251\n",
      "---------------------\n",
      "Iteration Number: 1836\n",
      "Loss: 60.309341424551064\n",
      "l2 norm of gradients: 0.6067189674288161\n",
      "l2 norm of weights: 6.268587496453617\n",
      "---------------------\n",
      "Iteration Number: 1837\n",
      "Loss: 60.29122488921047\n",
      "l2 norm of gradients: 0.6064273043642645\n",
      "l2 norm of weights: 6.268435202187916\n",
      "---------------------\n",
      "Iteration Number: 1838\n",
      "Loss: 60.273125569565835\n",
      "l2 norm of gradients: 0.6061359445854423\n",
      "l2 norm of weights: 6.268282959293393\n",
      "---------------------\n",
      "Iteration Number: 1839\n",
      "Loss: 60.25504343952347\n",
      "l2 norm of gradients: 0.6058448877620172\n",
      "l2 norm of weights: 6.268130767716391\n",
      "---------------------\n",
      "Iteration Number: 1840\n",
      "Loss: 60.23697847317852\n",
      "l2 norm of gradients: 0.6055541335628568\n",
      "l2 norm of weights: 6.26797862740335\n",
      "---------------------\n",
      "Iteration Number: 1841\n",
      "Loss: 60.21893064462038\n",
      "l2 norm of gradients: 0.6052636816560374\n",
      "l2 norm of weights: 6.267826538300808\n",
      "---------------------\n",
      "Iteration Number: 1842\n",
      "Loss: 60.200899927930955\n",
      "l2 norm of gradients: 0.6049735317088505\n",
      "l2 norm of weights: 6.267674500355399\n",
      "---------------------\n",
      "Iteration Number: 1843\n",
      "Loss: 60.18288629735027\n",
      "l2 norm of gradients: 0.6046836833878113\n",
      "l2 norm of weights: 6.267522513513858\n",
      "---------------------\n",
      "Iteration Number: 1844\n",
      "Loss: 60.16488972709025\n",
      "l2 norm of gradients: 0.6043941363586666\n",
      "l2 norm of weights: 6.267370577723011\n",
      "---------------------\n",
      "Iteration Number: 1845\n",
      "Loss: 60.14691019135943\n",
      "l2 norm of gradients: 0.6041048902864025\n",
      "l2 norm of weights: 6.267218692929786\n",
      "---------------------\n",
      "Iteration Number: 1846\n",
      "Loss: 60.1289476645615\n",
      "l2 norm of gradients: 0.6038159448352524\n",
      "l2 norm of weights: 6.267066859081206\n",
      "---------------------\n",
      "Iteration Number: 1847\n",
      "Loss: 60.11100212094703\n",
      "l2 norm of gradients: 0.6035272996687042\n",
      "l2 norm of weights: 6.266915076124388\n",
      "---------------------\n",
      "Iteration Number: 1848\n",
      "Loss: 60.09307353495884\n",
      "l2 norm of gradients: 0.603238954449509\n",
      "l2 norm of weights: 6.266763344006549\n",
      "---------------------\n",
      "Iteration Number: 1849\n",
      "Loss: 60.07516188105073\n",
      "l2 norm of gradients: 0.6029509088396878\n",
      "l2 norm of weights: 6.266611662674998\n",
      "---------------------\n",
      "Iteration Number: 1850\n",
      "Loss: 60.05726713375949\n",
      "l2 norm of gradients: 0.6026631625005404\n",
      "l2 norm of weights: 6.266460032077144\n",
      "---------------------\n",
      "Iteration Number: 1851\n",
      "Loss: 60.039389267535086\n",
      "l2 norm of gradients: 0.6023757150926523\n",
      "l2 norm of weights: 6.266308452160489\n",
      "---------------------\n",
      "Iteration Number: 1852\n",
      "Loss: 60.02152825700705\n",
      "l2 norm of gradients: 0.6020885662759027\n",
      "l2 norm of weights: 6.2661569228726295\n",
      "---------------------\n",
      "Iteration Number: 1853\n",
      "Loss: 60.00368407676165\n",
      "l2 norm of gradients: 0.6018017157094723\n",
      "l2 norm of weights: 6.2660054441612605\n",
      "---------------------\n",
      "Iteration Number: 1854\n",
      "Loss: 59.98585670151084\n",
      "l2 norm of gradients: 0.601515163051851\n",
      "l2 norm of weights: 6.265854015974169\n",
      "---------------------\n",
      "Iteration Number: 1855\n",
      "Loss: 59.96804610597661\n",
      "l2 norm of gradients: 0.6012289079608455\n",
      "l2 norm of weights: 6.2657026382592385\n",
      "---------------------\n",
      "Iteration Number: 1856\n",
      "Loss: 59.95025226489798\n",
      "l2 norm of gradients: 0.6009429500935872\n",
      "l2 norm of weights: 6.265551310964447\n",
      "---------------------\n",
      "Iteration Number: 1857\n",
      "Loss: 59.93247515308394\n",
      "l2 norm of gradients: 0.6006572891065395\n",
      "l2 norm of weights: 6.265400034037866\n",
      "---------------------\n",
      "Iteration Number: 1858\n",
      "Loss: 59.91471474535819\n",
      "l2 norm of gradients: 0.6003719246555057\n",
      "l2 norm of weights: 6.265248807427663\n",
      "---------------------\n",
      "Iteration Number: 1859\n",
      "Loss: 59.896971016619304\n",
      "l2 norm of gradients: 0.6000868563956366\n",
      "l2 norm of weights: 6.265097631082098\n",
      "---------------------\n",
      "Iteration Number: 1860\n",
      "Loss: 59.879243941810664\n",
      "l2 norm of gradients: 0.599802083981438\n",
      "l2 norm of weights: 6.264946504949524\n",
      "---------------------\n",
      "Iteration Number: 1861\n",
      "Loss: 59.86153349591482\n",
      "l2 norm of gradients: 0.599517607066779\n",
      "l2 norm of weights: 6.264795428978393\n",
      "---------------------\n",
      "Iteration Number: 1862\n",
      "Loss: 59.84383965396129\n",
      "l2 norm of gradients: 0.5992334253048983\n",
      "l2 norm of weights: 6.264644403117242\n",
      "---------------------\n",
      "Iteration Number: 1863\n",
      "Loss: 59.82616239097852\n",
      "l2 norm of gradients: 0.5989495383484126\n",
      "l2 norm of weights: 6.26449342731471\n",
      "---------------------\n",
      "Iteration Number: 1864\n",
      "Loss: 59.80850168217735\n",
      "l2 norm of gradients: 0.5986659458493248\n",
      "l2 norm of weights: 6.264342501519522\n",
      "---------------------\n",
      "Iteration Number: 1865\n",
      "Loss: 59.79085750262871\n",
      "l2 norm of gradients: 0.5983826474590292\n",
      "l2 norm of weights: 6.2641916256805015\n",
      "---------------------\n",
      "Iteration Number: 1866\n",
      "Loss: 59.77322982761456\n",
      "l2 norm of gradients: 0.5980996428283224\n",
      "l2 norm of weights: 6.264040799746561\n",
      "---------------------\n",
      "Iteration Number: 1867\n",
      "Loss: 59.75561863228966\n",
      "l2 norm of gradients: 0.5978169316074077\n",
      "l2 norm of weights: 6.263890023666707\n",
      "---------------------\n",
      "Iteration Number: 1868\n",
      "Loss: 59.73802389201613\n",
      "l2 norm of gradients: 0.5975345134459044\n",
      "l2 norm of weights: 6.263739297390039\n",
      "---------------------\n",
      "Iteration Number: 1869\n",
      "Loss: 59.72044558209355\n",
      "l2 norm of gradients: 0.5972523879928546\n",
      "l2 norm of weights: 6.263588620865748\n",
      "---------------------\n",
      "Iteration Number: 1870\n",
      "Loss: 59.70288367795386\n",
      "l2 norm of gradients: 0.5969705548967307\n",
      "l2 norm of weights: 6.263437994043116\n",
      "---------------------\n",
      "Iteration Number: 1871\n",
      "Loss: 59.685338154924885\n",
      "l2 norm of gradients: 0.596689013805443\n",
      "l2 norm of weights: 6.263287416871518\n",
      "---------------------\n",
      "Iteration Number: 1872\n",
      "Loss: 59.66780898858194\n",
      "l2 norm of gradients: 0.5964077643663471\n",
      "l2 norm of weights: 6.26313688930042\n",
      "---------------------\n",
      "Iteration Number: 1873\n",
      "Loss: 59.65029615434432\n",
      "l2 norm of gradients: 0.5961268062262512\n",
      "l2 norm of weights: 6.262986411279382\n",
      "---------------------\n",
      "Iteration Number: 1874\n",
      "Loss: 59.632799627867364\n",
      "l2 norm of gradients: 0.5958461390314231\n",
      "l2 norm of weights: 6.262835982758051\n",
      "---------------------\n",
      "Iteration Number: 1875\n",
      "Loss: 59.61531938466211\n",
      "l2 norm of gradients: 0.5955657624275982\n",
      "l2 norm of weights: 6.262685603686167\n",
      "---------------------\n",
      "Iteration Number: 1876\n",
      "Loss: 59.597855400425885\n",
      "l2 norm of gradients: 0.5952856760599865\n",
      "l2 norm of weights: 6.262535274013562\n",
      "---------------------\n",
      "Iteration Number: 1877\n",
      "Loss: 59.58040765087673\n",
      "l2 norm of gradients: 0.5950058795732799\n",
      "l2 norm of weights: 6.262384993690156\n",
      "---------------------\n",
      "Iteration Number: 1878\n",
      "Loss: 59.56297611166058\n",
      "l2 norm of gradients: 0.5947263726116593\n",
      "l2 norm of weights: 6.262234762665963\n",
      "---------------------\n",
      "Iteration Number: 1879\n",
      "Loss: 59.54556075862738\n",
      "l2 norm of gradients: 0.5944471548188023\n",
      "l2 norm of weights: 6.262084580891083\n",
      "---------------------\n",
      "Iteration Number: 1880\n",
      "Loss: 59.528161567547045\n",
      "l2 norm of gradients: 0.5941682258378902\n",
      "l2 norm of weights: 6.261934448315712\n",
      "---------------------\n",
      "Iteration Number: 1881\n",
      "Loss: 59.510778514346555\n",
      "l2 norm of gradients: 0.5938895853116151\n",
      "l2 norm of weights: 6.261784364890127\n",
      "---------------------\n",
      "Iteration Number: 1882\n",
      "Loss: 59.493411574843954\n",
      "l2 norm of gradients: 0.5936112328821871\n",
      "l2 norm of weights: 6.2616343305647035\n",
      "---------------------\n",
      "Iteration Number: 1883\n",
      "Loss: 59.47606072508378\n",
      "l2 norm of gradients: 0.5933331681913421\n",
      "l2 norm of weights: 6.261484345289903\n",
      "---------------------\n",
      "Iteration Number: 1884\n",
      "Loss: 59.45872594103063\n",
      "l2 norm of gradients: 0.5930553908803475\n",
      "l2 norm of weights: 6.261334409016273\n",
      "---------------------\n",
      "Iteration Number: 1885\n",
      "Loss: 59.44140719871833\n",
      "l2 norm of gradients: 0.5927779005900111\n",
      "l2 norm of weights: 6.261184521694458\n",
      "---------------------\n",
      "Iteration Number: 1886\n",
      "Loss: 59.42410447419397\n",
      "l2 norm of gradients: 0.5925006969606871\n",
      "l2 norm of weights: 6.2610346832751835\n",
      "---------------------\n",
      "Iteration Number: 1887\n",
      "Loss: 59.40681774370442\n",
      "l2 norm of gradients: 0.5922237796322831\n",
      "l2 norm of weights: 6.260884893709268\n",
      "---------------------\n",
      "Iteration Number: 1888\n",
      "Loss: 59.389546983283104\n",
      "l2 norm of gradients: 0.591947148244268\n",
      "l2 norm of weights: 6.2607351529476185\n",
      "---------------------\n",
      "Iteration Number: 1889\n",
      "Loss: 59.37229216921827\n",
      "l2 norm of gradients: 0.591670802435678\n",
      "l2 norm of weights: 6.260585460941227\n",
      "---------------------\n",
      "Iteration Number: 1890\n",
      "Loss: 59.355053277779916\n",
      "l2 norm of gradients: 0.5913947418451246\n",
      "l2 norm of weights: 6.260435817641178\n",
      "---------------------\n",
      "Iteration Number: 1891\n",
      "Loss: 59.337830285174164\n",
      "l2 norm of gradients: 0.5911189661108006\n",
      "l2 norm of weights: 6.260286222998641\n",
      "---------------------\n",
      "Iteration Number: 1892\n",
      "Loss: 59.320623167890936\n",
      "l2 norm of gradients: 0.590843474870488\n",
      "l2 norm of weights: 6.260136676964875\n",
      "---------------------\n",
      "Iteration Number: 1893\n",
      "Loss: 59.30343190218734\n",
      "l2 norm of gradients: 0.5905682677615642\n",
      "l2 norm of weights: 6.259987179491226\n",
      "---------------------\n",
      "Iteration Number: 1894\n",
      "Loss: 59.2862564645882\n",
      "l2 norm of gradients: 0.5902933444210096\n",
      "l2 norm of weights: 6.259837730529127\n",
      "---------------------\n",
      "Iteration Number: 1895\n",
      "Loss: 59.26909683153142\n",
      "l2 norm of gradients: 0.5900187044854134\n",
      "l2 norm of weights: 6.259688330030099\n",
      "---------------------\n",
      "Iteration Number: 1896\n",
      "Loss: 59.251952979532824\n",
      "l2 norm of gradients: 0.5897443475909817\n",
      "l2 norm of weights: 6.2595389779457475\n",
      "---------------------\n",
      "Iteration Number: 1897\n",
      "Loss: 59.234824885142366\n",
      "l2 norm of gradients: 0.5894702733735434\n",
      "l2 norm of weights: 6.25938967422777\n",
      "---------------------\n",
      "Iteration Number: 1898\n",
      "Loss: 59.21771252501083\n",
      "l2 norm of gradients: 0.5891964814685577\n",
      "l2 norm of weights: 6.259240418827945\n",
      "---------------------\n",
      "Iteration Number: 1899\n",
      "Loss: 59.200615875725866\n",
      "l2 norm of gradients: 0.5889229715111205\n",
      "l2 norm of weights: 6.2590912116981405\n",
      "---------------------\n",
      "Iteration Number: 1900\n",
      "Loss: 59.18353491402747\n",
      "l2 norm of gradients: 0.5886497431359708\n",
      "l2 norm of weights: 6.258942052790311\n",
      "---------------------\n",
      "Iteration Number: 1901\n",
      "Loss: 59.16646961665288\n",
      "l2 norm of gradients: 0.5883767959774986\n",
      "l2 norm of weights: 6.258792942056496\n",
      "---------------------\n",
      "Iteration Number: 1902\n",
      "Loss: 59.149419960341454\n",
      "l2 norm of gradients: 0.5881041296697498\n",
      "l2 norm of weights: 6.25864387944882\n",
      "---------------------\n",
      "Iteration Number: 1903\n",
      "Loss: 59.13238592196116\n",
      "l2 norm of gradients: 0.5878317438464351\n",
      "l2 norm of weights: 6.258494864919496\n",
      "---------------------\n",
      "Iteration Number: 1904\n",
      "Loss: 59.11536747833477\n",
      "l2 norm of gradients: 0.5875596381409345\n",
      "l2 norm of weights: 6.258345898420819\n",
      "---------------------\n",
      "Iteration Number: 1905\n",
      "Loss: 59.098364606371845\n",
      "l2 norm of gradients: 0.5872878121863054\n",
      "l2 norm of weights: 6.258196979905172\n",
      "---------------------\n",
      "Iteration Number: 1906\n",
      "Loss: 59.08137728302745\n",
      "l2 norm of gradients: 0.5870162656152886\n",
      "l2 norm of weights: 6.2580481093250215\n",
      "---------------------\n",
      "Iteration Number: 1907\n",
      "Loss: 59.064405485362464\n",
      "l2 norm of gradients: 0.5867449980603147\n",
      "l2 norm of weights: 6.257899286632921\n",
      "---------------------\n",
      "Iteration Number: 1908\n",
      "Loss: 59.04744919028935\n",
      "l2 norm of gradients: 0.5864740091535112\n",
      "l2 norm of weights: 6.257750511781506\n",
      "---------------------\n",
      "Iteration Number: 1909\n",
      "Loss: 59.03050837499495\n",
      "l2 norm of gradients: 0.5862032985267087\n",
      "l2 norm of weights: 6.257601784723498\n",
      "---------------------\n",
      "Iteration Number: 1910\n",
      "Loss: 59.01358301651987\n",
      "l2 norm of gradients: 0.5859328658114472\n",
      "l2 norm of weights: 6.257453105411704\n",
      "---------------------\n",
      "Iteration Number: 1911\n",
      "Loss: 58.99667309202319\n",
      "l2 norm of gradients: 0.5856627106389828\n",
      "l2 norm of weights: 6.257304473799012\n",
      "---------------------\n",
      "Iteration Number: 1912\n",
      "Loss: 58.979778578807796\n",
      "l2 norm of gradients: 0.5853928326402941\n",
      "l2 norm of weights: 6.257155889838398\n",
      "---------------------\n",
      "Iteration Number: 1913\n",
      "Loss: 58.962899454043836\n",
      "l2 norm of gradients: 0.5851232314460888\n",
      "l2 norm of weights: 6.257007353482917\n",
      "---------------------\n",
      "Iteration Number: 1914\n",
      "Loss: 58.94603569501678\n",
      "l2 norm of gradients: 0.5848539066868095\n",
      "l2 norm of weights: 6.256858864685713\n",
      "---------------------\n",
      "Iteration Number: 1915\n",
      "Loss: 58.9291872791165\n",
      "l2 norm of gradients: 0.5845848579926408\n",
      "l2 norm of weights: 6.2567104234000075\n",
      "---------------------\n",
      "Iteration Number: 1916\n",
      "Loss: 58.91235418371303\n",
      "l2 norm of gradients: 0.584316084993515\n",
      "l2 norm of weights: 6.2565620295791105\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 1917\n",
      "Loss: 58.89553638617321\n",
      "l2 norm of gradients: 0.5840475873191184\n",
      "l2 norm of weights: 6.2564136831764126\n",
      "---------------------\n",
      "Iteration Number: 1918\n",
      "Loss: 58.878733863952405\n",
      "l2 norm of gradients: 0.5837793645988986\n",
      "l2 norm of weights: 6.256265384145387\n",
      "---------------------\n",
      "Iteration Number: 1919\n",
      "Loss: 58.86194659462987\n",
      "l2 norm of gradients: 0.583511416462069\n",
      "l2 norm of weights: 6.25611713243959\n",
      "---------------------\n",
      "Iteration Number: 1920\n",
      "Loss: 58.84517455566259\n",
      "l2 norm of gradients: 0.5832437425376165\n",
      "l2 norm of weights: 6.25596892801266\n",
      "---------------------\n",
      "Iteration Number: 1921\n",
      "Loss: 58.82841772473166\n",
      "l2 norm of gradients: 0.5829763424543067\n",
      "l2 norm of weights: 6.255820770818319\n",
      "---------------------\n",
      "Iteration Number: 1922\n",
      "Loss: 58.81167607938357\n",
      "l2 norm of gradients: 0.5827092158406912\n",
      "l2 norm of weights: 6.2556726608103705\n",
      "---------------------\n",
      "Iteration Number: 1923\n",
      "Loss: 58.79494959733685\n",
      "l2 norm of gradients: 0.582442362325112\n",
      "l2 norm of weights: 6.255524597942697\n",
      "---------------------\n",
      "Iteration Number: 1924\n",
      "Loss: 58.778238256264444\n",
      "l2 norm of gradients: 0.5821757815357094\n",
      "l2 norm of weights: 6.255376582169268\n",
      "---------------------\n",
      "Iteration Number: 1925\n",
      "Loss: 58.76154203396704\n",
      "l2 norm of gradients: 0.5819094731004268\n",
      "l2 norm of weights: 6.255228613444131\n",
      "---------------------\n",
      "Iteration Number: 1926\n",
      "Loss: 58.744860908210015\n",
      "l2 norm of gradients: 0.5816434366470172\n",
      "l2 norm of weights: 6.255080691721415\n",
      "---------------------\n",
      "Iteration Number: 1927\n",
      "Loss: 58.72819485689634\n",
      "l2 norm of gradients: 0.5813776718030493\n",
      "l2 norm of weights: 6.25493281695533\n",
      "---------------------\n",
      "Iteration Number: 1928\n",
      "Loss: 58.71154385782135\n",
      "l2 norm of gradients: 0.5811121781959137\n",
      "l2 norm of weights: 6.25478498910017\n",
      "---------------------\n",
      "Iteration Number: 1929\n",
      "Loss: 58.6949078889409\n",
      "l2 norm of gradients: 0.5808469554528276\n",
      "l2 norm of weights: 6.254637208110305\n",
      "---------------------\n",
      "Iteration Number: 1930\n",
      "Loss: 58.678286928221446\n",
      "l2 norm of gradients: 0.5805820032008425\n",
      "l2 norm of weights: 6.254489473940189\n",
      "---------------------\n",
      "Iteration Number: 1931\n",
      "Loss: 58.66168095366859\n",
      "l2 norm of gradients: 0.5803173210668489\n",
      "l2 norm of weights: 6.254341786544354\n",
      "---------------------\n",
      "Iteration Number: 1932\n",
      "Loss: 58.64508994333962\n",
      "l2 norm of gradients: 0.5800529086775822\n",
      "l2 norm of weights: 6.254194145877417\n",
      "---------------------\n",
      "Iteration Number: 1933\n",
      "Loss: 58.62851387532944\n",
      "l2 norm of gradients: 0.5797887656596293\n",
      "l2 norm of weights: 6.254046551894069\n",
      "---------------------\n",
      "Iteration Number: 1934\n",
      "Loss: 58.611952727724486\n",
      "l2 norm of gradients: 0.5795248916394335\n",
      "l2 norm of weights: 6.253899004549083\n",
      "---------------------\n",
      "Iteration Number: 1935\n",
      "Loss: 58.595406478747314\n",
      "l2 norm of gradients: 0.5792612862433011\n",
      "l2 norm of weights: 6.2537515037973135\n",
      "---------------------\n",
      "Iteration Number: 1936\n",
      "Loss: 58.578875106640005\n",
      "l2 norm of gradients: 0.578997949097406\n",
      "l2 norm of weights: 6.253604049593692\n",
      "---------------------\n",
      "Iteration Number: 1937\n",
      "Loss: 58.562358589571396\n",
      "l2 norm of gradients: 0.5787348798277969\n",
      "l2 norm of weights: 6.253456641893233\n",
      "---------------------\n",
      "Iteration Number: 1938\n",
      "Loss: 58.54585690590843\n",
      "l2 norm of gradients: 0.5784720780604019\n",
      "l2 norm of weights: 6.253309280651022\n",
      "---------------------\n",
      "Iteration Number: 1939\n",
      "Loss: 58.529370033947\n",
      "l2 norm of gradients: 0.5782095434210343\n",
      "l2 norm of weights: 6.253161965822234\n",
      "---------------------\n",
      "Iteration Number: 1940\n",
      "Loss: 58.51289795207668\n",
      "l2 norm of gradients: 0.5779472755353987\n",
      "l2 norm of weights: 6.253014697362116\n",
      "---------------------\n",
      "Iteration Number: 1941\n",
      "Loss: 58.49644063871988\n",
      "l2 norm of gradients: 0.5776852740290961\n",
      "l2 norm of weights: 6.252867475225994\n",
      "---------------------\n",
      "Iteration Number: 1942\n",
      "Loss: 58.47999807234274\n",
      "l2 norm of gradients: 0.5774235385276296\n",
      "l2 norm of weights: 6.252720299369273\n",
      "---------------------\n",
      "Iteration Number: 1943\n",
      "Loss: 58.46357023144744\n",
      "l2 norm of gradients: 0.5771620686564101\n",
      "l2 norm of weights: 6.252573169747439\n",
      "---------------------\n",
      "Iteration Number: 1944\n",
      "Loss: 58.44715709458389\n",
      "l2 norm of gradients: 0.5769008640407616\n",
      "l2 norm of weights: 6.252426086316052\n",
      "---------------------\n",
      "Iteration Number: 1945\n",
      "Loss: 58.43075864029483\n",
      "l2 norm of gradients: 0.5766399243059267\n",
      "l2 norm of weights: 6.252279049030752\n",
      "---------------------\n",
      "Iteration Number: 1946\n",
      "Loss: 58.41437484728306\n",
      "l2 norm of gradients: 0.5763792490770727\n",
      "l2 norm of weights: 6.252132057847254\n",
      "---------------------\n",
      "Iteration Number: 1947\n",
      "Loss: 58.39800569416729\n",
      "l2 norm of gradients: 0.5761188379792954\n",
      "l2 norm of weights: 6.251985112721354\n",
      "---------------------\n",
      "Iteration Number: 1948\n",
      "Loss: 58.3816511596827\n",
      "l2 norm of gradients: 0.5758586906376262\n",
      "l2 norm of weights: 6.2518382136089246\n",
      "---------------------\n",
      "Iteration Number: 1949\n",
      "Loss: 58.36531122249598\n",
      "l2 norm of gradients: 0.5755988066770364\n",
      "l2 norm of weights: 6.251691360465912\n",
      "---------------------\n",
      "Iteration Number: 1950\n",
      "Loss: 58.34898586148455\n",
      "l2 norm of gradients: 0.5753391857224436\n",
      "l2 norm of weights: 6.251544553248344\n",
      "---------------------\n",
      "Iteration Number: 1951\n",
      "Loss: 58.33267505548468\n",
      "l2 norm of gradients: 0.5750798273987153\n",
      "l2 norm of weights: 6.251397791912322\n",
      "---------------------\n",
      "Iteration Number: 1952\n",
      "Loss: 58.316378783278914\n",
      "l2 norm of gradients: 0.5748207313306758\n",
      "l2 norm of weights: 6.2512510764140234\n",
      "---------------------\n",
      "Iteration Number: 1953\n",
      "Loss: 58.30009702386985\n",
      "l2 norm of gradients: 0.5745618971431106\n",
      "l2 norm of weights: 6.251104406709706\n",
      "---------------------\n",
      "Iteration Number: 1954\n",
      "Loss: 58.28382975612837\n",
      "l2 norm of gradients: 0.5743033244607715\n",
      "l2 norm of weights: 6.250957782755699\n",
      "---------------------\n",
      "Iteration Number: 1955\n",
      "Loss: 58.267576959109434\n",
      "l2 norm of gradients: 0.5740450129083827\n",
      "l2 norm of weights: 6.25081120450841\n",
      "---------------------\n",
      "Iteration Number: 1956\n",
      "Loss: 58.25133861180869\n",
      "l2 norm of gradients: 0.5737869621106448\n",
      "l2 norm of weights: 6.250664671924322\n",
      "---------------------\n",
      "Iteration Number: 1957\n",
      "Loss: 58.235114693323865\n",
      "l2 norm of gradients: 0.5735291716922408\n",
      "l2 norm of weights: 6.2505181849599944\n",
      "---------------------\n",
      "Iteration Number: 1958\n",
      "Loss: 58.21890518272537\n",
      "l2 norm of gradients: 0.5732716412778402\n",
      "l2 norm of weights: 6.250371743572059\n",
      "---------------------\n",
      "Iteration Number: 1959\n",
      "Loss: 58.202710059212194\n",
      "l2 norm of gradients: 0.5730143704921055\n",
      "l2 norm of weights: 6.250225347717228\n",
      "---------------------\n",
      "Iteration Number: 1960\n",
      "Loss: 58.18652930194031\n",
      "l2 norm of gradients: 0.5727573589596956\n",
      "l2 norm of weights: 6.250078997352284\n",
      "---------------------\n",
      "Iteration Number: 1961\n",
      "Loss: 58.17036289018751\n",
      "l2 norm of gradients: 0.5725006063052722\n",
      "l2 norm of weights: 6.249932692434086\n",
      "---------------------\n",
      "Iteration Number: 1962\n",
      "Loss: 58.1542108031896\n",
      "l2 norm of gradients: 0.5722441121535039\n",
      "l2 norm of weights: 6.249786432919569\n",
      "---------------------\n",
      "Iteration Number: 1963\n",
      "Loss: 58.138073020259014\n",
      "l2 norm of gradients: 0.5719878761290715\n",
      "l2 norm of weights: 6.249640218765739\n",
      "---------------------\n",
      "Iteration Number: 1964\n",
      "Loss: 58.12194952077306\n",
      "l2 norm of gradients: 0.5717318978566727\n",
      "l2 norm of weights: 6.249494049929682\n",
      "---------------------\n",
      "Iteration Number: 1965\n",
      "Loss: 58.105840284088565\n",
      "l2 norm of gradients: 0.571476176961027\n",
      "l2 norm of weights: 6.249347926368552\n",
      "---------------------\n",
      "Iteration Number: 1966\n",
      "Loss: 58.089745289660115\n",
      "l2 norm of gradients: 0.5712207130668809\n",
      "l2 norm of weights: 6.249201848039582\n",
      "---------------------\n",
      "Iteration Number: 1967\n",
      "Loss: 58.07366451696042\n",
      "l2 norm of gradients: 0.5709655057990124\n",
      "l2 norm of weights: 6.249055814900076\n",
      "---------------------\n",
      "Iteration Number: 1968\n",
      "Loss: 58.0575979454666\n",
      "l2 norm of gradients: 0.5707105547822358\n",
      "l2 norm of weights: 6.24890982690741\n",
      "---------------------\n",
      "Iteration Number: 1969\n",
      "Loss: 58.0415455547787\n",
      "l2 norm of gradients: 0.5704558596414062\n",
      "l2 norm of weights: 6.248763884019039\n",
      "---------------------\n",
      "Iteration Number: 1970\n",
      "Loss: 58.02550732448494\n",
      "l2 norm of gradients: 0.570201420001425\n",
      "l2 norm of weights: 6.248617986192487\n",
      "---------------------\n",
      "Iteration Number: 1971\n",
      "Loss: 58.009483234175256\n",
      "l2 norm of gradients: 0.5699472354872441\n",
      "l2 norm of weights: 6.24847213338535\n",
      "---------------------\n",
      "Iteration Number: 1972\n",
      "Loss: 57.99347326352613\n",
      "l2 norm of gradients: 0.5696933057238702\n",
      "l2 norm of weights: 6.248326325555302\n",
      "---------------------\n",
      "Iteration Number: 1973\n",
      "Loss: 57.977477392313666\n",
      "l2 norm of gradients: 0.5694396303363702\n",
      "l2 norm of weights: 6.248180562660086\n",
      "---------------------\n",
      "Iteration Number: 1974\n",
      "Loss: 57.961495600204564\n",
      "l2 norm of gradients: 0.5691862089498753\n",
      "l2 norm of weights: 6.248034844657515\n",
      "---------------------\n",
      "Iteration Number: 1975\n",
      "Loss: 57.94552786701301\n",
      "l2 norm of gradients: 0.5689330411895859\n",
      "l2 norm of weights: 6.247889171505481\n",
      "---------------------\n",
      "Iteration Number: 1976\n",
      "Loss: 57.92957417252052\n",
      "l2 norm of gradients: 0.5686801266807758\n",
      "l2 norm of weights: 6.247743543161945\n",
      "---------------------\n",
      "Iteration Number: 1977\n",
      "Loss: 57.913634496686385\n",
      "l2 norm of gradients: 0.5684274650487973\n",
      "l2 norm of weights: 6.247597959584937\n",
      "---------------------\n",
      "Iteration Number: 1978\n",
      "Loss: 57.897708819373605\n",
      "l2 norm of gradients: 0.5681750559190847\n",
      "l2 norm of weights: 6.247452420732564\n",
      "---------------------\n",
      "Iteration Number: 1979\n",
      "Loss: 57.881797120492124\n",
      "l2 norm of gradients: 0.5679228989171597\n",
      "l2 norm of weights: 6.2473069265630015\n",
      "---------------------\n",
      "Iteration Number: 1980\n",
      "Loss: 57.86589938006849\n",
      "l2 norm of gradients: 0.5676709936686358\n",
      "l2 norm of weights: 6.2471614770344965\n",
      "---------------------\n",
      "Iteration Number: 1981\n",
      "Loss: 57.85001557812228\n",
      "l2 norm of gradients: 0.5674193397992221\n",
      "l2 norm of weights: 6.24701607210537\n",
      "---------------------\n",
      "Iteration Number: 1982\n",
      "Loss: 57.83414569470615\n",
      "l2 norm of gradients: 0.5671679369347279\n",
      "l2 norm of weights: 6.246870711734012\n",
      "---------------------\n",
      "Iteration Number: 1983\n",
      "Loss: 57.81828970986894\n",
      "l2 norm of gradients: 0.5669167847010675\n",
      "l2 norm of weights: 6.246725395878881\n",
      "---------------------\n",
      "Iteration Number: 1984\n",
      "Loss: 57.80244760382756\n",
      "l2 norm of gradients: 0.5666658827242639\n",
      "l2 norm of weights: 6.246580124498513\n",
      "---------------------\n",
      "Iteration Number: 1985\n",
      "Loss: 57.78661935670897\n",
      "l2 norm of gradients: 0.5664152306304536\n",
      "l2 norm of weights: 6.246434897551508\n",
      "---------------------\n",
      "Iteration Number: 1986\n",
      "Loss: 57.770804948741706\n",
      "l2 norm of gradients: 0.56616482804589\n",
      "l2 norm of weights: 6.246289714996538\n",
      "---------------------\n",
      "Iteration Number: 1987\n",
      "Loss: 57.75500436013714\n",
      "l2 norm of gradients: 0.5659146745969494\n",
      "l2 norm of weights: 6.24614457679235\n",
      "---------------------\n",
      "Iteration Number: 1988\n",
      "Loss: 57.739217571269215\n",
      "l2 norm of gradients: 0.565664769910133\n",
      "l2 norm of weights: 6.245999482897756\n",
      "---------------------\n",
      "Iteration Number: 1989\n",
      "Loss: 57.723444562435084\n",
      "l2 norm of gradients: 0.5654151136120728\n",
      "l2 norm of weights: 6.245854433271637\n",
      "---------------------\n",
      "Iteration Number: 1990\n",
      "Loss: 57.70768531396676\n",
      "l2 norm of gradients: 0.5651657053295346\n",
      "l2 norm of weights: 6.245709427872949\n",
      "---------------------\n",
      "Iteration Number: 1991\n",
      "Loss: 57.691939806382145\n",
      "l2 norm of gradients: 0.564916544689423\n",
      "l2 norm of weights: 6.2455644666607135\n",
      "---------------------\n",
      "Iteration Number: 1992\n",
      "Loss: 57.676208019950124\n",
      "l2 norm of gradients: 0.564667631318785\n",
      "l2 norm of weights: 6.245419549594023\n",
      "---------------------\n",
      "Iteration Number: 1993\n",
      "Loss: 57.66048993527376\n",
      "l2 norm of gradients: 0.5644189648448139\n",
      "l2 norm of weights: 6.245274676632039\n",
      "---------------------\n",
      "Iteration Number: 1994\n",
      "Loss: 57.64478553287563\n",
      "l2 norm of gradients: 0.564170544894854\n",
      "l2 norm of weights: 6.24512984773399\n",
      "---------------------\n",
      "Iteration Number: 1995\n",
      "Loss: 57.62909479323478\n",
      "l2 norm of gradients: 0.5639223710964042\n",
      "l2 norm of weights: 6.244985062859178\n",
      "---------------------\n",
      "Iteration Number: 1996\n",
      "Loss: 57.61341769703356\n",
      "l2 norm of gradients: 0.5636744430771214\n",
      "l2 norm of weights: 6.2448403219669695\n",
      "---------------------\n",
      "Iteration Number: 1997\n",
      "Loss: 57.59775422491549\n",
      "l2 norm of gradients: 0.563426760464826\n",
      "l2 norm of weights: 6.2446956250168\n",
      "---------------------\n",
      "Iteration Number: 1998\n",
      "Loss: 57.582104357467124\n",
      "l2 norm of gradients: 0.5631793228875037\n",
      "l2 norm of weights: 6.244550971968176\n",
      "---------------------\n",
      "Iteration Number: 1999\n",
      "Loss: 57.566468075452775\n",
      "l2 norm of gradients: 0.5629321299733119\n",
      "l2 norm of weights: 6.244406362780672\n",
      "---------------------\n",
      "Iteration Number: 2000\n",
      "Loss: 57.55084535959812\n",
      "l2 norm of gradients: 0.5626851813505807\n",
      "l2 norm of weights: 6.244261797413926\n",
      "---------------------\n",
      "Iteration Number: 2001\n",
      "Loss: 57.53523619073403\n",
      "l2 norm of gradients: 0.5624384766478194\n",
      "l2 norm of weights: 6.244117275827648\n",
      "---------------------\n",
      "Iteration Number: 2002\n",
      "Loss: 57.519640549628775\n",
      "l2 norm of gradients: 0.5621920154937188\n",
      "l2 norm of weights: 6.243972797981615\n",
      "---------------------\n",
      "Iteration Number: 2003\n",
      "Loss: 57.50405841718877\n",
      "l2 norm of gradients: 0.5619457975171551\n",
      "l2 norm of weights: 6.2438283638356715\n",
      "---------------------\n",
      "Iteration Number: 2004\n",
      "Loss: 57.48848977427689\n",
      "l2 norm of gradients: 0.5616998223471945\n",
      "l2 norm of weights: 6.243683973349729\n",
      "---------------------\n",
      "Iteration Number: 2005\n",
      "Loss: 57.472934601868495\n",
      "l2 norm of gradients: 0.5614540896130958\n",
      "l2 norm of weights: 6.243539626483766\n",
      "---------------------\n",
      "Iteration Number: 2006\n",
      "Loss: 57.457392880905736\n",
      "l2 norm of gradients: 0.561208598944315\n",
      "l2 norm of weights: 6.243395323197828\n",
      "---------------------\n",
      "Iteration Number: 2007\n",
      "Loss: 57.44186459239839\n",
      "l2 norm of gradients: 0.5609633499705081\n",
      "l2 norm of weights: 6.243251063452028\n",
      "---------------------\n",
      "Iteration Number: 2008\n",
      "Loss: 57.426349717440765\n",
      "l2 norm of gradients: 0.5607183423215356\n",
      "l2 norm of weights: 6.243106847206547\n",
      "---------------------\n",
      "Iteration Number: 2009\n",
      "Loss: 57.41084823706299\n",
      "l2 norm of gradients: 0.5604735756274661\n",
      "l2 norm of weights: 6.242962674421627\n",
      "---------------------\n",
      "Iteration Number: 2010\n",
      "Loss: 57.39536013238373\n",
      "l2 norm of gradients: 0.5602290495185787\n",
      "l2 norm of weights: 6.242818545057584\n",
      "---------------------\n",
      "Iteration Number: 2011\n",
      "Loss: 57.37988538459313\n",
      "l2 norm of gradients: 0.5599847636253683\n",
      "l2 norm of weights: 6.242674459074794\n",
      "---------------------\n",
      "Iteration Number: 2012\n",
      "Loss: 57.36442397489084\n",
      "l2 norm of gradients: 0.5597407175785478\n",
      "l2 norm of weights: 6.242530416433703\n",
      "---------------------\n",
      "Iteration Number: 2013\n",
      "Loss: 57.34897588446955\n",
      "l2 norm of gradients: 0.5594969110090521\n",
      "l2 norm of weights: 6.242386417094819\n",
      "---------------------\n",
      "Iteration Number: 2014\n",
      "Loss: 57.33354109464183\n",
      "l2 norm of gradients: 0.5592533435480416\n",
      "l2 norm of weights: 6.242242461018719\n",
      "---------------------\n",
      "Iteration Number: 2015\n",
      "Loss: 57.31811958672582\n",
      "l2 norm of gradients: 0.5590100148269056\n",
      "l2 norm of weights: 6.242098548166047\n",
      "---------------------\n",
      "Iteration Number: 2016\n",
      "Loss: 57.30271134201897\n",
      "l2 norm of gradients: 0.558766924477266\n",
      "l2 norm of weights: 6.241954678497506\n",
      "---------------------\n",
      "Iteration Number: 2017\n",
      "Loss: 57.28731634193128\n",
      "l2 norm of gradients: 0.5585240721309799\n",
      "l2 norm of weights: 6.24181085197387\n",
      "---------------------\n",
      "Iteration Number: 2018\n",
      "Loss: 57.271934567840866\n",
      "l2 norm of gradients: 0.5582814574201441\n",
      "l2 norm of weights: 6.241667068555976\n",
      "---------------------\n",
      "Iteration Number: 2019\n",
      "Loss: 57.256566001266165\n",
      "l2 norm of gradients: 0.5580390799770978\n",
      "l2 norm of weights: 6.241523328204724\n",
      "---------------------\n",
      "Iteration Number: 2020\n",
      "Loss: 57.24121062361308\n",
      "l2 norm of gradients: 0.5577969394344257\n",
      "l2 norm of weights: 6.241379630881083\n",
      "---------------------\n",
      "Iteration Number: 2021\n",
      "Loss: 57.225868416501896\n",
      "l2 norm of gradients: 0.5575550354249617\n",
      "l2 norm of weights: 6.241235976546082\n",
      "---------------------\n",
      "Iteration Number: 2022\n",
      "Loss: 57.21053936140977\n",
      "l2 norm of gradients: 0.5573133675817922\n",
      "l2 norm of weights: 6.241092365160819\n",
      "---------------------\n",
      "Iteration Number: 2023\n",
      "Loss: 57.1952234399821\n",
      "l2 norm of gradients: 0.5570719355382592\n",
      "l2 norm of weights: 6.240948796686452\n",
      "---------------------\n",
      "Iteration Number: 2024\n",
      "Loss: 57.17992063385997\n",
      "l2 norm of gradients: 0.5568307389279635\n",
      "l2 norm of weights: 6.240805271084205\n",
      "---------------------\n",
      "Iteration Number: 2025\n",
      "Loss: 57.164630924670725\n",
      "l2 norm of gradients: 0.5565897773847681\n",
      "l2 norm of weights: 6.240661788315365\n",
      "---------------------\n",
      "Iteration Number: 2026\n",
      "Loss: 57.149354294190175\n",
      "l2 norm of gradients: 0.556349050542801\n",
      "l2 norm of weights: 6.2405183483412845\n",
      "---------------------\n",
      "Iteration Number: 2027\n",
      "Loss: 57.1340907240841\n",
      "l2 norm of gradients: 0.5561085580364589\n",
      "l2 norm of weights: 6.240374951123378\n",
      "---------------------\n",
      "Iteration Number: 2028\n",
      "Loss: 57.118840196156164\n",
      "l2 norm of gradients: 0.5558682995004095\n",
      "l2 norm of weights: 6.240231596623123\n",
      "---------------------\n",
      "Iteration Number: 2029\n",
      "Loss: 57.10360269226149\n",
      "l2 norm of gradients: 0.5556282745695954\n",
      "l2 norm of weights: 6.240088284802062\n",
      "---------------------\n",
      "Iteration Number: 2030\n",
      "Loss: 57.088378194210776\n",
      "l2 norm of gradients: 0.5553884828792368\n",
      "l2 norm of weights: 6.2399450156217995\n",
      "---------------------\n",
      "Iteration Number: 2031\n",
      "Loss: 57.07316668386514\n",
      "l2 norm of gradients: 0.5551489240648346\n",
      "l2 norm of weights: 6.239801789044002\n",
      "---------------------\n",
      "Iteration Number: 2032\n",
      "Loss: 57.057968143218524\n",
      "l2 norm of gradients: 0.5549095977621735\n",
      "l2 norm of weights: 6.2396586050304\n",
      "---------------------\n",
      "Iteration Number: 2033\n",
      "Loss: 57.04278255415856\n",
      "l2 norm of gradients: 0.5546705036073247\n",
      "l2 norm of weights: 6.239515463542788\n",
      "---------------------\n",
      "Iteration Number: 2034\n",
      "Loss: 57.02760989877777\n",
      "l2 norm of gradients: 0.5544316412366491\n",
      "l2 norm of weights: 6.239372364543019\n",
      "---------------------\n",
      "Iteration Number: 2035\n",
      "Loss: 57.01245015895068\n",
      "l2 norm of gradients: 0.5541930102868007\n",
      "l2 norm of weights: 6.239229307993013\n",
      "---------------------\n",
      "Iteration Number: 2036\n",
      "Loss: 56.99730331688711\n",
      "l2 norm of gradients: 0.5539546103947283\n",
      "l2 norm of weights: 6.239086293854748\n",
      "---------------------\n",
      "Iteration Number: 2037\n",
      "Loss: 56.982169354601716\n",
      "l2 norm of gradients: 0.5537164411976794\n",
      "l2 norm of weights: 6.238943322090265\n",
      "---------------------\n",
      "Iteration Number: 2038\n",
      "Loss: 56.96704825424249\n",
      "l2 norm of gradients: 0.5534785023332033\n",
      "l2 norm of weights: 6.23880039266167\n",
      "---------------------\n",
      "Iteration Number: 2039\n",
      "Loss: 56.95193999797539\n",
      "l2 norm of gradients: 0.5532407934391527\n",
      "l2 norm of weights: 6.238657505531125\n",
      "---------------------\n",
      "Iteration Number: 2040\n",
      "Loss: 56.93684456805455\n",
      "l2 norm of gradients: 0.553003314153688\n",
      "l2 norm of weights: 6.23851466066086\n",
      "---------------------\n",
      "Iteration Number: 2041\n",
      "Loss: 56.92176194668007\n",
      "l2 norm of gradients: 0.5527660641152784\n",
      "l2 norm of weights: 6.238371858013159\n",
      "---------------------\n",
      "Iteration Number: 2042\n",
      "Loss: 56.90669211613053\n",
      "l2 norm of gradients: 0.552529042962707\n",
      "l2 norm of weights: 6.2382290975503745\n",
      "---------------------\n",
      "Iteration Number: 2043\n",
      "Loss: 56.89163505869925\n",
      "l2 norm of gradients: 0.552292250335071\n",
      "l2 norm of weights: 6.238086379234915\n",
      "---------------------\n",
      "Iteration Number: 2044\n",
      "Loss: 56.876590756775\n",
      "l2 norm of gradients: 0.5520556858717863\n",
      "l2 norm of weights: 6.23794370302925\n",
      "---------------------\n",
      "Iteration Number: 2045\n",
      "Loss: 56.86155919267938\n",
      "l2 norm of gradients: 0.5518193492125892\n",
      "l2 norm of weights: 6.237801068895914\n",
      "---------------------\n",
      "Iteration Number: 2046\n",
      "Loss: 56.84654034887451\n",
      "l2 norm of gradients: 0.5515832399975399\n",
      "l2 norm of weights: 6.237658476797499\n",
      "---------------------\n",
      "Iteration Number: 2047\n",
      "Loss: 56.83153420781403\n",
      "l2 norm of gradients: 0.5513473578670239\n",
      "l2 norm of weights: 6.2375159266966556\n",
      "---------------------\n",
      "Iteration Number: 2048\n",
      "Loss: 56.8165407518985\n",
      "l2 norm of gradients: 0.5511117024617563\n",
      "l2 norm of weights: 6.237373418556098\n",
      "---------------------\n",
      "Iteration Number: 2049\n",
      "Loss: 56.80155996377411\n",
      "l2 norm of gradients: 0.5508762734227826\n",
      "l2 norm of weights: 6.2372309523385985\n",
      "---------------------\n",
      "Iteration Number: 2050\n",
      "Loss: 56.78659182587909\n",
      "l2 norm of gradients: 0.550641070391483\n",
      "l2 norm of weights: 6.237088528006989\n",
      "---------------------\n",
      "Iteration Number: 2051\n",
      "Loss: 56.7716363208972\n",
      "l2 norm of gradients: 0.5504060930095734\n",
      "l2 norm of weights: 6.236946145524166\n",
      "---------------------\n",
      "Iteration Number: 2052\n",
      "Loss: 56.75669343138135\n",
      "l2 norm of gradients: 0.5501713409191098\n",
      "l2 norm of weights: 6.236803804853076\n",
      "---------------------\n",
      "Iteration Number: 2053\n",
      "Loss: 56.741763139963865\n",
      "l2 norm of gradients: 0.5499368137624884\n",
      "l2 norm of weights: 6.236661505956736\n",
      "---------------------\n",
      "Iteration Number: 2054\n",
      "Loss: 56.72684542942485\n",
      "l2 norm of gradients: 0.5497025111824506\n",
      "l2 norm of weights: 6.236519248798214\n",
      "---------------------\n",
      "Iteration Number: 2055\n",
      "Loss: 56.711940282408506\n",
      "l2 norm of gradients: 0.5494684328220834\n",
      "l2 norm of weights: 6.236377033340641\n",
      "---------------------\n",
      "Iteration Number: 2056\n",
      "Loss: 56.697047681761\n",
      "l2 norm of gradients: 0.5492345783248233\n",
      "l2 norm of weights: 6.236234859547206\n",
      "---------------------\n",
      "Iteration Number: 2057\n",
      "Loss: 56.68216761015423\n",
      "l2 norm of gradients: 0.5490009473344578\n",
      "l2 norm of weights: 6.236092727381157\n",
      "---------------------\n",
      "Iteration Number: 2058\n",
      "Loss: 56.6673000505362\n",
      "l2 norm of gradients: 0.5487675394951285\n",
      "l2 norm of weights: 6.235950636805803\n",
      "---------------------\n",
      "Iteration Number: 2059\n",
      "Loss: 56.65244498567891\n",
      "l2 norm of gradients: 0.5485343544513331\n",
      "l2 norm of weights: 6.235808587784505\n",
      "---------------------\n",
      "Iteration Number: 2060\n",
      "Loss: 56.63760239851272\n",
      "l2 norm of gradients: 0.5483013918479277\n",
      "l2 norm of weights: 6.235666580280689\n",
      "---------------------\n",
      "Iteration Number: 2061\n",
      "Loss: 56.62277227199111\n",
      "l2 norm of gradients: 0.5480686513301293\n",
      "l2 norm of weights: 6.235524614257838\n",
      "---------------------\n",
      "Iteration Number: 2062\n",
      "Loss: 56.607954589002254\n",
      "l2 norm of gradients: 0.5478361325435182\n",
      "l2 norm of weights: 6.235382689679493\n",
      "---------------------\n",
      "Iteration Number: 2063\n",
      "Loss: 56.59314933261966\n",
      "l2 norm of gradients: 0.5476038351340397\n",
      "l2 norm of weights: 6.235240806509248\n",
      "---------------------\n",
      "Iteration Number: 2064\n",
      "Loss: 56.57835648580262\n",
      "l2 norm of gradients: 0.5473717587480074\n",
      "l2 norm of weights: 6.235098964710761\n",
      "---------------------\n",
      "Iteration Number: 2065\n",
      "Loss: 56.56357603169073\n",
      "l2 norm of gradients: 0.5471399030321049\n",
      "l2 norm of weights: 6.234957164247745\n",
      "---------------------\n",
      "Iteration Number: 2066\n",
      "Loss: 56.54880795330724\n",
      "l2 norm of gradients: 0.5469082676333876\n",
      "l2 norm of weights: 6.234815405083973\n",
      "---------------------\n",
      "Iteration Number: 2067\n",
      "Loss: 56.53405223384259\n",
      "l2 norm of gradients: 0.5466768521992859\n",
      "l2 norm of weights: 6.234673687183271\n",
      "---------------------\n",
      "Iteration Number: 2068\n",
      "Loss: 56.51930885638336\n",
      "l2 norm of gradients: 0.5464456563776061\n",
      "l2 norm of weights: 6.234532010509526\n",
      "---------------------\n",
      "Iteration Number: 2069\n",
      "Loss: 56.504577804192756\n",
      "l2 norm of gradients: 0.5462146798165342\n",
      "l2 norm of weights: 6.23439037502668\n",
      "---------------------\n",
      "Iteration Number: 2070\n",
      "Loss: 56.489859060458784\n",
      "l2 norm of gradients: 0.5459839221646363\n",
      "l2 norm of weights: 6.234248780698733\n",
      "---------------------\n",
      "Iteration Number: 2071\n",
      "Loss: 56.47515260848791\n",
      "l2 norm of gradients: 0.5457533830708622\n",
      "l2 norm of weights: 6.2341072274897416\n",
      "---------------------\n",
      "Iteration Number: 2072\n",
      "Loss: 56.46045843153961\n",
      "l2 norm of gradients: 0.5455230621845465\n",
      "l2 norm of weights: 6.2339657153638175\n",
      "---------------------\n",
      "Iteration Number: 2073\n",
      "Loss: 56.44577651295021\n",
      "l2 norm of gradients: 0.5452929591554113\n",
      "l2 norm of weights: 6.233824244285131\n",
      "---------------------\n",
      "Iteration Number: 2074\n",
      "Loss: 56.43110683607289\n",
      "l2 norm of gradients: 0.545063073633568\n",
      "l2 norm of weights: 6.233682814217908\n",
      "---------------------\n",
      "Iteration Number: 2075\n",
      "Loss: 56.41644938433743\n",
      "l2 norm of gradients: 0.5448334052695188\n",
      "l2 norm of weights: 6.233541425126431\n",
      "---------------------\n",
      "Iteration Number: 2076\n",
      "Loss: 56.40180414112741\n",
      "l2 norm of gradients: 0.5446039537141603\n",
      "l2 norm of weights: 6.233400076975036\n",
      "---------------------\n",
      "Iteration Number: 2077\n",
      "Loss: 56.3871710899302\n",
      "l2 norm of gradients: 0.544374718618784\n",
      "l2 norm of weights: 6.233258769728119\n",
      "---------------------\n",
      "Iteration Number: 2078\n",
      "Loss: 56.372550214197055\n",
      "l2 norm of gradients: 0.5441456996350784\n",
      "l2 norm of weights: 6.233117503350129\n",
      "---------------------\n",
      "Iteration Number: 2079\n",
      "Loss: 56.35794149749834\n",
      "l2 norm of gradients: 0.5439168964151322\n",
      "l2 norm of weights: 6.23297627780557\n",
      "---------------------\n",
      "Iteration Number: 2080\n",
      "Loss: 56.34334492335587\n",
      "l2 norm of gradients: 0.5436883086114345\n",
      "l2 norm of weights: 6.232835093059005\n",
      "---------------------\n",
      "Iteration Number: 2081\n",
      "Loss: 56.32876047540299\n",
      "l2 norm of gradients: 0.5434599358768782\n",
      "l2 norm of weights: 6.232693949075048\n",
      "---------------------\n",
      "Iteration Number: 2082\n",
      "Loss: 56.31418813722102\n",
      "l2 norm of gradients: 0.5432317778647614\n",
      "l2 norm of weights: 6.232552845818371\n",
      "---------------------\n",
      "Iteration Number: 2083\n",
      "Loss: 56.29962789249523\n",
      "l2 norm of gradients: 0.5430038342287884\n",
      "l2 norm of weights: 6.2324117832537\n",
      "---------------------\n",
      "Iteration Number: 2084\n",
      "Loss: 56.2850797248734\n",
      "l2 norm of gradients: 0.5427761046230735\n",
      "l2 norm of weights: 6.232270761345816\n",
      "---------------------\n",
      "Iteration Number: 2085\n",
      "Loss: 56.270543618133615\n",
      "l2 norm of gradients: 0.5425485887021406\n",
      "l2 norm of weights: 6.232129780059555\n",
      "---------------------\n",
      "Iteration Number: 2086\n",
      "Loss: 56.256019555958595\n",
      "l2 norm of gradients: 0.5423212861209271\n",
      "l2 norm of weights: 6.231988839359809\n",
      "---------------------\n",
      "Iteration Number: 2087\n",
      "Loss: 56.24150752216556\n",
      "l2 norm of gradients: 0.5420941965347842\n",
      "l2 norm of weights: 6.231847939211521\n",
      "---------------------\n",
      "Iteration Number: 2088\n",
      "Loss: 56.22700750058318\n",
      "l2 norm of gradients: 0.5418673195994792\n",
      "l2 norm of weights: 6.23170707957969\n",
      "---------------------\n",
      "Iteration Number: 2089\n",
      "Loss: 56.21251947501506\n",
      "l2 norm of gradients: 0.5416406549711977\n",
      "l2 norm of weights: 6.2315662604293705\n",
      "---------------------\n",
      "Iteration Number: 2090\n",
      "Loss: 56.198043429388726\n",
      "l2 norm of gradients: 0.5414142023065447\n",
      "l2 norm of weights: 6.23142548172567\n",
      "---------------------\n",
      "Iteration Number: 2091\n",
      "Loss: 56.18357934759169\n",
      "l2 norm of gradients: 0.5411879612625464\n",
      "l2 norm of weights: 6.231284743433749\n",
      "---------------------\n",
      "Iteration Number: 2092\n",
      "Loss: 56.16912721358141\n",
      "l2 norm of gradients: 0.5409619314966526\n",
      "l2 norm of weights: 6.231144045518823\n",
      "---------------------\n",
      "Iteration Number: 2093\n",
      "Loss: 56.15468701130739\n",
      "l2 norm of gradients: 0.5407361126667374\n",
      "l2 norm of weights: 6.231003387946162\n",
      "---------------------\n",
      "Iteration Number: 2094\n",
      "Loss: 56.14025872477714\n",
      "l2 norm of gradients: 0.5405105044311019\n",
      "l2 norm of weights: 6.230862770681086\n",
      "---------------------\n",
      "Iteration Number: 2095\n",
      "Loss: 56.12584233808146\n",
      "l2 norm of gradients: 0.5402851064484746\n",
      "l2 norm of weights: 6.230722193688972\n",
      "---------------------\n",
      "Iteration Number: 2096\n",
      "Loss: 56.11143783520494\n",
      "l2 norm of gradients: 0.5400599183780146\n",
      "l2 norm of weights: 6.230581656935248\n",
      "---------------------\n",
      "Iteration Number: 2097\n",
      "Loss: 56.09704520029564\n",
      "l2 norm of gradients: 0.5398349398793119\n",
      "l2 norm of weights: 6.2304411603853955\n",
      "---------------------\n",
      "Iteration Number: 2098\n",
      "Loss: 56.082664417483755\n",
      "l2 norm of gradients: 0.5396101706123897\n",
      "l2 norm of weights: 6.230300704004952\n",
      "---------------------\n",
      "Iteration Number: 2099\n",
      "Loss: 56.06829547096173\n",
      "l2 norm of gradients: 0.5393856102377055\n",
      "l2 norm of weights: 6.230160287759501\n",
      "---------------------\n",
      "Iteration Number: 2100\n",
      "Loss: 56.05393834484445\n",
      "l2 norm of gradients: 0.5391612584161534\n",
      "l2 norm of weights: 6.2300199116146855\n",
      "---------------------\n",
      "Iteration Number: 2101\n",
      "Loss: 56.03959302342383\n",
      "l2 norm of gradients: 0.5389371148090649\n",
      "l2 norm of weights: 6.229879575536197\n",
      "---------------------\n",
      "Iteration Number: 2102\n",
      "Loss: 56.02525949091763\n",
      "l2 norm of gradients: 0.5387131790782108\n",
      "l2 norm of weights: 6.22973927948978\n",
      "---------------------\n",
      "Iteration Number: 2103\n",
      "Loss: 56.01093773161604\n",
      "l2 norm of gradients: 0.538489450885803\n",
      "l2 norm of weights: 6.229599023441235\n",
      "---------------------\n",
      "Iteration Number: 2104\n",
      "Loss: 55.99662772988228\n",
      "l2 norm of gradients: 0.5382659298944953\n",
      "l2 norm of weights: 6.229458807356408\n",
      "---------------------\n",
      "Iteration Number: 2105\n",
      "Loss: 55.98232947003403\n",
      "l2 norm of gradients: 0.5380426157673849\n",
      "l2 norm of weights: 6.229318631201201\n",
      "---------------------\n",
      "Iteration Number: 2106\n",
      "Loss: 55.968042936429114\n",
      "l2 norm of gradients: 0.537819508168015\n",
      "l2 norm of weights: 6.229178494941567\n",
      "---------------------\n",
      "Iteration Number: 2107\n",
      "Loss: 55.95376811352319\n",
      "l2 norm of gradients: 0.5375966067603751\n",
      "l2 norm of weights: 6.229038398543513\n",
      "---------------------\n",
      "Iteration Number: 2108\n",
      "Loss: 55.93950498574677\n",
      "l2 norm of gradients: 0.537373911208903\n",
      "l2 norm of weights: 6.2288983419730934\n",
      "---------------------\n",
      "Iteration Number: 2109\n",
      "Loss: 55.92525353752659\n",
      "l2 norm of gradients: 0.5371514211784855\n",
      "l2 norm of weights: 6.2287583251964165\n",
      "---------------------\n",
      "Iteration Number: 2110\n",
      "Loss: 55.911013753422424\n",
      "l2 norm of gradients: 0.5369291363344609\n",
      "l2 norm of weights: 6.228618348179642\n",
      "---------------------\n",
      "Iteration Number: 2111\n",
      "Loss: 55.89678561795859\n",
      "l2 norm of gradients: 0.5367070563426192\n",
      "l2 norm of weights: 6.22847841088898\n",
      "---------------------\n",
      "Iteration Number: 2112\n",
      "Loss: 55.882569115670094\n",
      "l2 norm of gradients: 0.5364851808692048\n",
      "l2 norm of weights: 6.2283385132906925\n",
      "---------------------\n",
      "Iteration Number: 2113\n",
      "Loss: 55.868364231156775\n",
      "l2 norm of gradients: 0.5362635095809165\n",
      "l2 norm of weights: 6.22819865535109\n",
      "---------------------\n",
      "Iteration Number: 2114\n",
      "Loss: 55.854170949099895\n",
      "l2 norm of gradients: 0.5360420421449096\n",
      "l2 norm of weights: 6.228058837036537\n",
      "---------------------\n",
      "Iteration Number: 2115\n",
      "Loss: 55.83998925408078\n",
      "l2 norm of gradients: 0.5358207782287973\n",
      "l2 norm of weights: 6.227919058313447\n",
      "---------------------\n",
      "Iteration Number: 2116\n",
      "Loss: 55.82581913080873\n",
      "l2 norm of gradients: 0.5355997175006513\n",
      "l2 norm of weights: 6.227779319148284\n",
      "---------------------\n",
      "Iteration Number: 2117\n",
      "Loss: 55.8116605640062\n",
      "l2 norm of gradients: 0.5353788596290041\n",
      "l2 norm of weights: 6.227639619507563\n",
      "---------------------\n",
      "Iteration Number: 2118\n",
      "Loss: 55.79751353841997\n",
      "l2 norm of gradients: 0.5351582042828494\n",
      "l2 norm of weights: 6.227499959357847\n",
      "---------------------\n",
      "Iteration Number: 2119\n",
      "Loss: 55.78337803879039\n",
      "l2 norm of gradients: 0.5349377511316437\n",
      "l2 norm of weights: 6.227360338665753\n",
      "---------------------\n",
      "Iteration Number: 2120\n",
      "Loss: 55.76925405000249\n",
      "l2 norm of gradients: 0.5347174998453075\n",
      "l2 norm of weights: 6.227220757397945\n",
      "---------------------\n",
      "Iteration Number: 2121\n",
      "Loss: 55.75514155683471\n",
      "l2 norm of gradients: 0.5344974500942266\n",
      "l2 norm of weights: 6.227081215521139\n",
      "---------------------\n",
      "Iteration Number: 2122\n",
      "Loss: 55.74104054412567\n",
      "l2 norm of gradients: 0.5342776015492531\n",
      "l2 norm of weights: 6.2269417130020965\n",
      "---------------------\n",
      "Iteration Number: 2123\n",
      "Loss: 55.72695099682297\n",
      "l2 norm of gradients: 0.5340579538817072\n",
      "l2 norm of weights: 6.226802249807633\n",
      "---------------------\n",
      "Iteration Number: 2124\n",
      "Loss: 55.7128728998194\n",
      "l2 norm of gradients: 0.5338385067633771\n",
      "l2 norm of weights: 6.226662825904612\n",
      "---------------------\n",
      "Iteration Number: 2125\n",
      "Loss: 55.6988062381136\n",
      "l2 norm of gradients: 0.5336192598665218\n",
      "l2 norm of weights: 6.226523441259948\n",
      "---------------------\n",
      "Iteration Number: 2126\n",
      "Loss: 55.684750996612415\n",
      "l2 norm of gradients: 0.5334002128638711\n",
      "l2 norm of weights: 6.2263840958406\n",
      "---------------------\n",
      "Iteration Number: 2127\n",
      "Loss: 55.6707071604085\n",
      "l2 norm of gradients: 0.5331813654286267\n",
      "l2 norm of weights: 6.22624478961358\n",
      "---------------------\n",
      "Iteration Number: 2128\n",
      "Loss: 55.656674714517656\n",
      "l2 norm of gradients: 0.5329627172344644\n",
      "l2 norm of weights: 6.226105522545949\n",
      "---------------------\n",
      "Iteration Number: 2129\n",
      "Loss: 55.642653643984914\n",
      "l2 norm of gradients: 0.5327442679555338\n",
      "l2 norm of weights: 6.225966294604815\n",
      "---------------------\n",
      "Iteration Number: 2130\n",
      "Loss: 55.62864393392029\n",
      "l2 norm of gradients: 0.5325260172664606\n",
      "l2 norm of weights: 6.225827105757335\n",
      "---------------------\n",
      "Iteration Number: 2131\n",
      "Loss: 55.61464556947773\n",
      "l2 norm of gradients: 0.5323079648423468\n",
      "l2 norm of weights: 6.225687955970717\n",
      "---------------------\n",
      "Iteration Number: 2132\n",
      "Loss: 55.600658535821964\n",
      "l2 norm of gradients: 0.5320901103587723\n",
      "l2 norm of weights: 6.225548845212211\n",
      "---------------------\n",
      "Iteration Number: 2133\n",
      "Loss: 55.58668281810436\n",
      "l2 norm of gradients: 0.5318724534917953\n",
      "l2 norm of weights: 6.225409773449125\n",
      "---------------------\n",
      "Iteration Number: 2134\n",
      "Loss: 55.572718401547434\n",
      "l2 norm of gradients: 0.5316549939179546\n",
      "l2 norm of weights: 6.225270740648807\n",
      "---------------------\n",
      "Iteration Number: 2135\n",
      "Loss: 55.55876527143859\n",
      "l2 norm of gradients: 0.5314377313142692\n",
      "l2 norm of weights: 6.225131746778657\n",
      "---------------------\n",
      "Iteration Number: 2136\n",
      "Loss: 55.54482341300496\n",
      "l2 norm of gradients: 0.53122066535824\n",
      "l2 norm of weights: 6.224992791806121\n",
      "---------------------\n",
      "Iteration Number: 2137\n",
      "Loss: 55.530892811586554\n",
      "l2 norm of gradients: 0.5310037957278507\n",
      "l2 norm of weights: 6.224853875698695\n",
      "---------------------\n",
      "Iteration Number: 2138\n",
      "Loss: 55.5169734525048\n",
      "l2 norm of gradients: 0.530787122101569\n",
      "l2 norm of weights: 6.22471499842392\n",
      "---------------------\n",
      "Iteration Number: 2139\n",
      "Loss: 55.50306532112508\n",
      "l2 norm of gradients: 0.5305706441583471\n",
      "l2 norm of weights: 6.224576159949387\n",
      "---------------------\n",
      "Iteration Number: 2140\n",
      "Loss: 55.48916840280262\n",
      "l2 norm of gradients: 0.5303543615776227\n",
      "l2 norm of weights: 6.224437360242733\n",
      "---------------------\n",
      "Iteration Number: 2141\n",
      "Loss: 55.475282683037626\n",
      "l2 norm of gradients: 0.5301382740393208\n",
      "l2 norm of weights: 6.224298599271643\n",
      "---------------------\n",
      "Iteration Number: 2142\n",
      "Loss: 55.461408147168775\n",
      "l2 norm of gradients: 0.5299223812238532\n",
      "l2 norm of weights: 6.2241598770038475\n",
      "---------------------\n",
      "Iteration Number: 2143\n",
      "Loss: 55.44754478080606\n",
      "l2 norm of gradients: 0.5297066828121205\n",
      "l2 norm of weights: 6.224021193407127\n",
      "---------------------\n",
      "Iteration Number: 2144\n",
      "Loss: 55.43369256934916\n",
      "l2 norm of gradients: 0.5294911784855126\n",
      "l2 norm of weights: 6.223882548449308\n",
      "---------------------\n",
      "Iteration Number: 2145\n",
      "Loss: 55.41985149834609\n",
      "l2 norm of gradients: 0.5292758679259095\n",
      "l2 norm of weights: 6.22374394209826\n",
      "---------------------\n",
      "Iteration Number: 2146\n",
      "Loss: 55.406021553371275\n",
      "l2 norm of gradients: 0.5290607508156827\n",
      "l2 norm of weights: 6.223605374321906\n",
      "---------------------\n",
      "Iteration Number: 2147\n",
      "Loss: 55.39220272004843\n",
      "l2 norm of gradients: 0.5288458268376953\n",
      "l2 norm of weights: 6.223466845088209\n",
      "---------------------\n",
      "Iteration Number: 2148\n",
      "Loss: 55.37839498392845\n",
      "l2 norm of gradients: 0.5286310956753032\n",
      "l2 norm of weights: 6.223328354365183\n",
      "---------------------\n",
      "Iteration Number: 2149\n",
      "Loss: 55.364598330734445\n",
      "l2 norm of gradients: 0.5284165570123561\n",
      "l2 norm of weights: 6.223189902120887\n",
      "---------------------\n",
      "Iteration Number: 2150\n",
      "Loss: 55.35081274607137\n",
      "l2 norm of gradients: 0.528202210533198\n",
      "l2 norm of weights: 6.223051488323425\n",
      "---------------------\n",
      "Iteration Number: 2151\n",
      "Loss: 55.337038215696104\n",
      "l2 norm of gradients: 0.5279880559226685\n",
      "l2 norm of weights: 6.222913112940948\n",
      "---------------------\n",
      "Iteration Number: 2152\n",
      "Loss: 55.32327472530868\n",
      "l2 norm of gradients: 0.5277740928661024\n",
      "l2 norm of weights: 6.222774775941654\n",
      "---------------------\n",
      "Iteration Number: 2153\n",
      "Loss: 55.30952226065641\n",
      "l2 norm of gradients: 0.5275603210493326\n",
      "l2 norm of weights: 6.222636477293784\n",
      "---------------------\n",
      "Iteration Number: 2154\n",
      "Loss: 55.295780807557854\n",
      "l2 norm of gradients: 0.5273467401586885\n",
      "l2 norm of weights: 6.222498216965628\n",
      "---------------------\n",
      "Iteration Number: 2155\n",
      "Loss: 55.28205035179468\n",
      "l2 norm of gradients: 0.5271333498809985\n",
      "l2 norm of weights: 6.222359994925521\n",
      "---------------------\n",
      "Iteration Number: 2156\n",
      "Loss: 55.268330879216705\n",
      "l2 norm of gradients: 0.5269201499035897\n",
      "l2 norm of weights: 6.222221811141839\n",
      "---------------------\n",
      "Iteration Number: 2157\n",
      "Loss: 55.25462237568717\n",
      "l2 norm of gradients: 0.5267071399142894\n",
      "l2 norm of weights: 6.222083665583011\n",
      "---------------------\n",
      "Iteration Number: 2158\n",
      "Loss: 55.24092482717048\n",
      "l2 norm of gradients: 0.5264943196014253\n",
      "l2 norm of weights: 6.221945558217505\n",
      "---------------------\n",
      "Iteration Number: 2159\n",
      "Loss: 55.22723821951946\n",
      "l2 norm of gradients: 0.5262816886538263\n",
      "l2 norm of weights: 6.2218074890138375\n",
      "---------------------\n",
      "Iteration Number: 2160\n",
      "Loss: 55.213562538664114\n",
      "l2 norm of gradients: 0.5260692467608235\n",
      "l2 norm of weights: 6.221669457940569\n",
      "---------------------\n",
      "Iteration Number: 2161\n",
      "Loss: 55.199897770628084\n",
      "l2 norm of gradients: 0.5258569936122501\n",
      "l2 norm of weights: 6.221531464966303\n",
      "---------------------\n",
      "Iteration Number: 2162\n",
      "Loss: 55.1862439014251\n",
      "l2 norm of gradients: 0.5256449288984437\n",
      "l2 norm of weights: 6.221393510059691\n",
      "---------------------\n",
      "Iteration Number: 2163\n",
      "Loss: 55.172600917085674\n",
      "l2 norm of gradients: 0.5254330523102446\n",
      "l2 norm of weights: 6.221255593189428\n",
      "---------------------\n",
      "Iteration Number: 2164\n",
      "Loss: 55.15896880363761\n",
      "l2 norm of gradients: 0.5252213635389988\n",
      "l2 norm of weights: 6.221117714324253\n",
      "---------------------\n",
      "Iteration Number: 2165\n",
      "Loss: 55.14534754718919\n",
      "l2 norm of gradients: 0.5250098622765569\n",
      "l2 norm of weights: 6.220979873432948\n",
      "---------------------\n",
      "Iteration Number: 2166\n",
      "Loss: 55.13173713386266\n",
      "l2 norm of gradients: 0.5247985482152759\n",
      "l2 norm of weights: 6.220842070484344\n",
      "---------------------\n",
      "Iteration Number: 2167\n",
      "Loss: 55.11813754978252\n",
      "l2 norm of gradients: 0.5245874210480189\n",
      "l2 norm of weights: 6.2207043054473115\n",
      "---------------------\n",
      "Iteration Number: 2168\n",
      "Loss: 55.104548781123654\n",
      "l2 norm of gradients: 0.5243764804681567\n",
      "l2 norm of weights: 6.220566578290767\n",
      "---------------------\n",
      "Iteration Number: 2169\n",
      "Loss: 55.090970814079306\n",
      "l2 norm of gradients: 0.5241657261695672\n",
      "l2 norm of weights: 6.22042888898367\n",
      "---------------------\n",
      "Iteration Number: 2170\n",
      "Loss: 55.077403634882955\n",
      "l2 norm of gradients: 0.5239551578466372\n",
      "l2 norm of weights: 6.220291237495027\n",
      "---------------------\n",
      "Iteration Number: 2171\n",
      "Loss: 55.06384722974566\n",
      "l2 norm of gradients: 0.5237447751942619\n",
      "l2 norm of weights: 6.220153623793884\n",
      "---------------------\n",
      "Iteration Number: 2172\n",
      "Loss: 55.0503015850422\n",
      "l2 norm of gradients: 0.5235345779078464\n",
      "l2 norm of weights: 6.220016047849332\n",
      "---------------------\n",
      "Iteration Number: 2173\n",
      "Loss: 55.03676668696376\n",
      "l2 norm of gradients: 0.5233245656833057\n",
      "l2 norm of weights: 6.219878509630507\n",
      "---------------------\n",
      "Iteration Number: 2174\n",
      "Loss: 55.0232425219004\n",
      "l2 norm of gradients: 0.523114738217065\n",
      "l2 norm of weights: 6.219741009106589\n",
      "---------------------\n",
      "Iteration Number: 2175\n",
      "Loss: 55.0097290761743\n",
      "l2 norm of gradients: 0.5229050952060614\n",
      "l2 norm of weights: 6.219603546246797\n",
      "---------------------\n",
      "Iteration Number: 2176\n",
      "Loss: 54.99622633617249\n",
      "l2 norm of gradients: 0.5226956363477426\n",
      "l2 norm of weights: 6.219466121020397\n",
      "---------------------\n",
      "Iteration Number: 2177\n",
      "Loss: 54.98273428834295\n",
      "l2 norm of gradients: 0.5224863613400693\n",
      "l2 norm of weights: 6.219328733396697\n",
      "---------------------\n",
      "Iteration Number: 2178\n",
      "Loss: 54.96925291904941\n",
      "l2 norm of gradients: 0.5222772698815146\n",
      "l2 norm of weights: 6.2191913833450485\n",
      "---------------------\n",
      "Iteration Number: 2179\n",
      "Loss: 54.955782214851034\n",
      "l2 norm of gradients: 0.5220683616710643\n",
      "l2 norm of weights: 6.219054070834844\n",
      "---------------------\n",
      "Iteration Number: 2180\n",
      "Loss: 54.94232216216099\n",
      "l2 norm of gradients: 0.5218596364082184\n",
      "l2 norm of weights: 6.21891679583552\n",
      "---------------------\n",
      "Iteration Number: 2181\n",
      "Loss: 54.928872747478934\n",
      "l2 norm of gradients: 0.5216510937929906\n",
      "l2 norm of weights: 6.218779558316557\n",
      "---------------------\n",
      "Iteration Number: 2182\n",
      "Loss: 54.91543395742992\n",
      "l2 norm of gradients: 0.5214427335259093\n",
      "l2 norm of weights: 6.218642358247475\n",
      "---------------------\n",
      "Iteration Number: 2183\n",
      "Loss: 54.90200577849516\n",
      "l2 norm of gradients: 0.5212345553080181\n",
      "l2 norm of weights: 6.21850519559784\n",
      "---------------------\n",
      "Iteration Number: 2184\n",
      "Loss: 54.88858819728374\n",
      "l2 norm of gradients: 0.5210265588408756\n",
      "l2 norm of weights: 6.218368070337256\n",
      "---------------------\n",
      "Iteration Number: 2185\n",
      "Loss: 54.87518120044106\n",
      "l2 norm of gradients: 0.5208187438265566\n",
      "l2 norm of weights: 6.218230982435373\n",
      "---------------------\n",
      "Iteration Number: 2186\n",
      "Loss: 54.861784774644576\n",
      "l2 norm of gradients: 0.5206111099676519\n",
      "l2 norm of weights: 6.218093931861881\n",
      "---------------------\n",
      "Iteration Number: 2187\n",
      "Loss: 54.848398906447635\n",
      "l2 norm of gradients: 0.5204036569672694\n",
      "l2 norm of weights: 6.217956918586512\n",
      "---------------------\n",
      "Iteration Number: 2188\n",
      "Loss: 54.835023582645796\n",
      "l2 norm of gradients: 0.520196384529034\n",
      "l2 norm of weights: 6.21781994257904\n",
      "---------------------\n",
      "Iteration Number: 2189\n",
      "Loss: 54.82165878992285\n",
      "l2 norm of gradients: 0.5199892923570882\n",
      "l2 norm of weights: 6.217683003809284\n",
      "---------------------\n",
      "Iteration Number: 2190\n",
      "Loss: 54.80830451503656\n",
      "l2 norm of gradients: 0.5197823801560919\n",
      "l2 norm of weights: 6.2175461022470975\n",
      "---------------------\n",
      "Iteration Number: 2191\n",
      "Loss: 54.794960744713386\n",
      "l2 norm of gradients: 0.5195756476312238\n",
      "l2 norm of weights: 6.217409237862382\n",
      "---------------------\n",
      "Iteration Number: 2192\n",
      "Loss: 54.7816274658381\n",
      "l2 norm of gradients: 0.5193690944881812\n",
      "l2 norm of weights: 6.217272410625078\n",
      "---------------------\n",
      "Iteration Number: 2193\n",
      "Loss: 54.7683046651757\n",
      "l2 norm of gradients: 0.5191627204331801\n",
      "l2 norm of weights: 6.217135620505167\n",
      "---------------------\n",
      "Iteration Number: 2194\n",
      "Loss: 54.754992329583935\n",
      "l2 norm of gradients: 0.5189565251729563\n",
      "l2 norm of weights: 6.216998867472673\n",
      "---------------------\n",
      "Iteration Number: 2195\n",
      "Loss: 54.74169044591465\n",
      "l2 norm of gradients: 0.5187505084147649\n",
      "l2 norm of weights: 6.216862151497659\n",
      "---------------------\n",
      "Iteration Number: 2196\n",
      "Loss: 54.72839900111501\n",
      "l2 norm of gradients: 0.5185446698663811\n",
      "l2 norm of weights: 6.21672547255023\n",
      "---------------------\n",
      "Iteration Number: 2197\n",
      "Loss: 54.71511798206685\n",
      "l2 norm of gradients: 0.5183390092361008\n",
      "l2 norm of weights: 6.216588830600535\n",
      "---------------------\n",
      "Iteration Number: 2198\n",
      "Loss: 54.701847375731546\n",
      "l2 norm of gradients: 0.5181335262327403\n",
      "l2 norm of weights: 6.216452225618757\n",
      "---------------------\n",
      "Iteration Number: 2199\n",
      "Loss: 54.688587169083945\n",
      "l2 norm of gradients: 0.5179282205656369\n",
      "l2 norm of weights: 6.216315657575126\n",
      "---------------------\n",
      "Iteration Number: 2200\n",
      "Loss: 54.675337349142474\n",
      "l2 norm of gradients: 0.5177230919446488\n",
      "l2 norm of weights: 6.216179126439909\n",
      "---------------------\n",
      "Iteration Number: 2201\n",
      "Loss: 54.66209790290168\n",
      "l2 norm of gradients: 0.517518140080157\n",
      "l2 norm of weights: 6.216042632183415\n",
      "---------------------\n",
      "Iteration Number: 2202\n",
      "Loss: 54.6488688174321\n",
      "l2 norm of gradients: 0.5173133646830625\n",
      "l2 norm of weights: 6.215906174775992\n",
      "---------------------\n",
      "Iteration Number: 2203\n",
      "Loss: 54.635650079777115\n",
      "l2 norm of gradients: 0.5171087654647902\n",
      "l2 norm of weights: 6.215769754188031\n",
      "---------------------\n",
      "Iteration Number: 2204\n",
      "Loss: 54.622441677108974\n",
      "l2 norm of gradients: 0.5169043421372862\n",
      "l2 norm of weights: 6.21563337038996\n",
      "---------------------\n",
      "Iteration Number: 2205\n",
      "Loss: 54.609243596446795\n",
      "l2 norm of gradients: 0.5167000944130198\n",
      "l2 norm of weights: 6.215497023352248\n",
      "---------------------\n",
      "Iteration Number: 2206\n",
      "Loss: 54.59605582503253\n",
      "l2 norm of gradients: 0.5164960220049831\n",
      "l2 norm of weights: 6.2153607130454045\n",
      "---------------------\n",
      "Iteration Number: 2207\n",
      "Loss: 54.58287835000726\n",
      "l2 norm of gradients: 0.5162921246266912\n",
      "l2 norm of weights: 6.2152244394399805\n",
      "---------------------\n",
      "Iteration Number: 2208\n",
      "Loss: 54.56971115852455\n",
      "l2 norm of gradients: 0.5160884019921828\n",
      "l2 norm of weights: 6.215088202506562\n",
      "---------------------\n",
      "Iteration Number: 2209\n",
      "Loss: 54.55655423787102\n",
      "l2 norm of gradients: 0.5158848538160197\n",
      "l2 norm of weights: 6.214952002215779\n",
      "---------------------\n",
      "Iteration Number: 2210\n",
      "Loss: 54.54340757526545\n",
      "l2 norm of gradients: 0.5156814798132883\n",
      "l2 norm of weights: 6.2148158385383\n",
      "---------------------\n",
      "Iteration Number: 2211\n",
      "Loss: 54.530271157950565\n",
      "l2 norm of gradients: 0.5154782796995984\n",
      "l2 norm of weights: 6.2146797114448304\n",
      "---------------------\n",
      "Iteration Number: 2212\n",
      "Loss: 54.51714497329505\n",
      "l2 norm of gradients: 0.5152752531910842\n",
      "l2 norm of weights: 6.2145436209061184\n",
      "---------------------\n",
      "Iteration Number: 2213\n",
      "Loss: 54.504029008564665\n",
      "l2 norm of gradients: 0.5150724000044041\n",
      "l2 norm of weights: 6.21440756689295\n",
      "---------------------\n",
      "Iteration Number: 2214\n",
      "Loss: 54.49092325111374\n",
      "l2 norm of gradients: 0.5148697198567418\n",
      "l2 norm of weights: 6.21427154937615\n",
      "---------------------\n",
      "Iteration Number: 2215\n",
      "Loss: 54.4778276882892\n",
      "l2 norm of gradients: 0.5146672124658048\n",
      "l2 norm of weights: 6.214135568326583\n",
      "---------------------\n",
      "Iteration Number: 2216\n",
      "Loss: 54.464742307525974\n",
      "l2 norm of gradients: 0.5144648775498264\n",
      "l2 norm of weights: 6.213999623715151\n",
      "---------------------\n",
      "Iteration Number: 2217\n",
      "Loss: 54.45166709620719\n",
      "l2 norm of gradients: 0.5142627148275644\n",
      "l2 norm of weights: 6.213863715512796\n",
      "---------------------\n",
      "Iteration Number: 2218\n",
      "Loss: 54.438602041780484\n",
      "l2 norm of gradients: 0.5140607240183024\n",
      "l2 norm of weights: 6.213727843690499\n",
      "---------------------\n",
      "Iteration Number: 2219\n",
      "Loss: 54.425547131708335\n",
      "l2 norm of gradients: 0.5138589048418487\n",
      "l2 norm of weights: 6.21359200821928\n",
      "---------------------\n",
      "Iteration Number: 2220\n",
      "Loss: 54.412502353540404\n",
      "l2 norm of gradients: 0.5136572570185381\n",
      "l2 norm of weights: 6.213456209070195\n",
      "---------------------\n",
      "Iteration Number: 2221\n",
      "Loss: 54.399467694683175\n",
      "l2 norm of gradients: 0.5134557802692306\n",
      "l2 norm of weights: 6.2133204462143405\n",
      "---------------------\n",
      "Iteration Number: 2222\n",
      "Loss: 54.38644314273172\n",
      "l2 norm of gradients: 0.5132544743153117\n",
      "l2 norm of weights: 6.213184719622852\n",
      "---------------------\n",
      "Iteration Number: 2223\n",
      "Loss: 54.373428685247326\n",
      "l2 norm of gradients: 0.5130533388786932\n",
      "l2 norm of weights: 6.213049029266901\n",
      "---------------------\n",
      "Iteration Number: 2224\n",
      "Loss: 54.36042430982008\n",
      "l2 norm of gradients: 0.5128523736818135\n",
      "l2 norm of weights: 6.212913375117697\n",
      "---------------------\n",
      "Iteration Number: 2225\n",
      "Loss: 54.34743000401692\n",
      "l2 norm of gradients: 0.512651578447636\n",
      "l2 norm of weights: 6.212777757146493\n",
      "---------------------\n",
      "Iteration Number: 2226\n",
      "Loss: 54.33444575549766\n",
      "l2 norm of gradients: 0.5124509528996513\n",
      "l2 norm of weights: 6.2126421753245715\n",
      "---------------------\n",
      "Iteration Number: 2227\n",
      "Loss: 54.3214715519187\n",
      "l2 norm of gradients: 0.512250496761876\n",
      "l2 norm of weights: 6.21250662962326\n",
      "---------------------\n",
      "Iteration Number: 2228\n",
      "Loss: 54.30850738093688\n",
      "l2 norm of gradients: 0.5120502097588532\n",
      "l2 norm of weights: 6.212371120013918\n",
      "---------------------\n",
      "Iteration Number: 2229\n",
      "Loss: 54.29555323026375\n",
      "l2 norm of gradients: 0.5118500916156525\n",
      "l2 norm of weights: 6.2122356464679465\n",
      "---------------------\n",
      "Iteration Number: 2230\n",
      "Loss: 54.282609087663566\n",
      "l2 norm of gradients: 0.5116501420578704\n",
      "l2 norm of weights: 6.212100208956785\n",
      "---------------------\n",
      "Iteration Number: 2231\n",
      "Loss: 54.269674940810454\n",
      "l2 norm of gradients: 0.5114503608116295\n",
      "l2 norm of weights: 6.2119648074519045\n",
      "---------------------\n",
      "Iteration Number: 2232\n",
      "Loss: 54.256750777522534\n",
      "l2 norm of gradients: 0.5112507476035796\n",
      "l2 norm of weights: 6.211829441924821\n",
      "---------------------\n",
      "Iteration Number: 2233\n",
      "Loss: 54.243836585606175\n",
      "l2 norm of gradients: 0.511051302160897\n",
      "l2 norm of weights: 6.211694112347082\n",
      "---------------------\n",
      "Iteration Number: 2234\n",
      "Loss: 54.23093235281494\n",
      "l2 norm of gradients: 0.5108520242112852\n",
      "l2 norm of weights: 6.211558818690274\n",
      "---------------------\n",
      "Iteration Number: 2235\n",
      "Loss: 54.21803806704693\n",
      "l2 norm of gradients: 0.510652913482974\n",
      "l2 norm of weights: 6.211423560926021\n",
      "---------------------\n",
      "Iteration Number: 2236\n",
      "Loss: 54.20515371616429\n",
      "l2 norm of gradients: 0.5104539697047209\n",
      "l2 norm of weights: 6.211288339025985\n",
      "---------------------\n",
      "Iteration Number: 2237\n",
      "Loss: 54.19227928798789\n",
      "l2 norm of gradients: 0.5102551926058096\n",
      "l2 norm of weights: 6.211153152961861\n",
      "---------------------\n",
      "Iteration Number: 2238\n",
      "Loss: 54.17941477050694\n",
      "l2 norm of gradients: 0.5100565819160511\n",
      "l2 norm of weights: 6.211018002705385\n",
      "---------------------\n",
      "Iteration Number: 2239\n",
      "Loss: 54.16656015163491\n",
      "l2 norm of gradients: 0.5098581373657833\n",
      "l2 norm of weights: 6.210882888228328\n",
      "---------------------\n",
      "Iteration Number: 2240\n",
      "Loss: 54.153715419328336\n",
      "l2 norm of gradients: 0.5096598586858713\n",
      "l2 norm of weights: 6.2107478095024975\n",
      "---------------------\n",
      "Iteration Number: 2241\n",
      "Loss: 54.140880561513356\n",
      "l2 norm of gradients: 0.5094617456077069\n",
      "l2 norm of weights: 6.210612766499737\n",
      "---------------------\n",
      "Iteration Number: 2242\n",
      "Loss: 54.12805556623695\n",
      "l2 norm of gradients: 0.5092637978632087\n",
      "l2 norm of weights: 6.210477759191926\n",
      "---------------------\n",
      "Iteration Number: 2243\n",
      "Loss: 54.115240421517335\n",
      "l2 norm of gradients: 0.5090660151848233\n",
      "l2 norm of weights: 6.210342787550985\n",
      "---------------------\n",
      "Iteration Number: 2244\n",
      "Loss: 54.10243511537358\n",
      "l2 norm of gradients: 0.508868397305523\n",
      "l2 norm of weights: 6.210207851548862\n",
      "---------------------\n",
      "Iteration Number: 2245\n",
      "Loss: 54.0896396359254\n",
      "l2 norm of gradients: 0.5086709439588077\n",
      "l2 norm of weights: 6.21007295115755\n",
      "---------------------\n",
      "Iteration Number: 2246\n",
      "Loss: 54.07685397119914\n",
      "l2 norm of gradients: 0.5084736548787043\n",
      "l2 norm of weights: 6.209938086349072\n",
      "---------------------\n",
      "Iteration Number: 2247\n",
      "Loss: 54.06407810936857\n",
      "l2 norm of gradients: 0.5082765297997663\n",
      "l2 norm of weights: 6.209803257095491\n",
      "---------------------\n",
      "Iteration Number: 2248\n",
      "Loss: 54.05131203852594\n",
      "l2 norm of gradients: 0.5080795684570744\n",
      "l2 norm of weights: 6.209668463368901\n",
      "---------------------\n",
      "Iteration Number: 2249\n",
      "Loss: 54.038555746840856\n",
      "l2 norm of gradients: 0.5078827705862357\n",
      "l2 norm of weights: 6.209533705141437\n",
      "---------------------\n",
      "Iteration Number: 2250\n",
      "Loss: 54.02580922249227\n",
      "l2 norm of gradients: 0.5076861359233844\n",
      "l2 norm of weights: 6.2093989823852676\n",
      "---------------------\n",
      "Iteration Number: 2251\n",
      "Loss: 54.013072453728014\n",
      "l2 norm of gradients: 0.5074896642051813\n",
      "l2 norm of weights: 6.209264295072595\n",
      "---------------------\n",
      "Iteration Number: 2252\n",
      "Loss: 54.00034542871086\n",
      "l2 norm of gradients: 0.5072933551688138\n",
      "l2 norm of weights: 6.209129643175659\n",
      "---------------------\n",
      "Iteration Number: 2253\n",
      "Loss: 53.98762813566648\n",
      "l2 norm of gradients: 0.5070972085519958\n",
      "l2 norm of weights: 6.208995026666736\n",
      "---------------------\n",
      "Iteration Number: 2254\n",
      "Loss: 53.97492056295066\n",
      "l2 norm of gradients: 0.5069012240929683\n",
      "l2 norm of weights: 6.208860445518134\n",
      "---------------------\n",
      "Iteration Number: 2255\n",
      "Loss: 53.96222269878039\n",
      "l2 norm of gradients: 0.5067054015304981\n",
      "l2 norm of weights: 6.208725899702199\n",
      "---------------------\n",
      "Iteration Number: 2256\n",
      "Loss: 53.94953453148378\n",
      "l2 norm of gradients: 0.5065097406038788\n",
      "l2 norm of weights: 6.208591389191313\n",
      "---------------------\n",
      "Iteration Number: 2257\n",
      "Loss: 53.93685604942503\n",
      "l2 norm of gradients: 0.5063142410529295\n",
      "l2 norm of weights: 6.208456913957889\n",
      "---------------------\n",
      "Iteration Number: 2258\n",
      "Loss: 53.92418724094689\n",
      "l2 norm of gradients: 0.506118902617997\n",
      "l2 norm of weights: 6.208322473974378\n",
      "---------------------\n",
      "Iteration Number: 2259\n",
      "Loss: 53.911528094394185\n",
      "l2 norm of gradients: 0.5059237250399524\n",
      "l2 norm of weights: 6.208188069213267\n",
      "---------------------\n",
      "Iteration Number: 2260\n",
      "Loss: 53.898878598192205\n",
      "l2 norm of gradients: 0.5057287080601942\n",
      "l2 norm of weights: 6.208053699647073\n",
      "---------------------\n",
      "Iteration Number: 2261\n",
      "Loss: 53.88623874076419\n",
      "l2 norm of gradients: 0.5055338514206461\n",
      "l2 norm of weights: 6.207919365248353\n",
      "---------------------\n",
      "Iteration Number: 2262\n",
      "Loss: 53.873608510555044\n",
      "l2 norm of gradients: 0.505339154863758\n",
      "l2 norm of weights: 6.207785065989697\n",
      "---------------------\n",
      "Iteration Number: 2263\n",
      "Loss: 53.86098789603156\n",
      "l2 norm of gradients: 0.5051446181325047\n",
      "l2 norm of weights: 6.207650801843726\n",
      "---------------------\n",
      "Iteration Number: 2264\n",
      "Loss: 53.84837688570323\n",
      "l2 norm of gradients: 0.5049502409703875\n",
      "l2 norm of weights: 6.2075165727831\n",
      "---------------------\n",
      "Iteration Number: 2265\n",
      "Loss: 53.83577546800871\n",
      "l2 norm of gradients: 0.5047560231214324\n",
      "l2 norm of weights: 6.207382378780511\n",
      "---------------------\n",
      "Iteration Number: 2266\n",
      "Loss: 53.82318363149386\n",
      "l2 norm of gradients: 0.5045619643301913\n",
      "l2 norm of weights: 6.207248219808687\n",
      "---------------------\n",
      "Iteration Number: 2267\n",
      "Loss: 53.81060136475515\n",
      "l2 norm of gradients: 0.5043680643417405\n",
      "l2 norm of weights: 6.207114095840388\n",
      "---------------------\n",
      "Iteration Number: 2268\n",
      "Loss: 53.79802865632836\n",
      "l2 norm of gradients: 0.5041743229016821\n",
      "l2 norm of weights: 6.206980006848408\n",
      "---------------------\n",
      "Iteration Number: 2269\n",
      "Loss: 53.78546549481055\n",
      "l2 norm of gradients: 0.5039807397561423\n",
      "l2 norm of weights: 6.206845952805577\n",
      "---------------------\n",
      "Iteration Number: 2270\n",
      "Loss: 53.7729118688201\n",
      "l2 norm of gradients: 0.5037873146517727\n",
      "l2 norm of weights: 6.206711933684758\n",
      "---------------------\n",
      "Iteration Number: 2271\n",
      "Loss: 53.760367767007956\n",
      "l2 norm of gradients: 0.5035940473357491\n",
      "l2 norm of weights: 6.206577949458848\n",
      "---------------------\n",
      "Iteration Number: 2272\n",
      "Loss: 53.74783317800353\n",
      "l2 norm of gradients: 0.5034009375557715\n",
      "l2 norm of weights: 6.206444000100777\n",
      "---------------------\n",
      "Iteration Number: 2273\n",
      "Loss: 53.73530809050501\n",
      "l2 norm of gradients: 0.5032079850600648\n",
      "l2 norm of weights: 6.206310085583508\n",
      "---------------------\n",
      "Iteration Number: 2274\n",
      "Loss: 53.72279249321111\n",
      "l2 norm of gradients: 0.5030151895973772\n",
      "l2 norm of weights: 6.20617620588004\n",
      "---------------------\n",
      "Iteration Number: 2275\n",
      "Loss: 53.710286374812135\n",
      "l2 norm of gradients: 0.5028225509169812\n",
      "l2 norm of weights: 6.206042360963404\n",
      "---------------------\n",
      "Iteration Number: 2276\n",
      "Loss: 53.697789724048675\n",
      "l2 norm of gradients: 0.5026300687686733\n",
      "l2 norm of weights: 6.205908550806665\n",
      "---------------------\n",
      "Iteration Number: 2277\n",
      "Loss: 53.68530252973886\n",
      "l2 norm of gradients: 0.5024377429027728\n",
      "l2 norm of weights: 6.205774775382918\n",
      "---------------------\n",
      "Iteration Number: 2278\n",
      "Loss: 53.67282478060103\n",
      "l2 norm of gradients: 0.5022455730701229\n",
      "l2 norm of weights: 6.205641034665298\n",
      "---------------------\n",
      "Iteration Number: 2279\n",
      "Loss: 53.66035646545324\n",
      "l2 norm of gradients: 0.5020535590220897\n",
      "l2 norm of weights: 6.205507328626966\n",
      "---------------------\n",
      "Iteration Number: 2280\n",
      "Loss: 53.64789757312491\n",
      "l2 norm of gradients: 0.5018617005105624\n",
      "l2 norm of weights: 6.205373657241121\n",
      "---------------------\n",
      "Iteration Number: 2281\n",
      "Loss: 53.63544809246399\n",
      "l2 norm of gradients: 0.5016699972879531\n",
      "l2 norm of weights: 6.205240020480993\n",
      "---------------------\n",
      "Iteration Number: 2282\n",
      "Loss: 53.62300801233964\n",
      "l2 norm of gradients: 0.501478449107196\n",
      "l2 norm of weights: 6.205106418319843\n",
      "---------------------\n",
      "Iteration Number: 2283\n",
      "Loss: 53.61057732165535\n",
      "l2 norm of gradients: 0.501287055721748\n",
      "l2 norm of weights: 6.20497285073097\n",
      "---------------------\n",
      "Iteration Number: 2284\n",
      "Loss: 53.598156009245784\n",
      "l2 norm of gradients: 0.5010958168855879\n",
      "l2 norm of weights: 6.2048393176877\n",
      "---------------------\n",
      "Iteration Number: 2285\n",
      "Loss: 53.58574406410158\n",
      "l2 norm of gradients: 0.5009047323532169\n",
      "l2 norm of weights: 6.2047058191633955\n",
      "---------------------\n",
      "Iteration Number: 2286\n",
      "Loss: 53.57334147512854\n",
      "l2 norm of gradients: 0.5007138018796572\n",
      "l2 norm of weights: 6.204572355131451\n",
      "---------------------\n",
      "Iteration Number: 2287\n",
      "Loss: 53.56094823134029\n",
      "l2 norm of gradients: 0.500523025220453\n",
      "l2 norm of weights: 6.204438925565292\n",
      "---------------------\n",
      "Iteration Number: 2288\n",
      "Loss: 53.54856432168479\n",
      "l2 norm of gradients: 0.5003324021316695\n",
      "l2 norm of weights: 6.204305530438375\n",
      "---------------------\n",
      "Iteration Number: 2289\n",
      "Loss: 53.53618973519497\n",
      "l2 norm of gradients: 0.500141932369893\n",
      "l2 norm of weights: 6.2041721697241945\n",
      "---------------------\n",
      "Iteration Number: 2290\n",
      "Loss: 53.523824460866074\n",
      "l2 norm of gradients: 0.4999516156922305\n",
      "l2 norm of weights: 6.204038843396272\n",
      "---------------------\n",
      "Iteration Number: 2291\n",
      "Loss: 53.51146848776587\n",
      "l2 norm of gradients: 0.49976145185630955\n",
      "l2 norm of weights: 6.2039055514281625\n",
      "---------------------\n",
      "Iteration Number: 2292\n",
      "Loss: 53.499121804961284\n",
      "l2 norm of gradients: 0.4995714406202781\n",
      "l2 norm of weights: 6.203772293793454\n",
      "---------------------\n",
      "Iteration Number: 2293\n",
      "Loss: 53.486784401513944\n",
      "l2 norm of gradients: 0.49938158174280384\n",
      "l2 norm of weights: 6.2036390704657665\n",
      "---------------------\n",
      "Iteration Number: 2294\n",
      "Loss: 53.4744562665724\n",
      "l2 norm of gradients: 0.49919187498307455\n",
      "l2 norm of weights: 6.203505881418751\n",
      "---------------------\n",
      "Iteration Number: 2295\n",
      "Loss: 53.4621373892774\n",
      "l2 norm of gradients: 0.4990023201007973\n",
      "l2 norm of weights: 6.203372726626091\n",
      "---------------------\n",
      "Iteration Number: 2296\n",
      "Loss: 53.44982775871695\n",
      "l2 norm of gradients: 0.4988129168561986\n",
      "l2 norm of weights: 6.2032396060615\n",
      "---------------------\n",
      "Iteration Number: 2297\n",
      "Loss: 53.437527364071684\n",
      "l2 norm of gradients: 0.4986236650100238\n",
      "l2 norm of weights: 6.203106519698726\n",
      "---------------------\n",
      "Iteration Number: 2298\n",
      "Loss: 53.42523619457286\n",
      "l2 norm of gradients: 0.49843456432353717\n",
      "l2 norm of weights: 6.202973467511547\n",
      "---------------------\n",
      "Iteration Number: 2299\n",
      "Loss: 53.412954239393635\n",
      "l2 norm of gradients: 0.4982456145585211\n",
      "l2 norm of weights: 6.202840449473772\n",
      "---------------------\n",
      "Iteration Number: 2300\n",
      "Loss: 53.40068148773177\n",
      "l2 norm of gradients: 0.4980568154772765\n",
      "l2 norm of weights: 6.202707465559245\n",
      "---------------------\n",
      "Iteration Number: 2301\n",
      "Loss: 53.38841792892183\n",
      "l2 norm of gradients: 0.4978681668426219\n",
      "l2 norm of weights: 6.2025745157418335\n",
      "---------------------\n",
      "Iteration Number: 2302\n",
      "Loss: 53.376163552164186\n",
      "l2 norm of gradients: 0.49767966841789363\n",
      "l2 norm of weights: 6.202441599995446\n",
      "---------------------\n",
      "Iteration Number: 2303\n",
      "Loss: 53.36391834673044\n",
      "l2 norm of gradients: 0.4974913199669454\n",
      "l2 norm of weights: 6.202308718294015\n",
      "---------------------\n",
      "Iteration Number: 2304\n",
      "Loss: 53.351682301961596\n",
      "l2 norm of gradients: 0.4973031212541477\n",
      "l2 norm of weights: 6.202175870611508\n",
      "---------------------\n",
      "Iteration Number: 2305\n",
      "Loss: 53.33945540715707\n",
      "l2 norm of gradients: 0.49711507204438776\n",
      "l2 norm of weights: 6.202043056921922\n",
      "---------------------\n",
      "Iteration Number: 2306\n",
      "Loss: 53.32723765168713\n",
      "l2 norm of gradients: 0.4969271721030696\n",
      "l2 norm of weights: 6.201910277199285\n",
      "---------------------\n",
      "Iteration Number: 2307\n",
      "Loss: 53.31502902491355\n",
      "l2 norm of gradients: 0.49673942119611286\n",
      "l2 norm of weights: 6.2017775314176555\n",
      "---------------------\n",
      "Iteration Number: 2308\n",
      "Loss: 53.302829516179266\n",
      "l2 norm of gradients: 0.4965518190899536\n",
      "l2 norm of weights: 6.201644819551125\n",
      "---------------------\n",
      "Iteration Number: 2309\n",
      "Loss: 53.29063911491314\n",
      "l2 norm of gradients: 0.4963643655515429\n",
      "l2 norm of weights: 6.201512141573812\n",
      "---------------------\n",
      "Iteration Number: 2310\n",
      "Loss: 53.27845781056969\n",
      "l2 norm of gradients: 0.49617706034834724\n",
      "l2 norm of weights: 6.20137949745987\n",
      "---------------------\n",
      "Iteration Number: 2311\n",
      "Loss: 53.26628559252614\n",
      "l2 norm of gradients: 0.4959899032483479\n",
      "l2 norm of weights: 6.201246887183482\n",
      "---------------------\n",
      "Iteration Number: 2312\n",
      "Loss: 53.25412245027201\n",
      "l2 norm of gradients: 0.4958028940200408\n",
      "l2 norm of weights: 6.2011143107188555\n",
      "---------------------\n",
      "Iteration Number: 2313\n",
      "Loss: 53.24196837331882\n",
      "l2 norm of gradients: 0.49561603243243607\n",
      "l2 norm of weights: 6.200981768040238\n",
      "---------------------\n",
      "Iteration Number: 2314\n",
      "Loss: 53.22982335108708\n",
      "l2 norm of gradients: 0.49542931825505804\n",
      "l2 norm of weights: 6.200849259121902\n",
      "---------------------\n",
      "Iteration Number: 2315\n",
      "Loss: 53.21768737315853\n",
      "l2 norm of gradients: 0.49524275125794404\n",
      "l2 norm of weights: 6.200716783938151\n",
      "---------------------\n",
      "Iteration Number: 2316\n",
      "Loss: 53.2055604290285\n",
      "l2 norm of gradients: 0.4950563312116451\n",
      "l2 norm of weights: 6.200584342463317\n",
      "---------------------\n",
      "Iteration Number: 2317\n",
      "Loss: 53.19344250825666\n",
      "l2 norm of gradients: 0.494870057887225\n",
      "l2 norm of weights: 6.200451934671768\n",
      "---------------------\n",
      "Iteration Number: 2318\n",
      "Loss: 53.18133360047693\n",
      "l2 norm of gradients: 0.49468393105626\n",
      "l2 norm of weights: 6.200319560537894\n",
      "---------------------\n",
      "Iteration Number: 2319\n",
      "Loss: 53.16923369520799\n",
      "l2 norm of gradients: 0.49449795049083894\n",
      "l2 norm of weights: 6.2001872200361206\n",
      "---------------------\n",
      "Iteration Number: 2320\n",
      "Loss: 53.15714278205412\n",
      "l2 norm of gradients: 0.4943121159635623\n",
      "l2 norm of weights: 6.200054913140902\n",
      "---------------------\n",
      "Iteration Number: 2321\n",
      "Loss: 53.14506085070955\n",
      "l2 norm of gradients: 0.49412642724754197\n",
      "l2 norm of weights: 6.199922639826723\n",
      "---------------------\n",
      "Iteration Number: 2322\n",
      "Loss: 53.13298789076163\n",
      "l2 norm of gradients: 0.49394088411640114\n",
      "l2 norm of weights: 6.199790400068096\n",
      "---------------------\n",
      "Iteration Number: 2323\n",
      "Loss: 53.120923891911666\n",
      "l2 norm of gradients: 0.49375548634427385\n",
      "l2 norm of weights: 6.199658193839564\n",
      "---------------------\n",
      "Iteration Number: 2324\n",
      "Loss: 53.10886884386797\n",
      "l2 norm of gradients: 0.4935702337058047\n",
      "l2 norm of weights: 6.199526021115702\n",
      "---------------------\n",
      "Iteration Number: 2325\n",
      "Loss: 53.09682273627109\n",
      "l2 norm of gradients: 0.4933851259761481\n",
      "l2 norm of weights: 6.19939388187111\n",
      "---------------------\n",
      "Iteration Number: 2326\n",
      "Loss: 53.08478555885623\n",
      "l2 norm of gradients: 0.4932001629309684\n",
      "l2 norm of weights: 6.199261776080423\n",
      "---------------------\n",
      "Iteration Number: 2327\n",
      "Loss: 53.072757301409176\n",
      "l2 norm of gradients: 0.4930153443464393\n",
      "l2 norm of weights: 6.1991297037183\n",
      "---------------------\n",
      "Iteration Number: 2328\n",
      "Loss: 53.06073795365182\n",
      "l2 norm of gradients: 0.4928306699992433\n",
      "l2 norm of weights: 6.198997664759433\n",
      "---------------------\n",
      "Iteration Number: 2329\n",
      "Loss: 53.048727505337204\n",
      "l2 norm of gradients: 0.4926461396665717\n",
      "l2 norm of weights: 6.1988656591785425\n",
      "---------------------\n",
      "Iteration Number: 2330\n",
      "Loss: 53.03672594634226\n",
      "l2 norm of gradients: 0.4924617531261241\n",
      "l2 norm of weights: 6.198733686950377\n",
      "---------------------\n",
      "Iteration Number: 2331\n",
      "Loss: 53.02473326641837\n",
      "l2 norm of gradients: 0.4922775101561076\n",
      "l2 norm of weights: 6.1986017480497155\n",
      "---------------------\n",
      "Iteration Number: 2332\n",
      "Loss: 53.01274945539008\n",
      "l2 norm of gradients: 0.4920934105352372\n",
      "l2 norm of weights: 6.198469842451366\n",
      "---------------------\n",
      "Iteration Number: 2333\n",
      "Loss: 53.000774503137805\n",
      "l2 norm of gradients: 0.4919094540427345\n",
      "l2 norm of weights: 6.198337970130163\n",
      "---------------------\n",
      "Iteration Number: 2334\n",
      "Loss: 52.98880839953526\n",
      "l2 norm of gradients: 0.4917256404583282\n",
      "l2 norm of weights: 6.198206131060975\n",
      "---------------------\n",
      "Iteration Number: 2335\n",
      "Loss: 52.976851134436345\n",
      "l2 norm of gradients: 0.4915419695622532\n",
      "l2 norm of weights: 6.198074325218695\n",
      "---------------------\n",
      "Iteration Number: 2336\n",
      "Loss: 52.96490269777122\n",
      "l2 norm of gradients: 0.4913584411352499\n",
      "l2 norm of weights: 6.197942552578246\n",
      "---------------------\n",
      "Iteration Number: 2337\n",
      "Loss: 52.952963079458215\n",
      "l2 norm of gradients: 0.4911750549585646\n",
      "l2 norm of weights: 6.197810813114579\n",
      "---------------------\n",
      "Iteration Number: 2338\n",
      "Loss: 52.94103226939802\n",
      "l2 norm of gradients: 0.4909918108139487\n",
      "l2 norm of weights: 6.197679106802676\n",
      "---------------------\n",
      "Iteration Number: 2339\n",
      "Loss: 52.92911025761202\n",
      "l2 norm of gradients: 0.49080870848365804\n",
      "l2 norm of weights: 6.1975474336175465\n",
      "---------------------\n",
      "Iteration Number: 2340\n",
      "Loss: 52.91719703405086\n",
      "l2 norm of gradients: 0.49062574775045265\n",
      "l2 norm of weights: 6.197415793534226\n",
      "---------------------\n",
      "Iteration Number: 2341\n",
      "Loss: 52.9052925886928\n",
      "l2 norm of gradients: 0.49044292839759673\n",
      "l2 norm of weights: 6.197284186527782\n",
      "---------------------\n",
      "Iteration Number: 2342\n",
      "Loss: 52.893396911550056\n",
      "l2 norm of gradients: 0.4902602502088578\n",
      "l2 norm of weights: 6.197152612573309\n",
      "---------------------\n",
      "Iteration Number: 2343\n",
      "Loss: 52.881509992678716\n",
      "l2 norm of gradients: 0.4900777129685063\n",
      "l2 norm of weights: 6.19702107164593\n",
      "---------------------\n",
      "Iteration Number: 2344\n",
      "Loss: 52.86963182208842\n",
      "l2 norm of gradients: 0.48989531646131534\n",
      "l2 norm of weights: 6.196889563720795\n",
      "---------------------\n",
      "Iteration Number: 2345\n",
      "Loss: 52.85776238988292\n",
      "l2 norm of gradients: 0.48971306047255997\n",
      "l2 norm of weights: 6.196758088773085\n",
      "---------------------\n",
      "Iteration Number: 2346\n",
      "Loss: 52.84590168611586\n",
      "l2 norm of gradients: 0.48953094478801745\n",
      "l2 norm of weights: 6.196626646778004\n",
      "---------------------\n",
      "Iteration Number: 2347\n",
      "Loss: 52.83404970090716\n",
      "l2 norm of gradients: 0.4893489691939661\n",
      "l2 norm of weights: 6.19649523771079\n",
      "---------------------\n",
      "Iteration Number: 2348\n",
      "Loss: 52.822206424374556\n",
      "l2 norm of gradients: 0.48916713347718493\n",
      "l2 norm of weights: 6.196363861546707\n",
      "---------------------\n",
      "Iteration Number: 2349\n",
      "Loss: 52.81037184660361\n",
      "l2 norm of gradients: 0.48898543742495376\n",
      "l2 norm of weights: 6.196232518261044\n",
      "---------------------\n",
      "Iteration Number: 2350\n",
      "Loss: 52.79854595778463\n",
      "l2 norm of gradients: 0.488803880825052\n",
      "l2 norm of weights: 6.19610120782912\n",
      "---------------------\n",
      "Iteration Number: 2351\n",
      "Loss: 52.78672874809456\n",
      "l2 norm of gradients: 0.4886224634657591\n",
      "l2 norm of weights: 6.195969930226283\n",
      "---------------------\n",
      "Iteration Number: 2352\n",
      "Loss: 52.77492020769957\n",
      "l2 norm of gradients: 0.48844118513585316\n",
      "l2 norm of weights: 6.1958386854279075\n",
      "---------------------\n",
      "Iteration Number: 2353\n",
      "Loss: 52.763120326846305\n",
      "l2 norm of gradients: 0.4882600456246112\n",
      "l2 norm of weights: 6.195707473409395\n",
      "---------------------\n",
      "Iteration Number: 2354\n",
      "Loss: 52.75132909567314\n",
      "l2 norm of gradients: 0.4880790447218086\n",
      "l2 norm of weights: 6.195576294146176\n",
      "---------------------\n",
      "Iteration Number: 2355\n",
      "Loss: 52.739546504482846\n",
      "l2 norm of gradients: 0.48789818221771825\n",
      "l2 norm of weights: 6.195445147613707\n",
      "---------------------\n",
      "Iteration Number: 2356\n",
      "Loss: 52.727772543541846\n",
      "l2 norm of gradients: 0.48771745790311016\n",
      "l2 norm of weights: 6.195314033787475\n",
      "---------------------\n",
      "Iteration Number: 2357\n",
      "Loss: 52.71600720305325\n",
      "l2 norm of gradients: 0.48753687156925185\n",
      "l2 norm of weights: 6.195182952642988\n",
      "---------------------\n",
      "Iteration Number: 2358\n",
      "Loss: 52.704250473399355\n",
      "l2 norm of gradients: 0.4873564230079066\n",
      "l2 norm of weights: 6.195051904155789\n",
      "---------------------\n",
      "Iteration Number: 2359\n",
      "Loss: 52.69250234478334\n",
      "l2 norm of gradients: 0.48717611201133404\n",
      "l2 norm of weights: 6.194920888301442\n",
      "---------------------\n",
      "Iteration Number: 2360\n",
      "Loss: 52.680762807583406\n",
      "l2 norm of gradients: 0.48699593837228894\n",
      "l2 norm of weights: 6.194789905055542\n",
      "---------------------\n",
      "Iteration Number: 2361\n",
      "Loss: 52.66903185214815\n",
      "l2 norm of gradients: 0.48681590188402135\n",
      "l2 norm of weights: 6.19465895439371\n",
      "---------------------\n",
      "Iteration Number: 2362\n",
      "Loss: 52.65730946883128\n",
      "l2 norm of gradients: 0.4866360023402755\n",
      "l2 norm of weights: 6.1945280362915955\n",
      "---------------------\n",
      "Iteration Number: 2363\n",
      "Loss: 52.6455956479964\n",
      "l2 norm of gradients: 0.4864562395352901\n",
      "l2 norm of weights: 6.194397150724871\n",
      "---------------------\n",
      "Iteration Number: 2364\n",
      "Loss: 52.63389038003579\n",
      "l2 norm of gradients: 0.4862766132637969\n",
      "l2 norm of weights: 6.19426629766924\n",
      "---------------------\n",
      "Iteration Number: 2365\n",
      "Loss: 52.622193655345846\n",
      "l2 norm of gradients: 0.48609712332102123\n",
      "l2 norm of weights: 6.19413547710043\n",
      "---------------------\n",
      "Iteration Number: 2366\n",
      "Loss: 52.61050546435241\n",
      "l2 norm of gradients: 0.4859177695026809\n",
      "l2 norm of weights: 6.194004688994199\n",
      "---------------------\n",
      "Iteration Number: 2367\n",
      "Loss: 52.598825797503196\n",
      "l2 norm of gradients: 0.4857385516049855\n",
      "l2 norm of weights: 6.193873933326327\n",
      "---------------------\n",
      "Iteration Number: 2368\n",
      "Loss: 52.58715464528409\n",
      "l2 norm of gradients: 0.4855594694246369\n",
      "l2 norm of weights: 6.193743210072625\n",
      "---------------------\n",
      "Iteration Number: 2369\n",
      "Loss: 52.57549199812969\n",
      "l2 norm of gradients: 0.4853805227588273\n",
      "l2 norm of weights: 6.193612519208926\n",
      "---------------------\n",
      "Iteration Number: 2370\n",
      "Loss: 52.56383784653118\n",
      "l2 norm of gradients: 0.48520171140524027\n",
      "l2 norm of weights: 6.193481860711095\n",
      "---------------------\n",
      "Iteration Number: 2371\n",
      "Loss: 52.5521921810149\n",
      "l2 norm of gradients: 0.4850230351620491\n",
      "l2 norm of weights: 6.193351234555019\n",
      "---------------------\n",
      "Iteration Number: 2372\n",
      "Loss: 52.54055499207594\n",
      "l2 norm of gradients: 0.48484449382791694\n",
      "l2 norm of weights: 6.193220640716614\n",
      "---------------------\n",
      "Iteration Number: 2373\n",
      "Loss: 52.52892627028676\n",
      "l2 norm of gradients: 0.48466608720199605\n",
      "l2 norm of weights: 6.193090079171822\n",
      "---------------------\n",
      "Iteration Number: 2374\n",
      "Loss: 52.517306006196485\n",
      "l2 norm of gradients: 0.48448781508392735\n",
      "l2 norm of weights: 6.192959549896609\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 2375\n",
      "Loss: 52.50569419037724\n",
      "l2 norm of gradients: 0.4843096772738397\n",
      "l2 norm of weights: 6.192829052866971\n",
      "---------------------\n",
      "Iteration Number: 2376\n",
      "Loss: 52.494090813370605\n",
      "l2 norm of gradients: 0.4841316735723501\n",
      "l2 norm of weights: 6.192698588058927\n",
      "---------------------\n",
      "Iteration Number: 2377\n",
      "Loss: 52.48249586585039\n",
      "l2 norm of gradients: 0.483953803780562\n",
      "l2 norm of weights: 6.192568155448525\n",
      "---------------------\n",
      "Iteration Number: 2378\n",
      "Loss: 52.47090933841179\n",
      "l2 norm of gradients: 0.4837760677000661\n",
      "l2 norm of weights: 6.192437755011836\n",
      "---------------------\n",
      "Iteration Number: 2379\n",
      "Loss: 52.45933122169106\n",
      "l2 norm of gradients: 0.48359846513293864\n",
      "l2 norm of weights: 6.19230738672496\n",
      "---------------------\n",
      "Iteration Number: 2380\n",
      "Loss: 52.44776150633282\n",
      "l2 norm of gradients: 0.48342099588174203\n",
      "l2 norm of weights: 6.192177050564019\n",
      "---------------------\n",
      "Iteration Number: 2381\n",
      "Loss: 52.436200183008914\n",
      "l2 norm of gradients: 0.4832436597495232\n",
      "l2 norm of weights: 6.192046746505165\n",
      "---------------------\n",
      "Iteration Number: 2382\n",
      "Loss: 52.42464724241754\n",
      "l2 norm of gradients: 0.4830664565398139\n",
      "l2 norm of weights: 6.191916474524575\n",
      "---------------------\n",
      "Iteration Number: 2383\n",
      "Loss: 52.413102675241284\n",
      "l2 norm of gradients: 0.4828893860566296\n",
      "l2 norm of weights: 6.19178623459845\n",
      "---------------------\n",
      "Iteration Number: 2384\n",
      "Loss: 52.401566472197075\n",
      "l2 norm of gradients: 0.4827124481044699\n",
      "l2 norm of weights: 6.191656026703017\n",
      "---------------------\n",
      "Iteration Number: 2385\n",
      "Loss: 52.390038624066584\n",
      "l2 norm of gradients: 0.48253564248831654\n",
      "l2 norm of weights: 6.191525850814531\n",
      "---------------------\n",
      "Iteration Number: 2386\n",
      "Loss: 52.37851912150799\n",
      "l2 norm of gradients: 0.4823589690136341\n",
      "l2 norm of weights: 6.191395706909269\n",
      "---------------------\n",
      "Iteration Number: 2387\n",
      "Loss: 52.36700795536928\n",
      "l2 norm of gradients: 0.4821824274863691\n",
      "l2 norm of weights: 6.191265594963537\n",
      "---------------------\n",
      "Iteration Number: 2388\n",
      "Loss: 52.35550511640557\n",
      "l2 norm of gradients: 0.4820060177129492\n",
      "l2 norm of weights: 6.191135514953663\n",
      "---------------------\n",
      "Iteration Number: 2389\n",
      "Loss: 52.34401059537875\n",
      "l2 norm of gradients: 0.4818297395002829\n",
      "l2 norm of weights: 6.191005466856004\n",
      "---------------------\n",
      "Iteration Number: 2390\n",
      "Loss: 52.33252438319084\n",
      "l2 norm of gradients: 0.4816535926557589\n",
      "l2 norm of weights: 6.190875450646941\n",
      "---------------------\n",
      "Iteration Number: 2391\n",
      "Loss: 52.321046470542754\n",
      "l2 norm of gradients: 0.48147757698724614\n",
      "l2 norm of weights: 6.190745466302879\n",
      "---------------------\n",
      "Iteration Number: 2392\n",
      "Loss: 52.30957684838951\n",
      "l2 norm of gradients: 0.4813016923030918\n",
      "l2 norm of weights: 6.19061551380025\n",
      "---------------------\n",
      "Iteration Number: 2393\n",
      "Loss: 52.298115507498814\n",
      "l2 norm of gradients: 0.4811259384121223\n",
      "l2 norm of weights: 6.190485593115509\n",
      "---------------------\n",
      "Iteration Number: 2394\n",
      "Loss: 52.28666243881867\n",
      "l2 norm of gradients: 0.48095031512364206\n",
      "l2 norm of weights: 6.190355704225139\n",
      "---------------------\n",
      "Iteration Number: 2395\n",
      "Loss: 52.27521763323663\n",
      "l2 norm of gradients: 0.4807748222474328\n",
      "l2 norm of weights: 6.190225847105647\n",
      "---------------------\n",
      "Iteration Number: 2396\n",
      "Loss: 52.26378108161044\n",
      "l2 norm of gradients: 0.4805994595937533\n",
      "l2 norm of weights: 6.190096021733564\n",
      "---------------------\n",
      "Iteration Number: 2397\n",
      "Loss: 52.2523527748748\n",
      "l2 norm of gradients: 0.48042422697333875\n",
      "l2 norm of weights: 6.189966228085447\n",
      "---------------------\n",
      "Iteration Number: 2398\n",
      "Loss: 52.24093270397805\n",
      "l2 norm of gradients: 0.48024912419739996\n",
      "l2 norm of weights: 6.189836466137876\n",
      "---------------------\n",
      "Iteration Number: 2399\n",
      "Loss: 52.22952085987454\n",
      "l2 norm of gradients: 0.48007415107762347\n",
      "l2 norm of weights: 6.189706735867459\n",
      "---------------------\n",
      "Iteration Number: 2400\n",
      "Loss: 52.21811723353419\n",
      "l2 norm of gradients: 0.47989930742616993\n",
      "l2 norm of weights: 6.189577037250827\n",
      "---------------------\n",
      "Iteration Number: 2401\n",
      "Loss: 52.20672181593553\n",
      "l2 norm of gradients: 0.47972459305567455\n",
      "l2 norm of weights: 6.189447370264635\n",
      "---------------------\n",
      "Iteration Number: 2402\n",
      "Loss: 52.19533459803602\n",
      "l2 norm of gradients: 0.4795500077792459\n",
      "l2 norm of weights: 6.189317734885565\n",
      "---------------------\n",
      "Iteration Number: 2403\n",
      "Loss: 52.18395557090588\n",
      "l2 norm of gradients: 0.47937555141046534\n",
      "l2 norm of weights: 6.189188131090321\n",
      "---------------------\n",
      "Iteration Number: 2404\n",
      "Loss: 52.172584725542315\n",
      "l2 norm of gradients: 0.47920122376338714\n",
      "l2 norm of weights: 6.189058558855635\n",
      "---------------------\n",
      "Iteration Number: 2405\n",
      "Loss: 52.16122205302706\n",
      "l2 norm of gradients: 0.479027024652537\n",
      "l2 norm of weights: 6.188929018158258\n",
      "---------------------\n",
      "Iteration Number: 2406\n",
      "Loss: 52.14986754436672\n",
      "l2 norm of gradients: 0.4788529538929116\n",
      "l2 norm of weights: 6.188799508974971\n",
      "---------------------\n",
      "Iteration Number: 2407\n",
      "Loss: 52.138521190641626\n",
      "l2 norm of gradients: 0.47867901129997875\n",
      "l2 norm of weights: 6.188670031282577\n",
      "---------------------\n",
      "Iteration Number: 2408\n",
      "Loss: 52.12718298296079\n",
      "l2 norm of gradients: 0.47850519668967645\n",
      "l2 norm of weights: 6.188540585057903\n",
      "---------------------\n",
      "Iteration Number: 2409\n",
      "Loss: 52.11585291242756\n",
      "l2 norm of gradients: 0.47833150987841155\n",
      "l2 norm of weights: 6.188411170277801\n",
      "---------------------\n",
      "Iteration Number: 2410\n",
      "Loss: 52.104530970131776\n",
      "l2 norm of gradients: 0.4781579506830604\n",
      "l2 norm of weights: 6.1882817869191475\n",
      "---------------------\n",
      "Iteration Number: 2411\n",
      "Loss: 52.093217147259686\n",
      "l2 norm of gradients: 0.4779845189209673\n",
      "l2 norm of weights: 6.188152434958844\n",
      "---------------------\n",
      "Iteration Number: 2412\n",
      "Loss: 52.08191143489538\n",
      "l2 norm of gradients: 0.47781121440994423\n",
      "l2 norm of weights: 6.188023114373812\n",
      "---------------------\n",
      "Iteration Number: 2413\n",
      "Loss: 52.07061382425696\n",
      "l2 norm of gradients: 0.47763803696827034\n",
      "l2 norm of weights: 6.187893825141002\n",
      "---------------------\n",
      "Iteration Number: 2414\n",
      "Loss: 52.05932430645329\n",
      "l2 norm of gradients: 0.4774649864146917\n",
      "l2 norm of weights: 6.1877645672373855\n",
      "---------------------\n",
      "Iteration Number: 2415\n",
      "Loss: 52.04804287275173\n",
      "l2 norm of gradients: 0.4772920625684193\n",
      "l2 norm of weights: 6.187635340639961\n",
      "---------------------\n",
      "Iteration Number: 2416\n",
      "Loss: 52.0367695143405\n",
      "l2 norm of gradients: 0.47711926524913034\n",
      "l2 norm of weights: 6.187506145325747\n",
      "---------------------\n",
      "Iteration Number: 2417\n",
      "Loss: 52.025504222384086\n",
      "l2 norm of gradients: 0.4769465942769665\n",
      "l2 norm of weights: 6.18737698127179\n",
      "---------------------\n",
      "Iteration Number: 2418\n",
      "Loss: 52.014246988198934\n",
      "l2 norm of gradients: 0.4767740494725332\n",
      "l2 norm of weights: 6.187247848455155\n",
      "---------------------\n",
      "Iteration Number: 2419\n",
      "Loss: 52.00299780297711\n",
      "l2 norm of gradients: 0.4766016306568997\n",
      "l2 norm of weights: 6.187118746852937\n",
      "---------------------\n",
      "Iteration Number: 2420\n",
      "Loss: 51.99175665798474\n",
      "l2 norm of gradients: 0.4764293376515978\n",
      "l2 norm of weights: 6.18698967644225\n",
      "---------------------\n",
      "Iteration Number: 2421\n",
      "Loss: 51.9805235445385\n",
      "l2 norm of gradients: 0.47625717027862197\n",
      "l2 norm of weights: 6.186860637200234\n",
      "---------------------\n",
      "Iteration Number: 2422\n",
      "Loss: 51.969298453916196\n",
      "l2 norm of gradients: 0.4760851283604279\n",
      "l2 norm of weights: 6.186731629104052\n",
      "---------------------\n",
      "Iteration Number: 2423\n",
      "Loss: 51.95808137743632\n",
      "l2 norm of gradients: 0.4759132117199323\n",
      "l2 norm of weights: 6.18660265213089\n",
      "---------------------\n",
      "Iteration Number: 2424\n",
      "Loss: 51.94687230637107\n",
      "l2 norm of gradients: 0.4757414201805125\n",
      "l2 norm of weights: 6.186473706257959\n",
      "---------------------\n",
      "Iteration Number: 2425\n",
      "Loss: 51.93567123212466\n",
      "l2 norm of gradients: 0.47556975356600556\n",
      "l2 norm of weights: 6.186344791462492\n",
      "---------------------\n",
      "Iteration Number: 2426\n",
      "Loss: 51.92447814599124\n",
      "l2 norm of gradients: 0.4753982117007074\n",
      "l2 norm of weights: 6.186215907721746\n",
      "---------------------\n",
      "Iteration Number: 2427\n",
      "Loss: 51.91329303941885\n",
      "l2 norm of gradients: 0.47522679440937277\n",
      "l2 norm of weights: 6.186087055013\n",
      "---------------------\n",
      "Iteration Number: 2428\n",
      "Loss: 51.90211590369026\n",
      "l2 norm of gradients: 0.475055501517214\n",
      "l2 norm of weights: 6.18595823331356\n",
      "---------------------\n",
      "Iteration Number: 2429\n",
      "Loss: 51.89094673022724\n",
      "l2 norm of gradients: 0.47488433284990095\n",
      "l2 norm of weights: 6.185829442600751\n",
      "---------------------\n",
      "Iteration Number: 2430\n",
      "Loss: 51.879785510467926\n",
      "l2 norm of gradients: 0.4747132882335598\n",
      "l2 norm of weights: 6.185700682851924\n",
      "---------------------\n",
      "Iteration Number: 2431\n",
      "Loss: 51.86863223583829\n",
      "l2 norm of gradients: 0.47454236749477285\n",
      "l2 norm of weights: 6.185571954044453\n",
      "---------------------\n",
      "Iteration Number: 2432\n",
      "Loss: 51.857486897732244\n",
      "l2 norm of gradients: 0.4743715704605778\n",
      "l2 norm of weights: 6.1854432561557315\n",
      "---------------------\n",
      "Iteration Number: 2433\n",
      "Loss: 51.846349487635095\n",
      "l2 norm of gradients: 0.47420089695846673\n",
      "l2 norm of weights: 6.185314589163182\n",
      "---------------------\n",
      "Iteration Number: 2434\n",
      "Loss: 51.83521999699759\n",
      "l2 norm of gradients: 0.4740303468163862\n",
      "l2 norm of weights: 6.185185953044246\n",
      "---------------------\n",
      "Iteration Number: 2435\n",
      "Loss: 51.824098417300135\n",
      "l2 norm of gradients: 0.4738599198627355\n",
      "l2 norm of weights: 6.185057347776388\n",
      "---------------------\n",
      "Iteration Number: 2436\n",
      "Loss: 51.812984740010265\n",
      "l2 norm of gradients: 0.4736896159263676\n",
      "l2 norm of weights: 6.184928773337099\n",
      "---------------------\n",
      "Iteration Number: 2437\n",
      "Loss: 51.80187895671549\n",
      "l2 norm of gradients: 0.4735194348365868\n",
      "l2 norm of weights: 6.184800229703885\n",
      "---------------------\n",
      "Iteration Number: 2438\n",
      "Loss: 51.79078105884131\n",
      "l2 norm of gradients: 0.4733493764231489\n",
      "l2 norm of weights: 6.184671716854286\n",
      "---------------------\n",
      "Iteration Number: 2439\n",
      "Loss: 51.77969103794981\n",
      "l2 norm of gradients: 0.47317944051626104\n",
      "l2 norm of weights: 6.184543234765854\n",
      "---------------------\n",
      "Iteration Number: 2440\n",
      "Loss: 51.768608885614654\n",
      "l2 norm of gradients: 0.4730096269465799\n",
      "l2 norm of weights: 6.184414783416172\n",
      "---------------------\n",
      "Iteration Number: 2441\n",
      "Loss: 51.757534593373286\n",
      "l2 norm of gradients: 0.4728399355452119\n",
      "l2 norm of weights: 6.18428636278284\n",
      "---------------------\n",
      "Iteration Number: 2442\n",
      "Loss: 51.74646815285054\n",
      "l2 norm of gradients: 0.47267036614371205\n",
      "l2 norm of weights: 6.184157972843485\n",
      "---------------------\n",
      "Iteration Number: 2443\n",
      "Loss: 51.73540955554684\n",
      "l2 norm of gradients: 0.47250091857408394\n",
      "l2 norm of weights: 6.184029613575752\n",
      "---------------------\n",
      "Iteration Number: 2444\n",
      "Loss: 51.72435879314167\n",
      "l2 norm of gradients: 0.47233159266877817\n",
      "l2 norm of weights: 6.183901284957312\n",
      "---------------------\n",
      "Iteration Number: 2445\n",
      "Loss: 51.71331585719838\n",
      "l2 norm of gradients: 0.47216238826069246\n",
      "l2 norm of weights: 6.183772986965857\n",
      "---------------------\n",
      "Iteration Number: 2446\n",
      "Loss: 51.702280739379525\n",
      "l2 norm of gradients: 0.4719933051831705\n",
      "l2 norm of weights: 6.183644719579102\n",
      "---------------------\n",
      "Iteration Number: 2447\n",
      "Loss: 51.69125343134091\n",
      "l2 norm of gradients: 0.4718243432700017\n",
      "l2 norm of weights: 6.183516482774785\n",
      "---------------------\n",
      "Iteration Number: 2448\n",
      "Loss: 51.68023392469168\n",
      "l2 norm of gradients: 0.47165550235542003\n",
      "l2 norm of weights: 6.183388276530665\n",
      "---------------------\n",
      "Iteration Number: 2449\n",
      "Loss: 51.669222211142674\n",
      "l2 norm of gradients: 0.4714867822741036\n",
      "l2 norm of weights: 6.183260100824524\n",
      "---------------------\n",
      "Iteration Number: 2450\n",
      "Loss: 51.65821828235652\n",
      "l2 norm of gradients: 0.47131818286117394\n",
      "l2 norm of weights: 6.183131955634165\n",
      "---------------------\n",
      "Iteration Number: 2451\n",
      "Loss: 51.647222130028176\n",
      "l2 norm of gradients: 0.47114970395219574\n",
      "l2 norm of weights: 6.183003840937417\n",
      "---------------------\n",
      "Iteration Number: 2452\n",
      "Loss: 51.636233745897215\n",
      "l2 norm of gradients: 0.4709813453831751\n",
      "l2 norm of weights: 6.182875756712126\n",
      "---------------------\n",
      "Iteration Number: 2453\n",
      "Loss: 51.625253121676195\n",
      "l2 norm of gradients: 0.47081310699056006\n",
      "l2 norm of weights: 6.182747702936164\n",
      "---------------------\n",
      "Iteration Number: 2454\n",
      "Loss: 51.614280249069964\n",
      "l2 norm of gradients: 0.47064498861123916\n",
      "l2 norm of weights: 6.1826196795874235\n",
      "---------------------\n",
      "Iteration Number: 2455\n",
      "Loss: 51.60331511986598\n",
      "l2 norm of gradients: 0.4704769900825412\n",
      "l2 norm of weights: 6.182491686643819\n",
      "---------------------\n",
      "Iteration Number: 2456\n",
      "Loss: 51.59235772583517\n",
      "l2 norm of gradients: 0.47030911124223396\n",
      "l2 norm of weights: 6.182363724083288\n",
      "---------------------\n",
      "Iteration Number: 2457\n",
      "Loss: 51.5814080587023\n",
      "l2 norm of gradients: 0.4701413519285239\n",
      "l2 norm of weights: 6.182235791883787\n",
      "---------------------\n",
      "Iteration Number: 2458\n",
      "Loss: 51.570466110312296\n",
      "l2 norm of gradients: 0.4699737119800559\n",
      "l2 norm of weights: 6.182107890023299\n",
      "---------------------\n",
      "Iteration Number: 2459\n",
      "Loss: 51.55953187243175\n",
      "l2 norm of gradients: 0.4698061912359115\n",
      "l2 norm of weights: 6.181980018479825\n",
      "---------------------\n",
      "Iteration Number: 2460\n",
      "Loss: 51.548605336918634\n",
      "l2 norm of gradients: 0.46963878953560917\n",
      "l2 norm of weights: 6.181852177231389\n",
      "---------------------\n",
      "Iteration Number: 2461\n",
      "Loss: 51.53768649555092\n",
      "l2 norm of gradients: 0.4694715067191031\n",
      "l2 norm of weights: 6.181724366256038\n",
      "---------------------\n",
      "Iteration Number: 2462\n",
      "Loss: 51.526775340216325\n",
      "l2 norm of gradients: 0.4693043426267828\n",
      "l2 norm of weights: 6.181596585531839\n",
      "---------------------\n",
      "Iteration Number: 2463\n",
      "Loss: 51.515871862722335\n",
      "l2 norm of gradients: 0.46913729709947205\n",
      "l2 norm of weights: 6.181468835036882\n",
      "---------------------\n",
      "Iteration Number: 2464\n",
      "Loss: 51.50497605500349\n",
      "l2 norm of gradients: 0.46897036997842845\n",
      "l2 norm of weights: 6.181341114749277\n",
      "---------------------\n",
      "Iteration Number: 2465\n",
      "Loss: 51.49408790887633\n",
      "l2 norm of gradients: 0.4688035611053425\n",
      "l2 norm of weights: 6.181213424647157\n",
      "---------------------\n",
      "Iteration Number: 2466\n",
      "Loss: 51.48320741626089\n",
      "l2 norm of gradients: 0.4686368703223374\n",
      "l2 norm of weights: 6.181085764708675\n",
      "---------------------\n",
      "Iteration Number: 2467\n",
      "Loss: 51.472334569070796\n",
      "l2 norm of gradients: 0.4684702974719677\n",
      "l2 norm of weights: 6.180958134912009\n",
      "---------------------\n",
      "Iteration Number: 2468\n",
      "Loss: 51.46146935923827\n",
      "l2 norm of gradients: 0.46830384239721873\n",
      "l2 norm of weights: 6.180830535235354\n",
      "---------------------\n",
      "Iteration Number: 2469\n",
      "Loss: 51.45061177866777\n",
      "l2 norm of gradients: 0.46813750494150624\n",
      "l2 norm of weights: 6.180702965656929\n",
      "---------------------\n",
      "Iteration Number: 2470\n",
      "Loss: 51.43976181931285\n",
      "l2 norm of gradients: 0.4679712849486756\n",
      "l2 norm of weights: 6.180575426154973\n",
      "---------------------\n",
      "Iteration Number: 2471\n",
      "Loss: 51.42891947314688\n",
      "l2 norm of gradients: 0.46780518226300055\n",
      "l2 norm of weights: 6.180447916707748\n",
      "---------------------\n",
      "Iteration Number: 2472\n",
      "Loss: 51.4180847321194\n",
      "l2 norm of gradients: 0.467639196729183\n",
      "l2 norm of weights: 6.1803204372935365\n",
      "---------------------\n",
      "Iteration Number: 2473\n",
      "Loss: 51.40725758822667\n",
      "l2 norm of gradients: 0.4674733281923525\n",
      "l2 norm of weights: 6.180192987890642\n",
      "---------------------\n",
      "Iteration Number: 2474\n",
      "Loss: 51.39643803343884\n",
      "l2 norm of gradients: 0.4673075764980646\n",
      "l2 norm of weights: 6.1800655684773895\n",
      "---------------------\n",
      "Iteration Number: 2475\n",
      "Loss: 51.3856260598305\n",
      "l2 norm of gradients: 0.4671419414923012\n",
      "l2 norm of weights: 6.179938179032124\n",
      "---------------------\n",
      "Iteration Number: 2476\n",
      "Loss: 51.374821659350765\n",
      "l2 norm of gradients: 0.4669764230214691\n",
      "l2 norm of weights: 6.1798108195332135\n",
      "---------------------\n",
      "Iteration Number: 2477\n",
      "Loss: 51.36402482409651\n",
      "l2 norm of gradients: 0.46681102093239957\n",
      "l2 norm of weights: 6.179683489959046\n",
      "---------------------\n",
      "Iteration Number: 2478\n",
      "Loss: 51.35323554604677\n",
      "l2 norm of gradients: 0.4666457350723476\n",
      "l2 norm of weights: 6.1795561902880305\n",
      "---------------------\n",
      "Iteration Number: 2479\n",
      "Loss: 51.34245381731446\n",
      "l2 norm of gradients: 0.4664805652889909\n",
      "l2 norm of weights: 6.179428920498596\n",
      "---------------------\n",
      "Iteration Number: 2480\n",
      "Loss: 51.331679629936964\n",
      "l2 norm of gradients: 0.4663155114304297\n",
      "l2 norm of weights: 6.179301680569196\n",
      "---------------------\n",
      "Iteration Number: 2481\n",
      "Loss: 51.32091297602828\n",
      "l2 norm of gradients: 0.46615057334518545\n",
      "l2 norm of weights: 6.179174470478301\n",
      "---------------------\n",
      "Iteration Number: 2482\n",
      "Loss: 51.31015384767483\n",
      "l2 norm of gradients: 0.46598575088220046\n",
      "l2 norm of weights: 6.179047290204406\n",
      "---------------------\n",
      "Iteration Number: 2483\n",
      "Loss: 51.299402236951586\n",
      "l2 norm of gradients: 0.46582104389083706\n",
      "l2 norm of weights: 6.178920139726021\n",
      "---------------------\n",
      "Iteration Number: 2484\n",
      "Loss: 51.288658136023365\n",
      "l2 norm of gradients: 0.46565645222087654\n",
      "l2 norm of weights: 6.178793019021683\n",
      "---------------------\n",
      "Iteration Number: 2485\n",
      "Loss: 51.27792153703472\n",
      "l2 norm of gradients: 0.4654919757225188\n",
      "l2 norm of weights: 6.178665928069947\n",
      "---------------------\n",
      "Iteration Number: 2486\n",
      "Loss: 51.267192432077906\n",
      "l2 norm of gradients: 0.4653276142463818\n",
      "l2 norm of weights: 6.178538866849388\n",
      "---------------------\n",
      "Iteration Number: 2487\n",
      "Loss: 51.25647081332487\n",
      "l2 norm of gradients: 0.46516336764350025\n",
      "l2 norm of weights: 6.178411835338604\n",
      "---------------------\n",
      "Iteration Number: 2488\n",
      "Loss: 51.24575667295172\n",
      "l2 norm of gradients: 0.46499923576532504\n",
      "l2 norm of weights: 6.178284833516211\n",
      "---------------------\n",
      "Iteration Number: 2489\n",
      "Loss: 51.23505000317269\n",
      "l2 norm of gradients: 0.4648352184637226\n",
      "l2 norm of weights: 6.178157861360848\n",
      "---------------------\n",
      "Iteration Number: 2490\n",
      "Loss: 51.224350796170825\n",
      "l2 norm of gradients: 0.46467131559097413\n",
      "l2 norm of weights: 6.178030918851173\n",
      "---------------------\n",
      "Iteration Number: 2491\n",
      "Loss: 51.21365904409237\n",
      "l2 norm of gradients: 0.4645075269997748\n",
      "l2 norm of weights: 6.177904005965864\n",
      "---------------------\n",
      "Iteration Number: 2492\n",
      "Loss: 51.20297473920516\n",
      "l2 norm of gradients: 0.4643438525432333\n",
      "l2 norm of weights: 6.1777771226836204\n",
      "---------------------\n",
      "Iteration Number: 2493\n",
      "Loss: 51.192297873748444\n",
      "l2 norm of gradients: 0.46418029207487027\n",
      "l2 norm of weights: 6.1776502689831645\n",
      "---------------------\n",
      "Iteration Number: 2494\n",
      "Loss: 51.181628439973764\n",
      "l2 norm of gradients: 0.46401684544861826\n",
      "l2 norm of weights: 6.177523444843233\n",
      "---------------------\n",
      "Iteration Number: 2495\n",
      "Loss: 51.17096643008373\n",
      "l2 norm of gradients: 0.46385351251882123\n",
      "l2 norm of weights: 6.177396650242588\n",
      "---------------------\n",
      "Iteration Number: 2496\n",
      "Loss: 51.16031183635785\n",
      "l2 norm of gradients: 0.46369029314023247\n",
      "l2 norm of weights: 6.17726988516001\n",
      "---------------------\n",
      "Iteration Number: 2497\n",
      "Loss: 51.14966465111517\n",
      "l2 norm of gradients: 0.4635271871680154\n",
      "l2 norm of weights: 6.177143149574301\n",
      "---------------------\n",
      "Iteration Number: 2498\n",
      "Loss: 51.13902486658571\n",
      "l2 norm of gradients: 0.4633641944577418\n",
      "l2 norm of weights: 6.177016443464282\n",
      "---------------------\n",
      "Iteration Number: 2499\n",
      "Loss: 51.12839247514711\n",
      "l2 norm of gradients: 0.46320131486539123\n",
      "l2 norm of weights: 6.176889766808793\n",
      "---------------------\n",
      "Iteration Number: 2500\n",
      "Loss: 51.117767469027115\n",
      "l2 norm of gradients: 0.4630385482473508\n",
      "l2 norm of weights: 6.176763119586699\n",
      "---------------------\n",
      "Iteration Number: 2501\n",
      "Loss: 51.10714984062702\n",
      "l2 norm of gradients: 0.4628758944604134\n",
      "l2 norm of weights: 6.176636501776879\n",
      "---------------------\n",
      "Iteration Number: 2502\n",
      "Loss: 51.09653958223244\n",
      "l2 norm of gradients: 0.46271335336177777\n",
      "l2 norm of weights: 6.176509913358235\n",
      "---------------------\n",
      "Iteration Number: 2503\n",
      "Loss: 51.08593668619381\n",
      "l2 norm of gradients: 0.4625509248090476\n",
      "l2 norm of weights: 6.17638335430969\n",
      "---------------------\n",
      "Iteration Number: 2504\n",
      "Loss: 51.07534114491252\n",
      "l2 norm of gradients: 0.46238860866023007\n",
      "l2 norm of weights: 6.176256824610187\n",
      "---------------------\n",
      "Iteration Number: 2505\n",
      "Loss: 51.064752950742296\n",
      "l2 norm of gradients: 0.4622264047737363\n",
      "l2 norm of weights: 6.176130324238685\n",
      "---------------------\n",
      "Iteration Number: 2506\n",
      "Loss: 51.05417209602653\n",
      "l2 norm of gradients: 0.4620643130083794\n",
      "l2 norm of weights: 6.1760038531741674\n",
      "---------------------\n",
      "Iteration Number: 2507\n",
      "Loss: 51.043598573216315\n",
      "l2 norm of gradients: 0.461902333223374\n",
      "l2 norm of weights: 6.1758774113956365\n",
      "---------------------\n",
      "Iteration Number: 2508\n",
      "Loss: 51.03303237471096\n",
      "l2 norm of gradients: 0.4617404652783361\n",
      "l2 norm of weights: 6.175750998882114\n",
      "---------------------\n",
      "Iteration Number: 2509\n",
      "Loss: 51.02247349290012\n",
      "l2 norm of gradients: 0.4615787090332816\n",
      "l2 norm of weights: 6.175624615612641\n",
      "---------------------\n",
      "Iteration Number: 2510\n",
      "Loss: 51.01192192020549\n",
      "l2 norm of gradients: 0.46141706434862545\n",
      "l2 norm of weights: 6.175498261566278\n",
      "---------------------\n",
      "Iteration Number: 2511\n",
      "Loss: 51.00137764909701\n",
      "l2 norm of gradients: 0.46125553108518164\n",
      "l2 norm of weights: 6.175371936722105\n",
      "---------------------\n",
      "Iteration Number: 2512\n",
      "Loss: 50.99084067203468\n",
      "l2 norm of gradients: 0.4610941091041613\n",
      "l2 norm of weights: 6.175245641059227\n",
      "---------------------\n",
      "Iteration Number: 2513\n",
      "Loss: 50.98031098146651\n",
      "l2 norm of gradients: 0.46093279826717315\n",
      "l2 norm of weights: 6.175119374556761\n",
      "---------------------\n",
      "Iteration Number: 2514\n",
      "Loss: 50.9697885698762\n",
      "l2 norm of gradients: 0.46077159843622145\n",
      "l2 norm of weights: 6.174993137193848\n",
      "---------------------\n",
      "Iteration Number: 2515\n",
      "Loss: 50.95927342973938\n",
      "l2 norm of gradients: 0.4606105094737063\n",
      "l2 norm of weights: 6.174866928949648\n",
      "---------------------\n",
      "Iteration Number: 2516\n",
      "Loss: 50.94876555355104\n",
      "l2 norm of gradients: 0.460449531242422\n",
      "l2 norm of weights: 6.174740749803339\n",
      "---------------------\n",
      "Iteration Number: 2517\n",
      "Loss: 50.938264933842504\n",
      "l2 norm of gradients: 0.4602886636055569\n",
      "l2 norm of weights: 6.174614599734121\n",
      "---------------------\n",
      "Iteration Number: 2518\n",
      "Loss: 50.92777156310841\n",
      "l2 norm of gradients: 0.46012790642669205\n",
      "l2 norm of weights: 6.174488478721211\n",
      "---------------------\n",
      "Iteration Number: 2519\n",
      "Loss: 50.917285433901846\n",
      "l2 norm of gradients: 0.45996725956980106\n",
      "l2 norm of weights: 6.17436238674385\n",
      "---------------------\n",
      "Iteration Number: 2520\n",
      "Loss: 50.90680653873275\n",
      "l2 norm of gradients: 0.45980672289924845\n",
      "l2 norm of weights: 6.174236323781291\n",
      "---------------------\n",
      "Iteration Number: 2521\n",
      "Loss: 50.89633487019536\n",
      "l2 norm of gradients: 0.4596462962797894\n",
      "l2 norm of weights: 6.1741102898128135\n",
      "---------------------\n",
      "Iteration Number: 2522\n",
      "Loss: 50.88587042086902\n",
      "l2 norm of gradients: 0.4594859795765692\n",
      "l2 norm of weights: 6.173984284817712\n",
      "---------------------\n",
      "Iteration Number: 2523\n",
      "Loss: 50.875413183274965\n",
      "l2 norm of gradients: 0.4593257726551216\n",
      "l2 norm of weights: 6.173858308775303\n",
      "---------------------\n",
      "Iteration Number: 2524\n",
      "Loss: 50.86496315002157\n",
      "l2 norm of gradients: 0.4591656753813688\n",
      "l2 norm of weights: 6.1737323616649205\n",
      "---------------------\n",
      "Iteration Number: 2525\n",
      "Loss: 50.854520313705756\n",
      "l2 norm of gradients: 0.45900568762162053\n",
      "l2 norm of weights: 6.173606443465918\n",
      "---------------------\n",
      "Iteration Number: 2526\n",
      "Loss: 50.84408466695846\n",
      "l2 norm of gradients: 0.45884580924257223\n",
      "l2 norm of weights: 6.173480554157669\n",
      "---------------------\n",
      "Iteration Number: 2527\n",
      "Loss: 50.83365620240821\n",
      "l2 norm of gradients: 0.458686040111306\n",
      "l2 norm of weights: 6.173354693719567\n",
      "---------------------\n",
      "Iteration Number: 2528\n",
      "Loss: 50.82323491263966\n",
      "l2 norm of gradients: 0.4585263800952882\n",
      "l2 norm of weights: 6.173228862131022\n",
      "---------------------\n",
      "Iteration Number: 2529\n",
      "Loss: 50.81282079032643\n",
      "l2 norm of gradients: 0.4583668290623695\n",
      "l2 norm of weights: 6.1731030593714635\n",
      "---------------------\n",
      "Iteration Number: 2530\n",
      "Loss: 50.802413828100676\n",
      "l2 norm of gradients: 0.45820738688078394\n",
      "l2 norm of weights: 6.172977285420345\n",
      "---------------------\n",
      "Iteration Number: 2531\n",
      "Loss: 50.7920140186756\n",
      "l2 norm of gradients: 0.4580480534191479\n",
      "l2 norm of weights: 6.172851540257131\n",
      "---------------------\n",
      "Iteration Number: 2532\n",
      "Loss: 50.781621354665596\n",
      "l2 norm of gradients: 0.4578888285464593\n",
      "l2 norm of weights: 6.172725823861311\n",
      "---------------------\n",
      "Iteration Number: 2533\n",
      "Loss: 50.77123582877743\n",
      "l2 norm of gradients: 0.457729712132097\n",
      "l2 norm of weights: 6.1726001362123935\n",
      "---------------------\n",
      "Iteration Number: 2534\n",
      "Loss: 50.76085743374509\n",
      "l2 norm of gradients: 0.45757070404581973\n",
      "l2 norm of weights: 6.172474477289904\n",
      "---------------------\n",
      "Iteration Number: 2535\n",
      "Loss: 50.75048616225821\n",
      "l2 norm of gradients: 0.4574118041577656\n",
      "l2 norm of weights: 6.172348847073384\n",
      "---------------------\n",
      "Iteration Number: 2536\n",
      "Loss: 50.74012200698445\n",
      "l2 norm of gradients: 0.45725301233845067\n",
      "l2 norm of weights: 6.1722232455423995\n",
      "---------------------\n",
      "Iteration Number: 2537\n",
      "Loss: 50.72976496072388\n",
      "l2 norm of gradients: 0.457094328458769\n",
      "l2 norm of weights: 6.172097672676534\n",
      "---------------------\n",
      "Iteration Number: 2538\n",
      "Loss: 50.71941501616917\n",
      "l2 norm of gradients: 0.45693575238999107\n",
      "l2 norm of weights: 6.171972128455386\n",
      "---------------------\n",
      "Iteration Number: 2539\n",
      "Loss: 50.709072166068445\n",
      "l2 norm of gradients: 0.4567772840037631\n",
      "l2 norm of weights: 6.171846612858578\n",
      "---------------------\n",
      "Iteration Number: 2540\n",
      "Loss: 50.698736403261485\n",
      "l2 norm of gradients: 0.4566189231721068\n",
      "l2 norm of weights: 6.171721125865748\n",
      "---------------------\n",
      "Iteration Number: 2541\n",
      "Loss: 50.6884077204036\n",
      "l2 norm of gradients: 0.4564606697674174\n",
      "l2 norm of weights: 6.171595667456554\n",
      "---------------------\n",
      "Iteration Number: 2542\n",
      "Loss: 50.67808611036022\n",
      "l2 norm of gradients: 0.4563025236624642\n",
      "l2 norm of weights: 6.171470237610672\n",
      "---------------------\n",
      "Iteration Number: 2543\n",
      "Loss: 50.66777156589123\n",
      "l2 norm of gradients: 0.4561444847303887\n",
      "l2 norm of weights: 6.171344836307797\n",
      "---------------------\n",
      "Iteration Number: 2544\n",
      "Loss: 50.657464079821864\n",
      "l2 norm of gradients: 0.45598655284470396\n",
      "l2 norm of weights: 6.171219463527642\n",
      "---------------------\n",
      "Iteration Number: 2545\n",
      "Loss: 50.64716364494203\n",
      "l2 norm of gradients: 0.45582872787929424\n",
      "l2 norm of weights: 6.171094119249941\n",
      "---------------------\n",
      "Iteration Number: 2546\n",
      "Loss: 50.63687025405977\n",
      "l2 norm of gradients: 0.45567100970841357\n",
      "l2 norm of weights: 6.170968803454444\n",
      "---------------------\n",
      "Iteration Number: 2547\n",
      "Loss: 50.62658390007132\n",
      "l2 norm of gradients: 0.4555133982066855\n",
      "l2 norm of weights: 6.170843516120921\n",
      "---------------------\n",
      "Iteration Number: 2548\n",
      "Loss: 50.61630457578858\n",
      "l2 norm of gradients: 0.45535589324910153\n",
      "l2 norm of weights: 6.170718257229158\n",
      "---------------------\n",
      "Iteration Number: 2549\n",
      "Loss: 50.60603227403624\n",
      "l2 norm of gradients: 0.4551984947110209\n",
      "l2 norm of weights: 6.170593026758963\n",
      "---------------------\n",
      "Iteration Number: 2550\n",
      "Loss: 50.595766987726186\n",
      "l2 norm of gradients: 0.45504120246816965\n",
      "l2 norm of weights: 6.170467824690161\n",
      "---------------------\n",
      "Iteration Number: 2551\n",
      "Loss: 50.58550870971475\n",
      "l2 norm of gradients: 0.4548840163966393\n",
      "l2 norm of weights: 6.170342651002596\n",
      "---------------------\n",
      "Iteration Number: 2552\n",
      "Loss: 50.57525743288362\n",
      "l2 norm of gradients: 0.45472693637288675\n",
      "l2 norm of weights: 6.170217505676128\n",
      "---------------------\n",
      "Iteration Number: 2553\n",
      "Loss: 50.565013150183766\n",
      "l2 norm of gradients: 0.4545699622737329\n",
      "l2 norm of weights: 6.170092388690638\n",
      "---------------------\n",
      "Iteration Number: 2554\n",
      "Loss: 50.554775854435576\n",
      "l2 norm of gradients: 0.45441309397636187\n",
      "l2 norm of weights: 6.169967300026025\n",
      "---------------------\n",
      "Iteration Number: 2555\n",
      "Loss: 50.5445455386058\n",
      "l2 norm of gradients: 0.45425633135832033\n",
      "l2 norm of weights: 6.169842239662205\n",
      "---------------------\n",
      "Iteration Number: 2556\n",
      "Loss: 50.53432219559023\n",
      "l2 norm of gradients: 0.45409967429751663\n",
      "l2 norm of weights: 6.169717207579113\n",
      "---------------------\n",
      "Iteration Number: 2557\n",
      "Loss: 50.524105818414206\n",
      "l2 norm of gradients: 0.45394312267221965\n",
      "l2 norm of weights: 6.169592203756703\n",
      "---------------------\n",
      "Iteration Number: 2558\n",
      "Loss: 50.5138963998999\n",
      "l2 norm of gradients: 0.4537866763610585\n",
      "l2 norm of weights: 6.169467228174947\n",
      "---------------------\n",
      "Iteration Number: 2559\n",
      "Loss: 50.5036939330905\n",
      "l2 norm of gradients: 0.4536303352430211\n",
      "l2 norm of weights: 6.169342280813832\n",
      "---------------------\n",
      "Iteration Number: 2560\n",
      "Loss: 50.49349841095495\n",
      "l2 norm of gradients: 0.45347409919745374\n",
      "l2 norm of weights: 6.16921736165337\n",
      "---------------------\n",
      "Iteration Number: 2561\n",
      "Loss: 50.483309826427174\n",
      "l2 norm of gradients: 0.45331796810406005\n",
      "l2 norm of weights: 6.169092470673583\n",
      "---------------------\n",
      "Iteration Number: 2562\n",
      "Loss: 50.473128172534146\n",
      "l2 norm of gradients: 0.4531619418429\n",
      "l2 norm of weights: 6.168967607854519\n",
      "---------------------\n",
      "Iteration Number: 2563\n",
      "Loss: 50.462953442231075\n",
      "l2 norm of gradients: 0.4530060202943894\n",
      "l2 norm of weights: 6.1688427731762365\n",
      "---------------------\n",
      "Iteration Number: 2564\n",
      "Loss: 50.45278562855631\n",
      "l2 norm of gradients: 0.452850203339299\n",
      "l2 norm of weights: 6.168717966618818\n",
      "---------------------\n",
      "Iteration Number: 2565\n",
      "Loss: 50.44262472453715\n",
      "l2 norm of gradients: 0.452694490858753\n",
      "l2 norm of weights: 6.168593188162362\n",
      "---------------------\n",
      "Iteration Number: 2566\n",
      "Loss: 50.432470723190214\n",
      "l2 norm of gradients: 0.4525388827342294\n",
      "l2 norm of weights: 6.168468437786984\n",
      "---------------------\n",
      "Iteration Number: 2567\n",
      "Loss: 50.42232361755005\n",
      "l2 norm of gradients: 0.45238337884755764\n",
      "l2 norm of weights: 6.168343715472818\n",
      "---------------------\n",
      "Iteration Number: 2568\n",
      "Loss: 50.412183400657455\n",
      "l2 norm of gradients: 0.4522279790809192\n",
      "l2 norm of weights: 6.1682190212000165\n",
      "---------------------\n",
      "Iteration Number: 2569\n",
      "Loss: 50.402050065608826\n",
      "l2 norm of gradients: 0.4520726833168455\n",
      "l2 norm of weights: 6.16809435494875\n",
      "---------------------\n",
      "Iteration Number: 2570\n",
      "Loss: 50.391923605421184\n",
      "l2 norm of gradients: 0.4519174914382184\n",
      "l2 norm of weights: 6.167969716699206\n",
      "---------------------\n",
      "Iteration Number: 2571\n",
      "Loss: 50.381804013214456\n",
      "l2 norm of gradients: 0.45176240332826756\n",
      "l2 norm of weights: 6.167845106431591\n",
      "---------------------\n",
      "Iteration Number: 2572\n",
      "Loss: 50.37169128202954\n",
      "l2 norm of gradients: 0.4516074188705714\n",
      "l2 norm of weights: 6.167720524126128\n",
      "---------------------\n",
      "Iteration Number: 2573\n",
      "Loss: 50.36158540502559\n",
      "l2 norm of gradients: 0.4514525379490549\n",
      "l2 norm of weights: 6.167595969763059\n",
      "---------------------\n",
      "Iteration Number: 2574\n",
      "Loss: 50.35148637526935\n",
      "l2 norm of gradients: 0.4512977604479896\n",
      "l2 norm of weights: 6.167471443322643\n",
      "---------------------\n",
      "Iteration Number: 2575\n",
      "Loss: 50.34139418585988\n",
      "l2 norm of gradients: 0.451143086251992\n",
      "l2 norm of weights: 6.167346944785156\n",
      "---------------------\n",
      "Iteration Number: 2576\n",
      "Loss: 50.331308829957436\n",
      "l2 norm of gradients: 0.45098851524602335\n",
      "l2 norm of weights: 6.167222474130895\n",
      "---------------------\n",
      "Iteration Number: 2577\n",
      "Loss: 50.321230300679595\n",
      "l2 norm of gradients: 0.45083404731538856\n",
      "l2 norm of weights: 6.16709803134017\n",
      "---------------------\n",
      "Iteration Number: 2578\n",
      "Loss: 50.311158591190264\n",
      "l2 norm of gradients: 0.4506796823457352\n",
      "l2 norm of weights: 6.166973616393312\n",
      "---------------------\n",
      "Iteration Number: 2579\n",
      "Loss: 50.30109369464658\n",
      "l2 norm of gradients: 0.4505254202230525\n",
      "l2 norm of weights: 6.16684922927067\n",
      "---------------------\n",
      "Iteration Number: 2580\n",
      "Loss: 50.291035604203095\n",
      "l2 norm of gradients: 0.45037126083367085\n",
      "l2 norm of weights: 6.166724869952607\n",
      "---------------------\n",
      "Iteration Number: 2581\n",
      "Loss: 50.280984313006726\n",
      "l2 norm of gradients: 0.45021720406426097\n",
      "l2 norm of weights: 6.166600538419508\n",
      "---------------------\n",
      "Iteration Number: 2582\n",
      "Loss: 50.270939814290365\n",
      "l2 norm of gradients: 0.4500632498018325\n",
      "l2 norm of weights: 6.166476234651772\n",
      "---------------------\n",
      "Iteration Number: 2583\n",
      "Loss: 50.26090210122032\n",
      "l2 norm of gradients: 0.44990939793373375\n",
      "l2 norm of weights: 6.166351958629818\n",
      "---------------------\n",
      "Iteration Number: 2584\n",
      "Loss: 50.25087116698784\n",
      "l2 norm of gradients: 0.4497556483476505\n",
      "l2 norm of weights: 6.166227710334081\n",
      "---------------------\n",
      "Iteration Number: 2585\n",
      "Loss: 50.24084700484182\n",
      "l2 norm of gradients: 0.4496020009316051\n",
      "l2 norm of weights: 6.166103489745012\n",
      "---------------------\n",
      "Iteration Number: 2586\n",
      "Loss: 50.23082960798898\n",
      "l2 norm of gradients: 0.4494484555739558\n",
      "l2 norm of weights: 6.165979296843085\n",
      "---------------------\n",
      "Iteration Number: 2587\n",
      "Loss: 50.220818969636504\n",
      "l2 norm of gradients: 0.4492950121633955\n",
      "l2 norm of weights: 6.165855131608786\n",
      "---------------------\n",
      "Iteration Number: 2588\n",
      "Loss: 50.21081508307436\n",
      "l2 norm of gradients: 0.44914167058895155\n",
      "l2 norm of weights: 6.165730994022621\n",
      "---------------------\n",
      "Iteration Number: 2589\n",
      "Loss: 50.200817941494215\n",
      "l2 norm of gradients: 0.4489884307399841\n",
      "l2 norm of weights: 6.16560688406511\n",
      "---------------------\n",
      "Iteration Number: 2590\n",
      "Loss: 50.19082753818766\n",
      "l2 norm of gradients: 0.4488352925061858\n",
      "l2 norm of weights: 6.165482801716798\n",
      "---------------------\n",
      "Iteration Number: 2591\n",
      "Loss: 50.180843866447276\n",
      "l2 norm of gradients: 0.4486822557775806\n",
      "l2 norm of weights: 6.165358746958237\n",
      "---------------------\n",
      "Iteration Number: 2592\n",
      "Loss: 50.170866919518616\n",
      "l2 norm of gradients: 0.4485293204445229\n",
      "l2 norm of weights: 6.165234719770005\n",
      "---------------------\n",
      "Iteration Number: 2593\n",
      "Loss: 50.160896690681824\n",
      "l2 norm of gradients: 0.4483764863976971\n",
      "l2 norm of weights: 6.165110720132693\n",
      "---------------------\n",
      "Iteration Number: 2594\n",
      "Loss: 50.1509331732362\n",
      "l2 norm of gradients: 0.4482237535281161\n",
      "l2 norm of weights: 6.1649867480269105\n",
      "---------------------\n",
      "Iteration Number: 2595\n",
      "Loss: 50.14097636053593\n",
      "l2 norm of gradients: 0.44807112172712066\n",
      "l2 norm of weights: 6.164862803433282\n",
      "---------------------\n",
      "Iteration Number: 2596\n",
      "Loss: 50.131026245841454\n",
      "l2 norm of gradients: 0.44791859088637875\n",
      "l2 norm of weights: 6.1647388863324535\n",
      "---------------------\n",
      "Iteration Number: 2597\n",
      "Loss: 50.12108282248787\n",
      "l2 norm of gradients: 0.4477661608978846\n",
      "l2 norm of weights: 6.164614996705084\n",
      "---------------------\n",
      "Iteration Number: 2598\n",
      "Loss: 50.111146083820046\n",
      "l2 norm of gradients: 0.4476138316539573\n",
      "l2 norm of weights: 6.164491134531852\n",
      "---------------------\n",
      "Iteration Number: 2599\n",
      "Loss: 50.101216023169854\n",
      "l2 norm of gradients: 0.44746160304724053\n",
      "l2 norm of weights: 6.164367299793454\n",
      "---------------------\n",
      "Iteration Number: 2600\n",
      "Loss: 50.091292633912694\n",
      "l2 norm of gradients: 0.44730947497070167\n",
      "l2 norm of weights: 6.164243492470599\n",
      "---------------------\n",
      "Iteration Number: 2601\n",
      "Loss: 50.08137590938221\n",
      "l2 norm of gradients: 0.44715744731763046\n",
      "l2 norm of weights: 6.164119712544019\n",
      "---------------------\n",
      "Iteration Number: 2602\n",
      "Loss: 50.071465842969346\n",
      "l2 norm of gradients: 0.4470055199816385\n",
      "l2 norm of weights: 6.163995959994459\n",
      "---------------------\n",
      "Iteration Number: 2603\n",
      "Loss: 50.06156242801839\n",
      "l2 norm of gradients: 0.4468536928566583\n",
      "l2 norm of weights: 6.163872234802683\n",
      "---------------------\n",
      "Iteration Number: 2604\n",
      "Loss: 50.05166565797936\n",
      "l2 norm of gradients: 0.44670196583694244\n",
      "l2 norm of weights: 6.16374853694947\n",
      "---------------------\n",
      "Iteration Number: 2605\n",
      "Loss: 50.041775526177574\n",
      "l2 norm of gradients: 0.44655033881706224\n",
      "l2 norm of weights: 6.163624866415618\n",
      "---------------------\n",
      "Iteration Number: 2606\n",
      "Loss: 50.03189202605088\n",
      "l2 norm of gradients: 0.4463988116919078\n",
      "l2 norm of weights: 6.163501223181942\n",
      "---------------------\n",
      "Iteration Number: 2607\n",
      "Loss: 50.02201515106461\n",
      "l2 norm of gradients: 0.44624738435668587\n",
      "l2 norm of weights: 6.163377607229272\n",
      "---------------------\n",
      "Iteration Number: 2608\n",
      "Loss: 50.012144894555654\n",
      "l2 norm of gradients: 0.4460960567069203\n",
      "l2 norm of weights: 6.163254018538457\n",
      "---------------------\n",
      "Iteration Number: 2609\n",
      "Loss: 50.00228124997772\n",
      "l2 norm of gradients: 0.44594482863845014\n",
      "l2 norm of weights: 6.163130457090362\n",
      "---------------------\n",
      "Iteration Number: 2610\n",
      "Loss: 49.992424210828744\n",
      "l2 norm of gradients: 0.44579370004742924\n",
      "l2 norm of weights: 6.163006922865868\n",
      "---------------------\n",
      "Iteration Number: 2611\n",
      "Loss: 49.982573770526564\n",
      "l2 norm of gradients: 0.4456426708303253\n",
      "l2 norm of weights: 6.162883415845875\n",
      "---------------------\n",
      "Iteration Number: 2612\n",
      "Loss: 49.97272992249816\n",
      "l2 norm of gradients: 0.4454917408839188\n",
      "l2 norm of weights: 6.162759936011297\n",
      "---------------------\n",
      "Iteration Number: 2613\n",
      "Loss: 49.962892660233095\n",
      "l2 norm of gradients: 0.4453409101053025\n",
      "l2 norm of weights: 6.162636483343068\n",
      "---------------------\n",
      "Iteration Number: 2614\n",
      "Loss: 49.95306197724037\n",
      "l2 norm of gradients: 0.4451901783918801\n",
      "l2 norm of weights: 6.162513057822135\n",
      "---------------------\n",
      "Iteration Number: 2615\n",
      "Loss: 49.94323786697022\n",
      "l2 norm of gradients: 0.44503954564136555\n",
      "l2 norm of weights: 6.162389659429466\n",
      "---------------------\n",
      "Iteration Number: 2616\n",
      "Loss: 49.93342032291017\n",
      "l2 norm of gradients: 0.4448890117517823\n",
      "l2 norm of weights: 6.162266288146042\n",
      "---------------------\n",
      "Iteration Number: 2617\n",
      "Loss: 49.923609338617396\n",
      "l2 norm of gradients: 0.44473857662146227\n",
      "l2 norm of weights: 6.162142943952864\n",
      "---------------------\n",
      "Iteration Number: 2618\n",
      "Loss: 49.913804907518596\n",
      "l2 norm of gradients: 0.4445882401490451\n",
      "l2 norm of weights: 6.162019626830946\n",
      "---------------------\n",
      "Iteration Number: 2619\n",
      "Loss: 49.90400702324151\n",
      "l2 norm of gradients: 0.44443800223347674\n",
      "l2 norm of weights: 6.161896336761322\n",
      "---------------------\n",
      "Iteration Number: 2620\n",
      "Loss: 49.894215679198744\n",
      "l2 norm of gradients: 0.44428786277400956\n",
      "l2 norm of weights: 6.161773073725041\n",
      "---------------------\n",
      "Iteration Number: 2621\n",
      "Loss: 49.884430869037324\n",
      "l2 norm of gradients: 0.44413782167020066\n",
      "l2 norm of weights: 6.161649837703169\n",
      "---------------------\n",
      "Iteration Number: 2622\n",
      "Loss: 49.87465258622011\n",
      "l2 norm of gradients: 0.44398787882191065\n",
      "l2 norm of weights: 6.161526628676788\n",
      "---------------------\n",
      "Iteration Number: 2623\n",
      "Loss: 49.86488082434731\n",
      "l2 norm of gradients: 0.4438380341293045\n",
      "l2 norm of weights: 6.161403446626998\n",
      "---------------------\n",
      "Iteration Number: 2624\n",
      "Loss: 49.8551155769569\n",
      "l2 norm of gradients: 0.4436882874928483\n",
      "l2 norm of weights: 6.161280291534913\n",
      "---------------------\n",
      "Iteration Number: 2625\n",
      "Loss: 49.845356837666806\n",
      "l2 norm of gradients: 0.44353863881331\n",
      "l2 norm of weights: 6.1611571633816675\n",
      "---------------------\n",
      "Iteration Number: 2626\n",
      "Loss: 49.835604599979085\n",
      "l2 norm of gradients: 0.44338908799175847\n",
      "l2 norm of weights: 6.161034062148409\n",
      "---------------------\n",
      "Iteration Number: 2627\n",
      "Loss: 49.82585885758739\n",
      "l2 norm of gradients: 0.4432396349295615\n",
      "l2 norm of weights: 6.160910987816303\n",
      "---------------------\n",
      "Iteration Number: 2628\n",
      "Loss: 49.81611960400532\n",
      "l2 norm of gradients: 0.4430902795283862\n",
      "l2 norm of weights: 6.1607879403665295\n",
      "---------------------\n",
      "Iteration Number: 2629\n",
      "Loss: 49.80638683285259\n",
      "l2 norm of gradients: 0.4429410216901971\n",
      "l2 norm of weights: 6.160664919780289\n",
      "---------------------\n",
      "Iteration Number: 2630\n",
      "Loss: 49.796660537780625\n",
      "l2 norm of gradients: 0.44279186131725606\n",
      "l2 norm of weights: 6.160541926038795\n",
      "---------------------\n",
      "Iteration Number: 2631\n",
      "Loss: 49.78694071240308\n",
      "l2 norm of gradients: 0.44264279831212094\n",
      "l2 norm of weights: 6.1604189591232785\n",
      "---------------------\n",
      "Iteration Number: 2632\n",
      "Loss: 49.77722735032343\n",
      "l2 norm of gradients: 0.4424938325776445\n",
      "l2 norm of weights: 6.160296019014988\n",
      "---------------------\n",
      "Iteration Number: 2633\n",
      "Loss: 49.76752044519864\n",
      "l2 norm of gradients: 0.4423449640169741\n",
      "l2 norm of weights: 6.1601731056951845\n",
      "---------------------\n",
      "Iteration Number: 2634\n",
      "Loss: 49.75781999066039\n",
      "l2 norm of gradients: 0.4421961925335507\n",
      "l2 norm of weights: 6.16005021914515\n",
      "---------------------\n",
      "Iteration Number: 2635\n",
      "Loss: 49.74812598042973\n",
      "l2 norm of gradients: 0.44204751803110715\n",
      "l2 norm of weights: 6.159927359346181\n",
      "---------------------\n",
      "Iteration Number: 2636\n",
      "Loss: 49.7384384080658\n",
      "l2 norm of gradients: 0.4418989404136687\n",
      "l2 norm of weights: 6.15980452627959\n",
      "---------------------\n",
      "Iteration Number: 2637\n",
      "Loss: 49.72875726734742\n",
      "l2 norm of gradients: 0.44175045958555087\n",
      "l2 norm of weights: 6.159681719926704\n",
      "---------------------\n",
      "Iteration Number: 2638\n",
      "Loss: 49.719082551867835\n",
      "l2 norm of gradients: 0.4416020754513592\n",
      "l2 norm of weights: 6.159558940268871\n",
      "---------------------\n",
      "Iteration Number: 2639\n",
      "Loss: 49.70941425539675\n",
      "l2 norm of gradients: 0.44145378791598816\n",
      "l2 norm of weights: 6.159436187287452\n",
      "---------------------\n",
      "Iteration Number: 2640\n",
      "Loss: 49.69975237157872\n",
      "l2 norm of gradients: 0.4413055968846206\n",
      "l2 norm of weights: 6.159313460963824\n",
      "---------------------\n",
      "Iteration Number: 2641\n",
      "Loss: 49.690096894165485\n",
      "l2 norm of gradients: 0.441157502262726\n",
      "l2 norm of weights: 6.159190761279379\n",
      "---------------------\n",
      "Iteration Number: 2642\n",
      "Loss: 49.680447816805\n",
      "l2 norm of gradients: 0.44100950395606103\n",
      "l2 norm of weights: 6.159068088215531\n",
      "---------------------\n",
      "Iteration Number: 2643\n",
      "Loss: 49.670805133281696\n",
      "l2 norm of gradients: 0.44086160187066664\n",
      "l2 norm of weights: 6.158945441753703\n",
      "---------------------\n",
      "Iteration Number: 2644\n",
      "Loss: 49.661168837282716\n",
      "l2 norm of gradients: 0.44071379591286963\n",
      "l2 norm of weights: 6.158822821875339\n",
      "---------------------\n",
      "Iteration Number: 2645\n",
      "Loss: 49.65153892258283\n",
      "l2 norm of gradients: 0.4405660859892793\n",
      "l2 norm of weights: 6.158700228561898\n",
      "---------------------\n",
      "Iteration Number: 2646\n",
      "Loss: 49.641915382943914\n",
      "l2 norm of gradients: 0.44041847200678874\n",
      "l2 norm of weights: 6.158577661794854\n",
      "---------------------\n",
      "Iteration Number: 2647\n",
      "Loss: 49.63229821204371\n",
      "l2 norm of gradients: 0.44027095387257215\n",
      "l2 norm of weights: 6.158455121555697\n",
      "---------------------\n",
      "Iteration Number: 2648\n",
      "Loss: 49.622687403717265\n",
      "l2 norm of gradients: 0.4401235314940851\n",
      "l2 norm of weights: 6.158332607825934\n",
      "---------------------\n",
      "Iteration Number: 2649\n",
      "Loss: 49.613082951712435\n",
      "l2 norm of gradients: 0.4399762047790632\n",
      "l2 norm of weights: 6.158210120587088\n",
      "---------------------\n",
      "Iteration Number: 2650\n",
      "Loss: 49.603484849826756\n",
      "l2 norm of gradients: 0.4398289736355215\n",
      "l2 norm of weights: 6.158087659820699\n",
      "---------------------\n",
      "Iteration Number: 2651\n",
      "Loss: 49.5938930917949\n",
      "l2 norm of gradients: 0.43968183797175303\n",
      "l2 norm of weights: 6.15796522550832\n",
      "---------------------\n",
      "Iteration Number: 2652\n",
      "Loss: 49.58430767148935\n",
      "l2 norm of gradients: 0.43953479769632864\n",
      "l2 norm of weights: 6.157842817631523\n",
      "---------------------\n",
      "Iteration Number: 2653\n",
      "Loss: 49.574728582652654\n",
      "l2 norm of gradients: 0.43938785271809555\n",
      "l2 norm of weights: 6.1577204361718945\n",
      "---------------------\n",
      "Iteration Number: 2654\n",
      "Loss: 49.56515581912147\n",
      "l2 norm of gradients: 0.43924100294617674\n",
      "l2 norm of weights: 6.157598081111038\n",
      "---------------------\n",
      "Iteration Number: 2655\n",
      "Loss: 49.55558937471546\n",
      "l2 norm of gradients: 0.4390942482899702\n",
      "l2 norm of weights: 6.157475752430571\n",
      "---------------------\n",
      "Iteration Number: 2656\n",
      "Loss: 49.54602924325212\n",
      "l2 norm of gradients: 0.43894758865914757\n",
      "l2 norm of weights: 6.157353450112128\n",
      "---------------------\n",
      "Iteration Number: 2657\n",
      "Loss: 49.53647541857132\n",
      "l2 norm of gradients: 0.43880102396365356\n",
      "l2 norm of weights: 6.157231174137362\n",
      "---------------------\n",
      "Iteration Number: 2658\n",
      "Loss: 49.526927894566356\n",
      "l2 norm of gradients: 0.43865455411370535\n",
      "l2 norm of weights: 6.157108924487936\n",
      "---------------------\n",
      "Iteration Number: 2659\n",
      "Loss: 49.517386664996764\n",
      "l2 norm of gradients: 0.438508179019791\n",
      "l2 norm of weights: 6.156986701145534\n",
      "---------------------\n",
      "Iteration Number: 2660\n",
      "Loss: 49.50785172377533\n",
      "l2 norm of gradients: 0.43836189859266916\n",
      "l2 norm of weights: 6.156864504091855\n",
      "---------------------\n",
      "Iteration Number: 2661\n",
      "Loss: 49.49832306474557\n",
      "l2 norm of gradients: 0.4382157127433683\n",
      "l2 norm of weights: 6.156742333308611\n",
      "---------------------\n",
      "Iteration Number: 2662\n",
      "Loss: 49.48880068184789\n",
      "l2 norm of gradients: 0.43806962138318495\n",
      "l2 norm of weights: 6.156620188777533\n",
      "---------------------\n",
      "Iteration Number: 2663\n",
      "Loss: 49.479284568864806\n",
      "l2 norm of gradients: 0.4379236244236839\n",
      "l2 norm of weights: 6.1564980704803665\n",
      "---------------------\n",
      "Iteration Number: 2664\n",
      "Loss: 49.46977471975384\n",
      "l2 norm of gradients: 0.4377777217766964\n",
      "l2 norm of weights: 6.156375978398874\n",
      "---------------------\n",
      "Iteration Number: 2665\n",
      "Loss: 49.460271128403924\n",
      "l2 norm of gradients: 0.43763191335432017\n",
      "l2 norm of weights: 6.156253912514829\n",
      "---------------------\n",
      "Iteration Number: 2666\n",
      "Loss: 49.45077378872849\n",
      "l2 norm of gradients: 0.43748619906891734\n",
      "l2 norm of weights: 6.156131872810027\n",
      "---------------------\n",
      "Iteration Number: 2667\n",
      "Loss: 49.44128269459612\n",
      "l2 norm of gradients: 0.43734057883311545\n",
      "l2 norm of weights: 6.156009859266276\n",
      "---------------------\n",
      "Iteration Number: 2668\n",
      "Loss: 49.43179783996594\n",
      "l2 norm of gradients: 0.43719505255980406\n",
      "l2 norm of weights: 6.155887871865399\n",
      "---------------------\n",
      "Iteration Number: 2669\n",
      "Loss: 49.42231921878194\n",
      "l2 norm of gradients: 0.4370496201621362\n",
      "l2 norm of weights: 6.155765910589239\n",
      "---------------------\n",
      "Iteration Number: 2670\n",
      "Loss: 49.412846824924685\n",
      "l2 norm of gradients: 0.436904281553526\n",
      "l2 norm of weights: 6.155643975419649\n",
      "---------------------\n",
      "Iteration Number: 2671\n",
      "Loss: 49.403380652387355\n",
      "l2 norm of gradients: 0.4367590366476484\n",
      "l2 norm of weights: 6.1555220663385\n",
      "---------------------\n",
      "Iteration Number: 2672\n",
      "Loss: 49.39392069510044\n",
      "l2 norm of gradients: 0.43661388535843837\n",
      "l2 norm of weights: 6.15540018332768\n",
      "---------------------\n",
      "Iteration Number: 2673\n",
      "Loss: 49.38446694706293\n",
      "l2 norm of gradients: 0.4364688276000896\n",
      "l2 norm of weights: 6.155278326369092\n",
      "---------------------\n",
      "Iteration Number: 2674\n",
      "Loss: 49.3750194021649\n",
      "l2 norm of gradients: 0.4363238632870542\n",
      "l2 norm of weights: 6.155156495444652\n",
      "---------------------\n",
      "Iteration Number: 2675\n",
      "Loss: 49.36557805445415\n",
      "l2 norm of gradients: 0.4361789923340412\n",
      "l2 norm of weights: 6.155034690536295\n",
      "---------------------\n",
      "Iteration Number: 2676\n",
      "Loss: 49.35614289788662\n",
      "l2 norm of gradients: 0.4360342146560162\n",
      "l2 norm of weights: 6.154912911625971\n",
      "---------------------\n",
      "Iteration Number: 2677\n",
      "Loss: 49.346713926441076\n",
      "l2 norm of gradients: 0.4358895301682001\n",
      "l2 norm of weights: 6.154791158695643\n",
      "---------------------\n",
      "Iteration Number: 2678\n",
      "Loss: 49.33729113412456\n",
      "l2 norm of gradients: 0.43574493878606846\n",
      "l2 norm of weights: 6.154669431727291\n",
      "---------------------\n",
      "Iteration Number: 2679\n",
      "Loss: 49.32787451495101\n",
      "l2 norm of gradients: 0.4356004404253505\n",
      "l2 norm of weights: 6.154547730702912\n",
      "---------------------\n",
      "Iteration Number: 2680\n",
      "Loss: 49.31846406292753\n",
      "l2 norm of gradients: 0.4354560350020286\n",
      "l2 norm of weights: 6.154426055604519\n",
      "---------------------\n",
      "Iteration Number: 2681\n",
      "Loss: 49.30905977205259\n",
      "l2 norm of gradients: 0.4353117224323367\n",
      "l2 norm of weights: 6.154304406414135\n",
      "---------------------\n",
      "Iteration Number: 2682\n",
      "Loss: 49.29966163638812\n",
      "l2 norm of gradients: 0.4351675026327601\n",
      "l2 norm of weights: 6.154182783113805\n",
      "---------------------\n",
      "Iteration Number: 2683\n",
      "Loss: 49.290269649954524\n",
      "l2 norm of gradients: 0.43502337552003434\n",
      "l2 norm of weights: 6.1540611856855865\n",
      "---------------------\n",
      "Iteration Number: 2684\n",
      "Loss: 49.2808838067776\n",
      "l2 norm of gradients: 0.43487934101114434\n",
      "l2 norm of weights: 6.15393961411155\n",
      "---------------------\n",
      "Iteration Number: 2685\n",
      "Loss: 49.271504100944064\n",
      "l2 norm of gradients: 0.43473539902332325\n",
      "l2 norm of weights: 6.153818068373787\n",
      "---------------------\n",
      "Iteration Number: 2686\n",
      "Loss: 49.262130526465086\n",
      "l2 norm of gradients: 0.43459154947405226\n",
      "l2 norm of weights: 6.153696548454399\n",
      "---------------------\n",
      "Iteration Number: 2687\n",
      "Loss: 49.25276307743976\n",
      "l2 norm of gradients: 0.4344477922810591\n",
      "l2 norm of weights: 6.1535750543355086\n",
      "---------------------\n",
      "Iteration Number: 2688\n",
      "Loss: 49.24340174793116\n",
      "l2 norm of gradients: 0.43430412736231744\n",
      "l2 norm of weights: 6.153453585999247\n",
      "---------------------\n",
      "Iteration Number: 2689\n",
      "Loss: 49.234046532023264\n",
      "l2 norm of gradients: 0.4341605546360459\n",
      "l2 norm of weights: 6.1533321434277655\n",
      "---------------------\n",
      "Iteration Number: 2690\n",
      "Loss: 49.22469742378331\n",
      "l2 norm of gradients: 0.43401707402070755\n",
      "l2 norm of weights: 6.1532107266032305\n",
      "---------------------\n",
      "Iteration Number: 2691\n",
      "Loss: 49.21535441732837\n",
      "l2 norm of gradients: 0.43387368543500826\n",
      "l2 norm of weights: 6.153089335507822\n",
      "---------------------\n",
      "Iteration Number: 2692\n",
      "Loss: 49.20601750673614\n",
      "l2 norm of gradients: 0.433730388797897\n",
      "l2 norm of weights: 6.1529679701237345\n",
      "---------------------\n",
      "Iteration Number: 2693\n",
      "Loss: 49.19668668613066\n",
      "l2 norm of gradients: 0.4335871840285637\n",
      "l2 norm of weights: 6.1528466304331815\n",
      "---------------------\n",
      "Iteration Number: 2694\n",
      "Loss: 49.18736194962984\n",
      "l2 norm of gradients: 0.4334440710464393\n",
      "l2 norm of weights: 6.1527253164183895\n",
      "---------------------\n",
      "Iteration Number: 2695\n",
      "Loss: 49.17804329137359\n",
      "l2 norm of gradients: 0.4333010497711946\n",
      "l2 norm of weights: 6.152604028061598\n",
      "---------------------\n",
      "Iteration Number: 2696\n",
      "Loss: 49.16873070544252\n",
      "l2 norm of gradients: 0.43315812012273935\n",
      "l2 norm of weights: 6.1524827653450656\n",
      "---------------------\n",
      "Iteration Number: 2697\n",
      "Loss: 49.15942418598808\n",
      "l2 norm of gradients: 0.4330152820212213\n",
      "l2 norm of weights: 6.152361528251065\n",
      "---------------------\n",
      "Iteration Number: 2698\n",
      "Loss: 49.15012372719943\n",
      "l2 norm of gradients: 0.4328725353870257\n",
      "l2 norm of weights: 6.152240316761883\n",
      "---------------------\n",
      "Iteration Number: 2699\n",
      "Loss: 49.14082932317792\n",
      "l2 norm of gradients: 0.43272988014077407\n",
      "l2 norm of weights: 6.1521191308598215\n",
      "---------------------\n",
      "Iteration Number: 2700\n",
      "Loss: 49.13154096809851\n",
      "l2 norm of gradients: 0.43258731620332347\n",
      "l2 norm of weights: 6.1519979705272005\n",
      "---------------------\n",
      "Iteration Number: 2701\n",
      "Loss: 49.12225865612845\n",
      "l2 norm of gradients: 0.43244484349576556\n",
      "l2 norm of weights: 6.151876835746352\n",
      "---------------------\n",
      "Iteration Number: 2702\n",
      "Loss: 49.11298238146159\n",
      "l2 norm of gradients: 0.43230246193942634\n",
      "l2 norm of weights: 6.151755726499623\n",
      "---------------------\n",
      "Iteration Number: 2703\n",
      "Loss: 49.10371213825044\n",
      "l2 norm of gradients: 0.4321601714558641\n",
      "l2 norm of weights: 6.151634642769379\n",
      "---------------------\n",
      "Iteration Number: 2704\n",
      "Loss: 49.09444792067282\n",
      "l2 norm of gradients: 0.4320179719668697\n",
      "l2 norm of weights: 6.151513584537995\n",
      "---------------------\n",
      "Iteration Number: 2705\n",
      "Loss: 49.08518972295408\n",
      "l2 norm of gradients: 0.4318758633944651\n",
      "l2 norm of weights: 6.151392551787869\n",
      "---------------------\n",
      "Iteration Number: 2706\n",
      "Loss: 49.07593753928265\n",
      "l2 norm of gradients: 0.4317338456609029\n",
      "l2 norm of weights: 6.151271544501407\n",
      "---------------------\n",
      "Iteration Number: 2707\n",
      "Loss: 49.06669136384782\n",
      "l2 norm of gradients: 0.431591918688665\n",
      "l2 norm of weights: 6.151150562661033\n",
      "---------------------\n",
      "Iteration Number: 2708\n",
      "Loss: 49.05745119087742\n",
      "l2 norm of gradients: 0.43145008240046206\n",
      "l2 norm of weights: 6.151029606249185\n",
      "---------------------\n",
      "Iteration Number: 2709\n",
      "Loss: 49.04821701463741\n",
      "l2 norm of gradients: 0.43130833671923285\n",
      "l2 norm of weights: 6.150908675248319\n",
      "---------------------\n",
      "Iteration Number: 2710\n",
      "Loss: 49.03898882928176\n",
      "l2 norm of gradients: 0.431166681568143\n",
      "l2 norm of weights: 6.150787769640902\n",
      "---------------------\n",
      "Iteration Number: 2711\n",
      "Loss: 49.02976662909059\n",
      "l2 norm of gradients: 0.4310251168705841\n",
      "l2 norm of weights: 6.15066688940942\n",
      "---------------------\n",
      "Iteration Number: 2712\n",
      "Loss: 49.02055040830716\n",
      "l2 norm of gradients: 0.4308836425501734\n",
      "l2 norm of weights: 6.15054603453637\n",
      "---------------------\n",
      "Iteration Number: 2713\n",
      "Loss: 49.0113401611761\n",
      "l2 norm of gradients: 0.43074225853075265\n",
      "l2 norm of weights: 6.150425205004266\n",
      "---------------------\n",
      "Iteration Number: 2714\n",
      "Loss: 49.00213588192633\n",
      "l2 norm of gradients: 0.4306009647363869\n",
      "l2 norm of weights: 6.150304400795638\n",
      "---------------------\n",
      "Iteration Number: 2715\n",
      "Loss: 48.99293756486357\n",
      "l2 norm of gradients: 0.4304597610913644\n",
      "l2 norm of weights: 6.150183621893028\n",
      "---------------------\n",
      "Iteration Number: 2716\n",
      "Loss: 48.98374520425567\n",
      "l2 norm of gradients: 0.4303186475201949\n",
      "l2 norm of weights: 6.150062868278997\n",
      "---------------------\n",
      "Iteration Number: 2717\n",
      "Loss: 48.97455879435062\n",
      "l2 norm of gradients: 0.43017762394760967\n",
      "l2 norm of weights: 6.149942139936117\n",
      "---------------------\n",
      "Iteration Number: 2718\n",
      "Loss: 48.9653783294816\n",
      "l2 norm of gradients: 0.43003669029855984\n",
      "l2 norm of weights: 6.1498214368469775\n",
      "---------------------\n",
      "Iteration Number: 2719\n",
      "Loss: 48.95620380385683\n",
      "l2 norm of gradients: 0.4298958464982166\n",
      "l2 norm of weights: 6.149700758994182\n",
      "---------------------\n",
      "Iteration Number: 2720\n",
      "Loss: 48.947035211875395\n",
      "l2 norm of gradients: 0.429755092471969\n",
      "l2 norm of weights: 6.149580106360349\n",
      "---------------------\n",
      "Iteration Number: 2721\n",
      "Loss: 48.937872547764734\n",
      "l2 norm of gradients: 0.42961442814542417\n",
      "l2 norm of weights: 6.149459478928112\n",
      "---------------------\n",
      "Iteration Number: 2722\n",
      "Loss: 48.92871580588035\n",
      "l2 norm of gradients: 0.42947385344440636\n",
      "l2 norm of weights: 6.149338876680118\n",
      "---------------------\n",
      "Iteration Number: 2723\n",
      "Loss: 48.919564980504155\n",
      "l2 norm of gradients: 0.4293333682949555\n",
      "l2 norm of weights: 6.149218299599031\n",
      "---------------------\n",
      "Iteration Number: 2724\n",
      "Loss: 48.91042006598687\n",
      "l2 norm of gradients: 0.42919297262332695\n",
      "l2 norm of weights: 6.14909774766753\n",
      "---------------------\n",
      "Iteration Number: 2725\n",
      "Loss: 48.901281056684944\n",
      "l2 norm of gradients: 0.4290526663559905\n",
      "l2 norm of weights: 6.148977220868306\n",
      "---------------------\n",
      "Iteration Number: 2726\n",
      "Loss: 48.892147946879014\n",
      "l2 norm of gradients: 0.4289124494196295\n",
      "l2 norm of weights: 6.148856719184067\n",
      "---------------------\n",
      "Iteration Number: 2727\n",
      "Loss: 48.883020730967864\n",
      "l2 norm of gradients: 0.42877232174114\n",
      "l2 norm of weights: 6.148736242597535\n",
      "---------------------\n",
      "Iteration Number: 2728\n",
      "Loss: 48.87389940324579\n",
      "l2 norm of gradients: 0.42863228324763\n",
      "l2 norm of weights: 6.14861579109145\n",
      "---------------------\n",
      "Iteration Number: 2729\n",
      "Loss: 48.86478395814148\n",
      "l2 norm of gradients: 0.4284923338664185\n",
      "l2 norm of weights: 6.14849536464856\n",
      "---------------------\n",
      "Iteration Number: 2730\n",
      "Loss: 48.8556743899877\n",
      "l2 norm of gradients: 0.42835247352503497\n",
      "l2 norm of weights: 6.148374963251634\n",
      "---------------------\n",
      "Iteration Number: 2731\n",
      "Loss: 48.846570693130296\n",
      "l2 norm of gradients: 0.42821270215121815\n",
      "l2 norm of weights: 6.148254586883452\n",
      "---------------------\n",
      "Iteration Number: 2732\n",
      "Loss: 48.837472861969346\n",
      "l2 norm of gradients: 0.4280730196729153\n",
      "l2 norm of weights: 6.148134235526811\n",
      "---------------------\n",
      "Iteration Number: 2733\n",
      "Loss: 48.82838089091628\n",
      "l2 norm of gradients: 0.42793342601828144\n",
      "l2 norm of weights: 6.148013909164522\n",
      "---------------------\n",
      "Iteration Number: 2734\n",
      "Loss: 48.81929477432669\n",
      "l2 norm of gradients: 0.427793921115679\n",
      "l2 norm of weights: 6.147893607779411\n",
      "---------------------\n",
      "Iteration Number: 2735\n",
      "Loss: 48.81021450662924\n",
      "l2 norm of gradients: 0.4276545048936759\n",
      "l2 norm of weights: 6.147773331354317\n",
      "---------------------\n",
      "Iteration Number: 2736\n",
      "Loss: 48.801140082194955\n",
      "l2 norm of gradients: 0.4275151772810458\n",
      "l2 norm of weights: 6.1476530798720965\n",
      "---------------------\n",
      "Iteration Number: 2737\n",
      "Loss: 48.79207149547704\n",
      "l2 norm of gradients: 0.4273759382067669\n",
      "l2 norm of weights: 6.147532853315618\n",
      "---------------------\n",
      "Iteration Number: 2738\n",
      "Loss: 48.783008740847755\n",
      "l2 norm of gradients: 0.4272367876000206\n",
      "l2 norm of weights: 6.147412651667767\n",
      "---------------------\n",
      "Iteration Number: 2739\n",
      "Loss: 48.773951812768225\n",
      "l2 norm of gradients: 0.4270977253901916\n",
      "l2 norm of weights: 6.147292474911441\n",
      "---------------------\n",
      "Iteration Number: 2740\n",
      "Loss: 48.76490070565275\n",
      "l2 norm of gradients: 0.42695875150686635\n",
      "l2 norm of weights: 6.1471723230295545\n",
      "---------------------\n",
      "Iteration Number: 2741\n",
      "Loss: 48.75585541395455\n",
      "l2 norm of gradients: 0.42681986587983295\n",
      "l2 norm of weights: 6.147052196005036\n",
      "---------------------\n",
      "Iteration Number: 2742\n",
      "Loss: 48.746815932094215\n",
      "l2 norm of gradients: 0.4266810684390794\n",
      "l2 norm of weights: 6.146932093820827\n",
      "---------------------\n",
      "Iteration Number: 2743\n",
      "Loss: 48.73778225453594\n",
      "l2 norm of gradients: 0.42654235911479355\n",
      "l2 norm of weights: 6.146812016459887\n",
      "---------------------\n",
      "Iteration Number: 2744\n",
      "Loss: 48.728754375763536\n",
      "l2 norm of gradients: 0.42640373783736196\n",
      "l2 norm of weights: 6.146691963905187\n",
      "---------------------\n",
      "Iteration Number: 2745\n",
      "Loss: 48.7197322902065\n",
      "l2 norm of gradients: 0.42626520453736944\n",
      "l2 norm of weights: 6.146571936139712\n",
      "---------------------\n",
      "Iteration Number: 2746\n",
      "Loss: 48.71071599233184\n",
      "l2 norm of gradients: 0.42612675914559756\n",
      "l2 norm of weights: 6.146451933146467\n",
      "---------------------\n",
      "Iteration Number: 2747\n",
      "Loss: 48.701705476659\n",
      "l2 norm of gradients: 0.4259884015930243\n",
      "l2 norm of weights: 6.146331954908463\n",
      "---------------------\n",
      "Iteration Number: 2748\n",
      "Loss: 48.69270073762024\n",
      "l2 norm of gradients: 0.42585013181082365\n",
      "l2 norm of weights: 6.146212001408735\n",
      "---------------------\n",
      "Iteration Number: 2749\n",
      "Loss: 48.683701769721594\n",
      "l2 norm of gradients: 0.4257119497303639\n",
      "l2 norm of weights: 6.146092072630323\n",
      "---------------------\n",
      "Iteration Number: 2750\n",
      "Loss: 48.67470856746233\n",
      "l2 norm of gradients: 0.42557385528320724\n",
      "l2 norm of weights: 6.145972168556291\n",
      "---------------------\n",
      "Iteration Number: 2751\n",
      "Loss: 48.66572112536116\n",
      "l2 norm of gradients: 0.4254358484011095\n",
      "l2 norm of weights: 6.145852289169709\n",
      "---------------------\n",
      "Iteration Number: 2752\n",
      "Loss: 48.65673943790746\n",
      "l2 norm of gradients: 0.42529792901601815\n",
      "l2 norm of weights: 6.1457324344536675\n",
      "---------------------\n",
      "Iteration Number: 2753\n",
      "Loss: 48.64776349963022\n",
      "l2 norm of gradients: 0.4251600970600728\n",
      "l2 norm of weights: 6.145612604391269\n",
      "---------------------\n",
      "Iteration Number: 2754\n",
      "Loss: 48.63879330503914\n",
      "l2 norm of gradients: 0.4250223524656036\n",
      "l2 norm of weights: 6.14549279896563\n",
      "---------------------\n",
      "Iteration Number: 2755\n",
      "Loss: 48.629828848669625\n",
      "l2 norm of gradients: 0.4248846951651304\n",
      "l2 norm of weights: 6.145373018159883\n",
      "---------------------\n",
      "Iteration Number: 2756\n",
      "Loss: 48.62087012504291\n",
      "l2 norm of gradients: 0.4247471250913626\n",
      "l2 norm of weights: 6.145253261957173\n",
      "---------------------\n",
      "Iteration Number: 2757\n",
      "Loss: 48.611917128700306\n",
      "l2 norm of gradients: 0.42460964217719765\n",
      "l2 norm of weights: 6.145133530340661\n",
      "---------------------\n",
      "Iteration Number: 2758\n",
      "Loss: 48.60296985423666\n",
      "l2 norm of gradients: 0.42447224635572073\n",
      "l2 norm of weights: 6.145013823293522\n",
      "---------------------\n",
      "Iteration Number: 2759\n",
      "Loss: 48.59402829612554\n",
      "l2 norm of gradients: 0.42433493756020385\n",
      "l2 norm of weights: 6.144894140798946\n",
      "---------------------\n",
      "Iteration Number: 2760\n",
      "Loss: 48.58509244897078\n",
      "l2 norm of gradients: 0.4241977157241049\n",
      "l2 norm of weights: 6.144774482840137\n",
      "---------------------\n",
      "Iteration Number: 2761\n",
      "Loss: 48.576162307355524\n",
      "l2 norm of gradients: 0.4240605807810668\n",
      "l2 norm of weights: 6.144654849400312\n",
      "---------------------\n",
      "Iteration Number: 2762\n",
      "Loss: 48.567237865824886\n",
      "l2 norm of gradients: 0.42392353266491706\n",
      "l2 norm of weights: 6.144535240462705\n",
      "---------------------\n",
      "Iteration Number: 2763\n",
      "Loss: 48.55831911894841\n",
      "l2 norm of gradients: 0.4237865713096669\n",
      "l2 norm of weights: 6.144415656010562\n",
      "---------------------\n",
      "Iteration Number: 2764\n",
      "Loss: 48.549406061325406\n",
      "l2 norm of gradients: 0.4236496966495102\n",
      "l2 norm of weights: 6.144296096027143\n",
      "---------------------\n",
      "Iteration Number: 2765\n",
      "Loss: 48.54049868755099\n",
      "l2 norm of gradients: 0.4235129086188229\n",
      "l2 norm of weights: 6.144176560495726\n",
      "---------------------\n",
      "Iteration Number: 2766\n",
      "Loss: 48.53159699218867\n",
      "l2 norm of gradients: 0.42337620715216234\n",
      "l2 norm of weights: 6.1440570493996\n",
      "---------------------\n",
      "Iteration Number: 2767\n",
      "Loss: 48.52270096989265\n",
      "l2 norm of gradients: 0.4232395921842661\n",
      "l2 norm of weights: 6.143937562722069\n",
      "---------------------\n",
      "Iteration Number: 2768\n",
      "Loss: 48.51381061526429\n",
      "l2 norm of gradients: 0.42310306365005185\n",
      "l2 norm of weights: 6.143818100446452\n",
      "---------------------\n",
      "Iteration Number: 2769\n",
      "Loss: 48.504925922863464\n",
      "l2 norm of gradients: 0.4229666214846158\n",
      "l2 norm of weights: 6.14369866255608\n",
      "---------------------\n",
      "Iteration Number: 2770\n",
      "Loss: 48.49604688736205\n",
      "l2 norm of gradients: 0.42283026562323267\n",
      "l2 norm of weights: 6.143579249034304\n",
      "---------------------\n",
      "Iteration Number: 2771\n",
      "Loss: 48.48717350339615\n",
      "l2 norm of gradients: 0.4226939960013541\n",
      "l2 norm of weights: 6.143459859864482\n",
      "---------------------\n",
      "Iteration Number: 2772\n",
      "Loss: 48.47830576555618\n",
      "l2 norm of gradients: 0.4225578125546089\n",
      "l2 norm of weights: 6.143340495029991\n",
      "---------------------\n",
      "Iteration Number: 2773\n",
      "Loss: 48.469443668495686\n",
      "l2 norm of gradients: 0.4224217152188012\n",
      "l2 norm of weights: 6.143221154514221\n",
      "---------------------\n",
      "Iteration Number: 2774\n",
      "Loss: 48.4605872068734\n",
      "l2 norm of gradients: 0.4222857039299105\n",
      "l2 norm of weights: 6.143101838300576\n",
      "---------------------\n",
      "Iteration Number: 2775\n",
      "Loss: 48.451736375348894\n",
      "l2 norm of gradients: 0.4221497786240905\n",
      "l2 norm of weights: 6.142982546372474\n",
      "---------------------\n",
      "Iteration Number: 2776\n",
      "Loss: 48.44289116854045\n",
      "l2 norm of gradients: 0.4220139392376684\n",
      "l2 norm of weights: 6.142863278713349\n",
      "---------------------\n",
      "Iteration Number: 2777\n",
      "Loss: 48.434051581155074\n",
      "l2 norm of gradients: 0.42187818570714436\n",
      "l2 norm of weights: 6.142744035306646\n",
      "---------------------\n",
      "Iteration Number: 2778\n",
      "Loss: 48.42521760784221\n",
      "l2 norm of gradients: 0.4217425179691903\n",
      "l2 norm of weights: 6.142624816135828\n",
      "---------------------\n",
      "Iteration Number: 2779\n",
      "Loss: 48.41638924326188\n",
      "l2 norm of gradients: 0.42160693596064946\n",
      "l2 norm of weights: 6.142505621184368\n",
      "---------------------\n",
      "Iteration Number: 2780\n",
      "Loss: 48.40756648212929\n",
      "l2 norm of gradients: 0.4214714396185354\n",
      "l2 norm of weights: 6.142386450435757\n",
      "---------------------\n",
      "Iteration Number: 2781\n",
      "Loss: 48.398749319117485\n",
      "l2 norm of gradients: 0.4213360288800318\n",
      "l2 norm of weights: 6.142267303873499\n",
      "---------------------\n",
      "Iteration Number: 2782\n",
      "Loss: 48.38993774890597\n",
      "l2 norm of gradients: 0.4212007036824909\n",
      "l2 norm of weights: 6.142148181481111\n",
      "---------------------\n",
      "Iteration Number: 2783\n",
      "Loss: 48.38113176620428\n",
      "l2 norm of gradients: 0.42106546396343325\n",
      "l2 norm of weights: 6.142029083242124\n",
      "---------------------\n",
      "Iteration Number: 2784\n",
      "Loss: 48.37233136573185\n",
      "l2 norm of gradients: 0.420930309660547\n",
      "l2 norm of weights: 6.141910009140086\n",
      "---------------------\n",
      "Iteration Number: 2785\n",
      "Loss: 48.363536542183766\n",
      "l2 norm of gradients: 0.42079524071168667\n",
      "l2 norm of weights: 6.141790959158556\n",
      "---------------------\n",
      "Iteration Number: 2786\n",
      "Loss: 48.354747290263276\n",
      "l2 norm of gradients: 0.4206602570548729\n",
      "l2 norm of weights: 6.141671933281108\n",
      "---------------------\n",
      "Iteration Number: 2787\n",
      "Loss: 48.34596360473895\n",
      "l2 norm of gradients: 0.4205253586282917\n",
      "l2 norm of weights: 6.141552931491332\n",
      "---------------------\n",
      "Iteration Number: 2788\n",
      "Loss: 48.33718548031243\n",
      "l2 norm of gradients: 0.4203905453702933\n",
      "l2 norm of weights: 6.141433953772828\n",
      "---------------------\n",
      "Iteration Number: 2789\n",
      "Loss: 48.328412911714295\n",
      "l2 norm of gradients: 0.4202558172193915\n",
      "l2 norm of weights: 6.1413150001092145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Iteration Number: 2790\n",
      "Loss: 48.31964589369701\n",
      "l2 norm of gradients: 0.4201211741142632\n",
      "l2 norm of weights: 6.1411960704841215\n",
      "---------------------\n",
      "Iteration Number: 2791\n",
      "Loss: 48.31088442098876\n",
      "l2 norm of gradients: 0.4199866159937477\n",
      "l2 norm of weights: 6.141077164881194\n",
      "---------------------\n",
      "Iteration Number: 2792\n",
      "Loss: 48.30212848832877\n",
      "l2 norm of gradients: 0.4198521427968453\n",
      "l2 norm of weights: 6.140958283284091\n",
      "---------------------\n",
      "Iteration Number: 2793\n",
      "Loss: 48.29337809053443\n",
      "l2 norm of gradients: 0.41971775446271736\n",
      "l2 norm of weights: 6.140839425676484\n",
      "---------------------\n",
      "Iteration Number: 2794\n",
      "Loss: 48.28463322235574\n",
      "l2 norm of gradients: 0.4195834509306852\n",
      "l2 norm of weights: 6.140720592042061\n",
      "---------------------\n",
      "Iteration Number: 2795\n",
      "Loss: 48.27589387851914\n",
      "l2 norm of gradients: 0.4194492321402291\n",
      "l2 norm of weights: 6.140601782364523\n",
      "---------------------\n",
      "Iteration Number: 2796\n",
      "Loss: 48.26716005381203\n",
      "l2 norm of gradients: 0.41931509803098815\n",
      "l2 norm of weights: 6.1404829966275845\n",
      "---------------------\n",
      "Iteration Number: 2797\n",
      "Loss: 48.25843174305963\n",
      "l2 norm of gradients: 0.419181048542759\n",
      "l2 norm of weights: 6.1403642348149745\n",
      "---------------------\n",
      "Iteration Number: 2798\n",
      "Loss: 48.24970894100505\n",
      "l2 norm of gradients: 0.41904708361549536\n",
      "l2 norm of weights: 6.140245496910436\n",
      "---------------------\n",
      "Iteration Number: 2799\n",
      "Loss: 48.240991642454226\n",
      "l2 norm of gradients: 0.41891320318930747\n",
      "l2 norm of weights: 6.140126782897725\n",
      "---------------------\n",
      "Iteration Number: 2800\n",
      "Loss: 48.23227984221993\n",
      "l2 norm of gradients: 0.4187794072044608\n",
      "l2 norm of weights: 6.140008092760614\n",
      "---------------------\n",
      "Iteration Number: 2801\n",
      "Loss: 48.22357353506349\n",
      "l2 norm of gradients: 0.4186456956013756\n",
      "l2 norm of weights: 6.139889426482887\n",
      "---------------------\n",
      "Iteration Number: 2802\n",
      "Loss: 48.21487271586051\n",
      "l2 norm of gradients: 0.41851206832062676\n",
      "l2 norm of weights: 6.139770784048342\n",
      "---------------------\n",
      "Iteration Number: 2803\n",
      "Loss: 48.20617737938942\n",
      "l2 norm of gradients: 0.4183785253029421\n",
      "l2 norm of weights: 6.139652165440794\n",
      "---------------------\n",
      "Iteration Number: 2804\n",
      "Loss: 48.19748752045212\n",
      "l2 norm of gradients: 0.4182450664892022\n",
      "l2 norm of weights: 6.139533570644068\n",
      "---------------------\n",
      "Iteration Number: 2805\n",
      "Loss: 48.18880313392291\n",
      "l2 norm of gradients: 0.41811169182043967\n",
      "l2 norm of weights: 6.139414999642005\n",
      "---------------------\n",
      "Iteration Number: 2806\n",
      "Loss: 48.18012421461734\n",
      "l2 norm of gradients: 0.41797840123783825\n",
      "l2 norm of weights: 6.13929645241846\n",
      "---------------------\n",
      "Iteration Number: 2807\n",
      "Loss: 48.17145075735204\n",
      "l2 norm of gradients: 0.4178451946827321\n",
      "l2 norm of weights: 6.1391779289573\n",
      "---------------------\n",
      "Iteration Number: 2808\n",
      "Loss: 48.16278275699445\n",
      "l2 norm of gradients: 0.41771207209660544\n",
      "l2 norm of weights: 6.139059429242409\n",
      "---------------------\n",
      "Iteration Number: 2809\n",
      "Loss: 48.15412020839839\n",
      "l2 norm of gradients: 0.4175790334210913\n",
      "l2 norm of weights: 6.138940953257683\n",
      "---------------------\n",
      "Iteration Number: 2810\n",
      "Loss: 48.14546310640086\n",
      "l2 norm of gradients: 0.41744607859797095\n",
      "l2 norm of weights: 6.138822500987032\n",
      "---------------------\n",
      "Iteration Number: 2811\n",
      "Loss: 48.1368114458745\n",
      "l2 norm of gradients: 0.4173132075691737\n",
      "l2 norm of weights: 6.138704072414379\n",
      "---------------------\n",
      "Iteration Number: 2812\n",
      "Loss: 48.128165221701565\n",
      "l2 norm of gradients: 0.41718042027677554\n",
      "l2 norm of weights: 6.138585667523663\n",
      "---------------------\n",
      "Iteration Number: 2813\n",
      "Loss: 48.11952442874166\n",
      "l2 norm of gradients: 0.41704771666299867\n",
      "l2 norm of weights: 6.138467286298836\n",
      "---------------------\n",
      "Iteration Number: 2814\n",
      "Loss: 48.11088906185621\n",
      "l2 norm of gradients: 0.4169150966702108\n",
      "l2 norm of weights: 6.138348928723862\n",
      "---------------------\n",
      "Iteration Number: 2815\n",
      "Loss: 48.10225911594273\n",
      "l2 norm of gradients: 0.41678256024092464\n",
      "l2 norm of weights: 6.138230594782724\n",
      "---------------------\n",
      "Iteration Number: 2816\n",
      "Loss: 48.09363458590438\n",
      "l2 norm of gradients: 0.41665010731779667\n",
      "l2 norm of weights: 6.13811228445941\n",
      "---------------------\n",
      "Iteration Number: 2817\n",
      "Loss: 48.08501546661688\n",
      "l2 norm of gradients: 0.416517737843627\n",
      "l2 norm of weights: 6.137993997737931\n",
      "---------------------\n",
      "Iteration Number: 2818\n",
      "Loss: 48.07640175297094\n",
      "l2 norm of gradients: 0.41638545176135844\n",
      "l2 norm of weights: 6.137875734602307\n",
      "---------------------\n",
      "Iteration Number: 2819\n",
      "Loss: 48.06779343994005\n",
      "l2 norm of gradients: 0.4162532490140757\n",
      "l2 norm of weights: 6.137757495036572\n",
      "---------------------\n",
      "Iteration Number: 2820\n",
      "Loss: 48.059190522335385\n",
      "l2 norm of gradients: 0.41612112954500474\n",
      "l2 norm of weights: 6.137639279024776\n",
      "---------------------\n",
      "Iteration Number: 2821\n",
      "Loss: 48.05059299514419\n",
      "l2 norm of gradients: 0.4159890932975124\n",
      "l2 norm of weights: 6.13752108655098\n",
      "---------------------\n",
      "Iteration Number: 2822\n",
      "Loss: 48.042000853281976\n",
      "l2 norm of gradients: 0.41585714021510506\n",
      "l2 norm of weights: 6.13740291759926\n",
      "---------------------\n",
      "Iteration Number: 2823\n",
      "Loss: 48.033414091673684\n",
      "l2 norm of gradients: 0.41572527024142863\n",
      "l2 norm of weights: 6.137284772153706\n",
      "---------------------\n",
      "Iteration Number: 2824\n",
      "Loss: 48.02483270522068\n",
      "l2 norm of gradients: 0.4155934833202674\n",
      "l2 norm of weights: 6.137166650198423\n",
      "---------------------\n",
      "Iteration Number: 2825\n",
      "Loss: 48.01625668890603\n",
      "l2 norm of gradients: 0.41546177939554346\n",
      "l2 norm of weights: 6.137048551717525\n",
      "---------------------\n",
      "Iteration Number: 2826\n",
      "Loss: 48.007686037626016\n",
      "l2 norm of gradients: 0.415330158411316\n",
      "l2 norm of weights: 6.136930476695146\n",
      "---------------------\n",
      "Iteration Number: 2827\n",
      "Loss: 47.99912074639627\n",
      "l2 norm of gradients: 0.41519862031178106\n",
      "l2 norm of weights: 6.13681242511543\n",
      "---------------------\n",
      "Iteration Number: 2828\n",
      "Loss: 47.99056081011155\n",
      "l2 norm of gradients: 0.41506716504126995\n",
      "l2 norm of weights: 6.136694396962535\n",
      "---------------------\n",
      "Iteration Number: 2829\n",
      "Loss: 47.982006223790876\n",
      "l2 norm of gradients: 0.41493579254424934\n",
      "l2 norm of weights: 6.1365763922206344\n",
      "---------------------\n",
      "Iteration Number: 2830\n",
      "Loss: 47.973456982340664\n",
      "l2 norm of gradients: 0.4148045027653205\n",
      "l2 norm of weights: 6.136458410873914\n",
      "---------------------\n",
      "Iteration Number: 2831\n",
      "Loss: 47.96491308076519\n",
      "l2 norm of gradients: 0.41467329564921795\n",
      "l2 norm of weights: 6.1363404529065715\n",
      "---------------------\n",
      "Iteration Number: 2832\n",
      "Loss: 47.95637451403581\n",
      "l2 norm of gradients: 0.4145421711408097\n",
      "l2 norm of weights: 6.136222518302822\n",
      "---------------------\n",
      "Iteration Number: 2833\n",
      "Loss: 47.947841277161054\n",
      "l2 norm of gradients: 0.4144111291850958\n",
      "l2 norm of weights: 6.136104607046892\n",
      "---------------------\n",
      "Iteration Number: 2834\n",
      "Loss: 47.93931336507909\n",
      "l2 norm of gradients: 0.41428016972720844\n",
      "l2 norm of weights: 6.135986719123023\n",
      "---------------------\n",
      "Iteration Number: 2835\n",
      "Loss: 47.93079077283014\n",
      "l2 norm of gradients: 0.4141492927124105\n",
      "l2 norm of weights: 6.135868854515468\n",
      "---------------------\n",
      "Iteration Number: 2836\n",
      "Loss: 47.9222734953833\n",
      "l2 norm of gradients: 0.4140184980860954\n",
      "l2 norm of weights: 6.135751013208496\n",
      "---------------------\n",
      "Iteration Number: 2837\n",
      "Loss: 47.91376152775981\n",
      "l2 norm of gradients: 0.41388778579378616\n",
      "l2 norm of weights: 6.135633195186387\n",
      "---------------------\n",
      "Iteration Number: 2838\n",
      "Loss: 47.90525486494495\n",
      "l2 norm of gradients: 0.4137571557811348\n",
      "l2 norm of weights: 6.135515400433438\n",
      "---------------------\n",
      "Iteration Number: 2839\n",
      "Loss: 47.896753501969705\n",
      "l2 norm of gradients: 0.41362660799392187\n",
      "l2 norm of weights: 6.1353976289339585\n",
      "---------------------\n",
      "Iteration Number: 2840\n",
      "Loss: 47.88825743389448\n",
      "l2 norm of gradients: 0.4134961423780556\n",
      "l2 norm of weights: 6.135279880672269\n",
      "---------------------\n",
      "Iteration Number: 2841\n",
      "Loss: 47.8797666556781\n",
      "l2 norm of gradients: 0.41336575887957105\n",
      "l2 norm of weights: 6.135162155632706\n",
      "---------------------\n",
      "Iteration Number: 2842\n",
      "Loss: 47.87128116238553\n",
      "l2 norm of gradients: 0.4132354574446299\n",
      "l2 norm of weights: 6.135044453799622\n",
      "---------------------\n",
      "Iteration Number: 2843\n",
      "Loss: 47.86280094904794\n",
      "l2 norm of gradients: 0.41310523801951954\n",
      "l2 norm of weights: 6.134926775157376\n",
      "---------------------\n",
      "Iteration Number: 2844\n",
      "Loss: 47.85432601071086\n",
      "l2 norm of gradients: 0.41297510055065234\n",
      "l2 norm of weights: 6.134809119690347\n",
      "---------------------\n",
      "Iteration Number: 2845\n",
      "Loss: 47.84585634240717\n",
      "l2 norm of gradients: 0.41284504498456503\n",
      "l2 norm of weights: 6.134691487382927\n",
      "---------------------\n",
      "Iteration Number: 2846\n",
      "Loss: 47.83739193921175\n",
      "l2 norm of gradients: 0.4127150712679182\n",
      "l2 norm of weights: 6.134573878219518\n",
      "---------------------\n",
      "Iteration Number: 2847\n",
      "Loss: 47.82893279617922\n",
      "l2 norm of gradients: 0.4125851793474955\n",
      "l2 norm of weights: 6.134456292184538\n",
      "---------------------\n",
      "Iteration Number: 2848\n",
      "Loss: 47.82047890834343\n",
      "l2 norm of gradients: 0.4124553691702032\n",
      "l2 norm of weights: 6.134338729262419\n",
      "---------------------\n",
      "Iteration Number: 2849\n",
      "Loss: 47.81203027079436\n",
      "l2 norm of gradients: 0.4123256406830691\n",
      "l2 norm of weights: 6.134221189437604\n",
      "---------------------\n",
      "Iteration Number: 2850\n",
      "Loss: 47.803586878620976\n",
      "l2 norm of gradients: 0.41219599383324246\n",
      "l2 norm of weights: 6.134103672694554\n",
      "---------------------\n",
      "Iteration Number: 2851\n",
      "Loss: 47.79514872688814\n",
      "l2 norm of gradients: 0.41206642856799286\n",
      "l2 norm of weights: 6.133986179017739\n",
      "---------------------\n",
      "Iteration Number: 2852\n",
      "Loss: 47.78671581065278\n",
      "l2 norm of gradients: 0.4119369448347097\n",
      "l2 norm of weights: 6.133868708391644\n",
      "---------------------\n",
      "Iteration Number: 2853\n",
      "Loss: 47.778288125028226\n",
      "l2 norm of gradients: 0.411807542580902\n",
      "l2 norm of weights: 6.133751260800768\n",
      "---------------------\n",
      "Iteration Number: 2854\n",
      "Loss: 47.76986566513312\n",
      "l2 norm of gradients: 0.41167822175419694\n",
      "l2 norm of weights: 6.133633836229624\n",
      "---------------------\n",
      "Iteration Number: 2855\n",
      "Loss: 47.76144842602721\n",
      "l2 norm of gradients: 0.41154898230233977\n",
      "l2 norm of weights: 6.133516434662738\n",
      "---------------------\n",
      "Iteration Number: 2856\n",
      "Loss: 47.75303640281362\n",
      "l2 norm of gradients: 0.41141982417319334\n",
      "l2 norm of weights: 6.133399056084649\n",
      "---------------------\n",
      "Iteration Number: 2857\n",
      "Loss: 47.74462959062608\n",
      "l2 norm of gradients: 0.411290747314737\n",
      "l2 norm of weights: 6.133281700479909\n",
      "---------------------\n",
      "Iteration Number: 2858\n",
      "Loss: 47.73622798459097\n",
      "l2 norm of gradients: 0.41116175167506586\n",
      "l2 norm of weights: 6.1331643678330865\n",
      "---------------------\n",
      "Iteration Number: 2859\n",
      "Loss: 47.727831579778986\n",
      "l2 norm of gradients: 0.41103283720239114\n",
      "l2 norm of weights: 6.133047058128758\n",
      "---------------------\n",
      "Iteration Number: 2860\n",
      "Loss: 47.71944037136292\n",
      "l2 norm of gradients: 0.41090400384503833\n",
      "l2 norm of weights: 6.13292977135152\n",
      "---------------------\n",
      "Iteration Number: 2861\n",
      "Loss: 47.71105435445748\n",
      "l2 norm of gradients: 0.4107752515514473\n",
      "l2 norm of weights: 6.132812507485977\n",
      "---------------------\n",
      "Iteration Number: 2862\n",
      "Loss: 47.70267352418014\n",
      "l2 norm of gradients: 0.41064658027017115\n",
      "l2 norm of weights: 6.13269526651675\n",
      "---------------------\n",
      "Iteration Number: 2863\n",
      "Loss: 47.69429787568595\n",
      "l2 norm of gradients: 0.4105179899498764\n",
      "l2 norm of weights: 6.132578048428473\n",
      "---------------------\n",
      "Iteration Number: 2864\n",
      "Loss: 47.68592740413501\n",
      "l2 norm of gradients: 0.4103894805393418\n",
      "l2 norm of weights: 6.1324608532057905\n",
      "---------------------\n",
      "Iteration Number: 2865\n",
      "Loss: 47.67756210465869\n",
      "l2 norm of gradients: 0.41026105198745744\n",
      "l2 norm of weights: 6.132343680833366\n",
      "---------------------\n",
      "Iteration Number: 2866\n",
      "Loss: 47.66920197240191\n",
      "l2 norm of gradients: 0.41013270424322473\n",
      "l2 norm of weights: 6.1322265312958715\n",
      "---------------------\n",
      "Iteration Number: 2867\n",
      "Loss: 47.660847002565845\n",
      "l2 norm of gradients: 0.4100044372557555\n",
      "l2 norm of weights: 6.132109404577995\n",
      "---------------------\n",
      "Iteration Number: 2868\n",
      "Loss: 47.652497190269784\n",
      "l2 norm of gradients: 0.40987625097427155\n",
      "l2 norm of weights: 6.131992300664438\n",
      "---------------------\n",
      "Iteration Number: 2869\n",
      "Loss: 47.64415253072553\n",
      "l2 norm of gradients: 0.4097481453481036\n",
      "l2 norm of weights: 6.1318752195399115\n",
      "---------------------\n",
      "Iteration Number: 2870\n",
      "Loss: 47.63581301909971\n",
      "l2 norm of gradients: 0.4096201203266912\n",
      "l2 norm of weights: 6.131758161189145\n",
      "---------------------\n",
      "Iteration Number: 2871\n",
      "Loss: 47.62747865055505\n",
      "l2 norm of gradients: 0.4094921758595818\n",
      "l2 norm of weights: 6.131641125596879\n",
      "---------------------\n",
      "Iteration Number: 2872\n",
      "Loss: 47.61914942026411\n",
      "l2 norm of gradients: 0.40936431189643047\n",
      "l2 norm of weights: 6.131524112747868\n",
      "---------------------\n",
      "Iteration Number: 2873\n",
      "Loss: 47.61082532348357\n",
      "l2 norm of gradients: 0.4092365283869988\n",
      "l2 norm of weights: 6.131407122626878\n",
      "---------------------\n",
      "Iteration Number: 2874\n",
      "Loss: 47.602506355343515\n",
      "l2 norm of gradients: 0.4091088252811546\n",
      "l2 norm of weights: 6.131290155218691\n",
      "---------------------\n",
      "Iteration Number: 2875\n",
      "Loss: 47.59419251105473\n",
      "l2 norm of gradients: 0.4089812025288717\n",
      "l2 norm of weights: 6.131173210508102\n",
      "---------------------\n",
      "Iteration Number: 2876\n",
      "Loss: 47.58588378586976\n",
      "l2 norm of gradients: 0.4088536600802283\n",
      "l2 norm of weights: 6.1310562884799165\n",
      "---------------------\n",
      "Iteration Number: 2877\n",
      "Loss: 47.57758017494753\n",
      "l2 norm of gradients: 0.40872619788540726\n",
      "l2 norm of weights: 6.130939389118955\n",
      "---------------------\n",
      "Iteration Number: 2878\n",
      "Loss: 47.569281673523676\n",
      "l2 norm of gradients: 0.4085988158946955\n",
      "l2 norm of weights: 6.130822512410055\n",
      "---------------------\n",
      "Iteration Number: 2879\n",
      "Loss: 47.56098827685467\n",
      "l2 norm of gradients: 0.40847151405848264\n",
      "l2 norm of weights: 6.130705658338061\n",
      "---------------------\n",
      "Iteration Number: 2880\n",
      "Loss: 47.552699980102396\n",
      "l2 norm of gradients: 0.4083442923272612\n",
      "l2 norm of weights: 6.130588826887835\n",
      "---------------------\n",
      "Iteration Number: 2881\n",
      "Loss: 47.54441677853569\n",
      "l2 norm of gradients: 0.40821715065162584\n",
      "l2 norm of weights: 6.13047201804425\n",
      "---------------------\n",
      "Iteration Number: 2882\n",
      "Loss: 47.536138667381245\n",
      "l2 norm of gradients: 0.40809008898227234\n",
      "l2 norm of weights: 6.130355231792194\n",
      "---------------------\n",
      "Iteration Number: 2883\n",
      "Loss: 47.527865641885754\n",
      "l2 norm of gradients: 0.40796310726999746\n",
      "l2 norm of weights: 6.130238468116569\n",
      "---------------------\n",
      "Iteration Number: 2884\n",
      "Loss: 47.51959769728224\n",
      "l2 norm of gradients: 0.4078362054656984\n",
      "l2 norm of weights: 6.130121727002286\n",
      "---------------------\n",
      "Iteration Number: 2885\n",
      "Loss: 47.51133482885997\n",
      "l2 norm of gradients: 0.40770938352037145\n",
      "l2 norm of weights: 6.130005008434273\n",
      "---------------------\n",
      "Iteration Number: 2886\n",
      "Loss: 47.50307703181309\n",
      "l2 norm of gradients: 0.4075826413851129\n",
      "l2 norm of weights: 6.1298883123974734\n",
      "---------------------\n",
      "Iteration Number: 2887\n",
      "Loss: 47.494824301455914\n",
      "l2 norm of gradients: 0.40745597901111674\n",
      "l2 norm of weights: 6.1297716388768375\n",
      "---------------------\n",
      "Iteration Number: 2888\n",
      "Loss: 47.48657663302859\n",
      "l2 norm of gradients: 0.40732939634967513\n",
      "l2 norm of weights: 6.129654987857333\n",
      "---------------------\n",
      "Iteration Number: 2889\n",
      "Loss: 47.47833402180278\n",
      "l2 norm of gradients: 0.4072028933521778\n",
      "l2 norm of weights: 6.1295383593239405\n",
      "---------------------\n",
      "Iteration Number: 2890\n",
      "Loss: 47.47009646307938\n",
      "l2 norm of gradients: 0.40707646997011104\n",
      "l2 norm of weights: 6.129421753261654\n",
      "---------------------\n",
      "Iteration Number: 2891\n",
      "Loss: 47.46186395209481\n",
      "l2 norm of gradients: 0.40695012615505743\n",
      "l2 norm of weights: 6.129305169655479\n",
      "---------------------\n",
      "Iteration Number: 2892\n",
      "Loss: 47.453636484152156\n",
      "l2 norm of gradients: 0.40682386185869507\n",
      "l2 norm of weights: 6.129188608490435\n",
      "---------------------\n",
      "Iteration Number: 2893\n",
      "Loss: 47.445414054574016\n",
      "l2 norm of gradients: 0.40669767703279713\n",
      "l2 norm of weights: 6.129072069751557\n",
      "---------------------\n",
      "Iteration Number: 2894\n",
      "Loss: 47.43719665859246\n",
      "l2 norm of gradients: 0.40657157162923163\n",
      "l2 norm of weights: 6.128955553423889\n",
      "---------------------\n",
      "Iteration Number: 2895\n",
      "Loss: 47.42898429157083\n",
      "l2 norm of gradients: 0.40644554559996005\n",
      "l2 norm of weights: 6.128839059492493\n",
      "---------------------\n",
      "Iteration Number: 2896\n",
      "Loss: 47.42077694877457\n",
      "l2 norm of gradients: 0.4063195988970374\n",
      "l2 norm of weights: 6.128722587942439\n",
      "---------------------\n",
      "Iteration Number: 2897\n",
      "Loss: 47.41257462552308\n",
      "l2 norm of gradients: 0.40619373147261173\n",
      "l2 norm of weights: 6.128606138758815\n",
      "---------------------\n",
      "Iteration Number: 2898\n",
      "Loss: 47.40437731714277\n",
      "l2 norm of gradients: 0.40606794327892287\n",
      "l2 norm of weights: 6.128489711926718\n",
      "---------------------\n",
      "Iteration Number: 2899\n",
      "Loss: 47.39618501894315\n",
      "l2 norm of gradients: 0.40594223426830267\n",
      "l2 norm of weights: 6.128373307431262\n",
      "---------------------\n",
      "Iteration Number: 2900\n",
      "Loss: 47.38799772622763\n",
      "l2 norm of gradients: 0.40581660439317396\n",
      "l2 norm of weights: 6.128256925257571\n",
      "---------------------\n",
      "Iteration Number: 2901\n",
      "Loss: 47.37981543434337\n",
      "l2 norm of gradients: 0.40569105360605034\n",
      "l2 norm of weights: 6.128140565390784\n",
      "---------------------\n",
      "Iteration Number: 2902\n",
      "Loss: 47.37163813865652\n",
      "l2 norm of gradients: 0.40556558185953534\n",
      "l2 norm of weights: 6.128024227816053\n",
      "---------------------\n",
      "Iteration Number: 2903\n",
      "Loss: 47.363465834442216\n",
      "l2 norm of gradients: 0.40544018910632157\n",
      "l2 norm of weights: 6.127907912518542\n",
      "---------------------\n",
      "Iteration Number: 2904\n",
      "Loss: 47.355298517079596\n",
      "l2 norm of gradients: 0.40531487529919114\n",
      "l2 norm of weights: 6.127791619483428\n",
      "---------------------\n",
      "Iteration Number: 2905\n",
      "Loss: 47.34713618193351\n",
      "l2 norm of gradients: 0.4051896403910143\n",
      "l2 norm of weights: 6.127675348695905\n",
      "---------------------\n",
      "Iteration Number: 2906\n",
      "Loss: 47.33897882431057\n",
      "l2 norm of gradients: 0.405064484334749\n",
      "l2 norm of weights: 6.127559100141175\n",
      "---------------------\n",
      "Iteration Number: 2907\n",
      "Loss: 47.33082643961958\n",
      "l2 norm of gradients: 0.40493940708344056\n",
      "l2 norm of weights: 6.127442873804455\n",
      "---------------------\n",
      "Iteration Number: 2908\n",
      "Loss: 47.32267902317737\n",
      "l2 norm of gradients: 0.40481440859022094\n",
      "l2 norm of weights: 6.1273266696709765\n",
      "---------------------\n",
      "Iteration Number: 2909\n",
      "Loss: 47.31453657036684\n",
      "l2 norm of gradients: 0.4046894888083087\n",
      "l2 norm of weights: 6.127210487725982\n",
      "---------------------\n",
      "Iteration Number: 2910\n",
      "Loss: 47.30639907657402\n",
      "l2 norm of gradients: 0.40456464769100736\n",
      "l2 norm of weights: 6.127094327954729\n",
      "---------------------\n",
      "Iteration Number: 2911\n",
      "Loss: 47.298266537132356\n",
      "l2 norm of gradients: 0.40443988519170615\n",
      "l2 norm of weights: 6.126978190342486\n",
      "---------------------\n",
      "Iteration Number: 2912\n",
      "Loss: 47.290138947485715\n",
      "l2 norm of gradients: 0.40431520126387865\n",
      "l2 norm of weights: 6.126862074874536\n",
      "---------------------\n",
      "Iteration Number: 2913\n",
      "Loss: 47.28201630297999\n",
      "l2 norm of gradients: 0.40419059586108236\n",
      "l2 norm of weights: 6.126745981536176\n",
      "---------------------\n",
      "Iteration Number: 2914\n",
      "Loss: 47.27389859901926\n",
      "l2 norm of gradients: 0.4040660689369586\n",
      "l2 norm of weights: 6.126629910312713\n",
      "---------------------\n",
      "Iteration Number: 2915\n",
      "Loss: 47.26578583098275\n",
      "l2 norm of gradients: 0.40394162044523146\n",
      "l2 norm of weights: 6.126513861189468\n",
      "---------------------\n",
      "Iteration Number: 2916\n",
      "Loss: 47.257677994286496\n",
      "l2 norm of gradients: 0.40381725033970745\n",
      "l2 norm of weights: 6.1263978341517795\n",
      "---------------------\n",
      "Iteration Number: 2917\n",
      "Loss: 47.24957508431846\n",
      "l2 norm of gradients: 0.403692958574275\n",
      "l2 norm of weights: 6.126281829184993\n",
      "---------------------\n",
      "Iteration Number: 2918\n",
      "Loss: 47.241477096498286\n",
      "l2 norm of gradients: 0.4035687451029043\n",
      "l2 norm of weights: 6.1261658462744695\n",
      "---------------------\n",
      "Iteration Number: 2919\n",
      "Loss: 47.233384026258776\n",
      "l2 norm of gradients: 0.40344460987964603\n",
      "l2 norm of weights: 6.1260498854055845\n",
      "---------------------\n",
      "Iteration Number: 2920\n",
      "Loss: 47.225295868991715\n",
      "l2 norm of gradients: 0.40332055285863117\n",
      "l2 norm of weights: 6.125933946563723\n",
      "---------------------\n",
      "Iteration Number: 2921\n",
      "Loss: 47.21721262012912\n",
      "l2 norm of gradients: 0.40319657399407094\n",
      "l2 norm of weights: 6.125818029734286\n",
      "---------------------\n",
      "Iteration Number: 2922\n",
      "Loss: 47.209134275101356\n",
      "l2 norm of gradients: 0.4030726732402555\n",
      "l2 norm of weights: 6.125702134902687\n",
      "---------------------\n",
      "Iteration Number: 2923\n",
      "Loss: 47.20106082933138\n",
      "l2 norm of gradients: 0.40294885055155405\n",
      "l2 norm of weights: 6.125586262054352\n",
      "---------------------\n",
      "Iteration Number: 2924\n",
      "Loss: 47.19299227826747\n",
      "l2 norm of gradients: 0.40282510588241405\n",
      "l2 norm of weights: 6.125470411174718\n",
      "---------------------\n",
      "Iteration Number: 2925\n",
      "Loss: 47.18492861733466\n",
      "l2 norm of gradients: 0.4027014391873605\n",
      "l2 norm of weights: 6.125354582249241\n",
      "---------------------\n",
      "Iteration Number: 2926\n",
      "Loss: 47.17686984199794\n",
      "l2 norm of gradients: 0.402577850420996\n",
      "l2 norm of weights: 6.125238775263382\n",
      "---------------------\n",
      "Iteration Number: 2927\n",
      "Loss: 47.1688159477139\n",
      "l2 norm of gradients: 0.40245433953799953\n",
      "l2 norm of weights: 6.125122990202621\n",
      "---------------------\n",
      "Iteration Number: 2928\n",
      "Loss: 47.16076692992613\n",
      "l2 norm of gradients: 0.40233090649312686\n",
      "l2 norm of weights: 6.125007227052449\n",
      "---------------------\n",
      "Iteration Number: 2929\n",
      "Loss: 47.15272278408172\n",
      "l2 norm of gradients: 0.40220755124120905\n",
      "l2 norm of weights: 6.1248914857983685\n",
      "---------------------\n",
      "Iteration Number: 2930\n",
      "Loss: 47.14468350566417\n",
      "l2 norm of gradients: 0.40208427373715233\n",
      "l2 norm of weights: 6.1247757664258975\n",
      "---------------------\n",
      "Iteration Number: 2931\n",
      "Loss: 47.13664909013545\n",
      "l2 norm of gradients: 0.4019610739359382\n",
      "l2 norm of weights: 6.124660068920567\n",
      "---------------------\n",
      "Iteration Number: 2932\n",
      "Loss: 47.128619532965004\n",
      "l2 norm of gradients: 0.4018379517926219\n",
      "l2 norm of weights: 6.124544393267916\n",
      "---------------------\n",
      "Iteration Number: 2933\n",
      "Loss: 47.120594829644624\n",
      "l2 norm of gradients: 0.4017149072623325\n",
      "l2 norm of weights: 6.124428739453504\n",
      "---------------------\n",
      "Iteration Number: 2934\n",
      "Loss: 47.112574975655484\n",
      "l2 norm of gradients: 0.4015919403002726\n",
      "l2 norm of weights: 6.124313107462898\n",
      "---------------------\n",
      "Iteration Number: 2935\n",
      "Loss: 47.104559966474376\n",
      "l2 norm of gradients: 0.40146905086171725\n",
      "l2 norm of weights: 6.124197497281679\n",
      "---------------------\n",
      "Iteration Number: 2936\n",
      "Loss: 47.09654979760014\n",
      "l2 norm of gradients: 0.4013462389020138\n",
      "l2 norm of weights: 6.124081908895443\n",
      "---------------------\n",
      "Iteration Number: 2937\n",
      "Loss: 47.088544464513475\n",
      "l2 norm of gradients: 0.40122350437658155\n",
      "l2 norm of weights: 6.123966342289795\n",
      "---------------------\n",
      "Iteration Number: 2938\n",
      "Loss: 47.080543962762654\n",
      "l2 norm of gradients: 0.4011008472409106\n",
      "l2 norm of weights: 6.123850797450356\n",
      "---------------------\n",
      "Iteration Number: 2939\n",
      "Loss: 47.07254828780224\n",
      "l2 norm of gradients: 0.40097826745056264\n",
      "l2 norm of weights: 6.123735274362761\n",
      "---------------------\n",
      "Iteration Number: 2940\n",
      "Loss: 47.06455743517128\n",
      "l2 norm of gradients: 0.40085576496116887\n",
      "l2 norm of weights: 6.1236197730126545\n",
      "---------------------\n",
      "Iteration Number: 2941\n",
      "Loss: 47.056571400367595\n",
      "l2 norm of gradients: 0.4007333397284306\n",
      "l2 norm of weights: 6.123504293385695\n",
      "---------------------\n",
      "Iteration Number: 2942\n",
      "Loss: 47.04859017892183\n",
      "l2 norm of gradients: 0.4006109917081186\n",
      "l2 norm of weights: 6.123388835467554\n",
      "---------------------\n",
      "Iteration Number: 2943\n",
      "Loss: 47.04061376634821\n",
      "l2 norm of gradients: 0.4004887208560725\n",
      "l2 norm of weights: 6.123273399243918\n",
      "---------------------\n",
      "Iteration Number: 2944\n",
      "Loss: 47.03264215818423\n",
      "l2 norm of gradients: 0.4003665271281999\n",
      "l2 norm of weights: 6.123157984700482\n",
      "---------------------\n",
      "Iteration Number: 2945\n",
      "Loss: 47.02467534994976\n",
      "l2 norm of gradients: 0.4002444104804769\n",
      "l2 norm of weights: 6.1230425918229585\n",
      "---------------------\n",
      "Iteration Number: 2946\n",
      "Loss: 47.01671333722102\n",
      "l2 norm of gradients: 0.4001223708689464\n",
      "l2 norm of weights: 6.12292722059707\n",
      "---------------------\n",
      "Iteration Number: 2947\n",
      "Loss: 47.00875611549329\n",
      "l2 norm of gradients: 0.4000004082497188\n",
      "l2 norm of weights: 6.12281187100855\n",
      "---------------------\n",
      "Iteration Number: 2948\n",
      "Loss: 47.00080368031808\n",
      "l2 norm of gradients: 0.3998785225789706\n",
      "l2 norm of weights: 6.122696543043151\n",
      "---------------------\n",
      "Iteration Number: 2949\n",
      "Loss: 46.99285602727537\n",
      "l2 norm of gradients: 0.3997567138129445\n",
      "l2 norm of weights: 6.122581236686633\n",
      "---------------------\n",
      "Iteration Number: 2950\n",
      "Loss: 46.984913151885934\n",
      "l2 norm of gradients: 0.3996349819079484\n",
      "l2 norm of weights: 6.122465951924771\n",
      "---------------------\n",
      "Iteration Number: 2951\n",
      "Loss: 46.97697504974748\n",
      "l2 norm of gradients: 0.3995133268203559\n",
      "l2 norm of weights: 6.122350688743351\n",
      "---------------------\n",
      "Iteration Number: 2952\n",
      "Loss: 46.96904171640317\n",
      "l2 norm of gradients: 0.3993917485066045\n",
      "l2 norm of weights: 6.122235447128174\n",
      "---------------------\n",
      "Iteration Number: 2953\n",
      "Loss: 46.96111314741418\n",
      "l2 norm of gradients: 0.3992702469231964\n",
      "l2 norm of weights: 6.122120227065052\n",
      "---------------------\n",
      "Iteration Number: 2954\n",
      "Loss: 46.95318933836793\n",
      "l2 norm of gradients: 0.39914882202669716\n",
      "l2 norm of weights: 6.122005028539812\n",
      "---------------------\n",
      "Iteration Number: 2955\n",
      "Loss: 46.9452702848214\n",
      "l2 norm of gradients: 0.39902747377373554\n",
      "l2 norm of weights: 6.121889851538293\n",
      "---------------------\n",
      "Iteration Number: 2956\n",
      "Loss: 46.93735598238384\n",
      "l2 norm of gradients: 0.3989062021210034\n",
      "l2 norm of weights: 6.1217746960463435\n",
      "---------------------\n",
      "Iteration Number: 2957\n",
      "Loss: 46.929446426620615\n",
      "l2 norm of gradients: 0.3987850070252543\n",
      "l2 norm of weights: 6.121659562049829\n",
      "---------------------\n",
      "Iteration Number: 2958\n",
      "Loss: 46.92154161313751\n",
      "l2 norm of gradients: 0.39866388844330447\n",
      "l2 norm of weights: 6.1215444495346265\n",
      "---------------------\n",
      "Iteration Number: 2959\n",
      "Loss: 46.91364153751784\n",
      "l2 norm of gradients: 0.39854284633203063\n",
      "l2 norm of weights: 6.121429358486625\n",
      "---------------------\n",
      "Iteration Number: 2960\n",
      "Loss: 46.905746195369346\n",
      "l2 norm of gradients: 0.39842188064837114\n",
      "l2 norm of weights: 6.121314288891727\n",
      "---------------------\n",
      "Iteration Number: 2961\n",
      "Loss: 46.89785558228486\n",
      "l2 norm of gradients: 0.3983009913493246\n",
      "l2 norm of weights: 6.121199240735848\n",
      "---------------------\n",
      "Iteration Number: 2962\n",
      "Loss: 46.88996969388523\n",
      "l2 norm of gradients: 0.3981801783919495\n",
      "l2 norm of weights: 6.121084214004916\n",
      "---------------------\n",
      "Iteration Number: 2963\n",
      "Loss: 46.882088525767465\n",
      "l2 norm of gradients: 0.39805944173336416\n",
      "l2 norm of weights: 6.120969208684869\n",
      "---------------------\n",
      "Iteration Number: 2964\n",
      "Loss: 46.874212073570035\n",
      "l2 norm of gradients: 0.39793878133074606\n",
      "l2 norm of weights: 6.120854224761663\n",
      "---------------------\n",
      "Iteration Number: 2965\n",
      "Loss: 46.866340332898154\n",
      "l2 norm of gradients: 0.39781819714133104\n",
      "l2 norm of weights: 6.120739262221262\n",
      "---------------------\n",
      "Iteration Number: 2966\n",
      "Loss: 46.85847329936743\n",
      "l2 norm of gradients: 0.39769768912241393\n",
      "l2 norm of weights: 6.120624321049647\n",
      "---------------------\n",
      "Iteration Number: 2967\n",
      "Loss: 46.85061096864806\n",
      "l2 norm of gradients: 0.3975772572313466\n",
      "l2 norm of weights: 6.120509401232805\n",
      "---------------------\n",
      "Iteration Number: 2968\n",
      "Loss: 46.842753336324435\n",
      "l2 norm of gradients: 0.3974569014255388\n",
      "l2 norm of weights: 6.120394502756746\n",
      "---------------------\n",
      "Iteration Number: 2969\n",
      "Loss: 46.8349003980922\n",
      "l2 norm of gradients: 0.39733662166245703\n",
      "l2 norm of weights: 6.120279625607482\n",
      "---------------------\n",
      "Iteration Number: 2970\n",
      "Loss: 46.827052149521506\n",
      "l2 norm of gradients: 0.39721641789962436\n",
      "l2 norm of weights: 6.120164769771044\n",
      "---------------------\n",
      "Iteration Number: 2971\n",
      "Loss: 46.81920858633114\n",
      "l2 norm of gradients: 0.3970962900946201\n",
      "l2 norm of weights: 6.120049935233474\n",
      "---------------------\n",
      "Iteration Number: 2972\n",
      "Loss: 46.8113697041304\n",
      "l2 norm of gradients: 0.39697623820507894\n",
      "l2 norm of weights: 6.119935121980827\n",
      "---------------------\n",
      "Iteration Number: 2973\n",
      "Loss: 46.80353549857918\n",
      "l2 norm of gradients: 0.3968562621886912\n",
      "l2 norm of weights: 6.11982032999917\n",
      "---------------------\n",
      "Iteration Number: 2974\n",
      "Loss: 46.79570596535922\n",
      "l2 norm of gradients: 0.3967363620032015\n",
      "l2 norm of weights: 6.119705559274584\n",
      "---------------------\n",
      "Iteration Number: 2975\n",
      "Loss: 46.78788110012416\n",
      "l2 norm of gradients: 0.39661653760640925\n",
      "l2 norm of weights: 6.11959080979316\n",
      "---------------------\n",
      "Iteration Number: 2976\n",
      "Loss: 46.780060898519785\n",
      "l2 norm of gradients: 0.3964967889561677\n",
      "l2 norm of weights: 6.119476081541006\n",
      "---------------------\n",
      "Iteration Number: 2977\n",
      "Loss: 46.77224535627115\n",
      "l2 norm of gradients: 0.3963771160103835\n",
      "l2 norm of weights: 6.119361374504236\n",
      "---------------------\n",
      "Iteration Number: 2978\n",
      "Loss: 46.76443446901379\n",
      "l2 norm of gradients: 0.3962575187270165\n",
      "l2 norm of weights: 6.119246688668985\n",
      "---------------------\n",
      "Iteration Number: 2979\n",
      "Loss: 46.75662823241996\n",
      "l2 norm of gradients: 0.3961379970640794\n",
      "l2 norm of weights: 6.119132024021393\n",
      "---------------------\n",
      "Iteration Number: 2980\n",
      "Loss: 46.748826642212386\n",
      "l2 norm of gradients: 0.39601855097963706\n",
      "l2 norm of weights: 6.11901738054762\n",
      "---------------------\n",
      "Iteration Number: 2981\n",
      "Loss: 46.74102969406653\n",
      "l2 norm of gradients: 0.39589918043180605\n",
      "l2 norm of weights: 6.118902758233829\n",
      "---------------------\n",
      "Iteration Number: 2982\n",
      "Loss: 46.733237383694856\n",
      "l2 norm of gradients: 0.39577988537875464\n",
      "l2 norm of weights: 6.118788157066206\n",
      "---------------------\n",
      "Iteration Number: 2983\n",
      "Loss: 46.725449706735034\n",
      "l2 norm of gradients: 0.39566066577870185\n",
      "l2 norm of weights: 6.1186735770309415\n",
      "---------------------\n",
      "Iteration Number: 2984\n",
      "Loss: 46.717666658952204\n",
      "l2 norm of gradients: 0.3955415215899178\n",
      "l2 norm of weights: 6.118559018114243\n",
      "---------------------\n",
      "Iteration Number: 2985\n",
      "Loss: 46.70988823604412\n",
      "l2 norm of gradients: 0.3954224527707222\n",
      "l2 norm of weights: 6.11844448030233\n",
      "---------------------\n",
      "Iteration Number: 2986\n",
      "Loss: 46.702114433704054\n",
      "l2 norm of gradients: 0.39530345927948507\n",
      "l2 norm of weights: 6.1183299635814326\n",
      "---------------------\n",
      "Iteration Number: 2987\n",
      "Loss: 46.69434524766634\n",
      "l2 norm of gradients: 0.3951845410746256\n",
      "l2 norm of weights: 6.118215467937796\n",
      "---------------------\n",
      "Iteration Number: 2988\n",
      "Loss: 46.686580673641515\n",
      "l2 norm of gradients: 0.39506569811461206\n",
      "l2 norm of weights: 6.118100993357677\n",
      "---------------------\n",
      "Iteration Number: 2989\n",
      "Loss: 46.678820707341046\n",
      "l2 norm of gradients: 0.39494693035796136\n",
      "l2 norm of weights: 6.117986539827344\n",
      "---------------------\n",
      "Iteration Number: 2990\n",
      "Loss: 46.671065344507326\n",
      "l2 norm of gradients: 0.39482823776323844\n",
      "l2 norm of weights: 6.1178721073330795\n",
      "---------------------\n",
      "Iteration Number: 2991\n",
      "Loss: 46.66331458087651\n",
      "l2 norm of gradients: 0.3947096202890562\n",
      "l2 norm of weights: 6.117757695861178\n",
      "---------------------\n",
      "Iteration Number: 2992\n",
      "Loss: 46.655568412192174\n",
      "l2 norm of gradients: 0.394591077894075\n",
      "l2 norm of weights: 6.1176433053979435\n",
      "---------------------\n",
      "Iteration Number: 2993\n",
      "Loss: 46.64782683417142\n",
      "l2 norm of gradients: 0.39447261053700194\n",
      "l2 norm of weights: 6.117528935929699\n",
      "---------------------\n",
      "Iteration Number: 2994\n",
      "Loss: 46.64008984257402\n",
      "l2 norm of gradients: 0.394354218176591\n",
      "l2 norm of weights: 6.117414587442775\n",
      "---------------------\n",
      "Iteration Number: 2995\n",
      "Loss: 46.632357433146645\n",
      "l2 norm of gradients: 0.39423590077164233\n",
      "l2 norm of weights: 6.117300259923517\n",
      "---------------------\n",
      "Iteration Number: 2996\n",
      "Loss: 46.62462960164316\n",
      "l2 norm of gradients: 0.39411765828100165\n",
      "l2 norm of weights: 6.1171859533582795\n",
      "---------------------\n",
      "Iteration Number: 2997\n",
      "Loss: 46.616906343843624\n",
      "l2 norm of gradients: 0.3939994906635607\n",
      "l2 norm of weights: 6.117071667733434\n",
      "---------------------\n",
      "Iteration Number: 2998\n",
      "Loss: 46.60918765547413\n",
      "l2 norm of gradients: 0.39388139787825555\n",
      "l2 norm of weights: 6.116957403035361\n",
      "---------------------\n",
      "Iteration Number: 2999\n",
      "Loss: 46.601473532311715\n",
      "l2 norm of gradients: 0.3937633798840677\n",
      "l2 norm of weights: 6.116843159250457\n",
      "---------------------\n",
      "Iteration Number: 3000\n",
      "Loss: 46.59376397016377\n",
      "l2 norm of gradients: 0.3936454366400222\n",
      "l2 norm of weights: 6.116728936365127\n",
      "---------------------\n",
      "Iteration Number: 3001\n",
      "Loss: 46.58605896474652\n",
      "l2 norm of gradients: 0.3935275681051885\n",
      "l2 norm of weights: 6.1166147343657915\n",
      "---------------------\n",
      "Iteration Number: 3002\n",
      "Loss: 46.57835851187578\n",
      "l2 norm of gradients: 0.39340977423867957\n",
      "l2 norm of weights: 6.1165005532388825\n",
      "---------------------\n",
      "Iteration Number: 3003\n",
      "Loss: 46.57066260732238\n",
      "l2 norm of gradients: 0.3932920549996511\n",
      "l2 norm of weights: 6.116386392970844\n",
      "---------------------\n",
      "Iteration Number: 3004\n",
      "Loss: 46.56297124686627\n",
      "l2 norm of gradients: 0.39317441034730216\n",
      "l2 norm of weights: 6.116272253548132\n",
      "---------------------\n",
      "Iteration Number: 3005\n",
      "Loss: 46.55528442633297\n",
      "l2 norm of gradients: 0.39305684024087384\n",
      "l2 norm of weights: 6.116158134957217\n",
      "---------------------\n",
      "Iteration Number: 3006\n",
      "Loss: 46.54760214145495\n",
      "l2 norm of gradients: 0.3929393446396492\n",
      "l2 norm of weights: 6.116044037184581\n",
      "---------------------\n",
      "Iteration Number: 3007\n",
      "Loss: 46.53992438812333\n",
      "l2 norm of gradients: 0.39282192350295303\n",
      "l2 norm of weights: 6.115929960216718\n",
      "---------------------\n",
      "Iteration Number: 3008\n",
      "Loss: 46.532251162056724\n",
      "l2 norm of gradients: 0.3927045767901515\n",
      "l2 norm of weights: 6.115815904040134\n",
      "---------------------\n",
      "Iteration Number: 3009\n",
      "Loss: 46.52458245907274\n",
      "l2 norm of gradients: 0.3925873044606518\n",
      "l2 norm of weights: 6.115701868641348\n",
      "---------------------\n",
      "Iteration Number: 3010\n",
      "Loss: 46.51691827504942\n",
      "l2 norm of gradients: 0.39247010647390135\n",
      "l2 norm of weights: 6.115587854006892\n",
      "---------------------\n",
      "Iteration Number: 3011\n",
      "Loss: 46.5092586057342\n",
      "l2 norm of gradients: 0.39235298278938774\n",
      "l2 norm of weights: 6.115473860123311\n",
      "---------------------\n",
      "Iteration Number: 3012\n",
      "Loss: 46.50160344699585\n",
      "l2 norm of gradients: 0.39223593336663887\n",
      "l2 norm of weights: 6.11535988697716\n",
      "---------------------\n",
      "Iteration Number: 3013\n",
      "Loss: 46.49395279461609\n",
      "l2 norm of gradients: 0.39211895816522135\n",
      "l2 norm of weights: 6.1152459345550065\n",
      "---------------------\n",
      "Iteration Number: 3014\n",
      "Loss: 46.48630664444332\n",
      "l2 norm of gradients: 0.3920020571447417\n",
      "l2 norm of weights: 6.115132002843434\n",
      "---------------------\n",
      "Iteration Number: 3015\n",
      "Loss: 46.47866499231522\n",
      "l2 norm of gradients: 0.39188523026484434\n",
      "l2 norm of weights: 6.115018091829036\n",
      "---------------------\n",
      "Iteration Number: 3016\n",
      "Loss: 46.47102783405628\n",
      "l2 norm of gradients: 0.3917684774852128\n",
      "l2 norm of weights: 6.114904201498417\n",
      "---------------------\n",
      "Iteration Number: 3017\n",
      "Loss: 46.46339516552926\n",
      "l2 norm of gradients: 0.3916517987655681\n",
      "l2 norm of weights: 6.114790331838195\n",
      "---------------------\n",
      "Iteration Number: 3018\n",
      "Loss: 46.45576698255898\n",
      "l2 norm of gradients: 0.39153519406566933\n",
      "l2 norm of weights: 6.114676482835003\n",
      "---------------------\n",
      "Iteration Number: 3019\n",
      "Loss: 46.44814328098573\n",
      "l2 norm of gradients: 0.3914186633453125\n",
      "l2 norm of weights: 6.114562654475482\n",
      "---------------------\n",
      "Iteration Number: 3020\n",
      "Loss: 46.44052405668068\n",
      "l2 norm of gradients: 0.3913022065643309\n",
      "l2 norm of weights: 6.114448846746289\n",
      "---------------------\n",
      "Iteration Number: 3021\n",
      "Loss: 46.43290930550049\n",
      "l2 norm of gradients: 0.39118582368259436\n",
      "l2 norm of weights: 6.11433505963409\n",
      "---------------------\n",
      "Iteration Number: 3022\n",
      "Loss: 46.42529902331061\n",
      "l2 norm of gradients: 0.3910695146600088\n",
      "l2 norm of weights: 6.114221293125566\n",
      "---------------------\n",
      "Iteration Number: 3023\n",
      "Loss: 46.41769320593591\n",
      "l2 norm of gradients: 0.3909532794565161\n",
      "l2 norm of weights: 6.11410754720741\n",
      "---------------------\n",
      "Iteration Number: 3024\n",
      "Loss: 46.41009184930132\n",
      "l2 norm of gradients: 0.390837118032094\n",
      "l2 norm of weights: 6.113993821866326\n",
      "---------------------\n",
      "Iteration Number: 3025\n",
      "Loss: 46.4024949492492\n",
      "l2 norm of gradients: 0.39072103034675515\n",
      "l2 norm of weights: 6.113880117089032\n",
      "---------------------\n",
      "Iteration Number: 3026\n",
      "Loss: 46.39490250166223\n",
      "l2 norm of gradients: 0.3906050163605473\n",
      "l2 norm of weights: 6.113766432862256\n",
      "---------------------\n",
      "Iteration Number: 3027\n",
      "Loss: 46.387314502424225\n",
      "l2 norm of gradients: 0.39048907603355254\n",
      "l2 norm of weights: 6.113652769172742\n",
      "---------------------\n",
      "Iteration Number: 3028\n",
      "Loss: 46.37973094740501\n",
      "l2 norm of gradients: 0.3903732093258875\n",
      "l2 norm of weights: 6.113539126007241\n",
      "---------------------\n",
      "Iteration Number: 3029\n",
      "Loss: 46.37215183253283\n",
      "l2 norm of gradients: 0.39025741619770227\n",
      "l2 norm of weights: 6.113425503352523\n",
      "---------------------\n",
      "Iteration Number: 3030\n",
      "Loss: 46.36457715366341\n",
      "l2 norm of gradients: 0.3901416966091808\n",
      "l2 norm of weights: 6.113311901195364\n",
      "---------------------\n",
      "Iteration Number: 3031\n",
      "Loss: 46.35700690669016\n",
      "l2 norm of gradients: 0.3900260505205401\n",
      "l2 norm of weights: 6.113198319522556\n",
      "---------------------\n",
      "Iteration Number: 3032\n",
      "Loss: 46.34944108754543\n",
      "l2 norm of gradients: 0.3899104778920301\n",
      "l2 norm of weights: 6.113084758320902\n",
      "---------------------\n",
      "Iteration Number: 3033\n",
      "Loss: 46.341879692112684\n",
      "l2 norm of gradients: 0.38979497868393337\n",
      "l2 norm of weights: 6.1129712175772175\n",
      "---------------------\n",
      "Iteration Number: 3034\n",
      "Loss: 46.33432271631539\n",
      "l2 norm of gradients: 0.38967955285656447\n",
      "l2 norm of weights: 6.112857697278331\n",
      "---------------------\n",
      "Iteration Number: 3035\n",
      "Loss: 46.32677015606304\n",
      "l2 norm of gradients: 0.38956420037026995\n",
      "l2 norm of weights: 6.112744197411081\n",
      "---------------------\n",
      "Iteration Number: 3036\n",
      "Loss: 46.31922200724414\n",
      "l2 norm of gradients: 0.38944892118542807\n",
      "l2 norm of weights: 6.112630717962321\n",
      "---------------------\n",
      "Iteration Number: 3037\n",
      "Loss: 46.31167826583332\n",
      "l2 norm of gradients: 0.3893337152624481\n",
      "l2 norm of weights: 6.112517258918916\n",
      "---------------------\n",
      "Iteration Number: 3038\n",
      "Loss: 46.304138927721475\n",
      "l2 norm of gradients: 0.3892185825617703\n",
      "l2 norm of weights: 6.112403820267742\n",
      "---------------------\n",
      "Iteration Number: 3039\n",
      "Loss: 46.29660398883973\n",
      "l2 norm of gradients: 0.38910352304386575\n",
      "l2 norm of weights: 6.112290401995689\n",
      "---------------------\n",
      "Iteration Number: 3040\n",
      "Loss: 46.28907344512227\n",
      "l2 norm of gradients: 0.3889885366692356\n",
      "l2 norm of weights: 6.1121770040896575\n",
      "---------------------\n",
      "Iteration Number: 3041\n",
      "Loss: 46.28154729253194\n",
      "l2 norm of gradients: 0.38887362339841086\n",
      "l2 norm of weights: 6.11206362653656\n",
      "---------------------\n",
      "Iteration Number: 3042\n",
      "Loss: 46.27402552696974\n",
      "l2 norm of gradients: 0.3887587831919524\n",
      "l2 norm of weights: 6.111950269323325\n",
      "---------------------\n",
      "Iteration Number: 3043\n",
      "Loss: 46.266508144393704\n",
      "l2 norm of gradients: 0.3886440160104504\n",
      "l2 norm of weights: 6.111836932436888\n",
      "---------------------\n",
      "Iteration Number: 3044\n",
      "Loss: 46.25899514077251\n",
      "l2 norm of gradients: 0.3885293218145237\n",
      "l2 norm of weights: 6.1117236158642\n",
      "---------------------\n",
      "Iteration Number: 3045\n",
      "Loss: 46.251486512030255\n",
      "l2 norm of gradients: 0.3884147005648205\n",
      "l2 norm of weights: 6.111610319592224\n",
      "---------------------\n",
      "Iteration Number: 3046\n",
      "Loss: 46.243982254155966\n",
      "l2 norm of gradients: 0.3883001522220168\n",
      "l2 norm of weights: 6.111497043607933\n",
      "---------------------\n",
      "Iteration Number: 3047\n",
      "Loss: 46.23648236309063\n",
      "l2 norm of gradients: 0.38818567674681714\n",
      "l2 norm of weights: 6.1113837878983155\n",
      "---------------------\n",
      "Iteration Number: 3048\n",
      "Loss: 46.22898683478847\n",
      "l2 norm of gradients: 0.3880712740999534\n",
      "l2 norm of weights: 6.111270552450369\n",
      "---------------------\n",
      "Iteration Number: 3049\n",
      "Loss: 46.22149566523981\n",
      "l2 norm of gradients: 0.3879569442421852\n",
      "l2 norm of weights: 6.111157337251106\n",
      "---------------------\n",
      "Iteration Number: 3050\n",
      "Loss: 46.21400885040929\n",
      "l2 norm of gradients: 0.38784268713429937\n",
      "l2 norm of weights: 6.111044142287548\n",
      "---------------------\n",
      "Iteration Number: 3051\n",
      "Loss: 46.20652638628757\n",
      "l2 norm of gradients: 0.3877285027371097\n",
      "l2 norm of weights: 6.110930967546732\n",
      "---------------------\n",
      "Iteration Number: 3052\n",
      "Loss: 46.19904826882927\n",
      "l2 norm of gradients: 0.38761439101145584\n",
      "l2 norm of weights: 6.110817813015705\n",
      "---------------------\n",
      "Iteration Number: 3053\n",
      "Loss: 46.19157449404614\n",
      "l2 norm of gradients: 0.3875003519182048\n",
      "l2 norm of weights: 6.110704678681527\n",
      "---------------------\n",
      "Iteration Number: 3054\n",
      "Loss: 46.18410505788657\n",
      "l2 norm of gradients: 0.3873863854182489\n",
      "l2 norm of weights: 6.110591564531269\n",
      "---------------------\n",
      "Iteration Number: 3055\n",
      "Loss: 46.17663995637706\n",
      "l2 norm of gradients: 0.3872724914725061\n",
      "l2 norm of weights: 6.1104784705520165\n",
      "---------------------\n",
      "Iteration Number: 3056\n",
      "Loss: 46.16917918552998\n",
      "l2 norm of gradients: 0.3871586700419199\n",
      "l2 norm of weights: 6.110365396730864\n",
      "---------------------\n",
      "Iteration Number: 3057\n",
      "Loss: 46.16172274128251\n",
      "l2 norm of gradients: 0.38704492108745897\n",
      "l2 norm of weights: 6.1102523430549205\n",
      "---------------------\n",
      "Iteration Number: 3058\n",
      "Loss: 46.15427061970677\n",
      "l2 norm of gradients: 0.3869312445701169\n",
      "l2 norm of weights: 6.110139309511307\n",
      "---------------------\n",
      "Iteration Number: 3059\n",
      "Loss: 46.14682281676678\n",
      "l2 norm of gradients: 0.38681764045091127\n",
      "l2 norm of weights: 6.110026296087154\n",
      "---------------------\n",
      "Iteration Number: 3060\n",
      "Loss: 46.13937932850675\n",
      "l2 norm of gradients: 0.38670410869088434\n",
      "l2 norm of weights: 6.109913302769608\n",
      "---------------------\n",
      "Iteration Number: 3061\n",
      "Loss: 46.13194015091613\n",
      "l2 norm of gradients: 0.38659064925110204\n",
      "l2 norm of weights: 6.109800329545825\n",
      "---------------------\n",
      "Iteration Number: 3062\n",
      "Loss: 46.12450528001479\n",
      "l2 norm of gradients: 0.38647726209265426\n",
      "l2 norm of weights: 6.1096873764029755\n",
      "---------------------\n",
      "Iteration Number: 3063\n",
      "Loss: 46.117074711831094\n",
      "l2 norm of gradients: 0.38636394717665395\n",
      "l2 norm of weights: 6.109574443328237\n",
      "---------------------\n",
      "Iteration Number: 3064\n",
      "Loss: 46.10964844240206\n",
      "l2 norm of gradients: 0.3862507044642374\n",
      "l2 norm of weights: 6.109461530308804\n",
      "---------------------\n",
      "Iteration Number: 3065\n",
      "Loss: 46.10222646776008\n",
      "l2 norm of gradients: 0.38613753391656336\n",
      "l2 norm of weights: 6.109348637331882\n",
      "---------------------\n",
      "Iteration Number: 3066\n",
      "Loss: 46.094808783918474\n",
      "l2 norm of gradients: 0.3860244354948134\n",
      "l2 norm of weights: 6.109235764384687\n",
      "---------------------\n",
      "Iteration Number: 3067\n",
      "Loss: 46.08739538694339\n",
      "l2 norm of gradients: 0.38591140916019123\n",
      "l2 norm of weights: 6.109122911454449\n",
      "---------------------\n",
      "Iteration Number: 3068\n",
      "Loss: 46.07998627286436\n",
      "l2 norm of gradients: 0.3857984548739226\n",
      "l2 norm of weights: 6.109010078528409\n",
      "---------------------\n",
      "Iteration Number: 3069\n",
      "Loss: 46.07258143773228\n",
      "l2 norm of gradients: 0.38568557259725483\n",
      "l2 norm of weights: 6.10889726559382\n",
      "---------------------\n",
      "Iteration Number: 3070\n",
      "Loss: 46.06518087759783\n",
      "l2 norm of gradients: 0.38557276229145665\n",
      "l2 norm of weights: 6.1087844726379465\n",
      "---------------------\n",
      "Iteration Number: 3071\n",
      "Loss: 46.05778458849049\n",
      "l2 norm of gradients: 0.38546002391781814\n",
      "l2 norm of weights: 6.108671699648067\n",
      "---------------------\n",
      "Iteration Number: 3072\n",
      "Loss: 46.050392566496235\n",
      "l2 norm of gradients: 0.38534735743765\n",
      "l2 norm of weights: 6.1085589466114705\n",
      "---------------------\n",
      "Iteration Number: 3073\n",
      "Loss: 46.04300480768088\n",
      "l2 norm of gradients: 0.3852347628122838\n",
      "l2 norm of weights: 6.1084462135154585\n",
      "---------------------\n",
      "Iteration Number: 3074\n",
      "Loss: 46.035621308089056\n",
      "l2 norm of gradients: 0.3851222400030712\n",
      "l2 norm of weights: 6.108333500347342\n",
      "---------------------\n",
      "Iteration Number: 3075\n",
      "Loss: 46.02824206381995\n",
      "l2 norm of gradients: 0.3850097889713841\n",
      "l2 norm of weights: 6.108220807094451\n",
      "---------------------\n",
      "Iteration Number: 3076\n",
      "Loss: 46.02086707090268\n",
      "l2 norm of gradients: 0.3848974096786143\n",
      "l2 norm of weights: 6.108108133744118\n",
      "---------------------\n",
      "Iteration Number: 3077\n",
      "Loss: 46.013496325458085\n",
      "l2 norm of gradients: 0.3847851020861727\n",
      "l2 norm of weights: 6.1079954802836935\n",
      "---------------------\n",
      "Iteration Number: 3078\n",
      "Loss: 46.006129823544725\n",
      "l2 norm of gradients: 0.3846728661554901\n",
      "l2 norm of weights: 6.107882846700541\n",
      "---------------------\n",
      "Iteration Number: 3079\n",
      "Loss: 45.998767561258155\n",
      "l2 norm of gradients: 0.384560701848016\n",
      "l2 norm of weights: 6.107770232982031\n",
      "---------------------\n",
      "Iteration Number: 3080\n",
      "Loss: 45.991409534670396\n",
      "l2 norm of gradients: 0.3844486091252185\n",
      "l2 norm of weights: 6.107657639115551\n",
      "---------------------\n",
      "Iteration Number: 3081\n",
      "Loss: 45.984055739893925\n",
      "l2 norm of gradients: 0.3843365879485845\n",
      "l2 norm of weights: 6.107545065088496\n",
      "---------------------\n",
      "Iteration Number: 3082\n",
      "Loss: 45.97670617300913\n",
      "l2 norm of gradients: 0.38422463827961917\n",
      "l2 norm of weights: 6.107432510888278\n",
      "---------------------\n",
      "Iteration Number: 3083\n",
      "Loss: 45.96936083014598\n",
      "l2 norm of gradients: 0.3841127600798455\n",
      "l2 norm of weights: 6.107319976502315\n",
      "---------------------\n",
      "Iteration Number: 3084\n",
      "Loss: 45.96201970736119\n",
      "l2 norm of gradients: 0.3840009533108042\n",
      "l2 norm of weights: 6.107207461918041\n",
      "---------------------\n",
      "Iteration Number: 3085\n",
      "Loss: 45.95468280077827\n",
      "l2 norm of gradients: 0.38388921793405373\n",
      "l2 norm of weights: 6.107094967122902\n",
      "---------------------\n",
      "Iteration Number: 3086\n",
      "Loss: 45.94735010654577\n",
      "l2 norm of gradients: 0.38377755391116974\n",
      "l2 norm of weights: 6.106982492104354\n",
      "---------------------\n",
      "Iteration Number: 3087\n",
      "Loss: 45.940021620734285\n",
      "l2 norm of gradients: 0.3836659612037445\n",
      "l2 norm of weights: 6.106870036849866\n",
      "---------------------\n",
      "Iteration Number: 3088\n",
      "Loss: 45.932697339496485\n",
      "l2 norm of gradients: 0.38355443977338755\n",
      "l2 norm of weights: 6.106757601346918\n",
      "---------------------\n",
      "Iteration Number: 3089\n",
      "Loss: 45.92537725890727\n",
      "l2 norm of gradients: 0.38344298958172474\n",
      "l2 norm of weights: 6.1066451855830035\n",
      "---------------------\n",
      "Iteration Number: 3090\n",
      "Loss: 45.918061375159354\n",
      "l2 norm of gradients: 0.38333161059039805\n",
      "l2 norm of weights: 6.106532789545628\n",
      "---------------------\n",
      "Iteration Number: 3091\n",
      "Loss: 45.91074968433099\n",
      "l2 norm of gradients: 0.3832203027610655\n",
      "l2 norm of weights: 6.106420413222306\n",
      "---------------------\n",
      "Iteration Number: 3092\n",
      "Loss: 45.90344218257408\n",
      "l2 norm of gradients: 0.383109066055401\n",
      "l2 norm of weights: 6.106308056600566\n",
      "---------------------\n",
      "Iteration Number: 3093\n",
      "Loss: 45.89613886603043\n",
      "l2 norm of gradients: 0.3829979004350943\n",
      "l2 norm of weights: 6.10619571966795\n",
      "---------------------\n",
      "Iteration Number: 3094\n",
      "Loss: 45.8888397308332\n",
      "l2 norm of gradients: 0.3828868058618497\n",
      "l2 norm of weights: 6.106083402412008\n",
      "---------------------\n",
      "Iteration Number: 3095\n",
      "Loss: 45.88154477312388\n",
      "l2 norm of gradients: 0.3827757822973873\n",
      "l2 norm of weights: 6.105971104820304\n",
      "---------------------\n",
      "Iteration Number: 3096\n",
      "Loss: 45.874253989071285\n",
      "l2 norm of gradients: 0.38266482970344146\n",
      "l2 norm of weights: 6.105858826880415\n",
      "---------------------\n",
      "Iteration Number: 3097\n",
      "Loss: 45.86696737481306\n",
      "l2 norm of gradients: 0.3825539480417617\n",
      "l2 norm of weights: 6.105746568579929\n",
      "---------------------\n",
      "Iteration Number: 3098\n",
      "Loss: 45.85968492650953\n",
      "l2 norm of gradients: 0.3824431372741114\n",
      "l2 norm of weights: 6.105634329906444\n",
      "---------------------\n",
      "Iteration Number: 3099\n",
      "Loss: 45.852406640344455\n",
      "l2 norm of gradients: 0.38233239736226826\n",
      "l2 norm of weights: 6.105522110847572\n",
      "---------------------\n",
      "Iteration Number: 3100\n",
      "Loss: 45.845132512418914\n",
      "l2 norm of gradients: 0.3822217282680241\n",
      "l2 norm of weights: 6.105409911390936\n",
      "---------------------\n",
      "Iteration Number: 3101\n",
      "Loss: 45.83786253896802\n",
      "l2 norm of gradients: 0.38211112995318397\n",
      "l2 norm of weights: 6.105297731524171\n",
      "---------------------\n",
      "Iteration Number: 3102\n",
      "Loss: 45.83059671612479\n",
      "l2 norm of gradients: 0.38200060237956673\n",
      "l2 norm of weights: 6.105185571234924\n",
      "---------------------\n",
      "Iteration Number: 3103\n",
      "Loss: 45.82333504009171\n",
      "l2 norm of gradients: 0.38189014550900435\n",
      "l2 norm of weights: 6.1050734305108545\n",
      "---------------------\n",
      "Iteration Number: 3104\n",
      "Loss: 45.816077507001886\n",
      "l2 norm of gradients: 0.3817797593033417\n",
      "l2 norm of weights: 6.1049613093396315\n",
      "---------------------\n",
      "Iteration Number: 3105\n",
      "Loss: 45.80882411307926\n",
      "l2 norm of gradients: 0.38166944372443656\n",
      "l2 norm of weights: 6.104849207708938\n",
      "---------------------\n",
      "Iteration Number: 3106\n",
      "Loss: 45.8015748545001\n",
      "l2 norm of gradients: 0.38155919873415917\n",
      "l2 norm of weights: 6.104737125606467\n",
      "---------------------\n",
      "Iteration Number: 3107\n",
      "Loss: 45.794329727455036\n",
      "l2 norm of gradients: 0.3814490242943922\n",
      "l2 norm of weights: 6.104625063019926\n",
      "---------------------\n",
      "Iteration Number: 3108\n",
      "Loss: 45.787088728145385\n",
      "l2 norm of gradients: 0.3813389203670304\n",
      "l2 norm of weights: 6.1045130199370305\n",
      "---------------------\n",
      "Iteration Number: 3109\n",
      "Loss: 45.779851852750056\n",
      "l2 norm of gradients: 0.38122888691398027\n",
      "l2 norm of weights: 6.104400996345512\n",
      "---------------------\n",
      "Iteration Number: 3110\n",
      "Loss: 45.77261909748798\n",
      "l2 norm of gradients: 0.3811189238971604\n",
      "l2 norm of weights: 6.104288992233111\n",
      "---------------------\n",
      "Iteration Number: 3111\n",
      "Loss: 45.765390458532195\n",
      "l2 norm of gradients: 0.3810090312785002\n",
      "l2 norm of weights: 6.104177007587579\n",
      "---------------------\n",
      "Iteration Number: 3112\n",
      "Loss: 45.758165932133714\n",
      "l2 norm of gradients: 0.38089920901994095\n",
      "l2 norm of weights: 6.104065042396682\n",
      "---------------------\n",
      "Iteration Number: 3113\n",
      "Loss: 45.75094551448798\n",
      "l2 norm of gradients: 0.3807894570834346\n",
      "l2 norm of weights: 6.103953096648198\n",
      "---------------------\n",
      "Iteration Number: 3114\n",
      "Loss: 45.74372920180859\n",
      "l2 norm of gradients: 0.38067977543094406\n",
      "l2 norm of weights: 6.103841170329912\n",
      "---------------------\n",
      "Iteration Number: 3115\n",
      "Loss: 45.736516990305475\n",
      "l2 norm of gradients: 0.3805701640244429\n",
      "l2 norm of weights: 6.103729263429625\n",
      "---------------------\n",
      "Iteration Number: 3116\n",
      "Loss: 45.72930887621049\n",
      "l2 norm of gradients: 0.3804606228259152\n",
      "l2 norm of weights: 6.103617375935148\n",
      "---------------------\n",
      "Iteration Number: 3117\n",
      "Loss: 45.722104855762794\n",
      "l2 norm of gradients: 0.3803511517973549\n",
      "l2 norm of weights: 6.103505507834306\n",
      "---------------------\n",
      "Iteration Number: 3118\n",
      "Loss: 45.714904925181614\n",
      "l2 norm of gradients: 0.3802417509007661\n",
      "l2 norm of weights: 6.103393659114932\n",
      "---------------------\n",
      "Iteration Number: 3119\n",
      "Loss: 45.70770908069335\n",
      "l2 norm of gradients: 0.380132420098163\n",
      "l2 norm of weights: 6.103281829764875\n",
      "---------------------\n",
      "Iteration Number: 3120\n",
      "Loss: 45.7005173185516\n",
      "l2 norm of gradients: 0.38002315935156905\n",
      "l2 norm of weights: 6.103170019771991\n",
      "---------------------\n",
      "Iteration Number: 3121\n",
      "Loss: 45.69332963499065\n",
      "l2 norm of gradients: 0.3799139686230173\n",
      "l2 norm of weights: 6.1030582291241515\n",
      "---------------------\n",
      "Iteration Number: 3122\n",
      "Loss: 45.68614602624419\n",
      "l2 norm of gradients: 0.3798048478745497\n",
      "l2 norm of weights: 6.102946457809239\n",
      "---------------------\n",
      "Iteration Number: 3123\n",
      "Loss: 45.6789664886048\n",
      "l2 norm of gradients: 0.37969579706821754\n",
      "l2 norm of weights: 6.102834705815144\n",
      "---------------------\n",
      "Iteration Number: 3124\n",
      "Loss: 45.671791018260365\n",
      "l2 norm of gradients: 0.37958681616608075\n",
      "l2 norm of weights: 6.102722973129775\n",
      "---------------------\n",
      "Iteration Number: 3125\n",
      "Loss: 45.66461961151847\n",
      "l2 norm of gradients: 0.37947790513020785\n",
      "l2 norm of weights: 6.102611259741047\n",
      "---------------------\n",
      "Iteration Number: 3126\n",
      "Loss: 45.6574522646182\n",
      "l2 norm of gradients: 0.37936906392267605\n",
      "l2 norm of weights: 6.1024995656368874\n",
      "---------------------\n",
      "Iteration Number: 3127\n",
      "Loss: 45.6502889738217\n",
      "l2 norm of gradients: 0.37926029250557025\n",
      "l2 norm of weights: 6.10238789080524\n",
      "---------------------\n",
      "Iteration Number: 3128\n",
      "Loss: 45.643129735387106\n",
      "l2 norm of gradients: 0.3791515908409838\n",
      "l2 norm of weights: 6.1022762352340525\n",
      "---------------------\n",
      "Iteration Number: 3129\n",
      "Loss: 45.635974545615746\n",
      "l2 norm of gradients: 0.3790429588910181\n",
      "l2 norm of weights: 6.10216459891129\n",
      "---------------------\n",
      "Iteration Number: 3130\n",
      "Loss: 45.62882340075167\n",
      "l2 norm of gradients: 0.3789343966177813\n",
      "l2 norm of weights: 6.102052981824928\n",
      "---------------------\n",
      "Iteration Number: 3131\n",
      "Loss: 45.62167629709805\n",
      "l2 norm of gradients: 0.37882590398339\n",
      "l2 norm of weights: 6.101941383962952\n",
      "---------------------\n",
      "Iteration Number: 3132\n",
      "Loss: 45.614533230919285\n",
      "l2 norm of gradients: 0.37871748094996766\n",
      "l2 norm of weights: 6.101829805313361\n",
      "---------------------\n",
      "Iteration Number: 3133\n",
      "Loss: 45.60739419849987\n",
      "l2 norm of gradients: 0.37860912747964465\n",
      "l2 norm of weights: 6.1017182458641654\n",
      "---------------------\n",
      "Iteration Number: 3134\n",
      "Loss: 45.60025919613758\n",
      "l2 norm of gradients: 0.3785008435345586\n",
      "l2 norm of weights: 6.101606705603385\n",
      "---------------------\n",
      "Iteration Number: 3135\n",
      "Loss: 45.593128220125465\n",
      "l2 norm of gradients: 0.3783926290768537\n",
      "l2 norm of weights: 6.101495184519052\n",
      "---------------------\n",
      "Iteration Number: 3136\n",
      "Loss: 45.58600126675976\n",
      "l2 norm of gradients: 0.37828448406868076\n",
      "l2 norm of weights: 6.101383682599215\n",
      "---------------------\n",
      "Iteration Number: 3137\n",
      "Loss: 45.578878332328436\n",
      "l2 norm of gradients: 0.37817640847219686\n",
      "l2 norm of weights: 6.101272199831927\n",
      "---------------------\n",
      "Iteration Number: 3138\n",
      "Loss: 45.5717594131486\n",
      "l2 norm of gradients: 0.37806840224956534\n",
      "l2 norm of weights: 6.101160736205257\n",
      "---------------------\n",
      "Iteration Number: 3139\n",
      "Loss: 45.56464450549623\n",
      "l2 norm of gradients: 0.37796046536295547\n",
      "l2 norm of weights: 6.101049291707283\n",
      "---------------------\n",
      "Iteration Number: 3140\n",
      "Loss: 45.55753360572806\n",
      "l2 norm of gradients: 0.37785259777454255\n",
      "l2 norm of weights: 6.100937866326097\n",
      "---------------------\n",
      "Iteration Number: 3141\n",
      "Loss: 45.55042671013519\n",
      "l2 norm of gradients: 0.37774479944650735\n",
      "l2 norm of weights: 6.1008264600498014\n",
      "---------------------\n",
      "Iteration Number: 3142\n",
      "Loss: 45.54332381503972\n",
      "l2 norm of gradients: 0.3776370703410361\n",
      "l2 norm of weights: 6.100715072866509\n",
      "---------------------\n",
      "Iteration Number: 3143\n",
      "Loss: 45.53622491674664\n",
      "l2 norm of gradients: 0.3775294104203206\n",
      "l2 norm of weights: 6.100603704764347\n",
      "---------------------\n",
      "Iteration Number: 3144\n",
      "Loss: 45.52913001158869\n",
      "l2 norm of gradients: 0.37742181964655763\n",
      "l2 norm of weights: 6.100492355731451\n",
      "---------------------\n",
      "Iteration Number: 3145\n",
      "Loss: 45.52203909591576\n",
      "l2 norm of gradients: 0.3773142979819489\n",
      "l2 norm of weights: 6.100381025755971\n",
      "---------------------\n",
      "Iteration Number: 3146\n",
      "Loss: 45.51495216603514\n",
      "l2 norm of gradients: 0.377206845388701\n",
      "l2 norm of weights: 6.100269714826066\n",
      "---------------------\n",
      "Iteration Number: 3147\n",
      "Loss: 45.50786921831116\n",
      "l2 norm of gradients: 0.37709946182902526\n",
      "l2 norm of weights: 6.100158422929907\n",
      "---------------------\n",
      "Iteration Number: 3148\n",
      "Loss: 45.50079024903949\n",
      "l2 norm of gradients: 0.37699214726513736\n",
      "l2 norm of weights: 6.100047150055678\n",
      "---------------------\n",
      "Iteration Number: 3149\n",
      "Loss: 45.49371525459277\n",
      "l2 norm of gradients: 0.37688490165925714\n",
      "l2 norm of weights: 6.099935896191574\n",
      "---------------------\n",
      "Iteration Number: 3150\n",
      "Loss: 45.48664423129495\n",
      "l2 norm of gradients: 0.37677772497360906\n",
      "l2 norm of weights: 6.099824661325799\n",
      "---------------------\n",
      "Iteration Number: 3151\n",
      "Loss: 45.47957717553605\n",
      "l2 norm of gradients: 0.3766706171704213\n",
      "l2 norm of weights: 6.099713445446573\n",
      "---------------------\n",
      "Iteration Number: 3152\n",
      "Loss: 45.47251408361935\n",
      "l2 norm of gradients: 0.37656357821192565\n",
      "l2 norm of weights: 6.099602248542124\n",
      "---------------------\n",
      "Iteration Number: 3153\n",
      "Loss: 45.4654549519413\n",
      "l2 norm of gradients: 0.37645660806035797\n",
      "l2 norm of weights: 6.099491070600692\n",
      "---------------------\n",
      "Iteration Number: 3154\n",
      "Loss: 45.45839977684561\n",
      "l2 norm of gradients: 0.3763497066779574\n",
      "l2 norm of weights: 6.099379911610528\n",
      "---------------------\n",
      "Iteration Number: 3155\n",
      "Loss: 45.45134855469941\n",
      "l2 norm of gradients: 0.37624287402696655\n",
      "l2 norm of weights: 6.099268771559899\n",
      "---------------------\n",
      "Iteration Number: 3156\n",
      "Loss: 45.4443012818507\n",
      "l2 norm of gradients: 0.3761361100696311\n",
      "l2 norm of weights: 6.099157650437075\n",
      "---------------------\n",
      "Iteration Number: 3157\n",
      "Loss: 45.43725795471595\n",
      "l2 norm of gradients: 0.3760294147682\n",
      "l2 norm of weights: 6.099046548230346\n",
      "---------------------\n",
      "Iteration Number: 3158\n",
      "Loss: 45.430218569611945\n",
      "l2 norm of gradients: 0.37592278808492496\n",
      "l2 norm of weights: 6.098935464928008\n",
      "---------------------\n",
      "Iteration Number: 3159\n",
      "Loss: 45.4231831229781\n",
      "l2 norm of gradients: 0.37581622998206016\n",
      "l2 norm of weights: 6.09882440051837\n",
      "---------------------\n",
      "Iteration Number: 3160\n",
      "Loss: 45.4161516111423\n",
      "l2 norm of gradients: 0.37570974042186306\n",
      "l2 norm of weights: 6.098713354989753\n",
      "---------------------\n",
      "Iteration Number: 3161\n",
      "Loss: 45.409124030530926\n",
      "l2 norm of gradients: 0.37560331936659286\n",
      "l2 norm of weights: 6.098602328330489\n",
      "---------------------\n",
      "Iteration Number: 3162\n",
      "Loss: 45.402100377496375\n",
      "l2 norm of gradients: 0.3754969667785117\n",
      "l2 norm of weights: 6.09849132052892\n",
      "---------------------\n",
      "Iteration Number: 3163\n",
      "Loss: 45.39508064847253\n",
      "l2 norm of gradients: 0.37539068261988306\n",
      "l2 norm of weights: 6.098380331573402\n",
      "---------------------\n",
      "Iteration Number: 3164\n",
      "Loss: 45.38806483981442\n",
      "l2 norm of gradients: 0.37528446685297323\n",
      "l2 norm of weights: 6.098269361452301\n",
      "---------------------\n",
      "Iteration Number: 3165\n",
      "Loss: 45.38105294792932\n",
      "l2 norm of gradients: 0.3751783194400501\n",
      "l2 norm of weights: 6.098158410153994\n",
      "---------------------\n",
      "Iteration Number: 3166\n",
      "Loss: 45.37404496924024\n",
      "l2 norm of gradients: 0.375072240343383\n",
      "l2 norm of weights: 6.098047477666871\n",
      "---------------------\n",
      "Iteration Number: 3167\n",
      "Loss: 45.3670409001412\n",
      "l2 norm of gradients: 0.3749662295252432\n",
      "l2 norm of weights: 6.09793656397933\n",
      "---------------------\n",
      "Iteration Number: 3168\n",
      "Loss: 45.36004073703798\n",
      "l2 norm of gradients: 0.3748602869479032\n",
      "l2 norm of weights: 6.097825669079783\n",
      "---------------------\n",
      "Iteration Number: 3169\n",
      "Loss: 45.35304447636989\n",
      "l2 norm of gradients: 0.374754412573637\n",
      "l2 norm of weights: 6.097714792956655\n",
      "---------------------\n",
      "Iteration Number: 3170\n",
      "Loss: 45.34605211449854\n",
      "l2 norm of gradients: 0.3746486063647194\n",
      "l2 norm of weights: 6.097603935598378\n",
      "---------------------\n",
      "Iteration Number: 3171\n",
      "Loss: 45.339063647893994\n",
      "l2 norm of gradients: 0.37454286828342676\n",
      "l2 norm of weights: 6.097493096993398\n",
      "---------------------\n",
      "Iteration Number: 3172\n",
      "Loss: 45.33207907297276\n",
      "l2 norm of gradients: 0.3744371982920359\n",
      "l2 norm of weights: 6.097382277130173\n",
      "---------------------\n",
      "Iteration Number: 3173\n",
      "Loss: 45.32509838612868\n",
      "l2 norm of gradients: 0.37433159635282437\n",
      "l2 norm of weights: 6.09727147599717\n",
      "---------------------\n",
      "Iteration Number: 3174\n",
      "Loss: 45.31812158385001\n",
      "l2 norm of gradients: 0.37422606242807066\n",
      "l2 norm of weights: 6.097160693582869\n",
      "---------------------\n",
      "Iteration Number: 3175\n",
      "Loss: 45.31114866251769\n",
      "l2 norm of gradients: 0.3741205964800536\n",
      "l2 norm of weights: 6.0970499298757606\n",
      "---------------------\n",
      "Iteration Number: 3176\n",
      "Loss: 45.30417961859122\n",
      "l2 norm of gradients: 0.3740151984710523\n",
      "l2 norm of weights: 6.096939184864347\n",
      "---------------------\n",
      "Iteration Number: 3177\n",
      "Loss: 45.29721444851877\n",
      "l2 norm of gradients: 0.3739098683633463\n",
      "l2 norm of weights: 6.096828458537141\n",
      "---------------------\n",
      "Iteration Number: 3178\n",
      "Loss: 45.2902531487195\n",
      "l2 norm of gradients: 0.373804606119215\n",
      "l2 norm of weights: 6.096717750882668\n",
      "---------------------\n",
      "Iteration Number: 3179\n",
      "Loss: 45.28329571566882\n",
      "l2 norm of gradients: 0.37369941170093773\n",
      "l2 norm of weights: 6.096607061889464\n",
      "---------------------\n",
      "Iteration Number: 3180\n",
      "Loss: 45.27634214582161\n",
      "l2 norm of gradients: 0.3735942850707942\n",
      "l2 norm of weights: 6.096496391546077\n",
      "---------------------\n",
      "Iteration Number: 3181\n",
      "Loss: 45.26939243559555\n",
      "l2 norm of gradients: 0.3734892261910633\n",
      "l2 norm of weights: 6.096385739841062\n",
      "---------------------\n",
      "Iteration Number: 3182\n",
      "Loss: 45.2624465814925\n",
      "l2 norm of gradients: 0.37338423502402346\n",
      "l2 norm of weights: 6.0962751067629934\n",
      "---------------------\n",
      "Iteration Number: 3183\n",
      "Loss: 45.25550457993629\n",
      "l2 norm of gradients: 0.3732793115319532\n",
      "l2 norm of weights: 6.096164492300448\n",
      "---------------------\n",
      "Iteration Number: 3184\n",
      "Loss: 45.2485664274151\n",
      "l2 norm of gradients: 0.37317445567712976\n",
      "l2 norm of weights: 6.0960538964420214\n",
      "---------------------\n",
      "Iteration Number: 3185\n",
      "Loss: 45.241632120402215\n",
      "l2 norm of gradients: 0.37306966742183006\n",
      "l2 norm of weights: 6.095943319176316\n",
      "---------------------\n",
      "Iteration Number: 3186\n",
      "Loss: 45.23470165534818\n",
      "l2 norm of gradients: 0.3729649467283297\n",
      "l2 norm of weights: 6.095832760491945\n",
      "---------------------\n",
      "Iteration Number: 3187\n",
      "Loss: 45.227775028740446\n",
      "l2 norm of gradients: 0.3728602935589037\n",
      "l2 norm of weights: 6.095722220377535\n",
      "---------------------\n",
      "Iteration Number: 3188\n",
      "Loss: 45.22085223707403\n",
      "l2 norm of gradients: 0.37275570787582585\n",
      "l2 norm of weights: 6.095611698821724\n",
      "---------------------\n",
      "Iteration Number: 3189\n",
      "Loss: 45.21393327681712\n",
      "l2 norm of gradients: 0.3726511896413686\n",
      "l2 norm of weights: 6.095501195813159\n",
      "---------------------\n",
      "Iteration Number: 3190\n",
      "Loss: 45.207018144457294\n",
      "l2 norm of gradients: 0.3725467388178031\n",
      "l2 norm of weights: 6.0953907113405\n",
      "---------------------\n",
      "Iteration Number: 3191\n",
      "Loss: 45.20010683646676\n",
      "l2 norm of gradients: 0.3724423553673989\n",
      "l2 norm of weights: 6.095280245392418\n",
      "---------------------\n",
      "Iteration Number: 3192\n",
      "Loss: 45.19319934936393\n",
      "l2 norm of gradients: 0.37233803925242437\n",
      "l2 norm of weights: 6.0951697979575945\n",
      "---------------------\n",
      "Iteration Number: 3193\n",
      "Loss: 45.18629567962747\n",
      "l2 norm of gradients: 0.3722337904351457\n",
      "l2 norm of weights: 6.095059369024724\n",
      "---------------------\n",
      "Iteration Number: 3194\n",
      "Loss: 45.17939582377666\n",
      "l2 norm of gradients: 0.37212960887782776\n",
      "l2 norm of weights: 6.094948958582507\n",
      "---------------------\n",
      "Iteration Number: 3195\n",
      "Loss: 45.1724997783041\n",
      "l2 norm of gradients: 0.37202549454273326\n",
      "l2 norm of weights: 6.094838566619662\n",
      "---------------------\n",
      "Iteration Number: 3196\n",
      "Loss: 45.1656075397124\n",
      "l2 norm of gradients: 0.37192144739212285\n",
      "l2 norm of weights: 6.094728193124913\n",
      "---------------------\n",
      "Iteration Number: 3197\n",
      "Loss: 45.15871910449912\n",
      "l2 norm of gradients: 0.3718174673882551\n",
      "l2 norm of weights: 6.094617838087001\n",
      "---------------------\n",
      "Iteration Number: 3198\n",
      "Loss: 45.15183446921561\n",
      "l2 norm of gradients: 0.37171355449338656\n",
      "l2 norm of weights: 6.094507501494672\n",
      "---------------------\n",
      "Iteration Number: 3199\n",
      "Loss: 45.14495363033233\n",
      "l2 norm of gradients: 0.3716097086697712\n",
      "l2 norm of weights: 6.094397183336686\n",
      "---------------------\n",
      "Iteration Number: 3200\n",
      "Loss: 45.13807658442114\n",
      "l2 norm of gradients: 0.3715059298796607\n",
      "l2 norm of weights: 6.094286883601815\n",
      "---------------------\n",
      "Iteration Number: 3201\n",
      "Loss: 45.1312033279534\n",
      "l2 norm of gradients: 0.3714022180853042\n",
      "l2 norm of weights: 6.094176602278841\n",
      "---------------------\n",
      "Iteration Number: 3202\n",
      "Loss: 45.12433385750059\n",
      "l2 norm of gradients: 0.37129857324894827\n",
      "l2 norm of weights: 6.094066339356556\n",
      "---------------------\n",
      "Iteration Number: 3203\n",
      "Loss: 45.11746816955441\n",
      "l2 norm of gradients: 0.3711949953328364\n",
      "l2 norm of weights: 6.093956094823766\n",
      "---------------------\n",
      "Iteration Number: 3204\n",
      "Loss: 45.1106062606885\n",
      "l2 norm of gradients: 0.3710914842992098\n",
      "l2 norm of weights: 6.093845868669285\n",
      "---------------------\n",
      "Iteration Number: 3205\n",
      "Loss: 45.10374812741259\n",
      "l2 norm of gradients: 0.3709880401103062\n",
      "l2 norm of weights: 6.09373566088194\n",
      "---------------------\n",
      "Iteration Number: 3206\n",
      "Loss: 45.096893766282896\n",
      "l2 norm of gradients: 0.3708846627283608\n",
      "l2 norm of weights: 6.093625471450569\n",
      "---------------------\n",
      "Iteration Number: 3207\n",
      "Loss: 45.090043173824334\n",
      "l2 norm of gradients: 0.3707813521156054\n",
      "l2 norm of weights: 6.093515300364017\n",
      "---------------------\n",
      "Iteration Number: 3208\n",
      "Loss: 45.08319634659267\n",
      "l2 norm of gradients: 0.37067810823426856\n",
      "l2 norm of weights: 6.093405147611149\n",
      "---------------------\n",
      "Iteration Number: 3209\n",
      "Loss: 45.07635328114973\n",
      "l2 norm of gradients: 0.3705749310465758\n",
      "l2 norm of weights: 6.093295013180833\n",
      "---------------------\n",
      "Iteration Number: 3210\n",
      "Loss: 45.069513974044575\n",
      "l2 norm of gradients: 0.3704718205147488\n",
      "l2 norm of weights: 6.09318489706195\n",
      "---------------------\n",
      "Iteration Number: 3211\n",
      "Loss: 45.06267842183554\n",
      "l2 norm of gradients: 0.37036877660100614\n",
      "l2 norm of weights: 6.093074799243395\n",
      "---------------------\n",
      "Iteration Number: 3212\n",
      "Loss: 45.055846621065555\n",
      "l2 norm of gradients: 0.3702657992675627\n",
      "l2 norm of weights: 6.092964719714068\n",
      "---------------------\n",
      "Iteration Number: 3213\n",
      "Loss: 45.0490185683258\n",
      "l2 norm of gradients: 0.37016288847662965\n",
      "l2 norm of weights: 6.092854658462888\n",
      "---------------------\n",
      "Iteration Number: 3214\n",
      "Loss: 45.04219426016493\n",
      "l2 norm of gradients: 0.3700600441904144\n",
      "l2 norm of weights: 6.092744615478779\n",
      "---------------------\n",
      "Iteration Number: 3215\n",
      "Loss: 45.03537369315603\n",
      "l2 norm of gradients: 0.36995726637112075\n",
      "l2 norm of weights: 6.092634590750678\n",
      "---------------------\n",
      "Iteration Number: 3216\n",
      "Loss: 45.02855686387781\n",
      "l2 norm of gradients: 0.36985455498094805\n",
      "l2 norm of weights: 6.092524584267532\n",
      "---------------------\n",
      "Iteration Number: 3217\n",
      "Loss: 45.021743768929696\n",
      "l2 norm of gradients: 0.3697519099820921\n",
      "l2 norm of weights: 6.0924145960183\n",
      "---------------------\n",
      "Iteration Number: 3218\n",
      "Loss: 45.01493440485897\n",
      "l2 norm of gradients: 0.36964933133674466\n",
      "l2 norm of weights: 6.092304625991953\n",
      "---------------------\n",
      "Iteration Number: 3219\n",
      "Loss: 45.00812876825543\n",
      "l2 norm of gradients: 0.36954681900709274\n",
      "l2 norm of weights: 6.09219467417747\n",
      "---------------------\n",
      "Iteration Number: 3220\n",
      "Loss: 45.00132685572177\n",
      "l2 norm of gradients: 0.3694443729553197\n",
      "l2 norm of weights: 6.092084740563844\n",
      "---------------------\n",
      "Iteration Number: 3221\n",
      "Loss: 44.994528663855284\n",
      "l2 norm of gradients: 0.3693419931436042\n",
      "l2 norm of weights: 6.091974825140077\n",
      "---------------------\n",
      "Iteration Number: 3222\n",
      "Loss: 44.987734189215985\n",
      "l2 norm of gradients: 0.36923967953412073\n",
      "l2 norm of weights: 6.091864927895182\n",
      "---------------------\n",
      "Iteration Number: 3223\n",
      "Loss: 44.98094342844422\n",
      "l2 norm of gradients: 0.36913743208903904\n",
      "l2 norm of weights: 6.091755048818185\n",
      "---------------------\n",
      "Iteration Number: 3224\n",
      "Loss: 44.974156378103\n",
      "l2 norm of gradients: 0.3690352507705244\n",
      "l2 norm of weights: 6.09164518789812\n",
      "---------------------\n",
      "Iteration Number: 3225\n",
      "Loss: 44.96737303483952\n",
      "l2 norm of gradients: 0.3689331355407375\n",
      "l2 norm of weights: 6.091535345124035\n",
      "---------------------\n",
      "Iteration Number: 3226\n",
      "Loss: 44.96059339520664\n",
      "l2 norm of gradients: 0.36883108636183426\n",
      "l2 norm of weights: 6.091425520484986\n",
      "---------------------\n",
      "Iteration Number: 3227\n",
      "Loss: 44.95381745587901\n",
      "l2 norm of gradients: 0.36872910319596564\n",
      "l2 norm of weights: 6.091315713970041\n",
      "---------------------\n",
      "Iteration Number: 3228\n",
      "Loss: 44.94704521341124\n",
      "l2 norm of gradients: 0.3686271860052779\n",
      "l2 norm of weights: 6.091205925568281\n",
      "---------------------\n",
      "Iteration Number: 3229\n",
      "Loss: 44.94027666446944\n",
      "l2 norm of gradients: 0.36852533475191246\n",
      "l2 norm of weights: 6.091096155268794\n",
      "---------------------\n",
      "Iteration Number: 3230\n",
      "Loss: 44.933511805634296\n",
      "l2 norm of gradients: 0.36842354939800537\n",
      "l2 norm of weights: 6.090986403060681\n",
      "---------------------\n",
      "Iteration Number: 3231\n",
      "Loss: 44.92675063355609\n",
      "l2 norm of gradients: 0.3683218299056879\n",
      "l2 norm of weights: 6.0908766689330545\n",
      "---------------------\n",
      "Iteration Number: 3232\n",
      "Loss: 44.91999314485855\n",
      "l2 norm of gradients: 0.36822017623708597\n",
      "l2 norm of weights: 6.090766952875037\n",
      "---------------------\n",
      "Iteration Number: 3233\n",
      "Loss: 44.913239336182784\n",
      "l2 norm of gradients: 0.3681185883543204\n",
      "l2 norm of weights: 6.090657254875762\n",
      "---------------------\n",
      "Iteration Number: 3234\n",
      "Loss: 44.90648920414704\n",
      "l2 norm of gradients: 0.36801706621950675\n",
      "l2 norm of weights: 6.090547574924375\n",
      "---------------------\n",
      "Iteration Number: 3235\n",
      "Loss: 44.899742745386305\n",
      "l2 norm of gradients: 0.36791560979475496\n",
      "l2 norm of weights: 6.090437913010028\n",
      "---------------------\n",
      "Iteration Number: 3236\n",
      "Loss: 44.892999956562605\n",
      "l2 norm of gradients: 0.3678142190421698\n",
      "l2 norm of weights: 6.0903282691218905\n",
      "---------------------\n",
      "Iteration Number: 3237\n",
      "Loss: 44.88626083430829\n",
      "l2 norm of gradients: 0.3677128939238506\n",
      "l2 norm of weights: 6.090218643249138\n",
      "---------------------\n",
      "Iteration Number: 3238\n",
      "Loss: 44.87952537525587\n",
      "l2 norm of gradients: 0.3676116344018909\n",
      "l2 norm of weights: 6.090109035380957\n",
      "---------------------\n",
      "Iteration Number: 3239\n",
      "Loss: 44.87279357608086\n",
      "l2 norm of gradients: 0.3675104404383786\n",
      "l2 norm of weights: 6.089999445506548\n",
      "---------------------\n",
      "Iteration Number: 3240\n",
      "Loss: 44.86606543343577\n",
      "l2 norm of gradients: 0.36740931199539617\n",
      "l2 norm of weights: 6.089889873615119\n",
      "---------------------\n",
      "Iteration Number: 3241\n",
      "Loss: 44.859340943967545\n",
      "l2 norm of gradients: 0.3673082490350203\n",
      "l2 norm of weights: 6.08978031969589\n",
      "---------------------\n",
      "Iteration Number: 3242\n",
      "Loss: 44.85262010433318\n",
      "l2 norm of gradients: 0.36720725151932176\n",
      "l2 norm of weights: 6.089670783738094\n",
      "---------------------\n",
      "Iteration Number: 3243\n",
      "Loss: 44.84590291121017\n",
      "l2 norm of gradients: 0.3671063194103654\n",
      "l2 norm of weights: 6.08956126573097\n",
      "---------------------\n",
      "Iteration Number: 3244\n",
      "Loss: 44.83918936125947\n",
      "l2 norm of gradients: 0.36700545267021056\n",
      "l2 norm of weights: 6.089451765663772\n",
      "---------------------\n",
      "Iteration Number: 3245\n",
      "Loss: 44.832479451159536\n",
      "l2 norm of gradients: 0.36690465126091004\n",
      "l2 norm of weights: 6.089342283525762\n",
      "---------------------\n",
      "Iteration Number: 3246\n",
      "Loss: 44.825773177571705\n",
      "l2 norm of gradients: 0.3668039151445111\n",
      "l2 norm of weights: 6.089232819306216\n",
      "---------------------\n",
      "Iteration Number: 3247\n",
      "Loss: 44.81907053718751\n",
      "l2 norm of gradients: 0.3667032442830545\n",
      "l2 norm of weights: 6.0891233729944165\n",
      "---------------------\n",
      "Iteration Number: 3248\n",
      "Loss: 44.81237152667678\n",
      "l2 norm of gradients: 0.36660263863857523\n",
      "l2 norm of weights: 6.089013944579659\n",
      "---------------------\n",
      "Iteration Number: 3249\n",
      "Loss: 44.80567614272031\n",
      "l2 norm of gradients: 0.3665020981731021\n",
      "l2 norm of weights: 6.088904534051253\n",
      "---------------------\n",
      "Iteration Number: 3250\n",
      "Loss: 44.79898438203633\n",
      "l2 norm of gradients: 0.3664016228486573\n",
      "l2 norm of weights: 6.088795141398512\n",
      "---------------------\n",
      "Iteration Number: 3251\n",
      "Loss: 44.7922962412884\n",
      "l2 norm of gradients: 0.36630121262725696\n",
      "l2 norm of weights: 6.088685766610764\n",
      "---------------------\n",
      "Iteration Number: 3252\n",
      "Loss: 44.785611717158844\n",
      "l2 norm of gradients: 0.36620086747091113\n",
      "l2 norm of weights: 6.08857640967735\n",
      "---------------------\n",
      "Iteration Number: 3253\n",
      "Loss: 44.77893080637042\n",
      "l2 norm of gradients: 0.3661005873416231\n",
      "l2 norm of weights: 6.088467070587615\n",
      "---------------------\n",
      "Iteration Number: 3254\n",
      "Loss: 44.77225350560321\n",
      "l2 norm of gradients: 0.3660003722013897\n",
      "l2 norm of weights: 6.088357749330922\n",
      "---------------------\n",
      "Iteration Number: 3255\n",
      "Loss: 44.76557981156763\n",
      "l2 norm of gradients: 0.3659002220122016\n",
      "l2 norm of weights: 6.088248445896641\n",
      "---------------------\n",
      "Iteration Number: 3256\n",
      "Loss: 44.758909720993586\n",
      "l2 norm of gradients: 0.3658001367360427\n",
      "l2 norm of weights: 6.088139160274151\n",
      "---------------------\n",
      "Iteration Number: 3257\n",
      "Loss: 44.752243230554086\n",
      "l2 norm of gradients: 0.36570011633489063\n",
      "l2 norm of weights: 6.088029892452846\n",
      "---------------------\n",
      "Iteration Number: 3258\n",
      "Loss: 44.74558033698199\n",
      "l2 norm of gradients: 0.3656001607707159\n",
      "l2 norm of weights: 6.087920642422129\n",
      "---------------------\n",
      "Iteration Number: 3259\n",
      "Loss: 44.73892103701956\n",
      "l2 norm of gradients: 0.3655002700054827\n",
      "l2 norm of weights: 6.087811410171411\n",
      "---------------------\n",
      "Iteration Number: 3260\n",
      "Loss: 44.732265327332875\n",
      "l2 norm of gradients: 0.3654004440011484\n",
      "l2 norm of weights: 6.087702195690116\n",
      "---------------------\n",
      "Iteration Number: 3261\n",
      "Loss: 44.725613204683164\n",
      "l2 norm of gradients: 0.365300682719664\n",
      "l2 norm of weights: 6.08759299896768\n",
      "---------------------\n",
      "Iteration Number: 3262\n",
      "Loss: 44.71896466576608\n",
      "l2 norm of gradients: 0.36520098612297297\n",
      "l2 norm of weights: 6.087483819993546\n",
      "---------------------\n",
      "Iteration Number: 3263\n",
      "Loss: 44.71231970736757\n",
      "l2 norm of gradients: 0.3651013541730127\n",
      "l2 norm of weights: 6.087374658757172\n",
      "---------------------\n",
      "Iteration Number: 3264\n",
      "Loss: 44.70567832615798\n",
      "l2 norm of gradients: 0.3650017868317132\n",
      "l2 norm of weights: 6.0872655152480215\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 3265\n",
      "Loss: 44.69904051890746\n",
      "l2 norm of gradients: 0.36490228406099795\n",
      "l2 norm of weights: 6.087156389455575\n",
      "---------------------\n",
      "Iteration Number: 3266\n",
      "Loss: 44.692406282354774\n",
      "l2 norm of gradients: 0.36480284582278316\n",
      "l2 norm of weights: 6.087047281369316\n",
      "---------------------\n",
      "Iteration Number: 3267\n",
      "Loss: 44.685775613224905\n",
      "l2 norm of gradients: 0.3647034720789784\n",
      "l2 norm of weights: 6.086938190978746\n",
      "---------------------\n",
      "Iteration Number: 3268\n",
      "Loss: 44.67914850827286\n",
      "l2 norm of gradients: 0.36460416279148583\n",
      "l2 norm of weights: 6.0868291182733705\n",
      "---------------------\n",
      "Iteration Number: 3269\n",
      "Loss: 44.672524964248794\n",
      "l2 norm of gradients: 0.36450491792220097\n",
      "l2 norm of weights: 6.086720063242711\n",
      "---------------------\n",
      "Iteration Number: 3270\n",
      "Loss: 44.665904977923034\n",
      "l2 norm of gradients: 0.3644057374330121\n",
      "l2 norm of weights: 6.086611025876297\n",
      "---------------------\n",
      "Iteration Number: 3271\n",
      "Loss: 44.65928854601908\n",
      "l2 norm of gradients: 0.36430662128580027\n",
      "l2 norm of weights: 6.086502006163668\n",
      "---------------------\n",
      "Iteration Number: 3272\n",
      "Loss: 44.65267566531278\n",
      "l2 norm of gradients: 0.36420756944243954\n",
      "l2 norm of weights: 6.086393004094376\n",
      "---------------------\n",
      "Iteration Number: 3273\n",
      "Loss: 44.646066332588454\n",
      "l2 norm of gradients: 0.3641085818647967\n",
      "l2 norm of weights: 6.086284019657981\n",
      "---------------------\n",
      "Iteration Number: 3274\n",
      "Loss: 44.63946054456081\n",
      "l2 norm of gradients: 0.3640096585147314\n",
      "l2 norm of weights: 6.086175052844056\n",
      "---------------------\n",
      "Iteration Number: 3275\n",
      "Loss: 44.63285829804244\n",
      "l2 norm of gradients: 0.36391079935409615\n",
      "l2 norm of weights: 6.0860661036421835\n",
      "---------------------\n",
      "Iteration Number: 3276\n",
      "Loss: 44.62625958976055\n",
      "l2 norm of gradients: 0.36381200434473604\n",
      "l2 norm of weights: 6.085957172041956\n",
      "---------------------\n",
      "Iteration Number: 3277\n",
      "Loss: 44.6196644165435\n",
      "l2 norm of gradients: 0.3637132734484889\n",
      "l2 norm of weights: 6.085848258032978\n",
      "---------------------\n",
      "Iteration Number: 3278\n",
      "Loss: 44.613072775123364\n",
      "l2 norm of gradients: 0.3636146066271853\n",
      "l2 norm of weights: 6.085739361604861\n",
      "---------------------\n",
      "Iteration Number: 3279\n",
      "Loss: 44.606484662314394\n",
      "l2 norm of gradients: 0.3635160038426486\n",
      "l2 norm of weights: 6.0856304827472325\n",
      "---------------------\n",
      "Iteration Number: 3280\n",
      "Loss: 44.59990007487426\n",
      "l2 norm of gradients: 0.3634174650566943\n",
      "l2 norm of weights: 6.085521621449726\n",
      "---------------------\n",
      "Iteration Number: 3281\n",
      "Loss: 44.59331900961998\n",
      "l2 norm of gradients: 0.36331899023113107\n",
      "l2 norm of weights: 6.085412777701987\n",
      "---------------------\n",
      "Iteration Number: 3282\n",
      "Loss: 44.5867414633313\n",
      "l2 norm of gradients: 0.3632205793277599\n",
      "l2 norm of weights: 6.085303951493672\n",
      "---------------------\n",
      "Iteration Number: 3283\n",
      "Loss: 44.58016743277954\n",
      "l2 norm of gradients: 0.3631222323083744\n",
      "l2 norm of weights: 6.085195142814448\n",
      "---------------------\n",
      "Iteration Number: 3284\n",
      "Loss: 44.57359691480064\n",
      "l2 norm of gradients: 0.3630239491347607\n",
      "l2 norm of weights: 6.085086351653989\n",
      "---------------------\n",
      "Iteration Number: 3285\n",
      "Loss: 44.56702990614511\n",
      "l2 norm of gradients: 0.36292572976869747\n",
      "l2 norm of weights: 6.084977578001985\n",
      "---------------------\n",
      "Iteration Number: 3286\n",
      "Loss: 44.56046640367457\n",
      "l2 norm of gradients: 0.3628275741719558\n",
      "l2 norm of weights: 6.084868821848134\n",
      "---------------------\n",
      "Iteration Number: 3287\n",
      "Loss: 44.55390640415666\n",
      "l2 norm of gradients: 0.36272948230629953\n",
      "l2 norm of weights: 6.084760083182141\n",
      "---------------------\n",
      "Iteration Number: 3288\n",
      "Loss: 44.5473499044105\n",
      "l2 norm of gradients: 0.36263145413348463\n",
      "l2 norm of weights: 6.0846513619937275\n",
      "---------------------\n",
      "Iteration Number: 3289\n",
      "Loss: 44.540796901247454\n",
      "l2 norm of gradients: 0.36253348961525966\n",
      "l2 norm of weights: 6.084542658272622\n",
      "---------------------\n",
      "Iteration Number: 3290\n",
      "Loss: 44.53424739150726\n",
      "l2 norm of gradients: 0.3624355887133656\n",
      "l2 norm of weights: 6.084433972008563\n",
      "---------------------\n",
      "Iteration Number: 3291\n",
      "Loss: 44.52770137196919\n",
      "l2 norm of gradients: 0.36233775138953594\n",
      "l2 norm of weights: 6.084325303191301\n",
      "---------------------\n",
      "Iteration Number: 3292\n",
      "Loss: 44.52115883947951\n",
      "l2 norm of gradients: 0.3622399776054965\n",
      "l2 norm of weights: 6.084216651810596\n",
      "---------------------\n",
      "Iteration Number: 3293\n",
      "Loss: 44.5146197908741\n",
      "l2 norm of gradients: 0.36214226732296534\n",
      "l2 norm of weights: 6.084108017856218\n",
      "---------------------\n",
      "Iteration Number: 3294\n",
      "Loss: 44.50808422296167\n",
      "l2 norm of gradients: 0.362044620503653\n",
      "l2 norm of weights: 6.083999401317949\n",
      "---------------------\n",
      "Iteration Number: 3295\n",
      "Loss: 44.50155213257877\n",
      "l2 norm of gradients: 0.36194703710926246\n",
      "l2 norm of weights: 6.0838908021855795\n",
      "---------------------\n",
      "Iteration Number: 3296\n",
      "Loss: 44.495023516565595\n",
      "l2 norm of gradients: 0.361849517101489\n",
      "l2 norm of weights: 6.083782220448912\n",
      "---------------------\n",
      "Iteration Number: 3297\n",
      "Loss: 44.48849837175924\n",
      "l2 norm of gradients: 0.36175206044202024\n",
      "l2 norm of weights: 6.083673656097756\n",
      "---------------------\n",
      "Iteration Number: 3298\n",
      "Loss: 44.48197669501736\n",
      "l2 norm of gradients: 0.36165466709253596\n",
      "l2 norm of weights: 6.083565109121936\n",
      "---------------------\n",
      "Iteration Number: 3299\n",
      "Loss: 44.47545848313989\n",
      "l2 norm of gradients: 0.36155733701470855\n",
      "l2 norm of weights: 6.083456579511284\n",
      "---------------------\n",
      "Iteration Number: 3300\n",
      "Loss: 44.46894373303448\n",
      "l2 norm of gradients: 0.3614600701702025\n",
      "l2 norm of weights: 6.083348067255644\n",
      "---------------------\n",
      "Iteration Number: 3301\n",
      "Loss: 44.46243244149765\n",
      "l2 norm of gradients: 0.36136286652067445\n",
      "l2 norm of weights: 6.083239572344867\n",
      "---------------------\n",
      "Iteration Number: 3302\n",
      "Loss: 44.45592460542567\n",
      "l2 norm of gradients: 0.36126572602777385\n",
      "l2 norm of weights: 6.083131094768818\n",
      "---------------------\n",
      "Iteration Number: 3303\n",
      "Loss: 44.449420221637865\n",
      "l2 norm of gradients: 0.36116864865314185\n",
      "l2 norm of weights: 6.083022634517371\n",
      "---------------------\n",
      "Iteration Number: 3304\n",
      "Loss: 44.44291928703938\n",
      "l2 norm of gradients: 0.3610716343584123\n",
      "l2 norm of weights: 6.082914191580409\n",
      "---------------------\n",
      "Iteration Number: 3305\n",
      "Loss: 44.43642179846861\n",
      "l2 norm of gradients: 0.36097468310521086\n",
      "l2 norm of weights: 6.082805765947828\n",
      "---------------------\n",
      "Iteration Number: 3306\n",
      "Loss: 44.42992775276501\n",
      "l2 norm of gradients: 0.3608777948551562\n",
      "l2 norm of weights: 6.082697357609533\n",
      "---------------------\n",
      "Iteration Number: 3307\n",
      "Loss: 44.42343714685844\n",
      "l2 norm of gradients: 0.36078096956985833\n",
      "l2 norm of weights: 6.082588966555435\n",
      "---------------------\n",
      "Iteration Number: 3308\n",
      "Loss: 44.416949977568\n",
      "l2 norm of gradients: 0.3606842072109202\n",
      "l2 norm of weights: 6.082480592775463\n",
      "---------------------\n",
      "Iteration Number: 3309\n",
      "Loss: 44.4104662418132\n",
      "l2 norm of gradients: 0.360587507739937\n",
      "l2 norm of weights: 6.082372236259552\n",
      "---------------------\n",
      "Iteration Number: 3310\n",
      "Loss: 44.403985936436484\n",
      "l2 norm of gradients: 0.36049087111849565\n",
      "l2 norm of weights: 6.082263896997646\n",
      "---------------------\n",
      "Iteration Number: 3311\n",
      "Loss: 44.39750905835956\n",
      "l2 norm of gradients: 0.36039429730817574\n",
      "l2 norm of weights: 6.082155574979702\n",
      "---------------------\n",
      "Iteration Number: 3312\n",
      "Loss: 44.39103560442899\n",
      "l2 norm of gradients: 0.3602977862705492\n",
      "l2 norm of weights: 6.0820472701956865\n",
      "---------------------\n",
      "Iteration Number: 3313\n",
      "Loss: 44.38456557155759\n",
      "l2 norm of gradients: 0.3602013379671798\n",
      "l2 norm of weights: 6.081938982635575\n",
      "---------------------\n",
      "Iteration Number: 3314\n",
      "Loss: 44.37809895663065\n",
      "l2 norm of gradients: 0.360104952359624\n",
      "l2 norm of weights: 6.081830712289354\n",
      "---------------------\n",
      "Iteration Number: 3315\n",
      "Loss: 44.37163575651938\n",
      "l2 norm of gradients: 0.3600086294094302\n",
      "l2 norm of weights: 6.08172245914702\n",
      "---------------------\n",
      "Iteration Number: 3316\n",
      "Loss: 44.365175968182434\n",
      "l2 norm of gradients: 0.35991236907813934\n",
      "l2 norm of weights: 6.08161422319858\n",
      "---------------------\n",
      "Iteration Number: 3317\n",
      "Loss: 44.35871958845382\n",
      "l2 norm of gradients: 0.35981617132728433\n",
      "l2 norm of weights: 6.081506004434052\n",
      "---------------------\n",
      "Iteration Number: 3318\n",
      "Loss: 44.35226661429172\n",
      "l2 norm of gradients: 0.3597200361183906\n",
      "l2 norm of weights: 6.081397802843463\n",
      "---------------------\n",
      "Iteration Number: 3319\n",
      "Loss: 44.345817042581146\n",
      "l2 norm of gradients: 0.35962396341297564\n",
      "l2 norm of weights: 6.081289618416849\n",
      "---------------------\n",
      "Iteration Number: 3320\n",
      "Loss: 44.339370870206125\n",
      "l2 norm of gradients: 0.35952795317254943\n",
      "l2 norm of weights: 6.08118145114426\n",
      "---------------------\n",
      "Iteration Number: 3321\n",
      "Loss: 44.33292809412117\n",
      "l2 norm of gradients: 0.3594320053586141\n",
      "l2 norm of weights: 6.081073301015751\n",
      "---------------------\n",
      "Iteration Number: 3322\n",
      "Loss: 44.32648871122683\n",
      "l2 norm of gradients: 0.359336119932664\n",
      "l2 norm of weights: 6.080965168021391\n",
      "---------------------\n",
      "Iteration Number: 3323\n",
      "Loss: 44.320052718446554\n",
      "l2 norm of gradients: 0.35924029685618614\n",
      "l2 norm of weights: 6.080857052151258\n",
      "---------------------\n",
      "Iteration Number: 3324\n",
      "Loss: 44.31362011271738\n",
      "l2 norm of gradients: 0.35914453609065944\n",
      "l2 norm of weights: 6.08074895339544\n",
      "---------------------\n",
      "Iteration Number: 3325\n",
      "Loss: 44.30719089091467\n",
      "l2 norm of gradients: 0.3590488375975551\n",
      "l2 norm of weights: 6.080640871744035\n",
      "---------------------\n",
      "Iteration Number: 3326\n",
      "Loss: 44.30076505002076\n",
      "l2 norm of gradients: 0.35895320133833714\n",
      "l2 norm of weights: 6.080532807187151\n",
      "---------------------\n",
      "Iteration Number: 3327\n",
      "Loss: 44.29434258694029\n",
      "l2 norm of gradients: 0.35885762727446135\n",
      "l2 norm of weights: 6.080424759714907\n",
      "---------------------\n",
      "Iteration Number: 3328\n",
      "Loss: 44.287923498631194\n",
      "l2 norm of gradients: 0.3587621153673762\n",
      "l2 norm of weights: 6.080316729317431\n",
      "---------------------\n",
      "Iteration Number: 3329\n",
      "Loss: 44.281507782005974\n",
      "l2 norm of gradients: 0.3586666655785226\n",
      "l2 norm of weights: 6.080208715984861\n",
      "---------------------\n",
      "Iteration Number: 3330\n",
      "Loss: 44.27509543403748\n",
      "l2 norm of gradients: 0.3585712778693335\n",
      "l2 norm of weights: 6.080100719707347\n",
      "---------------------\n",
      "Iteration Number: 3331\n",
      "Loss: 44.268686451628675\n",
      "l2 norm of gradients: 0.35847595220123446\n",
      "l2 norm of weights: 6.079992740475046\n",
      "---------------------\n",
      "Iteration Number: 3332\n",
      "Loss: 44.26228083176213\n",
      "l2 norm of gradients: 0.35838068853564337\n",
      "l2 norm of weights: 6.079884778278127\n",
      "---------------------\n",
      "Iteration Number: 3333\n",
      "Loss: 44.255878571369145\n",
      "l2 norm of gradients: 0.3582854868339706\n",
      "l2 norm of weights: 6.079776833106769\n",
      "---------------------\n",
      "Iteration Number: 3334\n",
      "Loss: 44.24947966740985\n",
      "l2 norm of gradients: 0.3581903470576188\n",
      "l2 norm of weights: 6.079668904951161\n",
      "---------------------\n",
      "Iteration Number: 3335\n",
      "Loss: 44.24308411685869\n",
      "l2 norm of gradients: 0.35809526916798323\n",
      "l2 norm of weights: 6.079560993801502\n",
      "---------------------\n",
      "Iteration Number: 3336\n",
      "Loss: 44.236691916651026\n",
      "l2 norm of gradients: 0.3580002531264515\n",
      "l2 norm of weights: 6.079453099648\n",
      "---------------------\n",
      "Iteration Number: 3337\n",
      "Loss: 44.230303063748984\n",
      "l2 norm of gradients: 0.3579052988944037\n",
      "l2 norm of weights: 6.079345222480874\n",
      "---------------------\n",
      "Iteration Number: 3338\n",
      "Loss: 44.223917555128644\n",
      "l2 norm of gradients: 0.3578104064332126\n",
      "l2 norm of weights: 6.079237362290352\n",
      "---------------------\n",
      "Iteration Number: 3339\n",
      "Loss: 44.21753538775778\n",
      "l2 norm of gradients: 0.35771557570424306\n",
      "l2 norm of weights: 6.079129519066674\n",
      "---------------------\n",
      "Iteration Number: 3340\n",
      "Loss: 44.211156558618555\n",
      "l2 norm of gradients: 0.35762080666885293\n",
      "l2 norm of weights: 6.079021692800088\n",
      "---------------------\n",
      "Iteration Number: 3341\n",
      "Loss: 44.20478106468464\n",
      "l2 norm of gradients: 0.35752609928839235\n",
      "l2 norm of weights: 6.078913883480854\n",
      "---------------------\n",
      "Iteration Number: 3342\n",
      "Loss: 44.19840890291339\n",
      "l2 norm of gradients: 0.3574314535242041\n",
      "l2 norm of weights: 6.0788060910992385\n",
      "---------------------\n",
      "Iteration Number: 3343\n",
      "Loss: 44.192040070296436\n",
      "l2 norm of gradients: 0.3573368693376236\n",
      "l2 norm of weights: 6.078698315645521\n",
      "---------------------\n",
      "Iteration Number: 3344\n",
      "Loss: 44.18567456381373\n",
      "l2 norm of gradients: 0.3572423466899787\n",
      "l2 norm of weights: 6.07859055710999\n",
      "---------------------\n",
      "Iteration Number: 3345\n",
      "Loss: 44.17931238047005\n",
      "l2 norm of gradients: 0.3571478855425901\n",
      "l2 norm of weights: 6.078482815482945\n",
      "---------------------\n",
      "Iteration Number: 3346\n",
      "Loss: 44.17295351725165\n",
      "l2 norm of gradients: 0.35705348585677094\n",
      "l2 norm of weights: 6.078375090754694\n",
      "---------------------\n",
      "Iteration Number: 3347\n",
      "Loss: 44.16659797113809\n",
      "l2 norm of gradients: 0.35695914759382713\n",
      "l2 norm of weights: 6.078267382915555\n",
      "---------------------\n",
      "Iteration Number: 3348\n",
      "Loss: 44.16024573913764\n",
      "l2 norm of gradients: 0.3568648707150574\n",
      "l2 norm of weights: 6.078159691955855\n",
      "---------------------\n",
      "Iteration Number: 3349\n",
      "Loss: 44.15389681824848\n",
      "l2 norm of gradients: 0.35677065518175316\n",
      "l2 norm of weights: 6.078052017865934\n",
      "---------------------\n",
      "Iteration Number: 3350\n",
      "Loss: 44.1475512054623\n",
      "l2 norm of gradients: 0.3566765009551983\n",
      "l2 norm of weights: 6.077944360636139\n",
      "---------------------\n",
      "Iteration Number: 3351\n",
      "Loss: 44.14120889780774\n",
      "l2 norm of gradients: 0.3565824079966699\n",
      "l2 norm of weights: 6.077836720256829\n",
      "---------------------\n",
      "Iteration Number: 3352\n",
      "Loss: 44.134869892272896\n",
      "l2 norm of gradients: 0.3564883762674375\n",
      "l2 norm of weights: 6.077729096718369\n",
      "---------------------\n",
      "Iteration Number: 3353\n",
      "Loss: 44.128534185872574\n",
      "l2 norm of gradients: 0.3563944057287637\n",
      "l2 norm of weights: 6.077621490011141\n",
      "---------------------\n",
      "Iteration Number: 3354\n",
      "Loss: 44.12220177562282\n",
      "l2 norm of gradients: 0.3563004963419037\n",
      "l2 norm of weights: 6.077513900125529\n",
      "---------------------\n",
      "Iteration Number: 3355\n",
      "Loss: 44.11587265854996\n",
      "l2 norm of gradients: 0.35620664806810576\n",
      "l2 norm of weights: 6.077406327051931\n",
      "---------------------\n",
      "Iteration Number: 3356\n",
      "Loss: 44.10954683166519\n",
      "l2 norm of gradients: 0.35611286086861105\n",
      "l2 norm of weights: 6.077298770780756\n",
      "---------------------\n",
      "Iteration Number: 3357\n",
      "Loss: 44.1032242919984\n",
      "l2 norm of gradients: 0.3560191347046537\n",
      "l2 norm of weights: 6.077191231302419\n",
      "---------------------\n",
      "Iteration Number: 3358\n",
      "Loss: 44.09690503658568\n",
      "l2 norm of gradients: 0.35592546953746057\n",
      "l2 norm of weights: 6.077083708607348\n",
      "---------------------\n",
      "Iteration Number: 3359\n",
      "Loss: 44.09058906242258\n",
      "l2 norm of gradients: 0.3558318653282519\n",
      "l2 norm of weights: 6.076976202685978\n",
      "---------------------\n",
      "Iteration Number: 3360\n",
      "Loss: 44.08427636658159\n",
      "l2 norm of gradients: 0.35573832203824063\n",
      "l2 norm of weights: 6.076868713528759\n",
      "---------------------\n",
      "Iteration Number: 3361\n",
      "Loss: 44.07796694607287\n",
      "l2 norm of gradients: 0.3556448396286328\n",
      "l2 norm of weights: 6.076761241126144\n",
      "---------------------\n",
      "Iteration Number: 3362\n",
      "Loss: 44.07166079794918\n",
      "l2 norm of gradients: 0.35555141806062784\n",
      "l2 norm of weights: 6.0766537854686\n",
      "---------------------\n",
      "Iteration Number: 3363\n",
      "Loss: 44.06535791925084\n",
      "l2 norm of gradients: 0.35545805729541774\n",
      "l2 norm of weights: 6.0765463465466025\n",
      "---------------------\n",
      "Iteration Number: 3364\n",
      "Loss: 44.059058306997194\n",
      "l2 norm of gradients: 0.35536475729418826\n",
      "l2 norm of weights: 6.076438924350638\n",
      "---------------------\n",
      "Iteration Number: 3365\n",
      "Loss: 44.05276195828427\n",
      "l2 norm of gradients: 0.35527151801811785\n",
      "l2 norm of weights: 6.076331518871202\n",
      "---------------------\n",
      "Iteration Number: 3366\n",
      "Loss: 44.04646887012743\n",
      "l2 norm of gradients: 0.35517833942837845\n",
      "l2 norm of weights: 6.076224130098799\n",
      "---------------------\n",
      "Iteration Number: 3367\n",
      "Loss: 44.04017903958946\n",
      "l2 norm of gradients: 0.35508522148613525\n",
      "l2 norm of weights: 6.076116758023943\n",
      "---------------------\n",
      "Iteration Number: 3368\n",
      "Loss: 44.03389246373182\n",
      "l2 norm of gradients: 0.3549921641525465\n",
      "l2 norm of weights: 6.07600940263716\n",
      "---------------------\n",
      "Iteration Number: 3369\n",
      "Loss: 44.027609139622115\n",
      "l2 norm of gradients: 0.3548991673887641\n",
      "l2 norm of weights: 6.075902063928983\n",
      "---------------------\n",
      "Iteration Number: 3370\n",
      "Loss: 44.0213290642845\n",
      "l2 norm of gradients: 0.3548062311559328\n",
      "l2 norm of weights: 6.075794741889958\n",
      "---------------------\n",
      "Iteration Number: 3371\n",
      "Loss: 44.01505223484532\n",
      "l2 norm of gradients: 0.3547133554151915\n",
      "l2 norm of weights: 6.075687436510636\n",
      "---------------------\n",
      "Iteration Number: 3372\n",
      "Loss: 44.008778648311264\n",
      "l2 norm of gradients: 0.35462054012767186\n",
      "l2 norm of weights: 6.075580147781583\n",
      "---------------------\n",
      "Iteration Number: 3373\n",
      "Loss: 44.002508301788254\n",
      "l2 norm of gradients: 0.35452778525449924\n",
      "l2 norm of weights: 6.075472875693371\n",
      "---------------------\n",
      "Iteration Number: 3374\n",
      "Loss: 43.99624119235433\n",
      "l2 norm of gradients: 0.3544350907567926\n",
      "l2 norm of weights: 6.0753656202365836\n",
      "---------------------\n",
      "Iteration Number: 3375\n",
      "Loss: 43.98997731708305\n",
      "l2 norm of gradients: 0.35434245659566427\n",
      "l2 norm of weights: 6.075258381401811\n",
      "---------------------\n",
      "Iteration Number: 3376\n",
      "Loss: 43.98371667304432\n",
      "l2 norm of gradients: 0.3542498827322203\n",
      "l2 norm of weights: 6.075151159179659\n",
      "---------------------\n",
      "Iteration Number: 3377\n",
      "Loss: 43.97745925731607\n",
      "l2 norm of gradients: 0.3541573691275601\n",
      "l2 norm of weights: 6.075043953560737\n",
      "---------------------\n",
      "Iteration Number: 3378\n",
      "Loss: 43.97120506701212\n",
      "l2 norm of gradients: 0.3540649157427773\n",
      "l2 norm of weights: 6.074936764535667\n",
      "---------------------\n",
      "Iteration Number: 3379\n",
      "Loss: 43.96495409921576\n",
      "l2 norm of gradients: 0.3539725225389585\n",
      "l2 norm of weights: 6.074829592095079\n",
      "---------------------\n",
      "Iteration Number: 3380\n",
      "Loss: 43.95870635099685\n",
      "l2 norm of gradients: 0.35388018947718475\n",
      "l2 norm of weights: 6.074722436229616\n",
      "---------------------\n",
      "Iteration Number: 3381\n",
      "Loss: 43.952461819480654\n",
      "l2 norm of gradients: 0.35378791651853014\n",
      "l2 norm of weights: 6.0746152969299265\n",
      "---------------------\n",
      "Iteration Number: 3382\n",
      "Loss: 43.946220501752954\n",
      "l2 norm of gradients: 0.35369570362406316\n",
      "l2 norm of weights: 6.074508174186672\n",
      "---------------------\n",
      "Iteration Number: 3383\n",
      "Loss: 43.93998239488678\n",
      "l2 norm of gradients: 0.35360355075484606\n",
      "l2 norm of weights: 6.07440106799052\n",
      "---------------------\n",
      "Iteration Number: 3384\n",
      "Loss: 43.93374749604391\n",
      "l2 norm of gradients: 0.35351145787193483\n",
      "l2 norm of weights: 6.074293978332151\n",
      "---------------------\n",
      "Iteration Number: 3385\n",
      "Loss: 43.92751580227811\n",
      "l2 norm of gradients: 0.35341942493637923\n",
      "l2 norm of weights: 6.074186905202254\n",
      "---------------------\n",
      "Iteration Number: 3386\n",
      "Loss: 43.92128731074073\n",
      "l2 norm of gradients: 0.35332745190922366\n",
      "l2 norm of weights: 6.074079848591526\n",
      "---------------------\n",
      "Iteration Number: 3387\n",
      "Loss: 43.915062018518796\n",
      "l2 norm of gradients: 0.353235538751506\n",
      "l2 norm of weights: 6.0739728084906774\n",
      "---------------------\n",
      "Iteration Number: 3388\n",
      "Loss: 43.90883992272819\n",
      "l2 norm of gradients: 0.3531436854242582\n",
      "l2 norm of weights: 6.0738657848904225\n",
      "---------------------\n",
      "Iteration Number: 3389\n",
      "Loss: 43.90262102050152\n",
      "l2 norm of gradients: 0.35305189188850666\n",
      "l2 norm of weights: 6.073758777781491\n",
      "---------------------\n",
      "Iteration Number: 3390\n",
      "Loss: 43.89640530896313\n",
      "l2 norm of gradients: 0.3529601581052717\n",
      "l2 norm of weights: 6.073651787154619\n",
      "---------------------\n",
      "Iteration Number: 3391\n",
      "Loss: 43.89019278522418\n",
      "l2 norm of gradients: 0.3528684840355681\n",
      "l2 norm of weights: 6.07354481300055\n",
      "---------------------\n",
      "Iteration Number: 3392\n",
      "Loss: 43.88398344643873\n",
      "l2 norm of gradients: 0.3527768696404045\n",
      "l2 norm of weights: 6.073437855310042\n",
      "---------------------\n",
      "Iteration Number: 3393\n",
      "Loss: 43.87777728971212\n",
      "l2 norm of gradients: 0.3526853148807842\n",
      "l2 norm of weights: 6.07333091407386\n",
      "---------------------\n",
      "Iteration Number: 3394\n",
      "Loss: 43.871574312171745\n",
      "l2 norm of gradients: 0.3525938197177046\n",
      "l2 norm of weights: 6.073223989282778\n",
      "---------------------\n",
      "Iteration Number: 3395\n",
      "Loss: 43.865374510973254\n",
      "l2 norm of gradients: 0.3525023841121577\n",
      "l2 norm of weights: 6.073117080927581\n",
      "---------------------\n",
      "Iteration Number: 3396\n",
      "Loss: 43.85917788324593\n",
      "l2 norm of gradients: 0.3524110080251299\n",
      "l2 norm of weights: 6.07301018899906\n",
      "---------------------\n",
      "Iteration Number: 3397\n",
      "Loss: 43.8529844261506\n",
      "l2 norm of gradients: 0.3523196914176021\n",
      "l2 norm of weights: 6.0729033134880215\n",
      "---------------------\n",
      "Iteration Number: 3398\n",
      "Loss: 43.84679413680924\n",
      "l2 norm of gradients: 0.3522284342505496\n",
      "l2 norm of weights: 6.072796454385277\n",
      "---------------------\n",
      "Iteration Number: 3399\n",
      "Loss: 43.84060701238324\n",
      "l2 norm of gradients: 0.35213723648494244\n",
      "l2 norm of weights: 6.072689611681648\n",
      "---------------------\n",
      "Iteration Number: 3400\n",
      "Loss: 43.834423050043675\n",
      "l2 norm of gradients: 0.35204609808174514\n",
      "l2 norm of weights: 6.072582785367965\n",
      "---------------------\n",
      "Iteration Number: 3401\n",
      "Loss: 43.82824224690293\n",
      "l2 norm of gradients: 0.35195501900191717\n",
      "l2 norm of weights: 6.072475975435071\n",
      "---------------------\n",
      "Iteration Number: 3402\n",
      "Loss: 43.822064600130545\n",
      "l2 norm of gradients: 0.3518639992064124\n",
      "l2 norm of weights: 6.072369181873817\n",
      "---------------------\n",
      "Iteration Number: 3403\n",
      "Loss: 43.81589010690692\n",
      "l2 norm of gradients: 0.35177303865618\n",
      "l2 norm of weights: 6.072262404675058\n",
      "---------------------\n",
      "Iteration Number: 3404\n",
      "Loss: 43.80971876439109\n",
      "l2 norm of gradients: 0.35168213731216336\n",
      "l2 norm of weights: 6.072155643829668\n",
      "---------------------\n",
      "Iteration Number: 3405\n",
      "Loss: 43.80355056974226\n",
      "l2 norm of gradients: 0.35159129513530113\n",
      "l2 norm of weights: 6.072048899328523\n",
      "---------------------\n",
      "Iteration Number: 3406\n",
      "Loss: 43.79738552011111\n",
      "l2 norm of gradients: 0.35150051208652683\n",
      "l2 norm of weights: 6.0719421711625134\n",
      "---------------------\n",
      "Iteration Number: 3407\n",
      "Loss: 43.79122361269447\n",
      "l2 norm of gradients: 0.3514097881267691\n",
      "l2 norm of weights: 6.071835459322535\n",
      "---------------------\n",
      "Iteration Number: 3408\n",
      "Loss: 43.78506484467019\n",
      "l2 norm of gradients: 0.3513191232169513\n",
      "l2 norm of weights: 6.071728763799494\n",
      "---------------------\n",
      "Iteration Number: 3409\n",
      "Loss: 43.7789092131896\n",
      "l2 norm of gradients: 0.35122851731799226\n",
      "l2 norm of weights: 6.071622084584307\n",
      "---------------------\n",
      "Iteration Number: 3410\n",
      "Loss: 43.77275671546276\n",
      "l2 norm of gradients: 0.3511379703908057\n",
      "l2 norm of weights: 6.071515421667901\n",
      "---------------------\n",
      "Iteration Number: 3411\n",
      "Loss: 43.76660734865925\n",
      "l2 norm of gradients: 0.3510474823963005\n",
      "l2 norm of weights: 6.071408775041209\n",
      "---------------------\n",
      "Iteration Number: 3412\n",
      "Loss: 43.7604611099579\n",
      "l2 norm of gradients: 0.35095705329538124\n",
      "l2 norm of weights: 6.071302144695176\n",
      "---------------------\n",
      "Iteration Number: 3413\n",
      "Loss: 43.75431799656335\n",
      "l2 norm of gradients: 0.3508666830489474\n",
      "l2 norm of weights: 6.071195530620756\n",
      "---------------------\n",
      "Iteration Number: 3414\n",
      "Loss: 43.74817800565273\n",
      "l2 norm of gradients: 0.3507763716178938\n",
      "l2 norm of weights: 6.07108893280891\n",
      "---------------------\n",
      "Iteration Number: 3415\n",
      "Loss: 43.742041134428504\n",
      "l2 norm of gradients: 0.350686118963111\n",
      "l2 norm of weights: 6.0709823512506125\n",
      "---------------------\n",
      "Iteration Number: 3416\n",
      "Loss: 43.73590738010069\n",
      "l2 norm of gradients: 0.35059592504548487\n",
      "l2 norm of weights: 6.0708757859368445\n",
      "---------------------\n",
      "Iteration Number: 3417\n",
      "Loss: 43.72977673983817\n",
      "l2 norm of gradients: 0.3505057898258966\n",
      "l2 norm of weights: 6.070769236858595\n",
      "---------------------\n",
      "Iteration Number: 3418\n",
      "Loss: 43.723649210892226\n",
      "l2 norm of gradients: 0.35041571326522336\n",
      "l2 norm of weights: 6.070662704006867\n",
      "---------------------\n",
      "Iteration Number: 3419\n",
      "Loss: 43.71752479041603\n",
      "l2 norm of gradients: 0.35032569532433766\n",
      "l2 norm of weights: 6.0705561873726674\n",
      "---------------------\n",
      "Iteration Number: 3420\n",
      "Loss: 43.71140347565121\n",
      "l2 norm of gradients: 0.350235735964108\n",
      "l2 norm of weights: 6.070449686947017\n",
      "---------------------\n",
      "Iteration Number: 3421\n",
      "Loss: 43.70528526381912\n",
      "l2 norm of gradients: 0.3501458351453982\n",
      "l2 norm of weights: 6.070343202720941\n",
      "---------------------\n",
      "Iteration Number: 3422\n",
      "Loss: 43.69917015209942\n",
      "l2 norm of gradients: 0.35005599282906846\n",
      "l2 norm of weights: 6.070236734685478\n",
      "---------------------\n",
      "Iteration Number: 3423\n",
      "Loss: 43.6930581377482\n",
      "l2 norm of gradients: 0.3499662089759743\n",
      "l2 norm of weights: 6.070130282831676\n",
      "---------------------\n",
      "Iteration Number: 3424\n",
      "Loss: 43.68694921795714\n",
      "l2 norm of gradients: 0.34987648354696765\n",
      "l2 norm of weights: 6.070023847150589\n",
      "---------------------\n",
      "Iteration Number: 3425\n",
      "Loss: 43.68084338996673\n",
      "l2 norm of gradients: 0.3497868165028961\n",
      "l2 norm of weights: 6.069917427633281\n",
      "---------------------\n",
      "Iteration Number: 3426\n",
      "Loss: 43.67474065098674\n",
      "l2 norm of gradients: 0.34969720780460334\n",
      "l2 norm of weights: 6.069811024270828\n",
      "---------------------\n",
      "Iteration Number: 3427\n",
      "Loss: 43.66864099827711\n",
      "l2 norm of gradients: 0.3496076574129294\n",
      "l2 norm of weights: 6.069704637054312\n",
      "---------------------\n",
      "Iteration Number: 3428\n",
      "Loss: 43.66254442902856\n",
      "l2 norm of gradients: 0.34951816528871005\n",
      "l2 norm of weights: 6.069598265974827\n",
      "---------------------\n",
      "Iteration Number: 3429\n",
      "Loss: 43.65645094051503\n",
      "l2 norm of gradients: 0.34942873139277786\n",
      "l2 norm of weights: 6.069491911023475\n",
      "---------------------\n",
      "Iteration Number: 3430\n",
      "Loss: 43.65036052995651\n",
      "l2 norm of gradients: 0.34933935568596103\n",
      "l2 norm of weights: 6.069385572191363\n",
      "---------------------\n",
      "Iteration Number: 3431\n",
      "Loss: 43.64427319460039\n",
      "l2 norm of gradients: 0.3492500381290847\n",
      "l2 norm of weights: 6.0692792494696155\n",
      "---------------------\n",
      "Iteration Number: 3432\n",
      "Loss: 43.638188931683636\n",
      "l2 norm of gradients: 0.34916077868296996\n",
      "l2 norm of weights: 6.0691729428493595\n",
      "---------------------\n",
      "Iteration Number: 3433\n",
      "Loss: 43.632107738444645\n",
      "l2 norm of gradients: 0.3490715773084347\n",
      "l2 norm of weights: 6.069066652321735\n",
      "---------------------\n",
      "Iteration Number: 3434\n",
      "Loss: 43.626029612156835\n",
      "l2 norm of gradients: 0.3489824339662932\n",
      "l2 norm of weights: 6.068960377877888\n",
      "---------------------\n",
      "Iteration Number: 3435\n",
      "Loss: 43.61995455004578\n",
      "l2 norm of gradients: 0.3488933486173562\n",
      "l2 norm of weights: 6.068854119508976\n",
      "---------------------\n",
      "Iteration Number: 3436\n",
      "Loss: 43.61388254938224\n",
      "l2 norm of gradients: 0.3488043212224314\n",
      "l2 norm of weights: 6.068747877206164\n",
      "---------------------\n",
      "Iteration Number: 3437\n",
      "Loss: 43.6078136074244\n",
      "l2 norm of gradients: 0.3487153517423229\n",
      "l2 norm of weights: 6.068641650960629\n",
      "---------------------\n",
      "Iteration Number: 3438\n",
      "Loss: 43.60174772144286\n",
      "l2 norm of gradients: 0.3486264401378316\n",
      "l2 norm of weights: 6.068535440763553\n",
      "---------------------\n",
      "Iteration Number: 3439\n",
      "Loss: 43.59568488868116\n",
      "l2 norm of gradients: 0.3485375863697557\n",
      "l2 norm of weights: 6.068429246606129\n",
      "---------------------\n",
      "Iteration Number: 3440\n",
      "Loss: 43.58962510640627\n",
      "l2 norm of gradients: 0.34844879039888954\n",
      "l2 norm of weights: 6.068323068479562\n",
      "---------------------\n",
      "Iteration Number: 3441\n",
      "Loss: 43.58356837190007\n",
      "l2 norm of gradients: 0.34836005218602506\n",
      "l2 norm of weights: 6.06821690637506\n",
      "---------------------\n",
      "Iteration Number: 3442\n",
      "Loss: 43.57751468243002\n",
      "l2 norm of gradients: 0.3482713716919508\n",
      "l2 norm of weights: 6.0681107602838456\n",
      "---------------------\n",
      "Iteration Number: 3443\n",
      "Loss: 43.571464035270864\n",
      "l2 norm of gradients: 0.3481827488774528\n",
      "l2 norm of weights: 6.068004630197148\n",
      "---------------------\n",
      "Iteration Number: 3444\n",
      "Loss: 43.565416427698175\n",
      "l2 norm of gradients: 0.34809418370331396\n",
      "l2 norm of weights: 6.067898516106205\n",
      "---------------------\n",
      "Iteration Number: 3445\n",
      "Loss: 43.55937185699365\n",
      "l2 norm of gradients: 0.34800567613031436\n",
      "l2 norm of weights: 6.0677924180022655\n",
      "---------------------\n",
      "Iteration Number: 3446\n",
      "Loss: 43.55333032044826\n",
      "l2 norm of gradients: 0.34791722611923154\n",
      "l2 norm of weights: 6.0676863358765845\n",
      "---------------------\n",
      "Iteration Number: 3447\n",
      "Loss: 43.54729181534199\n",
      "l2 norm of gradients: 0.3478288336308402\n",
      "l2 norm of weights: 6.067580269720429\n",
      "---------------------\n",
      "Iteration Number: 3448\n",
      "Loss: 43.541256338951854\n",
      "l2 norm of gradients: 0.3477404986259128\n",
      "l2 norm of weights: 6.0674742195250735\n",
      "---------------------\n",
      "Iteration Number: 3449\n",
      "Loss: 43.53522388858052\n",
      "l2 norm of gradients: 0.34765222106521887\n",
      "l2 norm of weights: 6.067368185281801\n",
      "---------------------\n",
      "Iteration Number: 3450\n",
      "Loss: 43.52919446153154\n",
      "l2 norm of gradients: 0.3475640009095256\n",
      "l2 norm of weights: 6.067262166981904\n",
      "---------------------\n",
      "Iteration Number: 3451\n",
      "Loss: 43.52316805508737\n",
      "l2 norm of gradients: 0.3474758381195979\n",
      "l2 norm of weights: 6.067156164616687\n",
      "---------------------\n",
      "Iteration Number: 3452\n",
      "Loss: 43.51714466655317\n",
      "l2 norm of gradients: 0.3473877326561983\n",
      "l2 norm of weights: 6.067050178177457\n",
      "---------------------\n",
      "Iteration Number: 3453\n",
      "Loss: 43.51112429324095\n",
      "l2 norm of gradients: 0.34729968448008697\n",
      "l2 norm of weights: 6.066944207655536\n",
      "---------------------\n",
      "Iteration Number: 3454\n",
      "Loss: 43.50510693246223\n",
      "l2 norm of gradients: 0.347211693552022\n",
      "l2 norm of weights: 6.066838253042252\n",
      "---------------------\n",
      "Iteration Number: 3455\n",
      "Loss: 43.49909258149349\n",
      "l2 norm of gradients: 0.34712375983275945\n",
      "l2 norm of weights: 6.066732314328942\n",
      "---------------------\n",
      "Iteration Number: 3456\n",
      "Loss: 43.493081237666274\n",
      "l2 norm of gradients: 0.34703588328305285\n",
      "l2 norm of weights: 6.066626391506954\n",
      "---------------------\n",
      "Iteration Number: 3457\n",
      "Loss: 43.48707289829517\n",
      "l2 norm of gradients: 0.3469480638636543\n",
      "l2 norm of weights: 6.066520484567643\n",
      "---------------------\n",
      "Iteration Number: 3458\n",
      "Loss: 43.481067560689816\n",
      "l2 norm of gradients: 0.3468603015353138\n",
      "l2 norm of weights: 6.066414593502373\n",
      "---------------------\n",
      "Iteration Number: 3459\n",
      "Loss: 43.475065222187055\n",
      "l2 norm of gradients: 0.34677259625877915\n",
      "l2 norm of weights: 6.066308718302517\n",
      "---------------------\n",
      "Iteration Number: 3460\n",
      "Loss: 43.46906588010584\n",
      "l2 norm of gradients: 0.3466849479947967\n",
      "l2 norm of weights: 6.066202858959459\n",
      "---------------------\n",
      "Iteration Number: 3461\n",
      "Loss: 43.4630695317484\n",
      "l2 norm of gradients: 0.34659735670411124\n",
      "l2 norm of weights: 6.066097015464588\n",
      "---------------------\n",
      "Iteration Number: 3462\n",
      "Loss: 43.45707617446272\n",
      "l2 norm of gradients: 0.34650982234746536\n",
      "l2 norm of weights: 6.065991187809306\n",
      "---------------------\n",
      "Iteration Number: 3463\n",
      "Loss: 43.45108580558561\n",
      "l2 norm of gradients: 0.3464223448856005\n",
      "l2 norm of weights: 6.0658853759850215\n",
      "---------------------\n",
      "Iteration Number: 3464\n",
      "Loss: 43.44509842241947\n",
      "l2 norm of gradients: 0.34633492427925633\n",
      "l2 norm of weights: 6.065779579983151\n",
      "---------------------\n",
      "Iteration Number: 3465\n",
      "Loss: 43.43911402233772\n",
      "l2 norm of gradients: 0.3462475604891712\n",
      "l2 norm of weights: 6.065673799795124\n",
      "---------------------\n",
      "Iteration Number: 3466\n",
      "Loss: 43.433132602663555\n",
      "l2 norm of gradients: 0.3461602534760821\n",
      "l2 norm of weights: 6.065568035412373\n",
      "---------------------\n",
      "Iteration Number: 3467\n",
      "Loss: 43.427154160728556\n",
      "l2 norm of gradients: 0.3460730032007246\n",
      "l2 norm of weights: 6.065462286826344\n",
      "---------------------\n",
      "Iteration Number: 3468\n",
      "Loss: 43.42117869390424\n",
      "l2 norm of gradients: 0.3459858096238331\n",
      "l2 norm of weights: 6.065356554028491\n",
      "---------------------\n",
      "Iteration Number: 3469\n",
      "Loss: 43.415206199503174\n",
      "l2 norm of gradients: 0.34589867270614066\n",
      "l2 norm of weights: 6.065250837010275\n",
      "---------------------\n",
      "Iteration Number: 3470\n",
      "Loss: 43.409236674917466\n",
      "l2 norm of gradients: 0.34581159240837944\n",
      "l2 norm of weights: 6.065145135763167\n",
      "---------------------\n",
      "Iteration Number: 3471\n",
      "Loss: 43.403270117466036\n",
      "l2 norm of gradients: 0.34572456869128054\n",
      "l2 norm of weights: 6.065039450278647\n",
      "---------------------\n",
      "Iteration Number: 3472\n",
      "Loss: 43.3973065245177\n",
      "l2 norm of gradients: 0.3456376015155739\n",
      "l2 norm of weights: 6.064933780548205\n",
      "---------------------\n",
      "Iteration Number: 3473\n",
      "Loss: 43.39134589341764\n",
      "l2 norm of gradients: 0.3455506908419887\n",
      "l2 norm of weights: 6.064828126563336\n",
      "---------------------\n",
      "Iteration Number: 3474\n",
      "Loss: 43.38538822154394\n",
      "l2 norm of gradients: 0.34546383663125335\n",
      "l2 norm of weights: 6.064722488315547\n",
      "---------------------\n",
      "Iteration Number: 3475\n",
      "Loss: 43.37943350626392\n",
      "l2 norm of gradients: 0.3453770388440953\n",
      "l2 norm of weights: 6.064616865796354\n",
      "---------------------\n",
      "Iteration Number: 3476\n",
      "Loss: 43.373481744931325\n",
      "l2 norm of gradients: 0.3452902974412417\n",
      "l2 norm of weights: 6.064511258997278\n",
      "---------------------\n",
      "Iteration Number: 3477\n",
      "Loss: 43.367532934919595\n",
      "l2 norm of gradients: 0.3452036123834185\n",
      "l2 norm of weights: 6.064405667909855\n",
      "---------------------\n",
      "Iteration Number: 3478\n",
      "Loss: 43.36158707361247\n",
      "l2 norm of gradients: 0.3451169836313516\n",
      "l2 norm of weights: 6.064300092525625\n",
      "---------------------\n",
      "Iteration Number: 3479\n",
      "Loss: 43.355644158372016\n",
      "l2 norm of gradients: 0.3450304111457662\n",
      "l2 norm of weights: 6.064194532836136\n",
      "---------------------\n",
      "Iteration Number: 3480\n",
      "Loss: 43.349704186565006\n",
      "l2 norm of gradients: 0.3449438948873872\n",
      "l2 norm of weights: 6.064088988832949\n",
      "---------------------\n",
      "Iteration Number: 3481\n",
      "Loss: 43.343767155597085\n",
      "l2 norm of gradients: 0.344857434816939\n",
      "l2 norm of weights: 6.063983460507631\n",
      "---------------------\n",
      "Iteration Number: 3482\n",
      "Loss: 43.33783306282973\n",
      "l2 norm of gradients: 0.34477103089514605\n",
      "l2 norm of weights: 6.063877947851758\n",
      "---------------------\n",
      "Iteration Number: 3483\n",
      "Loss: 43.33190190566119\n",
      "l2 norm of gradients: 0.34468468308273204\n",
      "l2 norm of weights: 6.063772450856915\n",
      "---------------------\n",
      "Iteration Number: 3484\n",
      "Loss: 43.32597368149246\n",
      "l2 norm of gradients: 0.3445983913404212\n",
      "l2 norm of weights: 6.063666969514695\n",
      "---------------------\n",
      "Iteration Number: 3485\n",
      "Loss: 43.320048387684324\n",
      "l2 norm of gradients: 0.3445121556289373\n",
      "l2 norm of weights: 6.0635615038167\n",
      "---------------------\n",
      "Iteration Number: 3486\n",
      "Loss: 43.314126021657415\n",
      "l2 norm of gradients: 0.34442597590900437\n",
      "l2 norm of weights: 6.0634560537545426\n",
      "---------------------\n",
      "Iteration Number: 3487\n",
      "Loss: 43.3082065807876\n",
      "l2 norm of gradients: 0.3443398521413462\n",
      "l2 norm of weights: 6.0633506193198405\n",
      "---------------------\n",
      "Iteration Number: 3488\n",
      "Loss: 43.302290062493775\n",
      "l2 norm of gradients: 0.3442537842866872\n",
      "l2 norm of weights: 6.063245200504223\n",
      "---------------------\n",
      "Iteration Number: 3489\n",
      "Loss: 43.296376464172056\n",
      "l2 norm of gradients: 0.3441677723057517\n",
      "l2 norm of weights: 6.0631397972993275\n",
      "---------------------\n",
      "Iteration Number: 3490\n",
      "Loss: 43.2904657832318\n",
      "l2 norm of gradients: 0.3440818161592643\n",
      "l2 norm of weights: 6.063034409696797\n",
      "---------------------\n",
      "Iteration Number: 3491\n",
      "Loss: 43.28455801705608\n",
      "l2 norm of gradients: 0.34399591580795036\n",
      "l2 norm of weights: 6.06292903768829\n",
      "---------------------\n",
      "Iteration Number: 3492\n",
      "Loss: 43.2786531630752\n",
      "l2 norm of gradients: 0.3439100712125353\n",
      "l2 norm of weights: 6.062823681265467\n",
      "---------------------\n",
      "Iteration Number: 3493\n",
      "Loss: 43.2727512187184\n",
      "l2 norm of gradients: 0.3438242823337453\n",
      "l2 norm of weights: 6.062718340419998\n",
      "---------------------\n",
      "Iteration Number: 3494\n",
      "Loss: 43.2668521813533\n",
      "l2 norm of gradients: 0.34373854913230706\n",
      "l2 norm of weights: 6.062613015143566\n",
      "---------------------\n",
      "Iteration Number: 3495\n",
      "Loss: 43.26095604846126\n",
      "l2 norm of gradients: 0.3436528715689479\n",
      "l2 norm of weights: 6.062507705427857\n",
      "---------------------\n",
      "Iteration Number: 3496\n",
      "Loss: 43.25506281741698\n",
      "l2 norm of gradients: 0.3435672496043961\n",
      "l2 norm of weights: 6.06240241126457\n",
      "---------------------\n",
      "Iteration Number: 3497\n",
      "Loss: 43.249172485666186\n",
      "l2 norm of gradients: 0.3434816831993806\n",
      "l2 norm of weights: 6.06229713264541\n",
      "---------------------\n",
      "Iteration Number: 3498\n",
      "Loss: 43.24328505061615\n",
      "l2 norm of gradients: 0.34339617231463127\n",
      "l2 norm of weights: 6.062191869562092\n",
      "---------------------\n",
      "Iteration Number: 3499\n",
      "Loss: 43.23740050971611\n",
      "l2 norm of gradients: 0.3433107169108789\n",
      "l2 norm of weights: 6.062086622006339\n",
      "---------------------\n",
      "Iteration Number: 3500\n",
      "Loss: 43.231518860399845\n",
      "l2 norm of gradients: 0.34322531694885544\n",
      "l2 norm of weights: 6.061981389969882\n",
      "---------------------\n",
      "Iteration Number: 3501\n",
      "Loss: 43.225640100077456\n",
      "l2 norm of gradients: 0.3431399723892939\n",
      "l2 norm of weights: 6.061876173444461\n",
      "---------------------\n",
      "Iteration Number: 3502\n",
      "Loss: 43.2197642262092\n",
      "l2 norm of gradients: 0.3430546831929285\n",
      "l2 norm of weights: 6.061770972421825\n",
      "---------------------\n",
      "Iteration Number: 3503\n",
      "Loss: 43.21389123622244\n",
      "l2 norm of gradients: 0.3429694493204947\n",
      "l2 norm of weights: 6.061665786893731\n",
      "---------------------\n",
      "Iteration Number: 3504\n",
      "Loss: 43.20802112757626\n",
      "l2 norm of gradients: 0.3428842707327293\n",
      "l2 norm of weights: 6.061560616851946\n",
      "---------------------\n",
      "Iteration Number: 3505\n",
      "Loss: 43.202153897693364\n",
      "l2 norm of gradients: 0.3427991473903705\n",
      "l2 norm of weights: 6.061455462288241\n",
      "---------------------\n",
      "Iteration Number: 3506\n",
      "Loss: 43.19628954404167\n",
      "l2 norm of gradients: 0.34271407925415825\n",
      "l2 norm of weights: 6.061350323194402\n",
      "---------------------\n",
      "Iteration Number: 3507\n",
      "Loss: 43.1904280640478\n",
      "l2 norm of gradients: 0.34262906628483353\n",
      "l2 norm of weights: 6.061245199562217\n",
      "---------------------\n",
      "Iteration Number: 3508\n",
      "Loss: 43.18456945519619\n",
      "l2 norm of gradients: 0.3425441084431395\n",
      "l2 norm of weights: 6.061140091383489\n",
      "---------------------\n",
      "Iteration Number: 3509\n",
      "Loss: 43.17871371493075\n",
      "l2 norm of gradients: 0.3424592056898207\n",
      "l2 norm of weights: 6.061034998650025\n",
      "---------------------\n",
      "Iteration Number: 3510\n",
      "Loss: 43.172860840702285\n",
      "l2 norm of gradients: 0.3423743579856237\n",
      "l2 norm of weights: 6.06092992135364\n",
      "---------------------\n",
      "Iteration Number: 3511\n",
      "Loss: 43.167010829986324\n",
      "l2 norm of gradients: 0.3422895652912966\n",
      "l2 norm of weights: 6.060824859486161\n",
      "---------------------\n",
      "Iteration Number: 3512\n",
      "Loss: 43.1611636802361\n",
      "l2 norm of gradients: 0.3422048275675897\n",
      "l2 norm of weights: 6.0607198130394195\n",
      "---------------------\n",
      "Iteration Number: 3513\n",
      "Loss: 43.155319388912545\n",
      "l2 norm of gradients: 0.34212014477525526\n",
      "l2 norm of weights: 6.06061478200526\n",
      "---------------------\n",
      "Iteration Number: 3514\n",
      "Loss: 43.14947795350188\n",
      "l2 norm of gradients: 0.3420355168750477\n",
      "l2 norm of weights: 6.060509766375531\n",
      "---------------------\n",
      "Iteration Number: 3515\n",
      "Loss: 43.143639371474826\n",
      "l2 norm of gradients: 0.34195094382772345\n",
      "l2 norm of weights: 6.060404766142091\n",
      "---------------------\n",
      "Iteration Number: 3516\n",
      "Loss: 43.13780364028849\n",
      "l2 norm of gradients: 0.3418664255940413\n",
      "l2 norm of weights: 6.060299781296808\n",
      "---------------------\n",
      "Iteration Number: 3517\n",
      "Loss: 43.13197075743168\n",
      "l2 norm of gradients: 0.3417819621347621\n",
      "l2 norm of weights: 6.0601948118315585\n",
      "---------------------\n",
      "Iteration Number: 3518\n",
      "Loss: 43.126140720407335\n",
      "l2 norm of gradients: 0.3416975534106496\n",
      "l2 norm of weights: 6.060089857738225\n",
      "---------------------\n",
      "Iteration Number: 3519\n",
      "Loss: 43.12031352664807\n",
      "l2 norm of gradients: 0.34161319938246953\n",
      "l2 norm of weights: 6.0599849190087\n",
      "---------------------\n",
      "Iteration Number: 3520\n",
      "Loss: 43.11448917368375\n",
      "l2 norm of gradients: 0.34152890001099034\n",
      "l2 norm of weights: 6.059879995634884\n",
      "---------------------\n",
      "Iteration Number: 3521\n",
      "Loss: 43.10866765897722\n",
      "l2 norm of gradients: 0.3414446552569831\n",
      "l2 norm of weights: 6.059775087608688\n",
      "---------------------\n",
      "Iteration Number: 3522\n",
      "Loss: 43.10284898003827\n",
      "l2 norm of gradients: 0.34136046508122153\n",
      "l2 norm of weights: 6.059670194922028\n",
      "---------------------\n",
      "Iteration Number: 3523\n",
      "Loss: 43.09703313433891\n",
      "l2 norm of gradients: 0.34127632944448216\n",
      "l2 norm of weights: 6.059565317566832\n",
      "---------------------\n",
      "Iteration Number: 3524\n",
      "Loss: 43.091220119391316\n",
      "l2 norm of gradients: 0.3411922483075444\n",
      "l2 norm of weights: 6.059460455535029\n",
      "---------------------\n",
      "Iteration Number: 3525\n",
      "Loss: 43.08540993267834\n",
      "l2 norm of gradients: 0.34110822163119037\n",
      "l2 norm of weights: 6.059355608818566\n",
      "---------------------\n",
      "Iteration Number: 3526\n",
      "Loss: 43.07960257173541\n",
      "l2 norm of gradients: 0.3410242493762055\n",
      "l2 norm of weights: 6.059250777409392\n",
      "---------------------\n",
      "Iteration Number: 3527\n",
      "Loss: 43.073798034023504\n",
      "l2 norm of gradients: 0.340940331503378\n",
      "l2 norm of weights: 6.059145961299468\n",
      "---------------------\n",
      "Iteration Number: 3528\n",
      "Loss: 43.06799631707282\n",
      "l2 norm of gradients: 0.34085646797349944\n",
      "l2 norm of weights: 6.05904116048076\n",
      "---------------------\n",
      "Iteration Number: 3529\n",
      "Loss: 43.06219741839951\n",
      "l2 norm of gradients: 0.34077265874736434\n",
      "l2 norm of weights: 6.058936374945242\n",
      "---------------------\n",
      "Iteration Number: 3530\n",
      "Loss: 43.05640133549083\n",
      "l2 norm of gradients: 0.34068890378577077\n",
      "l2 norm of weights: 6.058831604684901\n",
      "---------------------\n",
      "Iteration Number: 3531\n",
      "Loss: 43.050608065886045\n",
      "l2 norm of gradients: 0.3406052030495202\n",
      "l2 norm of weights: 6.058726849691727\n",
      "---------------------\n",
      "Iteration Number: 3532\n",
      "Loss: 43.044817607085804\n",
      "l2 norm of gradients: 0.3405215564994172\n",
      "l2 norm of weights: 6.058622109957723\n",
      "---------------------\n",
      "Iteration Number: 3533\n",
      "Loss: 43.03902995662648\n",
      "l2 norm of gradients: 0.3404379640962702\n",
      "l2 norm of weights: 6.058517385474896\n",
      "---------------------\n",
      "Iteration Number: 3534\n",
      "Loss: 43.03324511199571\n",
      "l2 norm of gradients: 0.3403544258008911\n",
      "l2 norm of weights: 6.058412676235263\n",
      "---------------------\n",
      "Iteration Number: 3535\n",
      "Loss: 43.027463070760675\n",
      "l2 norm of gradients: 0.34027094157409543\n",
      "l2 norm of weights: 6.05830798223085\n",
      "---------------------\n",
      "Iteration Number: 3536\n",
      "Loss: 43.021683830427406\n",
      "l2 norm of gradients: 0.3401875113767026\n",
      "l2 norm of weights: 6.058203303453691\n",
      "---------------------\n",
      "Iteration Number: 3537\n",
      "Loss: 43.015907388526095\n",
      "l2 norm of gradients: 0.34010413516953564\n",
      "l2 norm of weights: 6.058098639895825\n",
      "---------------------\n",
      "Iteration Number: 3538\n",
      "Loss: 43.01013374258335\n",
      "l2 norm of gradients: 0.3400208129134219\n",
      "l2 norm of weights: 6.057993991549306\n",
      "---------------------\n",
      "Iteration Number: 3539\n",
      "Loss: 43.004362890161254\n",
      "l2 norm of gradients: 0.339937544569192\n",
      "l2 norm of weights: 6.0578893584061895\n",
      "---------------------\n",
      "Iteration Number: 3540\n",
      "Loss: 42.99859482876466\n",
      "l2 norm of gradients: 0.3398543300976813\n",
      "l2 norm of weights: 6.057784740458543\n",
      "---------------------\n",
      "Iteration Number: 3541\n",
      "Loss: 42.99282955596153\n",
      "l2 norm of gradients: 0.33977116945972896\n",
      "l2 norm of weights: 6.0576801376984415\n",
      "---------------------\n",
      "Iteration Number: 3542\n",
      "Loss: 42.987067069270466\n",
      "l2 norm of gradients: 0.3396880626161783\n",
      "l2 norm of weights: 6.057575550117965\n",
      "---------------------\n",
      "Iteration Number: 3543\n",
      "Loss: 42.981307366256104\n",
      "l2 norm of gradients: 0.33960500952787714\n",
      "l2 norm of weights: 6.057470977709207\n",
      "---------------------\n",
      "Iteration Number: 3544\n",
      "Loss: 42.975550444461504\n",
      "l2 norm of gradients: 0.3395220101556774\n",
      "l2 norm of weights: 6.057366420464266\n",
      "---------------------\n",
      "Iteration Number: 3545\n",
      "Loss: 42.969796301446316\n",
      "l2 norm of gradients: 0.33943906446043576\n",
      "l2 norm of weights: 6.057261878375249\n",
      "---------------------\n",
      "Iteration Number: 3546\n",
      "Loss: 42.96404493474564\n",
      "l2 norm of gradients: 0.33935617240301286\n",
      "l2 norm of weights: 6.057157351434272\n",
      "---------------------\n",
      "Iteration Number: 3547\n",
      "Loss: 42.95829634191303\n",
      "l2 norm of gradients: 0.33927333394427467\n",
      "l2 norm of weights: 6.057052839633457\n",
      "---------------------\n",
      "Iteration Number: 3548\n",
      "Loss: 42.952550520530224\n",
      "l2 norm of gradients: 0.33919054904509116\n",
      "l2 norm of weights: 6.056948342964938\n",
      "---------------------\n",
      "Iteration Number: 3549\n",
      "Loss: 42.94680746814568\n",
      "l2 norm of gradients: 0.3391078176663373\n",
      "l2 norm of weights: 6.056843861420852\n",
      "---------------------\n",
      "Iteration Number: 3550\n",
      "Loss: 42.9410671823175\n",
      "l2 norm of gradients: 0.339025139768893\n",
      "l2 norm of weights: 6.056739394993348\n",
      "---------------------\n",
      "Iteration Number: 3551\n",
      "Loss: 42.935329660615\n",
      "l2 norm of gradients: 0.3389425153136428\n",
      "l2 norm of weights: 6.056634943674582\n",
      "---------------------\n",
      "Iteration Number: 3552\n",
      "Loss: 42.929594900632026\n",
      "l2 norm of gradients: 0.3388599442614763\n",
      "l2 norm of weights: 6.056530507456718\n",
      "---------------------\n",
      "Iteration Number: 3553\n",
      "Loss: 42.92386289990176\n",
      "l2 norm of gradients: 0.33877742657328824\n",
      "l2 norm of weights: 6.056426086331928\n",
      "---------------------\n",
      "Iteration Number: 3554\n",
      "Loss: 42.91813365603422\n",
      "l2 norm of gradients: 0.33869496220997836\n",
      "l2 norm of weights: 6.056321680292394\n",
      "---------------------\n",
      "Iteration Number: 3555\n",
      "Loss: 42.912407166567064\n",
      "l2 norm of gradients: 0.3386125511324516\n",
      "l2 norm of weights: 6.0562172893303\n",
      "---------------------\n",
      "Iteration Number: 3556\n",
      "Loss: 42.90668342911009\n",
      "l2 norm of gradients: 0.3385301933016182\n",
      "l2 norm of weights: 6.056112913437846\n",
      "---------------------\n",
      "Iteration Number: 3557\n",
      "Loss: 42.900962441231535\n",
      "l2 norm of gradients: 0.33844788867839376\n",
      "l2 norm of weights: 6.056008552607236\n",
      "---------------------\n",
      "Iteration Number: 3558\n",
      "Loss: 42.895244200525745\n",
      "l2 norm of gradients: 0.3383656372236993\n",
      "l2 norm of weights: 6.0559042068306805\n",
      "---------------------\n",
      "Iteration Number: 3559\n",
      "Loss: 42.88952870455459\n",
      "l2 norm of gradients: 0.3382834388984611\n",
      "l2 norm of weights: 6.0557998761004015\n",
      "---------------------\n",
      "Iteration Number: 3560\n",
      "Loss: 42.88381595094738\n",
      "l2 norm of gradients: 0.33820129366361135\n",
      "l2 norm of weights: 6.055695560408627\n",
      "---------------------\n",
      "Iteration Number: 3561\n",
      "Loss: 42.87810593727922\n",
      "l2 norm of gradients: 0.3381192014800877\n",
      "l2 norm of weights: 6.055591259747593\n",
      "---------------------\n",
      "Iteration Number: 3562\n",
      "Loss: 42.87239866113528\n",
      "l2 norm of gradients: 0.3380371623088336\n",
      "l2 norm of weights: 6.055486974109544\n",
      "---------------------\n",
      "Iteration Number: 3563\n",
      "Loss: 42.86669412010777\n",
      "l2 norm of gradients: 0.3379551761107983\n",
      "l2 norm of weights: 6.055382703486734\n",
      "---------------------\n",
      "Iteration Number: 3564\n",
      "Loss: 42.86099231181089\n",
      "l2 norm of gradients: 0.3378732428469367\n",
      "l2 norm of weights: 6.05527844787142\n",
      "---------------------\n",
      "Iteration Number: 3565\n",
      "Loss: 42.85529323385006\n",
      "l2 norm of gradients: 0.3377913624782099\n",
      "l2 norm of weights: 6.055174207255874\n",
      "---------------------\n",
      "Iteration Number: 3566\n",
      "Loss: 42.849596883826095\n",
      "l2 norm of gradients: 0.337709534965585\n",
      "l2 norm of weights: 6.05506998163237\n",
      "---------------------\n",
      "Iteration Number: 3567\n",
      "Loss: 42.84390325932887\n",
      "l2 norm of gradients: 0.337627760270035\n",
      "l2 norm of weights: 6.054965770993193\n",
      "---------------------\n",
      "Iteration Number: 3568\n",
      "Loss: 42.83821235798416\n",
      "l2 norm of gradients: 0.33754603835253927\n",
      "l2 norm of weights: 6.054861575330635\n",
      "---------------------\n",
      "Iteration Number: 3569\n",
      "Loss: 42.83252417741642\n",
      "l2 norm of gradients: 0.33746436917408346\n",
      "l2 norm of weights: 6.054757394636997\n",
      "---------------------\n",
      "Iteration Number: 3570\n",
      "Loss: 42.826838715228234\n",
      "l2 norm of gradients: 0.33738275269565926\n",
      "l2 norm of weights: 6.054653228904586\n",
      "---------------------\n",
      "Iteration Number: 3571\n",
      "Loss: 42.82115596903514\n",
      "l2 norm of gradients: 0.33730118887826505\n",
      "l2 norm of weights: 6.054549078125719\n",
      "---------------------\n",
      "Iteration Number: 3572\n",
      "Loss: 42.81547593644874\n",
      "l2 norm of gradients: 0.33721967768290545\n",
      "l2 norm of weights: 6.05444494229272\n",
      "---------------------\n",
      "Iteration Number: 3573\n",
      "Loss: 42.80979861509839\n",
      "l2 norm of gradients: 0.337138219070592\n",
      "l2 norm of weights: 6.054340821397919\n",
      "---------------------\n",
      "Iteration Number: 3574\n",
      "Loss: 42.80412400262032\n",
      "l2 norm of gradients: 0.33705681300234236\n",
      "l2 norm of weights: 6.0542367154336585\n",
      "---------------------\n",
      "Iteration Number: 3575\n",
      "Loss: 42.7984520966389\n",
      "l2 norm of gradients: 0.3369754594391813\n",
      "l2 norm of weights: 6.054132624392284\n",
      "---------------------\n",
      "Iteration Number: 3576\n",
      "Loss: 42.79278289476808\n",
      "l2 norm of gradients: 0.33689415834214015\n",
      "l2 norm of weights: 6.0540285482661504\n",
      "---------------------\n",
      "Iteration Number: 3577\n",
      "Loss: 42.7871163946528\n",
      "l2 norm of gradients: 0.33681290967225697\n",
      "l2 norm of weights: 6.053924487047624\n",
      "---------------------\n",
      "Iteration Number: 3578\n",
      "Loss: 42.78145259393609\n",
      "l2 norm of gradients: 0.33673171339057717\n",
      "l2 norm of weights: 6.053820440729074\n",
      "---------------------\n",
      "Iteration Number: 3579\n",
      "Loss: 42.77579149023979\n",
      "l2 norm of gradients: 0.33665056945815275\n",
      "l2 norm of weights: 6.053716409302879\n",
      "---------------------\n",
      "Iteration Number: 3580\n",
      "Loss: 42.7701330812171\n",
      "l2 norm of gradients: 0.336569477836043\n",
      "l2 norm of weights: 6.053612392761428\n",
      "---------------------\n",
      "Iteration Number: 3581\n",
      "Loss: 42.764477364513844\n",
      "l2 norm of gradients: 0.3364884384853142\n",
      "l2 norm of weights: 6.053508391097113\n",
      "---------------------\n",
      "Iteration Number: 3582\n",
      "Loss: 42.758824337739306\n",
      "l2 norm of gradients: 0.3364074513670401\n",
      "l2 norm of weights: 6.053404404302339\n",
      "---------------------\n",
      "Iteration Number: 3583\n",
      "Loss: 42.753173998591606\n",
      "l2 norm of gradients: 0.3363265164423013\n",
      "l2 norm of weights: 6.0533004323695145\n",
      "---------------------\n",
      "Iteration Number: 3584\n",
      "Loss: 42.74752634468265\n",
      "l2 norm of gradients: 0.33624563367218635\n",
      "l2 norm of weights: 6.05319647529106\n",
      "---------------------\n",
      "Iteration Number: 3585\n",
      "Loss: 42.741881373685075\n",
      "l2 norm of gradients: 0.3361648030177906\n",
      "l2 norm of weights: 6.053092533059398\n",
      "---------------------\n",
      "Iteration Number: 3586\n",
      "Loss: 42.736239083236654\n",
      "l2 norm of gradients: 0.3360840244402177\n",
      "l2 norm of weights: 6.0529886056669655\n",
      "---------------------\n",
      "Iteration Number: 3587\n",
      "Loss: 42.730599471023375\n",
      "l2 norm of gradients: 0.3360032979005781\n",
      "l2 norm of weights: 6.052884693106202\n",
      "---------------------\n",
      "Iteration Number: 3588\n",
      "Loss: 42.72496253468571\n",
      "l2 norm of gradients: 0.3359226233599903\n",
      "l2 norm of weights: 6.05278079536956\n",
      "---------------------\n",
      "Iteration Number: 3589\n",
      "Loss: 42.71932827189028\n",
      "l2 norm of gradients: 0.3358420007795806\n",
      "l2 norm of weights: 6.052676912449492\n",
      "---------------------\n",
      "Iteration Number: 3590\n",
      "Loss: 42.71369668030462\n",
      "l2 norm of gradients: 0.33576143012048304\n",
      "l2 norm of weights: 6.052573044338465\n",
      "---------------------\n",
      "Iteration Number: 3591\n",
      "Loss: 42.70806775759163\n",
      "l2 norm of gradients: 0.3356809113438395\n",
      "l2 norm of weights: 6.052469191028952\n",
      "---------------------\n",
      "Iteration Number: 3592\n",
      "Loss: 42.702441501425405\n",
      "l2 norm of gradients: 0.3356004444107997\n",
      "l2 norm of weights: 6.0523653525134335\n",
      "---------------------\n",
      "Iteration Number: 3593\n",
      "Loss: 42.69681790947285\n",
      "l2 norm of gradients: 0.3355200292825218\n",
      "l2 norm of weights: 6.052261528784396\n",
      "---------------------\n",
      "Iteration Number: 3594\n",
      "Loss: 42.69119697942268\n",
      "l2 norm of gradients: 0.33543966592017155\n",
      "l2 norm of weights: 6.052157719834336\n",
      "---------------------\n",
      "Iteration Number: 3595\n",
      "Loss: 42.68557870892993\n",
      "l2 norm of gradients: 0.33535935428492336\n",
      "l2 norm of weights: 6.052053925655758\n",
      "---------------------\n",
      "Iteration Number: 3596\n",
      "Loss: 42.679963095686\n",
      "l2 norm of gradients: 0.33527909433795944\n",
      "l2 norm of weights: 6.051950146241173\n",
      "---------------------\n",
      "Iteration Number: 3597\n",
      "Loss: 42.6743501373839\n",
      "l2 norm of gradients: 0.3351988860404709\n",
      "l2 norm of weights: 6.051846381583097\n",
      "---------------------\n",
      "Iteration Number: 3598\n",
      "Loss: 42.66873983169587\n",
      "l2 norm of gradients: 0.3351187293536566\n",
      "l2 norm of weights: 6.051742631674061\n",
      "---------------------\n",
      "Iteration Number: 3599\n",
      "Loss: 42.663132176325135\n",
      "l2 norm of gradients: 0.3350386242387243\n",
      "l2 norm of weights: 6.051638896506595\n",
      "---------------------\n",
      "Iteration Number: 3600\n",
      "Loss: 42.657527168945734\n",
      "l2 norm of gradients: 0.33495857065689016\n",
      "l2 norm of weights: 6.051535176073243\n",
      "---------------------\n",
      "Iteration Number: 3601\n",
      "Loss: 42.6519248072344\n",
      "l2 norm of gradients: 0.3348785685693792\n",
      "l2 norm of weights: 6.051431470366555\n",
      "---------------------\n",
      "Iteration Number: 3602\n",
      "Loss: 42.64632508891831\n",
      "l2 norm of gradients: 0.33479861793742477\n",
      "l2 norm of weights: 6.051327779379087\n",
      "---------------------\n",
      "Iteration Number: 3603\n",
      "Loss: 42.64072801168583\n",
      "l2 norm of gradients: 0.334718718722269\n",
      "l2 norm of weights: 6.051224103103404\n",
      "---------------------\n",
      "Iteration Number: 3604\n",
      "Loss: 42.635133573244715\n",
      "l2 norm of gradients: 0.33463887088516325\n",
      "l2 norm of weights: 6.051120441532079\n",
      "---------------------\n",
      "Iteration Number: 3605\n",
      "Loss: 42.6295417712645\n",
      "l2 norm of gradients: 0.3345590743873675\n",
      "l2 norm of weights: 6.051016794657692\n",
      "---------------------\n",
      "Iteration Number: 3606\n",
      "Loss: 42.62395260348104\n",
      "l2 norm of gradients: 0.33447932919015066\n",
      "l2 norm of weights: 6.050913162472829\n",
      "---------------------\n",
      "Iteration Number: 3607\n",
      "Loss: 42.618366067584454\n",
      "l2 norm of gradients: 0.33439963525479094\n",
      "l2 norm of weights: 6.050809544970089\n",
      "---------------------\n",
      "Iteration Number: 3608\n",
      "Loss: 42.61278216130611\n",
      "l2 norm of gradients: 0.3343199925425754\n",
      "l2 norm of weights: 6.05070594214207\n",
      "---------------------\n",
      "Iteration Number: 3609\n",
      "Loss: 42.60720088232815\n",
      "l2 norm of gradients: 0.3342404010148005\n",
      "l2 norm of weights: 6.050602353981387\n",
      "---------------------\n",
      "Iteration Number: 3610\n",
      "Loss: 42.601622228383846\n",
      "l2 norm of gradients: 0.33416086063277184\n",
      "l2 norm of weights: 6.050498780480655\n",
      "---------------------\n",
      "Iteration Number: 3611\n",
      "Loss: 42.596046197202156\n",
      "l2 norm of gradients: 0.33408137135780447\n",
      "l2 norm of weights: 6.050395221632501\n",
      "---------------------\n",
      "Iteration Number: 3612\n",
      "Loss: 42.59047278646038\n",
      "l2 norm of gradients: 0.3340019331512229\n",
      "l2 norm of weights: 6.050291677429557\n",
      "---------------------\n",
      "Iteration Number: 3613\n",
      "Loss: 42.58490199391836\n",
      "l2 norm of gradients: 0.333922545974361\n",
      "l2 norm of weights: 6.050188147864466\n",
      "---------------------\n",
      "Iteration Number: 3614\n",
      "Loss: 42.579333817296714\n",
      "l2 norm of gradients: 0.3338432097885621\n",
      "l2 norm of weights: 6.050084632929874\n",
      "---------------------\n",
      "Iteration Number: 3615\n",
      "Loss: 42.5737682543162\n",
      "l2 norm of gradients: 0.33376392455517945\n",
      "l2 norm of weights: 6.049981132618436\n",
      "---------------------\n",
      "Iteration Number: 3616\n",
      "Loss: 42.5682053026856\n",
      "l2 norm of gradients: 0.3336846902355759\n",
      "l2 norm of weights: 6.049877646922819\n",
      "---------------------\n",
      "Iteration Number: 3617\n",
      "Loss: 42.562644960161954\n",
      "l2 norm of gradients: 0.3336055067911238\n",
      "l2 norm of weights: 6.04977417583569\n",
      "---------------------\n",
      "Iteration Number: 3618\n",
      "Loss: 42.55708722447763\n",
      "l2 norm of gradients: 0.33352637418320574\n",
      "l2 norm of weights: 6.049670719349731\n",
      "---------------------\n",
      "Iteration Number: 3619\n",
      "Loss: 42.55153209336813\n",
      "l2 norm of gradients: 0.3334472923732139\n",
      "l2 norm of weights: 6.0495672774576255\n",
      "---------------------\n",
      "Iteration Number: 3620\n",
      "Loss: 42.545979564557236\n",
      "l2 norm of gradients: 0.33336826132255076\n",
      "l2 norm of weights: 6.0494638501520654\n",
      "---------------------\n",
      "Iteration Number: 3621\n",
      "Loss: 42.54042963580476\n",
      "l2 norm of gradients: 0.3332892809926287\n",
      "l2 norm of weights: 6.049360437425755\n",
      "---------------------\n",
      "Iteration Number: 3622\n",
      "Loss: 42.53488230483887\n",
      "l2 norm of gradients: 0.33321035134487015\n",
      "l2 norm of weights: 6.0492570392714\n",
      "---------------------\n",
      "Iteration Number: 3623\n",
      "Loss: 42.529337569405314\n",
      "l2 norm of gradients: 0.3331314723407078\n",
      "l2 norm of weights: 6.049153655681718\n",
      "---------------------\n",
      "Iteration Number: 3624\n",
      "Loss: 42.52379542727073\n",
      "l2 norm of gradients: 0.3330526439415845\n",
      "l2 norm of weights: 6.049050286649432\n",
      "---------------------\n",
      "Iteration Number: 3625\n",
      "Loss: 42.51825587617724\n",
      "l2 norm of gradients: 0.3329738661089537\n",
      "l2 norm of weights: 6.0489469321672695\n",
      "---------------------\n",
      "Iteration Number: 3626\n",
      "Loss: 42.51271891387823\n",
      "l2 norm of gradients: 0.332895138804279\n",
      "l2 norm of weights: 6.048843592227972\n",
      "---------------------\n",
      "Iteration Number: 3627\n",
      "Loss: 42.507184538108554\n",
      "l2 norm of gradients: 0.33281646198903475\n",
      "l2 norm of weights: 6.048740266824285\n",
      "---------------------\n",
      "Iteration Number: 3628\n",
      "Loss: 42.50165274666493\n",
      "l2 norm of gradients: 0.3327378356247055\n",
      "l2 norm of weights: 6.048636955948959\n",
      "---------------------\n",
      "Iteration Number: 3629\n",
      "Loss: 42.49612353728147\n",
      "l2 norm of gradients: 0.33265925967278664\n",
      "l2 norm of weights: 6.048533659594757\n",
      "---------------------\n",
      "Iteration Number: 3630\n",
      "Loss: 42.4905969077263\n",
      "l2 norm of gradients: 0.33258073409478417\n",
      "l2 norm of weights: 6.048430377754445\n",
      "---------------------\n",
      "Iteration Number: 3631\n",
      "Loss: 42.48507285576466\n",
      "l2 norm of gradients: 0.33250225885221496\n",
      "l2 norm of weights: 6.048327110420799\n",
      "---------------------\n",
      "Iteration Number: 3632\n",
      "Loss: 42.47955137916378\n",
      "l2 norm of gradients: 0.33242383390660646\n",
      "l2 norm of weights: 6.048223857586602\n",
      "---------------------\n",
      "Iteration Number: 3633\n",
      "Loss: 42.47403247570268\n",
      "l2 norm of gradients: 0.3323454592194973\n",
      "l2 norm of weights: 6.048120619244643\n",
      "---------------------\n",
      "Iteration Number: 3634\n",
      "Loss: 42.468516143135304\n",
      "l2 norm of gradients: 0.3322671347524369\n",
      "l2 norm of weights: 6.048017395387719\n",
      "---------------------\n",
      "Iteration Number: 3635\n",
      "Loss: 42.463002379258135\n",
      "l2 norm of gradients: 0.3321888604669857\n",
      "l2 norm of weights: 6.047914186008636\n",
      "---------------------\n",
      "Iteration Number: 3636\n",
      "Loss: 42.45749118181963\n",
      "l2 norm of gradients: 0.3321106363247154\n",
      "l2 norm of weights: 6.047810991100204\n",
      "---------------------\n",
      "Iteration Number: 3637\n",
      "Loss: 42.45198254863759\n",
      "l2 norm of gradients: 0.3320324622872085\n",
      "l2 norm of weights: 6.047707810655245\n",
      "---------------------\n",
      "Iteration Number: 3638\n",
      "Loss: 42.446476477467854\n",
      "l2 norm of gradients: 0.33195433831605936\n",
      "l2 norm of weights: 6.047604644666584\n",
      "---------------------\n",
      "Iteration Number: 3639\n",
      "Loss: 42.44097296609847\n",
      "l2 norm of gradients: 0.33187626437287293\n",
      "l2 norm of weights: 6.047501493127054\n",
      "---------------------\n",
      "Iteration Number: 3640\n",
      "Loss: 42.43547201230685\n",
      "l2 norm of gradients: 0.33179824041926614\n",
      "l2 norm of weights: 6.047398356029499\n",
      "---------------------\n",
      "Iteration Number: 3641\n",
      "Loss: 42.429973613915514\n",
      "l2 norm of gradients: 0.33172026641686697\n",
      "l2 norm of weights: 6.047295233366765\n",
      "---------------------\n",
      "Iteration Number: 3642\n",
      "Loss: 42.42447776868291\n",
      "l2 norm of gradients: 0.33164234232731504\n",
      "l2 norm of weights: 6.047192125131711\n",
      "---------------------\n",
      "Iteration Number: 3643\n",
      "Loss: 42.41898447442046\n",
      "l2 norm of gradients: 0.3315644681122615\n",
      "l2 norm of weights: 6.047089031317197\n",
      "---------------------\n",
      "Iteration Number: 3644\n",
      "Loss: 42.41349372890855\n",
      "l2 norm of gradients: 0.3314866437333693\n",
      "l2 norm of weights: 6.0469859519160964\n",
      "---------------------\n",
      "Iteration Number: 3645\n",
      "Loss: 42.40800552995076\n",
      "l2 norm of gradients: 0.3314088691523129\n",
      "l2 norm of weights: 6.046882886921285\n",
      "---------------------\n",
      "Iteration Number: 3646\n",
      "Loss: 42.40251987536763\n",
      "l2 norm of gradients: 0.3313311443307785\n",
      "l2 norm of weights: 6.046779836325648\n",
      "---------------------\n",
      "Iteration Number: 3647\n",
      "Loss: 42.39703676292688\n",
      "l2 norm of gradients: 0.33125346923046445\n",
      "l2 norm of weights: 6.046676800122079\n",
      "---------------------\n",
      "Iteration Number: 3648\n",
      "Loss: 42.39155619047967\n",
      "l2 norm of gradients: 0.33117584381308074\n",
      "l2 norm of weights: 6.046573778303476\n",
      "---------------------\n",
      "Iteration Number: 3649\n",
      "Loss: 42.3860781557851\n",
      "l2 norm of gradients: 0.3310982680403495\n",
      "l2 norm of weights: 6.046470770862747\n",
      "---------------------\n",
      "Iteration Number: 3650\n",
      "Loss: 42.38060265669858\n",
      "l2 norm of gradients: 0.33102074187400465\n",
      "l2 norm of weights: 6.046367777792806\n",
      "---------------------\n",
      "Iteration Number: 3651\n",
      "Loss: 42.37512969099855\n",
      "l2 norm of gradients: 0.3309432652757925\n",
      "l2 norm of weights: 6.046264799086574\n",
      "---------------------\n",
      "Iteration Number: 3652\n",
      "Loss: 42.36965925650994\n",
      "l2 norm of gradients: 0.33086583820747145\n",
      "l2 norm of weights: 6.046161834736979\n",
      "---------------------\n",
      "Iteration Number: 3653\n",
      "Loss: 42.36419135105899\n",
      "l2 norm of gradients: 0.3307884606308122\n",
      "l2 norm of weights: 6.046058884736957\n",
      "---------------------\n",
      "Iteration Number: 3654\n",
      "Loss: 42.35872597245989\n",
      "l2 norm of gradients: 0.3307111325075975\n",
      "l2 norm of weights: 6.045955949079452\n",
      "---------------------\n",
      "Iteration Number: 3655\n",
      "Loss: 42.35326311852455\n",
      "l2 norm of gradients: 0.3306338537996228\n",
      "l2 norm of weights: 6.0458530277574125\n",
      "---------------------\n",
      "Iteration Number: 3656\n",
      "Loss: 42.3478027870984\n",
      "l2 norm of gradients: 0.33055662446869566\n",
      "l2 norm of weights: 6.045750120763796\n",
      "---------------------\n",
      "Iteration Number: 3657\n",
      "Loss: 42.34234497599449\n",
      "l2 norm of gradients: 0.33047944447663646\n",
      "l2 norm of weights: 6.0456472280915685\n",
      "---------------------\n",
      "Iteration Number: 3658\n",
      "Loss: 42.33688968304304\n",
      "l2 norm of gradients: 0.3304023137852781\n",
      "l2 norm of weights: 6.0455443497337\n",
      "---------------------\n",
      "Iteration Number: 3659\n",
      "Loss: 42.33143690608177\n",
      "l2 norm of gradients: 0.33032523235646594\n",
      "l2 norm of weights: 6.04544148568317\n",
      "---------------------\n",
      "Iteration Number: 3660\n",
      "Loss: 42.32598664291841\n",
      "l2 norm of gradients: 0.33024820015205814\n",
      "l2 norm of weights: 6.045338635932964\n",
      "---------------------\n",
      "Iteration Number: 3661\n",
      "Loss: 42.320538891428455\n",
      "l2 norm of gradients: 0.3301712171339256\n",
      "l2 norm of weights: 6.045235800476076\n",
      "---------------------\n",
      "Iteration Number: 3662\n",
      "Loss: 42.31509364942652\n",
      "l2 norm of gradients: 0.33009428326395224\n",
      "l2 norm of weights: 6.045132979305505\n",
      "---------------------\n",
      "Iteration Number: 3663\n",
      "Loss: 42.30965091475579\n",
      "l2 norm of gradients: 0.3300173985040344\n",
      "l2 norm of weights: 6.045030172414258\n",
      "---------------------\n",
      "Iteration Number: 3664\n",
      "Loss: 42.30421068526169\n",
      "l2 norm of gradients: 0.32994056281608186\n",
      "l2 norm of weights: 6.044927379795352\n",
      "---------------------\n",
      "Iteration Number: 3665\n",
      "Loss: 42.29877295879789\n",
      "l2 norm of gradients: 0.32986377616201723\n",
      "l2 norm of weights: 6.044824601441806\n",
      "---------------------\n",
      "Iteration Number: 3666\n",
      "Loss: 42.29333773318988\n",
      "l2 norm of gradients: 0.32978703850377633\n",
      "l2 norm of weights: 6.044721837346649\n",
      "---------------------\n",
      "Iteration Number: 3667\n",
      "Loss: 42.28790500631531\n",
      "l2 norm of gradients: 0.3297103498033079\n",
      "l2 norm of weights: 6.044619087502918\n",
      "---------------------\n",
      "Iteration Number: 3668\n",
      "Loss: 42.2824747760011\n",
      "l2 norm of gradients: 0.3296337100225739\n",
      "l2 norm of weights: 6.044516351903654\n",
      "---------------------\n",
      "Iteration Number: 3669\n",
      "Loss: 42.27704704011746\n",
      "l2 norm of gradients: 0.3295571191235499\n",
      "l2 norm of weights: 6.044413630541909\n",
      "---------------------\n",
      "Iteration Number: 3670\n",
      "Loss: 42.27162179651955\n",
      "l2 norm of gradients: 0.32948057706822453\n",
      "l2 norm of weights: 6.044310923410738\n",
      "---------------------\n",
      "Iteration Number: 3671\n",
      "Loss: 42.26619904305675\n",
      "l2 norm of gradients: 0.3294040838185999\n",
      "l2 norm of weights: 6.044208230503207\n",
      "---------------------\n",
      "Iteration Number: 3672\n",
      "Loss: 42.26077877760444\n",
      "l2 norm of gradients: 0.32932763933669146\n",
      "l2 norm of weights: 6.044105551812385\n",
      "---------------------\n",
      "Iteration Number: 3673\n",
      "Loss: 42.25536099802946\n",
      "l2 norm of gradients: 0.3292512435845284\n",
      "l2 norm of weights: 6.044002887331351\n",
      "---------------------\n",
      "Iteration Number: 3674\n",
      "Loss: 42.249945702172056\n",
      "l2 norm of gradients: 0.32917489652415344\n",
      "l2 norm of weights: 6.043900237053191\n",
      "---------------------\n",
      "Iteration Number: 3675\n",
      "Loss: 42.24453288792715\n",
      "l2 norm of gradients: 0.3290985981176227\n",
      "l2 norm of weights: 6.043797600970996\n",
      "---------------------\n",
      "Iteration Number: 3676\n",
      "Loss: 42.23912255314812\n",
      "l2 norm of gradients: 0.32902234832700644\n",
      "l2 norm of weights: 6.043694979077865\n",
      "---------------------\n",
      "Iteration Number: 3677\n",
      "Loss: 42.23371469572132\n",
      "l2 norm of gradients: 0.32894614711438847\n",
      "l2 norm of weights: 6.0435923713669055\n",
      "---------------------\n",
      "Iteration Number: 3678\n",
      "Loss: 42.2283093135059\n",
      "l2 norm of gradients: 0.32886999444186643\n",
      "l2 norm of weights: 6.043489777831229\n",
      "---------------------\n",
      "Iteration Number: 3679\n",
      "Loss: 42.22290640440455\n",
      "l2 norm of gradients: 0.32879389027155187\n",
      "l2 norm of weights: 6.043387198463957\n",
      "---------------------\n",
      "Iteration Number: 3680\n",
      "Loss: 42.217505966262685\n",
      "l2 norm of gradients: 0.3287178345655703\n",
      "l2 norm of weights: 6.043284633258216\n",
      "---------------------\n",
      "Iteration Number: 3681\n",
      "Loss: 42.21210799700045\n",
      "l2 norm of gradients: 0.32864182728606134\n",
      "l2 norm of weights: 6.0431820822071405\n",
      "---------------------\n",
      "Iteration Number: 3682\n",
      "Loss: 42.20671249447208\n",
      "l2 norm of gradients: 0.32856586839517854\n",
      "l2 norm of weights: 6.043079545303872\n",
      "---------------------\n",
      "Iteration Number: 3683\n",
      "Loss: 42.20131945658031\n",
      "l2 norm of gradients: 0.3284899578550898\n",
      "l2 norm of weights: 6.042977022541557\n",
      "---------------------\n",
      "Iteration Number: 3684\n",
      "Loss: 42.19592888119779\n",
      "l2 norm of gradients: 0.328414095627977\n",
      "l2 norm of weights: 6.042874513913351\n",
      "---------------------\n",
      "Iteration Number: 3685\n",
      "Loss: 42.19054076623136\n",
      "l2 norm of gradients: 0.32833828167603646\n",
      "l2 norm of weights: 6.042772019412417\n",
      "---------------------\n",
      "Iteration Number: 3686\n",
      "Loss: 42.18515510958533\n",
      "l2 norm of gradients: 0.3282625159614788\n",
      "l2 norm of weights: 6.042669539031923\n",
      "---------------------\n",
      "Iteration Number: 3687\n",
      "Loss: 42.17977190912865\n",
      "l2 norm of gradients: 0.3281867984465289\n",
      "l2 norm of weights: 6.042567072765045\n",
      "---------------------\n",
      "Iteration Number: 3688\n",
      "Loss: 42.17439116277349\n",
      "l2 norm of gradients: 0.3281111290934263\n",
      "l2 norm of weights: 6.0424646206049655\n",
      "---------------------\n",
      "Iteration Number: 3689\n",
      "Loss: 42.16901286841094\n",
      "l2 norm of gradients: 0.3280355078644248\n",
      "l2 norm of weights: 6.042362182544874\n",
      "---------------------\n",
      "Iteration Number: 3690\n",
      "Loss: 42.163637023944936\n",
      "l2 norm of gradients: 0.32795993472179297\n",
      "l2 norm of weights: 6.042259758577967\n",
      "---------------------\n",
      "Iteration Number: 3691\n",
      "Loss: 42.158263627304066\n",
      "l2 norm of gradients: 0.327884409627814\n",
      "l2 norm of weights: 6.042157348697449\n",
      "---------------------\n",
      "Iteration Number: 3692\n",
      "Loss: 42.15289267636452\n",
      "l2 norm of gradients: 0.3278089325447856\n",
      "l2 norm of weights: 6.04205495289653\n",
      "---------------------\n",
      "Iteration Number: 3693\n",
      "Loss: 42.14752416904624\n",
      "l2 norm of gradients: 0.32773350343502033\n",
      "l2 norm of weights: 6.041952571168426\n",
      "---------------------\n",
      "Iteration Number: 3694\n",
      "Loss: 42.14215810326893\n",
      "l2 norm of gradients: 0.3276581222608455\n",
      "l2 norm of weights: 6.041850203506361\n",
      "---------------------\n",
      "Iteration Number: 3695\n",
      "Loss: 42.13679447693522\n",
      "l2 norm of gradients: 0.32758278898460347\n",
      "l2 norm of weights: 6.041747849903567\n",
      "---------------------\n",
      "Iteration Number: 3696\n",
      "Loss: 42.13143328797385\n",
      "l2 norm of gradients: 0.32750750356865127\n",
      "l2 norm of weights: 6.041645510353281\n",
      "---------------------\n",
      "Iteration Number: 3697\n",
      "Loss: 42.126074534297075\n",
      "l2 norm of gradients: 0.32743226597536107\n",
      "l2 norm of weights: 6.041543184848748\n",
      "---------------------\n",
      "Iteration Number: 3698\n",
      "Loss: 42.120718213809496\n",
      "l2 norm of gradients: 0.32735707616711995\n",
      "l2 norm of weights: 6.041440873383219\n",
      "---------------------\n",
      "Iteration Number: 3699\n",
      "Loss: 42.115364324455214\n",
      "l2 norm of gradients: 0.3272819341063304\n",
      "l2 norm of weights: 6.041338575949952\n",
      "---------------------\n",
      "Iteration Number: 3700\n",
      "Loss: 42.11001286415501\n",
      "l2 norm of gradients: 0.3272068397554094\n",
      "l2 norm of weights: 6.041236292542213\n",
      "---------------------\n",
      "Iteration Number: 3701\n",
      "Loss: 42.10466383082938\n",
      "l2 norm of gradients: 0.32713179307678997\n",
      "l2 norm of weights: 6.0411340231532735\n",
      "---------------------\n",
      "Iteration Number: 3702\n",
      "Loss: 42.09931722240485\n",
      "l2 norm of gradients: 0.3270567940329198\n",
      "l2 norm of weights: 6.041031767776412\n",
      "---------------------\n",
      "Iteration Number: 3703\n",
      "Loss: 42.093973036827975\n",
      "l2 norm of gradients: 0.32698184258626206\n",
      "l2 norm of weights: 6.040929526404912\n",
      "---------------------\n",
      "Iteration Number: 3704\n",
      "Loss: 42.08863127202484\n",
      "l2 norm of gradients: 0.3269069386992955\n",
      "l2 norm of weights: 6.0408272990320695\n",
      "---------------------\n",
      "Iteration Number: 3705\n",
      "Loss: 42.08329192593874\n",
      "l2 norm of gradients: 0.32683208233451405\n",
      "l2 norm of weights: 6.040725085651181\n",
      "---------------------\n",
      "Iteration Number: 3706\n",
      "Loss: 42.077954996477835\n",
      "l2 norm of gradients: 0.3267572734544272\n",
      "l2 norm of weights: 6.040622886255552\n",
      "---------------------\n",
      "Iteration Number: 3707\n",
      "Loss: 42.07262048163239\n",
      "l2 norm of gradients: 0.32668251202156\n",
      "l2 norm of weights: 6.040520700838498\n",
      "---------------------\n",
      "Iteration Number: 3708\n",
      "Loss: 42.067288379302035\n",
      "l2 norm of gradients: 0.3266077979984532\n",
      "l2 norm of weights: 6.040418529393334\n",
      "---------------------\n",
      "Iteration Number: 3709\n",
      "Loss: 42.06195868744958\n",
      "l2 norm of gradients: 0.32653313134766326\n",
      "l2 norm of weights: 6.0403163719133905\n",
      "---------------------\n",
      "Iteration Number: 3710\n",
      "Loss: 42.056631404027016\n",
      "l2 norm of gradients: 0.32645851203176196\n",
      "l2 norm of weights: 6.040214228391997\n",
      "---------------------\n",
      "Iteration Number: 3711\n",
      "Loss: 42.051306526970656\n",
      "l2 norm of gradients: 0.32638394001333737\n",
      "l2 norm of weights: 6.040112098822492\n",
      "---------------------\n",
      "Iteration Number: 3712\n",
      "Loss: 42.04598405424494\n",
      "l2 norm of gradients: 0.3263094152549932\n",
      "l2 norm of weights: 6.0400099831982255\n",
      "---------------------\n",
      "Iteration Number: 3713\n",
      "Loss: 42.040663983789806\n",
      "l2 norm of gradients: 0.32623493771934875\n",
      "l2 norm of weights: 6.039907881512549\n",
      "---------------------\n",
      "Iteration Number: 3714\n",
      "Loss: 42.03534631356415\n",
      "l2 norm of gradients: 0.32616050736903973\n",
      "l2 norm of weights: 6.03980579375882\n",
      "---------------------\n",
      "Iteration Number: 3715\n",
      "Loss: 42.030031041546984\n",
      "l2 norm of gradients: 0.3260861241667175\n",
      "l2 norm of weights: 6.039703719930407\n",
      "---------------------\n",
      "Iteration Number: 3716\n",
      "Loss: 42.02471816567613\n",
      "l2 norm of gradients: 0.32601178807504977\n",
      "l2 norm of weights: 6.039601660020683\n",
      "---------------------\n",
      "Iteration Number: 3717\n",
      "Loss: 42.01940768393211\n",
      "l2 norm of gradients: 0.32593749905672004\n",
      "l2 norm of weights: 6.039499614023026\n",
      "---------------------\n",
      "Iteration Number: 3718\n",
      "Loss: 42.014099594263435\n",
      "l2 norm of gradients: 0.32586325707442815\n",
      "l2 norm of weights: 6.039397581930824\n",
      "---------------------\n",
      "Iteration Number: 3719\n",
      "Loss: 42.00879389464167\n",
      "l2 norm of gradients: 0.32578906209089015\n",
      "l2 norm of weights: 6.039295563737469\n",
      "---------------------\n",
      "Iteration Number: 3720\n",
      "Loss: 42.00349058303951\n",
      "l2 norm of gradients: 0.32571491406883835\n",
      "l2 norm of weights: 6.039193559436363\n",
      "---------------------\n",
      "Iteration Number: 3721\n",
      "Loss: 41.99818965741629\n",
      "l2 norm of gradients: 0.3256408129710212\n",
      "l2 norm of weights: 6.039091569020909\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 3722\n",
      "Loss: 41.992891115774285\n",
      "l2 norm of gradients: 0.32556675876020397\n",
      "l2 norm of weights: 6.038989592484522\n",
      "---------------------\n",
      "Iteration Number: 3723\n",
      "Loss: 41.98759495605461\n",
      "l2 norm of gradients: 0.3254927513991677\n",
      "l2 norm of weights: 6.03888762982062\n",
      "---------------------\n",
      "Iteration Number: 3724\n",
      "Loss: 41.982301176265686\n",
      "l2 norm of gradients: 0.32541879085071057\n",
      "l2 norm of weights: 6.038785681022631\n",
      "---------------------\n",
      "Iteration Number: 3725\n",
      "Loss: 41.97700977436333\n",
      "l2 norm of gradients: 0.32534487707764703\n",
      "l2 norm of weights: 6.038683746083986\n",
      "---------------------\n",
      "Iteration Number: 3726\n",
      "Loss: 41.97172074833837\n",
      "l2 norm of gradients: 0.32527101004280773\n",
      "l2 norm of weights: 6.0385818249981265\n",
      "---------------------\n",
      "Iteration Number: 3727\n",
      "Loss: 41.96643409618176\n",
      "l2 norm of gradients: 0.32519718970904066\n",
      "l2 norm of weights: 6.0384799177584965\n",
      "---------------------\n",
      "Iteration Number: 3728\n",
      "Loss: 41.96114981588423\n",
      "l2 norm of gradients: 0.32512341603921\n",
      "l2 norm of weights: 6.0383780243585505\n",
      "---------------------\n",
      "Iteration Number: 3729\n",
      "Loss: 41.9558679054258\n",
      "l2 norm of gradients: 0.3250496889961969\n",
      "l2 norm of weights: 6.038276144791746\n",
      "---------------------\n",
      "Iteration Number: 3730\n",
      "Loss: 41.95058836279384\n",
      "l2 norm of gradients: 0.3249760085428992\n",
      "l2 norm of weights: 6.03817427905155\n",
      "---------------------\n",
      "Iteration Number: 3731\n",
      "Loss: 41.94531118598661\n",
      "l2 norm of gradients: 0.3249023746422316\n",
      "l2 norm of weights: 6.038072427131434\n",
      "---------------------\n",
      "Iteration Number: 3732\n",
      "Loss: 41.940036372991464\n",
      "l2 norm of gradients: 0.32482878725712566\n",
      "l2 norm of weights: 6.037970589024877\n",
      "---------------------\n",
      "Iteration Number: 3733\n",
      "Loss: 41.93476392181922\n",
      "l2 norm of gradients: 0.32475524635053005\n",
      "l2 norm of weights: 6.037868764725365\n",
      "---------------------\n",
      "Iteration Number: 3734\n",
      "Loss: 41.92949383047293\n",
      "l2 norm of gradients: 0.3246817518854102\n",
      "l2 norm of weights: 6.03776695422639\n",
      "---------------------\n",
      "Iteration Number: 3735\n",
      "Loss: 41.92422609694899\n",
      "l2 norm of gradients: 0.3246083038247489\n",
      "l2 norm of weights: 6.037665157521449\n",
      "---------------------\n",
      "Iteration Number: 3736\n",
      "Loss: 41.918960719227414\n",
      "l2 norm of gradients: 0.3245349021315456\n",
      "l2 norm of weights: 6.037563374604049\n",
      "---------------------\n",
      "Iteration Number: 3737\n",
      "Loss: 41.91369769535413\n",
      "l2 norm of gradients: 0.3244615467688175\n",
      "l2 norm of weights: 6.0374616054677\n",
      "---------------------\n",
      "Iteration Number: 3738\n",
      "Loss: 41.90843702331086\n",
      "l2 norm of gradients: 0.32438823769959835\n",
      "l2 norm of weights: 6.037359850105921\n",
      "---------------------\n",
      "Iteration Number: 3739\n",
      "Loss: 41.90317870109951\n",
      "l2 norm of gradients: 0.32431497488693967\n",
      "l2 norm of weights: 6.037258108512238\n",
      "---------------------\n",
      "Iteration Number: 3740\n",
      "Loss: 41.89792272676994\n",
      "l2 norm of gradients: 0.32424175829391\n",
      "l2 norm of weights: 6.0371563806801785\n",
      "---------------------\n",
      "Iteration Number: 3741\n",
      "Loss: 41.89266909831471\n",
      "l2 norm of gradients: 0.3241685878835953\n",
      "l2 norm of weights: 6.037054666603283\n",
      "---------------------\n",
      "Iteration Number: 3742\n",
      "Loss: 41.88741781375408\n",
      "l2 norm of gradients: 0.32409546361909886\n",
      "l2 norm of weights: 6.036952966275094\n",
      "---------------------\n",
      "Iteration Number: 3743\n",
      "Loss: 41.88216887110794\n",
      "l2 norm of gradients: 0.3240223854635418\n",
      "l2 norm of weights: 6.036851279689163\n",
      "---------------------\n",
      "Iteration Number: 3744\n",
      "Loss: 41.87692226838964\n",
      "l2 norm of gradients: 0.32394935338006203\n",
      "l2 norm of weights: 6.036749606839046\n",
      "---------------------\n",
      "Iteration Number: 3745\n",
      "Loss: 41.87167800362236\n",
      "l2 norm of gradients: 0.32387636733181563\n",
      "l2 norm of weights: 6.036647947718307\n",
      "---------------------\n",
      "Iteration Number: 3746\n",
      "Loss: 41.866436074848224\n",
      "l2 norm of gradients: 0.32380342728197603\n",
      "l2 norm of weights: 6.036546302320517\n",
      "---------------------\n",
      "Iteration Number: 3747\n",
      "Loss: 41.861196480072664\n",
      "l2 norm of gradients: 0.3237305331937343\n",
      "l2 norm of weights: 6.0364446706392485\n",
      "---------------------\n",
      "Iteration Number: 3748\n",
      "Loss: 41.85595921734895\n",
      "l2 norm of gradients: 0.3236576850302993\n",
      "l2 norm of weights: 6.036343052668088\n",
      "---------------------\n",
      "Iteration Number: 3749\n",
      "Loss: 41.850724284693825\n",
      "l2 norm of gradients: 0.32358488275489755\n",
      "l2 norm of weights: 6.036241448400625\n",
      "---------------------\n",
      "Iteration Number: 3750\n",
      "Loss: 41.84549168013685\n",
      "l2 norm of gradients: 0.3235121263307732\n",
      "l2 norm of weights: 6.036139857830452\n",
      "---------------------\n",
      "Iteration Number: 3751\n",
      "Loss: 41.84026140173161\n",
      "l2 norm of gradients: 0.32343941572118867\n",
      "l2 norm of weights: 6.036038280951174\n",
      "---------------------\n",
      "Iteration Number: 3752\n",
      "Loss: 41.83503344750776\n",
      "l2 norm of gradients: 0.3233667508894238\n",
      "l2 norm of weights: 6.035936717756397\n",
      "---------------------\n",
      "Iteration Number: 3753\n",
      "Loss: 41.829807815504054\n",
      "l2 norm of gradients: 0.3232941317987767\n",
      "l2 norm of weights: 6.035835168239736\n",
      "---------------------\n",
      "Iteration Number: 3754\n",
      "Loss: 41.82458450375291\n",
      "l2 norm of gradients: 0.32322155841256317\n",
      "l2 norm of weights: 6.035733632394814\n",
      "---------------------\n",
      "Iteration Number: 3755\n",
      "Loss: 41.81936351032121\n",
      "l2 norm of gradients: 0.3231490306941173\n",
      "l2 norm of weights: 6.035632110215258\n",
      "---------------------\n",
      "Iteration Number: 3756\n",
      "Loss: 41.81414483323783\n",
      "l2 norm of gradients: 0.323076548606791\n",
      "l2 norm of weights: 6.035530601694701\n",
      "---------------------\n",
      "Iteration Number: 3757\n",
      "Loss: 41.808928470565775\n",
      "l2 norm of gradients: 0.3230041121139547\n",
      "l2 norm of weights: 6.035429106826784\n",
      "---------------------\n",
      "Iteration Number: 3758\n",
      "Loss: 41.80371442033805\n",
      "l2 norm of gradients: 0.3229317211789964\n",
      "l2 norm of weights: 6.035327625605152\n",
      "---------------------\n",
      "Iteration Number: 3759\n",
      "Loss: 41.79850268062996\n",
      "l2 norm of gradients: 0.32285937576532275\n",
      "l2 norm of weights: 6.035226158023461\n",
      "---------------------\n",
      "Iteration Number: 3760\n",
      "Loss: 41.793293249467325\n",
      "l2 norm of gradients: 0.3227870758363586\n",
      "l2 norm of weights: 6.0351247040753675\n",
      "---------------------\n",
      "Iteration Number: 3761\n",
      "Loss: 41.78808612495107\n",
      "l2 norm of gradients: 0.3227148213555468\n",
      "l2 norm of weights: 6.035023263754539\n",
      "---------------------\n",
      "Iteration Number: 3762\n",
      "Loss: 41.782881305091095\n",
      "l2 norm of gradients: 0.32264261228634916\n",
      "l2 norm of weights: 6.034921837054647\n",
      "---------------------\n",
      "Iteration Number: 3763\n",
      "Loss: 41.777678787989984\n",
      "l2 norm of gradients: 0.3225704485922452\n",
      "l2 norm of weights: 6.034820423969368\n",
      "---------------------\n",
      "Iteration Number: 3764\n",
      "Loss: 41.77247857168076\n",
      "l2 norm of gradients: 0.32249833023673324\n",
      "l2 norm of weights: 6.0347190244923885\n",
      "---------------------\n",
      "Iteration Number: 3765\n",
      "Loss: 41.767280654260645\n",
      "l2 norm of gradients: 0.32242625718333\n",
      "l2 norm of weights: 6.034617638617401\n",
      "---------------------\n",
      "Iteration Number: 3766\n",
      "Loss: 41.76208503376009\n",
      "l2 norm of gradients: 0.32235422939557073\n",
      "l2 norm of weights: 6.034516266338098\n",
      "---------------------\n",
      "Iteration Number: 3767\n",
      "Loss: 41.756891708272406\n",
      "l2 norm of gradients: 0.3222822468370094\n",
      "l2 norm of weights: 6.034414907648188\n",
      "---------------------\n",
      "Iteration Number: 3768\n",
      "Loss: 41.751700675867035\n",
      "l2 norm of gradients: 0.3222103094712182\n",
      "l2 norm of weights: 6.034313562541377\n",
      "---------------------\n",
      "Iteration Number: 3769\n",
      "Loss: 41.74651193461335\n",
      "l2 norm of gradients: 0.32213841726178843\n",
      "l2 norm of weights: 6.034212231011383\n",
      "---------------------\n",
      "Iteration Number: 3770\n",
      "Loss: 41.74132548259764\n",
      "l2 norm of gradients: 0.3220665701723297\n",
      "l2 norm of weights: 6.034110913051927\n",
      "---------------------\n",
      "Iteration Number: 3771\n",
      "Loss: 41.73614131789446\n",
      "l2 norm of gradients: 0.3219947681664707\n",
      "l2 norm of weights: 6.034009608656739\n",
      "---------------------\n",
      "Iteration Number: 3772\n",
      "Loss: 41.73095943857362\n",
      "l2 norm of gradients: 0.3219230112078587\n",
      "l2 norm of weights: 6.033908317819553\n",
      "---------------------\n",
      "Iteration Number: 3773\n",
      "Loss: 41.725779842722346\n",
      "l2 norm of gradients: 0.32185129926015965\n",
      "l2 norm of weights: 6.03380704053411\n",
      "---------------------\n",
      "Iteration Number: 3774\n",
      "Loss: 41.72060252842681\n",
      "l2 norm of gradients: 0.32177963228705897\n",
      "l2 norm of weights: 6.033705776794157\n",
      "---------------------\n",
      "Iteration Number: 3775\n",
      "Loss: 41.71542749377859\n",
      "l2 norm of gradients: 0.32170801025226026\n",
      "l2 norm of weights: 6.033604526593448\n",
      "---------------------\n",
      "Iteration Number: 3776\n",
      "Loss: 41.710254736847034\n",
      "l2 norm of gradients: 0.3216364331194866\n",
      "l2 norm of weights: 6.0335032899257435\n",
      "---------------------\n",
      "Iteration Number: 3777\n",
      "Loss: 41.7050842557453\n",
      "l2 norm of gradients: 0.32156490085247974\n",
      "l2 norm of weights: 6.033402066784807\n",
      "---------------------\n",
      "Iteration Number: 3778\n",
      "Loss: 41.69991604856947\n",
      "l2 norm of gradients: 0.32149341341500076\n",
      "l2 norm of weights: 6.033300857164412\n",
      "---------------------\n",
      "Iteration Number: 3779\n",
      "Loss: 41.69475011338826\n",
      "l2 norm of gradients: 0.32142197077082957\n",
      "l2 norm of weights: 6.033199661058337\n",
      "---------------------\n",
      "Iteration Number: 3780\n",
      "Loss: 41.68958644832678\n",
      "l2 norm of gradients: 0.32135057288376534\n",
      "l2 norm of weights: 6.033098478460365\n",
      "---------------------\n",
      "Iteration Number: 3781\n",
      "Loss: 41.68442505146076\n",
      "l2 norm of gradients: 0.3212792197176265\n",
      "l2 norm of weights: 6.03299730936429\n",
      "---------------------\n",
      "Iteration Number: 3782\n",
      "Loss: 41.679265920891666\n",
      "l2 norm of gradients: 0.32120791123625037\n",
      "l2 norm of weights: 6.032896153763906\n",
      "---------------------\n",
      "Iteration Number: 3783\n",
      "Loss: 41.67410905475329\n",
      "l2 norm of gradients: 0.32113664740349374\n",
      "l2 norm of weights: 6.032795011653016\n",
      "---------------------\n",
      "Iteration Number: 3784\n",
      "Loss: 41.66895445111472\n",
      "l2 norm of gradients: 0.32106542818323286\n",
      "l2 norm of weights: 6.03269388302543\n",
      "---------------------\n",
      "Iteration Number: 3785\n",
      "Loss: 41.663802108095915\n",
      "l2 norm of gradients: 0.32099425353936284\n",
      "l2 norm of weights: 6.032592767874962\n",
      "---------------------\n",
      "Iteration Number: 3786\n",
      "Loss: 41.658652023808806\n",
      "l2 norm of gradients: 0.32092312343579876\n",
      "l2 norm of weights: 6.032491666195435\n",
      "---------------------\n",
      "Iteration Number: 3787\n",
      "Loss: 41.653504196373525\n",
      "l2 norm of gradients: 0.3208520378364744\n",
      "l2 norm of weights: 6.032390577980677\n",
      "---------------------\n",
      "Iteration Number: 3788\n",
      "Loss: 41.648358623881904\n",
      "l2 norm of gradients: 0.32078099670534377\n",
      "l2 norm of weights: 6.032289503224519\n",
      "---------------------\n",
      "Iteration Number: 3789\n",
      "Loss: 41.64321530445593\n",
      "l2 norm of gradients: 0.3207100000063796\n",
      "l2 norm of weights: 6.0321884419208045\n",
      "---------------------\n",
      "Iteration Number: 3790\n",
      "Loss: 41.63807423622969\n",
      "l2 norm of gradients: 0.3206390477035748\n",
      "l2 norm of weights: 6.032087394063376\n",
      "---------------------\n",
      "Iteration Number: 3791\n",
      "Loss: 41.632935417301375\n",
      "l2 norm of gradients: 0.32056813976094145\n",
      "l2 norm of weights: 6.031986359646087\n",
      "---------------------\n",
      "Iteration Number: 3792\n",
      "Loss: 41.627798845803625\n",
      "l2 norm of gradients: 0.3204972761425115\n",
      "l2 norm of weights: 6.031885338662796\n",
      "---------------------\n",
      "Iteration Number: 3793\n",
      "Loss: 41.62266451986426\n",
      "l2 norm of gradients: 0.32042645681233617\n",
      "l2 norm of weights: 6.0317843311073664\n",
      "---------------------\n",
      "Iteration Number: 3794\n",
      "Loss: 41.61753243759503\n",
      "l2 norm of gradients: 0.3203556817344867\n",
      "l2 norm of weights: 6.031683336973669\n",
      "---------------------\n",
      "Iteration Number: 3795\n",
      "Loss: 41.61240259713296\n",
      "l2 norm of gradients: 0.3202849508730541\n",
      "l2 norm of weights: 6.031582356255579\n",
      "---------------------\n",
      "Iteration Number: 3796\n",
      "Loss: 41.60727499659955\n",
      "l2 norm of gradients: 0.32021426419214877\n",
      "l2 norm of weights: 6.03148138894698\n",
      "---------------------\n",
      "Iteration Number: 3797\n",
      "Loss: 41.602149634123855\n",
      "l2 norm of gradients: 0.3201436216559013\n",
      "l2 norm of weights: 6.03138043504176\n",
      "---------------------\n",
      "Iteration Number: 3798\n",
      "Loss: 41.59702650786945\n",
      "l2 norm of gradients: 0.32007302322846193\n",
      "l2 norm of weights: 6.031279494533814\n",
      "---------------------\n",
      "Iteration Number: 3799\n",
      "Loss: 41.591905615927246\n",
      "l2 norm of gradients: 0.32000246887400075\n",
      "l2 norm of weights: 6.031178567417042\n",
      "---------------------\n",
      "Iteration Number: 3800\n",
      "Loss: 41.58678695646698\n",
      "l2 norm of gradients: 0.3199319585567078\n",
      "l2 norm of weights: 6.031077653685351\n",
      "---------------------\n",
      "Iteration Number: 3801\n",
      "Loss: 41.58167052761896\n",
      "l2 norm of gradients: 0.31986149224079313\n",
      "l2 norm of weights: 6.030976753332653\n",
      "---------------------\n",
      "Iteration Number: 3802\n",
      "Loss: 41.5765563275257\n",
      "l2 norm of gradients: 0.31979106989048683\n",
      "l2 norm of weights: 6.030875866352867\n",
      "---------------------\n",
      "Iteration Number: 3803\n",
      "Loss: 41.57144435432243\n",
      "l2 norm of gradients: 0.3197206914700388\n",
      "l2 norm of weights: 6.030774992739918\n",
      "---------------------\n",
      "Iteration Number: 3804\n",
      "Loss: 41.566334606165626\n",
      "l2 norm of gradients: 0.31965035694371907\n",
      "l2 norm of weights: 6.030674132487735\n",
      "---------------------\n",
      "Iteration Number: 3805\n",
      "Loss: 41.56122708121317\n",
      "l2 norm of gradients: 0.319580066275818\n",
      "l2 norm of weights: 6.030573285590257\n",
      "---------------------\n",
      "Iteration Number: 3806\n",
      "Loss: 41.55612177758366\n",
      "l2 norm of gradients: 0.31950981943064577\n",
      "l2 norm of weights: 6.030472452041425\n",
      "---------------------\n",
      "Iteration Number: 3807\n",
      "Loss: 41.5510186934452\n",
      "l2 norm of gradients: 0.3194396163725329\n",
      "l2 norm of weights: 6.030371631835188\n",
      "---------------------\n",
      "Iteration Number: 3808\n",
      "Loss: 41.54591782695883\n",
      "l2 norm of gradients: 0.31936945706583003\n",
      "l2 norm of weights: 6.030270824965502\n",
      "---------------------\n",
      "Iteration Number: 3809\n",
      "Loss: 41.54081917626402\n",
      "l2 norm of gradients: 0.3192993414749082\n",
      "l2 norm of weights: 6.030170031426326\n",
      "---------------------\n",
      "Iteration Number: 3810\n",
      "Loss: 41.53572273953652\n",
      "l2 norm of gradients: 0.31922926956415854\n",
      "l2 norm of weights: 6.030069251211626\n",
      "---------------------\n",
      "Iteration Number: 3811\n",
      "Loss: 41.53062851492285\n",
      "l2 norm of gradients: 0.31915924129799256\n",
      "l2 norm of weights: 6.029968484315375\n",
      "---------------------\n",
      "Iteration Number: 3812\n",
      "Loss: 41.52553650059298\n",
      "l2 norm of gradients: 0.31908925664084214\n",
      "l2 norm of weights: 6.029867730731552\n",
      "---------------------\n",
      "Iteration Number: 3813\n",
      "Loss: 41.52044669469465\n",
      "l2 norm of gradients: 0.3190193155571596\n",
      "l2 norm of weights: 6.029766990454141\n",
      "---------------------\n",
      "Iteration Number: 3814\n",
      "Loss: 41.5153590954243\n",
      "l2 norm of gradients: 0.3189494180114175\n",
      "l2 norm of weights: 6.029666263477132\n",
      "---------------------\n",
      "Iteration Number: 3815\n",
      "Loss: 41.510273700908435\n",
      "l2 norm of gradients: 0.318879563968109\n",
      "l2 norm of weights: 6.029565549794523\n",
      "---------------------\n",
      "Iteration Number: 3816\n",
      "Loss: 41.50519050935402\n",
      "l2 norm of gradients: 0.31880975339174783\n",
      "l2 norm of weights: 6.029464849400313\n",
      "---------------------\n",
      "Iteration Number: 3817\n",
      "Loss: 41.50010951891448\n",
      "l2 norm of gradients: 0.31873998624686783\n",
      "l2 norm of weights: 6.0293641622885135\n",
      "---------------------\n",
      "Iteration Number: 3818\n",
      "Loss: 41.495030727765794\n",
      "l2 norm of gradients: 0.31867026249802394\n",
      "l2 norm of weights: 6.029263488453135\n",
      "---------------------\n",
      "Iteration Number: 3819\n",
      "Loss: 41.489954134077045\n",
      "l2 norm of gradients: 0.31860058210979136\n",
      "l2 norm of weights: 6.029162827888199\n",
      "---------------------\n",
      "Iteration Number: 3820\n",
      "Loss: 41.48487973603776\n",
      "l2 norm of gradients: 0.3185309450467659\n",
      "l2 norm of weights: 6.029062180587732\n",
      "---------------------\n",
      "Iteration Number: 3821\n",
      "Loss: 41.479807531818935\n",
      "l2 norm of gradients: 0.3184613512735641\n",
      "l2 norm of weights: 6.028961546545765\n",
      "---------------------\n",
      "Iteration Number: 3822\n",
      "Loss: 41.47473751960306\n",
      "l2 norm of gradients: 0.31839180075482326\n",
      "l2 norm of weights: 6.028860925756335\n",
      "---------------------\n",
      "Iteration Number: 3823\n",
      "Loss: 41.46966969757725\n",
      "l2 norm of gradients: 0.3183222934552014\n",
      "l2 norm of weights: 6.028760318213485\n",
      "---------------------\n",
      "Iteration Number: 3824\n",
      "Loss: 41.46460406392903\n",
      "l2 norm of gradients: 0.31825282933937704\n",
      "l2 norm of weights: 6.028659723911265\n",
      "---------------------\n",
      "Iteration Number: 3825\n",
      "Loss: 41.4595406168493\n",
      "l2 norm of gradients: 0.3181834083720497\n",
      "l2 norm of weights: 6.02855914284373\n",
      "---------------------\n",
      "Iteration Number: 3826\n",
      "Loss: 41.45447935449935\n",
      "l2 norm of gradients: 0.31811403051793985\n",
      "l2 norm of weights: 6.028458575004941\n",
      "---------------------\n",
      "Iteration Number: 3827\n",
      "Loss: 41.4494202751026\n",
      "l2 norm of gradients: 0.31804469574178845\n",
      "l2 norm of weights: 6.028358020388965\n",
      "---------------------\n",
      "Iteration Number: 3828\n",
      "Loss: 41.44436337683838\n",
      "l2 norm of gradients: 0.3179754040083576\n",
      "l2 norm of weights: 6.0282574789898735\n",
      "---------------------\n",
      "Iteration Number: 3829\n",
      "Loss: 41.4393086578938\n",
      "l2 norm of gradients: 0.3179061552824302\n",
      "l2 norm of weights: 6.028156950801745\n",
      "---------------------\n",
      "Iteration Number: 3830\n",
      "Loss: 41.434256116482906\n",
      "l2 norm of gradients: 0.31783694952881025\n",
      "l2 norm of weights: 6.028056435818664\n",
      "---------------------\n",
      "Iteration Number: 3831\n",
      "Loss: 41.42920575080747\n",
      "l2 norm of gradients: 0.31776778671232253\n",
      "l2 norm of weights: 6.027955934034722\n",
      "---------------------\n",
      "Iteration Number: 3832\n",
      "Loss: 41.4241575590608\n",
      "l2 norm of gradients: 0.31769866679781295\n",
      "l2 norm of weights: 6.027855445444012\n",
      "---------------------\n",
      "Iteration Number: 3833\n",
      "Loss: 41.41911153943707\n",
      "l2 norm of gradients: 0.3176295897501484\n",
      "l2 norm of weights: 6.027754970040638\n",
      "---------------------\n",
      "Iteration Number: 3834\n",
      "Loss: 41.414067690136385\n",
      "l2 norm of gradients: 0.31756055553421686\n",
      "l2 norm of weights: 6.027654507818705\n",
      "---------------------\n",
      "Iteration Number: 3835\n",
      "Loss: 41.40902600937965\n",
      "l2 norm of gradients: 0.3174915641149274\n",
      "l2 norm of weights: 6.027554058772329\n",
      "---------------------\n",
      "Iteration Number: 3836\n",
      "Loss: 41.40398649538359\n",
      "l2 norm of gradients: 0.3174226154572102\n",
      "l2 norm of weights: 6.027453622895626\n",
      "---------------------\n",
      "Iteration Number: 3837\n",
      "Loss: 41.39894914634143\n",
      "l2 norm of gradients: 0.3173537095260167\n",
      "l2 norm of weights: 6.027353200182723\n",
      "---------------------\n",
      "Iteration Number: 3838\n",
      "Loss: 41.39391396047404\n",
      "l2 norm of gradients: 0.3172848462863194\n",
      "l2 norm of weights: 6.02725279062775\n",
      "---------------------\n",
      "Iteration Number: 3839\n",
      "Loss: 41.38888093599723\n",
      "l2 norm of gradients: 0.3172160257031121\n",
      "l2 norm of weights: 6.027152394224842\n",
      "---------------------\n",
      "Iteration Number: 3840\n",
      "Loss: 41.3838500711137\n",
      "l2 norm of gradients: 0.31714724774140973\n",
      "l2 norm of weights: 6.027052010968142\n",
      "---------------------\n",
      "Iteration Number: 3841\n",
      "Loss: 41.378821364058744\n",
      "l2 norm of gradients: 0.31707851236624873\n",
      "l2 norm of weights: 6.026951640851798\n",
      "---------------------\n",
      "Iteration Number: 3842\n",
      "Loss: 41.373794813053685\n",
      "l2 norm of gradients: 0.31700981954268675\n",
      "l2 norm of weights: 6.026851283869963\n",
      "---------------------\n",
      "Iteration Number: 3843\n",
      "Loss: 41.36877041630293\n",
      "l2 norm of gradients: 0.3169411692358025\n",
      "l2 norm of weights: 6.026750940016795\n",
      "---------------------\n",
      "Iteration Number: 3844\n",
      "Loss: 41.363748172053384\n",
      "l2 norm of gradients: 0.3168725614106965\n",
      "l2 norm of weights: 6.0266506092864605\n",
      "---------------------\n",
      "Iteration Number: 3845\n",
      "Loss: 41.35872807851361\n",
      "l2 norm of gradients: 0.31680399603249026\n",
      "l2 norm of weights: 6.026550291673129\n",
      "---------------------\n",
      "Iteration Number: 3846\n",
      "Loss: 41.353710133916785\n",
      "l2 norm of gradients: 0.3167354730663271\n",
      "l2 norm of weights: 6.026449987170977\n",
      "---------------------\n",
      "Iteration Number: 3847\n",
      "Loss: 41.348694336492244\n",
      "l2 norm of gradients: 0.31666699247737146\n",
      "l2 norm of weights: 6.026349695774188\n",
      "---------------------\n",
      "Iteration Number: 3848\n",
      "Loss: 41.343680684470854\n",
      "l2 norm of gradients: 0.3165985542308094\n",
      "l2 norm of weights: 6.026249417476946\n",
      "---------------------\n",
      "Iteration Number: 3849\n",
      "Loss: 41.33866917609146\n",
      "l2 norm of gradients: 0.31653015829184844\n",
      "l2 norm of weights: 6.026149152273448\n",
      "---------------------\n",
      "Iteration Number: 3850\n",
      "Loss: 41.333659809584894\n",
      "l2 norm of gradients: 0.31646180462571794\n",
      "l2 norm of weights: 6.0260489001578925\n",
      "---------------------\n",
      "Iteration Number: 3851\n",
      "Loss: 41.32865258320145\n",
      "l2 norm of gradients: 0.3163934931976681\n",
      "l2 norm of weights: 6.025948661124482\n",
      "---------------------\n",
      "Iteration Number: 3852\n",
      "Loss: 41.323647495162156\n",
      "l2 norm of gradients: 0.3163252239729714\n",
      "l2 norm of weights: 6.025848435167428\n",
      "---------------------\n",
      "Iteration Number: 3853\n",
      "Loss: 41.31864454370971\n",
      "l2 norm of gradients: 0.31625699691692183\n",
      "l2 norm of weights: 6.025748222280947\n",
      "---------------------\n",
      "Iteration Number: 3854\n",
      "Loss: 41.31364372710976\n",
      "l2 norm of gradients: 0.31618881199483456\n",
      "l2 norm of weights: 6.02564802245926\n",
      "---------------------\n",
      "Iteration Number: 3855\n",
      "Loss: 41.30864504357857\n",
      "l2 norm of gradients: 0.316120669172047\n",
      "l2 norm of weights: 6.025547835696593\n",
      "---------------------\n",
      "Iteration Number: 3856\n",
      "Loss: 41.30364849137906\n",
      "l2 norm of gradients: 0.31605256841391793\n",
      "l2 norm of weights: 6.025447661987182\n",
      "---------------------\n",
      "Iteration Number: 3857\n",
      "Loss: 41.29865406875539\n",
      "l2 norm of gradients: 0.31598450968582786\n",
      "l2 norm of weights: 6.025347501325263\n",
      "---------------------\n",
      "Iteration Number: 3858\n",
      "Loss: 41.29366177396621\n",
      "l2 norm of gradients: 0.31591649295317914\n",
      "l2 norm of weights: 6.0252473537050815\n",
      "---------------------\n",
      "Iteration Number: 3859\n",
      "Loss: 41.288671605246954\n",
      "l2 norm of gradients: 0.315848518181396\n",
      "l2 norm of weights: 6.0251472191208855\n",
      "---------------------\n",
      "Iteration Number: 3860\n",
      "Loss: 41.283683560876455\n",
      "l2 norm of gradients: 0.3157805853359241\n",
      "l2 norm of weights: 6.025047097566932\n",
      "---------------------\n",
      "Iteration Number: 3861\n",
      "Loss: 41.27869763908798\n",
      "l2 norm of gradients: 0.3157126943822314\n",
      "l2 norm of weights: 6.024946989037481\n",
      "---------------------\n",
      "Iteration Number: 3862\n",
      "Loss: 41.273713838157434\n",
      "l2 norm of gradients: 0.3156448452858073\n",
      "l2 norm of weights: 6.024846893526799\n",
      "---------------------\n",
      "Iteration Number: 3863\n",
      "Loss: 41.26873215633972\n",
      "l2 norm of gradients: 0.31557703801216314\n",
      "l2 norm of weights: 6.02474681102916\n",
      "---------------------\n",
      "Iteration Number: 3864\n",
      "Loss: 41.263752591886075\n",
      "l2 norm of gradients: 0.31550927252683253\n",
      "l2 norm of weights: 6.024646741538838\n",
      "---------------------\n",
      "Iteration Number: 3865\n",
      "Loss: 41.258775143079795\n",
      "l2 norm of gradients: 0.3154415487953707\n",
      "l2 norm of weights: 6.02454668505012\n",
      "---------------------\n",
      "Iteration Number: 3866\n",
      "Loss: 41.25379980817707\n",
      "l2 norm of gradients: 0.31537386678335466\n",
      "l2 norm of weights: 6.024446641557293\n",
      "---------------------\n",
      "Iteration Number: 3867\n",
      "Loss: 41.24882658543526\n",
      "l2 norm of gradients: 0.3153062264563837\n",
      "l2 norm of weights: 6.024346611054651\n",
      "---------------------\n",
      "Iteration Number: 3868\n",
      "Loss: 41.24385547314597\n",
      "l2 norm of gradients: 0.315238627780079\n",
      "l2 norm of weights: 6.0242465935364935\n",
      "---------------------\n",
      "Iteration Number: 3869\n",
      "Loss: 41.238886469562736\n",
      "l2 norm of gradients: 0.3151710707200838\n",
      "l2 norm of weights: 6.024146588997128\n",
      "---------------------\n",
      "Iteration Number: 3870\n",
      "Loss: 41.233919572968354\n",
      "l2 norm of gradients: 0.31510355524206335\n",
      "l2 norm of weights: 6.024046597430863\n",
      "---------------------\n",
      "Iteration Number: 3871\n",
      "Loss: 41.228954781639196\n",
      "l2 norm of gradients: 0.315036081311705\n",
      "l2 norm of weights: 6.023946618832018\n",
      "---------------------\n",
      "Iteration Number: 3872\n",
      "Loss: 41.223992093839776\n",
      "l2 norm of gradients: 0.31496864889471804\n",
      "l2 norm of weights: 6.023846653194912\n",
      "---------------------\n",
      "Iteration Number: 3873\n",
      "Loss: 41.21903150786382\n",
      "l2 norm of gradients: 0.31490125795683416\n",
      "l2 norm of weights: 6.023746700513875\n",
      "---------------------\n",
      "Iteration Number: 3874\n",
      "Loss: 41.21407302198651\n",
      "l2 norm of gradients: 0.31483390846380693\n",
      "l2 norm of weights: 6.0236467607832385\n",
      "---------------------\n",
      "Iteration Number: 3875\n",
      "Loss: 41.20911663448658\n",
      "l2 norm of gradients: 0.31476660038141224\n",
      "l2 norm of weights: 6.023546833997341\n",
      "---------------------\n",
      "Iteration Number: 3876\n",
      "Loss: 41.20416234366256\n",
      "l2 norm of gradients: 0.3146993336754479\n",
      "l2 norm of weights: 6.023446920150527\n",
      "---------------------\n",
      "Iteration Number: 3877\n",
      "Loss: 41.19921014777415\n",
      "l2 norm of gradients: 0.31463210831173444\n",
      "l2 norm of weights: 6.023347019237146\n",
      "---------------------\n",
      "Iteration Number: 3878\n",
      "Loss: 41.19426004513554\n",
      "l2 norm of gradients: 0.31456492425611415\n",
      "l2 norm of weights: 6.023247131251553\n",
      "---------------------\n",
      "Iteration Number: 3879\n",
      "Loss: 41.18931203402448\n",
      "l2 norm of gradients: 0.31449778147445173\n",
      "l2 norm of weights: 6.023147256188108\n",
      "---------------------\n",
      "Iteration Number: 3880\n",
      "Loss: 41.1843661127338\n",
      "l2 norm of gradients: 0.31443067993263424\n",
      "l2 norm of weights: 6.023047394041178\n",
      "---------------------\n",
      "Iteration Number: 3881\n",
      "Loss: 41.179422279562694\n",
      "l2 norm of gradients: 0.3143636195965709\n",
      "l2 norm of weights: 6.022947544805133\n",
      "---------------------\n",
      "Iteration Number: 3882\n",
      "Loss: 41.17448053280465\n",
      "l2 norm of gradients: 0.3142966004321932\n",
      "l2 norm of weights: 6.022847708474351\n",
      "---------------------\n",
      "Iteration Number: 3883\n",
      "Loss: 41.16954087075315\n",
      "l2 norm of gradients: 0.31422962240545527\n",
      "l2 norm of weights: 6.022747885043214\n",
      "---------------------\n",
      "Iteration Number: 3884\n",
      "Loss: 41.164603291704104\n",
      "l2 norm of gradients: 0.3141626854823334\n",
      "l2 norm of weights: 6.022648074506108\n",
      "---------------------\n",
      "Iteration Number: 3885\n",
      "Loss: 41.15966779397839\n",
      "l2 norm of gradients: 0.31409578962882595\n",
      "l2 norm of weights: 6.022548276857429\n",
      "---------------------\n",
      "Iteration Number: 3886\n",
      "Loss: 41.154734375875\n",
      "l2 norm of gradients: 0.31402893481095434\n",
      "l2 norm of weights: 6.022448492091574\n",
      "---------------------\n",
      "Iteration Number: 3887\n",
      "Loss: 41.149803035670146\n",
      "l2 norm of gradients: 0.313962120994762\n",
      "l2 norm of weights: 6.022348720202948\n",
      "---------------------\n",
      "Iteration Number: 3888\n",
      "Loss: 41.14487377168993\n",
      "l2 norm of gradients: 0.3138953481463148\n",
      "l2 norm of weights: 6.022248961185959\n",
      "---------------------\n",
      "Iteration Number: 3889\n",
      "Loss: 41.13994658224517\n",
      "l2 norm of gradients: 0.3138286162317011\n",
      "l2 norm of weights: 6.022149215035022\n",
      "---------------------\n",
      "Iteration Number: 3890\n",
      "Loss: 41.135021465658824\n",
      "l2 norm of gradients: 0.31376192521703195\n",
      "l2 norm of weights: 6.022049481744559\n",
      "---------------------\n",
      "Iteration Number: 3891\n",
      "Loss: 41.130098420211446\n",
      "l2 norm of gradients: 0.31369527506844064\n",
      "l2 norm of weights: 6.021949761308994\n",
      "---------------------\n",
      "Iteration Number: 3892\n",
      "Loss: 41.12517744424101\n",
      "l2 norm of gradients: 0.31362866575208315\n",
      "l2 norm of weights: 6.0218500537227575\n",
      "---------------------\n",
      "Iteration Number: 3893\n",
      "Loss: 41.120258536048695\n",
      "l2 norm of gradients: 0.3135620972341379\n",
      "l2 norm of weights: 6.021750358980286\n",
      "---------------------\n",
      "Iteration Number: 3894\n",
      "Loss: 41.11534169396466\n",
      "l2 norm of gradients: 0.3134955694808059\n",
      "l2 norm of weights: 6.021650677076024\n",
      "---------------------\n",
      "Iteration Number: 3895\n",
      "Loss: 41.11042691631054\n",
      "l2 norm of gradients: 0.3134290824583109\n",
      "l2 norm of weights: 6.021551008004415\n",
      "---------------------\n",
      "Iteration Number: 3896\n",
      "Loss: 41.105514201394826\n",
      "l2 norm of gradients: 0.31336263613289894\n",
      "l2 norm of weights: 6.021451351759913\n",
      "---------------------\n",
      "Iteration Number: 3897\n",
      "Loss: 41.10060354753045\n",
      "l2 norm of gradients: 0.313296230470839\n",
      "l2 norm of weights: 6.021351708336975\n",
      "---------------------\n",
      "Iteration Number: 3898\n",
      "Loss: 41.09569495306772\n",
      "l2 norm of gradients: 0.3132298654384225\n",
      "l2 norm of weights: 6.021252077730065\n",
      "---------------------\n",
      "Iteration Number: 3899\n",
      "Loss: 41.090788416329495\n",
      "l2 norm of gradients: 0.3131635410019636\n",
      "l2 norm of weights: 6.021152459933652\n",
      "---------------------\n",
      "Iteration Number: 3900\n",
      "Loss: 41.085883935639835\n",
      "l2 norm of gradients: 0.31309725712779907\n",
      "l2 norm of weights: 6.021052854942209\n",
      "---------------------\n",
      "Iteration Number: 3901\n",
      "Loss: 41.08098150932116\n",
      "l2 norm of gradients: 0.3130310137822886\n",
      "l2 norm of weights: 6.020953262750215\n",
      "---------------------\n",
      "Iteration Number: 3902\n",
      "Loss: 41.076081135720266\n",
      "l2 norm of gradients: 0.31296481093181416\n",
      "l2 norm of weights: 6.020853683352154\n",
      "---------------------\n",
      "Iteration Number: 3903\n",
      "Loss: 41.07118281315656\n",
      "l2 norm of gradients: 0.31289864854278093\n",
      "l2 norm of weights: 6.020754116742517\n",
      "---------------------\n",
      "Iteration Number: 3904\n",
      "Loss: 41.06628653997491\n",
      "l2 norm of gradients: 0.3128325265816165\n",
      "l2 norm of weights: 6.020654562915799\n",
      "---------------------\n",
      "Iteration Number: 3905\n",
      "Loss: 41.06139231450507\n",
      "l2 norm of gradients: 0.3127664450147713\n",
      "l2 norm of weights: 6.020555021866499\n",
      "---------------------\n",
      "Iteration Number: 3906\n",
      "Loss: 41.056500135079084\n",
      "l2 norm of gradients: 0.3127004038087188\n",
      "l2 norm of weights: 6.0204554935891235\n",
      "---------------------\n",
      "Iteration Number: 3907\n",
      "Loss: 41.05161000006434\n",
      "l2 norm of gradients: 0.31263440292995487\n",
      "l2 norm of weights: 6.020355978078184\n",
      "---------------------\n",
      "Iteration Number: 3908\n",
      "Loss: 41.04672190779222\n",
      "l2 norm of gradients: 0.3125684423449985\n",
      "l2 norm of weights: 6.020256475328196\n",
      "---------------------\n",
      "Iteration Number: 3909\n",
      "Loss: 41.0418358566021\n",
      "l2 norm of gradients: 0.31250252202039147\n",
      "l2 norm of weights: 6.020156985333681\n",
      "---------------------\n",
      "Iteration Number: 3910\n",
      "Loss: 41.03695184483684\n",
      "l2 norm of gradients: 0.31243664192269827\n",
      "l2 norm of weights: 6.020057508089167\n",
      "---------------------\n",
      "Iteration Number: 3911\n",
      "Loss: 41.0320698708565\n",
      "l2 norm of gradients: 0.31237080201850637\n",
      "l2 norm of weights: 6.019958043589184\n",
      "---------------------\n",
      "Iteration Number: 3912\n",
      "Loss: 41.02718993300343\n",
      "l2 norm of gradients: 0.3123050022744261\n",
      "l2 norm of weights: 6.01985859182827\n",
      "---------------------\n",
      "Iteration Number: 3913\n",
      "Loss: 41.02231202962989\n",
      "l2 norm of gradients: 0.3122392426570908\n",
      "l2 norm of weights: 6.019759152800968\n",
      "---------------------\n",
      "Iteration Number: 3914\n",
      "Loss: 41.01743615909161\n",
      "l2 norm of gradients: 0.3121735231331565\n",
      "l2 norm of weights: 6.019659726501825\n",
      "---------------------\n",
      "Iteration Number: 3915\n",
      "Loss: 41.0125623197429\n",
      "l2 norm of gradients: 0.3121078436693024\n",
      "l2 norm of weights: 6.019560312925394\n",
      "---------------------\n",
      "Iteration Number: 3916\n",
      "Loss: 41.00769050993558\n",
      "l2 norm of gradients: 0.3120422042322305\n",
      "l2 norm of weights: 6.019460912066235\n",
      "---------------------\n",
      "Iteration Number: 3917\n",
      "Loss: 41.00282072804395\n",
      "l2 norm of gradients: 0.31197660478866585\n",
      "l2 norm of weights: 6.019361523918908\n",
      "---------------------\n",
      "Iteration Number: 3918\n",
      "Loss: 40.997952972415284\n",
      "l2 norm of gradients: 0.3119110453053565\n",
      "l2 norm of weights: 6.019262148477986\n",
      "---------------------\n",
      "Iteration Number: 3919\n",
      "Loss: 40.99308724141774\n",
      "l2 norm of gradients: 0.31184552574907337\n",
      "l2 norm of weights: 6.01916278573804\n",
      "---------------------\n",
      "Iteration Number: 3920\n",
      "Loss: 40.98822353340987\n",
      "l2 norm of gradients: 0.31178004608661053\n",
      "l2 norm of weights: 6.019063435693648\n",
      "---------------------\n",
      "Iteration Number: 3921\n",
      "Loss: 40.983361846757596\n",
      "l2 norm of gradients: 0.311714606284785\n",
      "l2 norm of weights: 6.0189640983393975\n",
      "---------------------\n",
      "Iteration Number: 3922\n",
      "Loss: 40.978502179832475\n",
      "l2 norm of gradients: 0.3116492063104368\n",
      "l2 norm of weights: 6.018864773669875\n",
      "---------------------\n",
      "Iteration Number: 3923\n",
      "Loss: 40.97364453100687\n",
      "l2 norm of gradients: 0.3115838461304292\n",
      "l2 norm of weights: 6.018765461679677\n",
      "---------------------\n",
      "Iteration Number: 3924\n",
      "Loss: 40.96878889864445\n",
      "l2 norm of gradients: 0.3115185257116485\n",
      "l2 norm of weights: 6.018666162363402\n",
      "---------------------\n",
      "Iteration Number: 3925\n",
      "Loss: 40.96393528113843\n",
      "l2 norm of gradients: 0.31145324502100374\n",
      "l2 norm of weights: 6.018566875715656\n",
      "---------------------\n",
      "Iteration Number: 3926\n",
      "Loss: 40.95908367683172\n",
      "l2 norm of gradients: 0.31138800402542766\n",
      "l2 norm of weights: 6.018467601731048\n",
      "---------------------\n",
      "Iteration Number: 3927\n",
      "Loss: 40.9542340841116\n",
      "l2 norm of gradients: 0.31132280269187546\n",
      "l2 norm of weights: 6.018368340404194\n",
      "---------------------\n",
      "Iteration Number: 3928\n",
      "Loss: 40.94938650137436\n",
      "l2 norm of gradients: 0.3112576409873259\n",
      "l2 norm of weights: 6.0182690917297155\n",
      "---------------------\n",
      "Iteration Number: 3929\n",
      "Loss: 40.94454092698359\n",
      "l2 norm of gradients: 0.31119251887878097\n",
      "l2 norm of weights: 6.018169855702236\n",
      "---------------------\n",
      "Iteration Number: 3930\n",
      "Loss: 40.93969735931675\n",
      "l2 norm of gradients: 0.3111274363332654\n",
      "l2 norm of weights: 6.018070632316387\n",
      "---------------------\n",
      "Iteration Number: 3931\n",
      "Loss: 40.93485579678206\n",
      "l2 norm of gradients: 0.3110623933178274\n",
      "l2 norm of weights: 6.017971421566804\n",
      "---------------------\n",
      "Iteration Number: 3932\n",
      "Loss: 40.930016237730136\n",
      "l2 norm of gradients: 0.3109973897995384\n",
      "l2 norm of weights: 6.017872223448128\n",
      "---------------------\n",
      "Iteration Number: 3933\n",
      "Loss: 40.92517868056398\n",
      "l2 norm of gradients: 0.3109324257454927\n",
      "l2 norm of weights: 6.017773037955006\n",
      "---------------------\n",
      "Iteration Number: 3934\n",
      "Loss: 40.92034312368621\n",
      "l2 norm of gradients: 0.31086750112280814\n",
      "l2 norm of weights: 6.017673865082088\n",
      "---------------------\n",
      "Iteration Number: 3935\n",
      "Loss: 40.91550956546506\n",
      "l2 norm of gradients: 0.3108026158986258\n",
      "l2 norm of weights: 6.017574704824031\n",
      "---------------------\n",
      "Iteration Number: 3936\n",
      "Loss: 40.91067800431254\n",
      "l2 norm of gradients: 0.3107377700401095\n",
      "l2 norm of weights: 6.017475557175496\n",
      "---------------------\n",
      "Iteration Number: 3937\n",
      "Loss: 40.905848438609624\n",
      "l2 norm of gradients: 0.31067296351444706\n",
      "l2 norm of weights: 6.017376422131149\n",
      "---------------------\n",
      "Iteration Number: 3938\n",
      "Loss: 40.90102086674978\n",
      "l2 norm of gradients: 0.31060819628884895\n",
      "l2 norm of weights: 6.017277299685663\n",
      "---------------------\n",
      "Iteration Number: 3939\n",
      "Loss: 40.89619528714166\n",
      "l2 norm of gradients: 0.3105434683305492\n",
      "l2 norm of weights: 6.017178189833713\n",
      "---------------------\n",
      "Iteration Number: 3940\n",
      "Loss: 40.89137169816864\n",
      "l2 norm of gradients: 0.3104787796068051\n",
      "l2 norm of weights: 6.017079092569982\n",
      "---------------------\n",
      "Iteration Number: 3941\n",
      "Loss: 40.88655009824184\n",
      "l2 norm of gradients: 0.31041413008489704\n",
      "l2 norm of weights: 6.016980007889154\n",
      "---------------------\n",
      "Iteration Number: 3942\n",
      "Loss: 40.8817304857769\n",
      "l2 norm of gradients: 0.3103495197321291\n",
      "l2 norm of weights: 6.016880935785925\n",
      "---------------------\n",
      "Iteration Number: 3943\n",
      "Loss: 40.87691285916031\n",
      "l2 norm of gradients: 0.3102849485158283\n",
      "l2 norm of weights: 6.0167818762549885\n",
      "---------------------\n",
      "Iteration Number: 3944\n",
      "Loss: 40.872097216782585\n",
      "l2 norm of gradients: 0.3102204164033452\n",
      "l2 norm of weights: 6.016682829291049\n",
      "---------------------\n",
      "Iteration Number: 3945\n",
      "Loss: 40.86728355708238\n",
      "l2 norm of gradients: 0.3101559233620536\n",
      "l2 norm of weights: 6.0165837948888115\n",
      "---------------------\n",
      "Iteration Number: 3946\n",
      "Loss: 40.86247187846651\n",
      "l2 norm of gradients: 0.31009146935935084\n",
      "l2 norm of weights: 6.016484773042988\n",
      "---------------------\n",
      "Iteration Number: 3947\n",
      "Loss: 40.8576621793247\n",
      "l2 norm of gradients: 0.31002705436265726\n",
      "l2 norm of weights: 6.016385763748297\n",
      "---------------------\n",
      "Iteration Number: 3948\n",
      "Loss: 40.85285445808903\n",
      "l2 norm of gradients: 0.30996267833941704\n",
      "l2 norm of weights: 6.0162867669994595\n",
      "---------------------\n",
      "Iteration Number: 3949\n",
      "Loss: 40.84804871316382\n",
      "l2 norm of gradients: 0.30989834125709753\n",
      "l2 norm of weights: 6.016187782791201\n",
      "---------------------\n",
      "Iteration Number: 3950\n",
      "Loss: 40.843244942967964\n",
      "l2 norm of gradients: 0.30983404308318924\n",
      "l2 norm of weights: 6.016088811118257\n",
      "---------------------\n",
      "Iteration Number: 3951\n",
      "Loss: 40.83844314591395\n",
      "l2 norm of gradients: 0.30976978378520653\n",
      "l2 norm of weights: 6.0159898519753625\n",
      "---------------------\n",
      "Iteration Number: 3952\n",
      "Loss: 40.83364332043872\n",
      "l2 norm of gradients: 0.3097055633306869\n",
      "l2 norm of weights: 6.015890905357259\n",
      "---------------------\n",
      "Iteration Number: 3953\n",
      "Loss: 40.8288454649429\n",
      "l2 norm of gradients: 0.30964138168719124\n",
      "l2 norm of weights: 6.015791971258695\n",
      "---------------------\n",
      "Iteration Number: 3954\n",
      "Loss: 40.82404957787009\n",
      "l2 norm of gradients: 0.30957723882230415\n",
      "l2 norm of weights: 6.015693049674422\n",
      "---------------------\n",
      "Iteration Number: 3955\n",
      "Loss: 40.81925565762287\n",
      "l2 norm of gradients: 0.30951313470363345\n",
      "l2 norm of weights: 6.015594140599196\n",
      "---------------------\n",
      "Iteration Number: 3956\n",
      "Loss: 40.8144637026473\n",
      "l2 norm of gradients: 0.3094490692988105\n",
      "l2 norm of weights: 6.015495244027782\n",
      "---------------------\n",
      "Iteration Number: 3957\n",
      "Loss: 40.80967371134945\n",
      "l2 norm of gradients: 0.30938504257549004\n",
      "l2 norm of weights: 6.015396359954942\n",
      "---------------------\n",
      "Iteration Number: 3958\n",
      "Loss: 40.804885682173754\n",
      "l2 norm of gradients: 0.30932105450135033\n",
      "l2 norm of weights: 6.015297488375453\n",
      "---------------------\n",
      "Iteration Number: 3959\n",
      "Loss: 40.800099613562544\n",
      "l2 norm of gradients: 0.3092571050440933\n",
      "l2 norm of weights: 6.015198629284089\n",
      "---------------------\n",
      "Iteration Number: 3960\n",
      "Loss: 40.79531550393385\n",
      "l2 norm of gradients: 0.30919319417144414\n",
      "l2 norm of weights: 6.015099782675631\n",
      "---------------------\n",
      "Iteration Number: 3961\n",
      "Loss: 40.79053335172827\n",
      "l2 norm of gradients: 0.3091293218511516\n",
      "l2 norm of weights: 6.015000948544869\n",
      "---------------------\n",
      "Iteration Number: 3962\n",
      "Loss: 40.78575315536264\n",
      "l2 norm of gradients: 0.30906548805098794\n",
      "l2 norm of weights: 6.014902126886591\n",
      "---------------------\n",
      "Iteration Number: 3963\n",
      "Loss: 40.780974913307624\n",
      "l2 norm of gradients: 0.3090016927387491\n",
      "l2 norm of weights: 6.014803317695597\n",
      "---------------------\n",
      "Iteration Number: 3964\n",
      "Loss: 40.77619862398447\n",
      "l2 norm of gradients: 0.30893793588225427\n",
      "l2 norm of weights: 6.014704520966686\n",
      "---------------------\n",
      "Iteration Number: 3965\n",
      "Loss: 40.7714242858423\n",
      "l2 norm of gradients: 0.3088742174493465\n",
      "l2 norm of weights: 6.014605736694665\n",
      "---------------------\n",
      "Iteration Number: 3966\n",
      "Loss: 40.76665189731319\n",
      "l2 norm of gradients: 0.30881053740789216\n",
      "l2 norm of weights: 6.0145069648743466\n",
      "---------------------\n",
      "Iteration Number: 3967\n",
      "Loss: 40.76188145683555\n",
      "l2 norm of gradients: 0.3087468957257811\n",
      "l2 norm of weights: 6.014408205500546\n",
      "---------------------\n",
      "Iteration Number: 3968\n",
      "Loss: 40.75711296288747\n",
      "l2 norm of gradients: 0.3086832923709271\n",
      "l2 norm of weights: 6.014309458568085\n",
      "---------------------\n",
      "Iteration Number: 3969\n",
      "Loss: 40.75234641390762\n",
      "l2 norm of gradients: 0.3086197273112671\n",
      "l2 norm of weights: 6.014210724071789\n",
      "---------------------\n",
      "Iteration Number: 3970\n",
      "Loss: 40.74758180832384\n",
      "l2 norm of gradients: 0.30855620051476196\n",
      "l2 norm of weights: 6.014112002006486\n",
      "---------------------\n",
      "Iteration Number: 3971\n",
      "Loss: 40.74281914460201\n",
      "l2 norm of gradients: 0.30849271194939576\n",
      "l2 norm of weights: 6.014013292367019\n",
      "---------------------\n",
      "Iteration Number: 3972\n",
      "Loss: 40.73805842118984\n",
      "l2 norm of gradients: 0.30842926158317663\n",
      "l2 norm of weights: 6.013914595148223\n",
      "---------------------\n",
      "Iteration Number: 3973\n",
      "Loss: 40.733299636556396\n",
      "l2 norm of gradients: 0.308365849384136\n",
      "l2 norm of weights: 6.013815910344945\n",
      "---------------------\n",
      "Iteration Number: 3974\n",
      "Loss: 40.72854278914778\n",
      "l2 norm of gradients: 0.30830247532032895\n",
      "l2 norm of weights: 6.013717237952037\n",
      "---------------------\n",
      "Iteration Number: 3975\n",
      "Loss: 40.72378787741812\n",
      "l2 norm of gradients: 0.3082391393598341\n",
      "l2 norm of weights: 6.013618577964352\n",
      "---------------------\n",
      "Iteration Number: 3976\n",
      "Loss: 40.71903489982973\n",
      "l2 norm of gradients: 0.30817584147075394\n",
      "l2 norm of weights: 6.013519930376751\n",
      "---------------------\n",
      "Iteration Number: 3977\n",
      "Loss: 40.714283854857705\n",
      "l2 norm of gradients: 0.3081125816212144\n",
      "l2 norm of weights: 6.013421295184099\n",
      "---------------------\n",
      "Iteration Number: 3978\n",
      "Loss: 40.70953474094932\n",
      "l2 norm of gradients: 0.3080493597793651\n",
      "l2 norm of weights: 6.013322672381265\n",
      "---------------------\n",
      "Iteration Number: 3979\n",
      "Loss: 40.70478755657555\n",
      "l2 norm of gradients: 0.3079861759133793\n",
      "l2 norm of weights: 6.013224061963125\n",
      "---------------------\n",
      "Iteration Number: 3980\n",
      "Loss: 40.70004230020832\n",
      "l2 norm of gradients: 0.3079230299914538\n",
      "l2 norm of weights: 6.013125463924558\n",
      "---------------------\n",
      "Iteration Number: 3981\n",
      "Loss: 40.6952989702937\n",
      "l2 norm of gradients: 0.3078599219818093\n",
      "l2 norm of weights: 6.013026878260448\n",
      "---------------------\n",
      "Iteration Number: 3982\n",
      "Loss: 40.69055756533328\n",
      "l2 norm of gradients: 0.30779685185268996\n",
      "l2 norm of weights: 6.012928304965684\n",
      "---------------------\n",
      "Iteration Number: 3983\n",
      "Loss: 40.68581808378147\n",
      "l2 norm of gradients: 0.3077338195723638\n",
      "l2 norm of weights: 6.0128297440351615\n",
      "---------------------\n",
      "Iteration Number: 3984\n",
      "Loss: 40.681080524102946\n",
      "l2 norm of gradients: 0.30767082510912225\n",
      "l2 norm of weights: 6.012731195463776\n",
      "---------------------\n",
      "Iteration Number: 3985\n",
      "Loss: 40.67634488478922\n",
      "l2 norm of gradients: 0.3076078684312806\n",
      "l2 norm of weights: 6.012632659246434\n",
      "---------------------\n",
      "Iteration Number: 3986\n",
      "Loss: 40.67161116431109\n",
      "l2 norm of gradients: 0.30754494950717776\n",
      "l2 norm of weights: 6.012534135378042\n",
      "---------------------\n",
      "Iteration Number: 3987\n",
      "Loss: 40.66687936113901\n",
      "l2 norm of gradients: 0.30748206830517655\n",
      "l2 norm of weights: 6.012435623853515\n",
      "---------------------\n",
      "Iteration Number: 3988\n",
      "Loss: 40.66214947375791\n",
      "l2 norm of gradients: 0.307419224793663\n",
      "l2 norm of weights: 6.012337124667768\n",
      "---------------------\n",
      "Iteration Number: 3989\n",
      "Loss: 40.65742150064554\n",
      "l2 norm of gradients: 0.3073564189410474\n",
      "l2 norm of weights: 6.0122386378157255\n",
      "---------------------\n",
      "Iteration Number: 3990\n",
      "Loss: 40.652695440293186\n",
      "l2 norm of gradients: 0.30729365071576337\n",
      "l2 norm of weights: 6.012140163292315\n",
      "---------------------\n",
      "Iteration Number: 3991\n",
      "Loss: 40.64797129119912\n",
      "l2 norm of gradients: 0.30723092008626834\n",
      "l2 norm of weights: 6.012041701092468\n",
      "---------------------\n",
      "Iteration Number: 3992\n",
      "Loss: 40.6432490518103\n",
      "l2 norm of gradients: 0.30716822702104357\n",
      "l2 norm of weights: 6.011943251211122\n",
      "---------------------\n",
      "Iteration Number: 3993\n",
      "Loss: 40.63852872063602\n",
      "l2 norm of gradients: 0.30710557148859385\n",
      "l2 norm of weights: 6.011844813643218\n",
      "---------------------\n",
      "Iteration Number: 3994\n",
      "Loss: 40.63381029617018\n",
      "l2 norm of gradients: 0.30704295345744786\n",
      "l2 norm of weights: 6.011746388383702\n",
      "---------------------\n",
      "Iteration Number: 3995\n",
      "Loss: 40.629093776901406\n",
      "l2 norm of gradients: 0.3069803728961579\n",
      "l2 norm of weights: 6.011647975427527\n",
      "---------------------\n",
      "Iteration Number: 3996\n",
      "Loss: 40.624379161323226\n",
      "l2 norm of gradients: 0.30691782977330023\n",
      "l2 norm of weights: 6.011549574769647\n",
      "---------------------\n",
      "Iteration Number: 3997\n",
      "Loss: 40.61966644792262\n",
      "l2 norm of gradients: 0.3068553240574746\n",
      "l2 norm of weights: 6.011451186405022\n",
      "---------------------\n",
      "Iteration Number: 3998\n",
      "Loss: 40.614955635190285\n",
      "l2 norm of gradients: 0.30679285571730447\n",
      "l2 norm of weights: 6.011352810328621\n",
      "---------------------\n",
      "Iteration Number: 3999\n",
      "Loss: 40.610246721644145\n",
      "l2 norm of gradients: 0.30673042472143724\n",
      "l2 norm of weights: 6.0112544465354105\n",
      "---------------------\n",
      "Iteration Number: 4000\n",
      "Loss: 40.60553970577379\n",
      "l2 norm of gradients: 0.30666803103854395\n",
      "l2 norm of weights: 6.0111560950203655\n",
      "---------------------\n",
      "Iteration Number: 4001\n",
      "Loss: 40.60083458607004\n",
      "l2 norm of gradients: 0.30660567463731947\n",
      "l2 norm of weights: 6.011057755778467\n",
      "---------------------\n",
      "Iteration Number: 4002\n",
      "Loss: 40.59613136103647\n",
      "l2 norm of gradients: 0.3065433554864824\n",
      "l2 norm of weights: 6.010959428804698\n",
      "---------------------\n",
      "Iteration Number: 4003\n",
      "Loss: 40.59143002919977\n",
      "l2 norm of gradients: 0.3064810735547751\n",
      "l2 norm of weights: 6.010861114094048\n",
      "---------------------\n",
      "Iteration Number: 4004\n",
      "Loss: 40.58673058903574\n",
      "l2 norm of gradients: 0.3064188288109636\n",
      "l2 norm of weights: 6.01076281164151\n",
      "---------------------\n",
      "Iteration Number: 4005\n",
      "Loss: 40.58203303907664\n",
      "l2 norm of gradients: 0.30635662122383794\n",
      "l2 norm of weights: 6.010664521442083\n",
      "---------------------\n",
      "Iteration Number: 4006\n",
      "Loss: 40.577337377811254\n",
      "l2 norm of gradients: 0.3062944507622116\n",
      "l2 norm of weights: 6.010566243490768\n",
      "---------------------\n",
      "Iteration Number: 4007\n",
      "Loss: 40.57264360376528\n",
      "l2 norm of gradients: 0.3062323173949222\n",
      "l2 norm of weights: 6.010467977782574\n",
      "---------------------\n",
      "Iteration Number: 4008\n",
      "Loss: 40.567951715442625\n",
      "l2 norm of gradients: 0.3061702210908309\n",
      "l2 norm of weights: 6.010369724312513\n",
      "---------------------\n",
      "Iteration Number: 4009\n",
      "Loss: 40.5632617113579\n",
      "l2 norm of gradients: 0.30610816181882267\n",
      "l2 norm of weights: 6.010271483075601\n",
      "---------------------\n",
      "Iteration Number: 4010\n",
      "Loss: 40.55857359002775\n",
      "l2 norm of gradients: 0.30604613954780624\n",
      "l2 norm of weights: 6.010173254066861\n",
      "---------------------\n",
      "Iteration Number: 4011\n",
      "Loss: 40.55388734996107\n",
      "l2 norm of gradients: 0.3059841542467143\n",
      "l2 norm of weights: 6.010075037281318\n",
      "---------------------\n",
      "Iteration Number: 4012\n",
      "Loss: 40.54920298968007\n",
      "l2 norm of gradients: 0.3059222058845032\n",
      "l2 norm of weights: 6.009976832714003\n",
      "---------------------\n",
      "Iteration Number: 4013\n",
      "Loss: 40.544520507706935\n",
      "l2 norm of gradients: 0.30586029443015306\n",
      "l2 norm of weights: 6.009878640359951\n",
      "---------------------\n",
      "Iteration Number: 4014\n",
      "Loss: 40.539839902575814\n",
      "l2 norm of gradients: 0.3057984198526678\n",
      "l2 norm of weights: 6.009780460214203\n",
      "---------------------\n",
      "Iteration Number: 4015\n",
      "Loss: 40.53516117278639\n",
      "l2 norm of gradients: 0.3057365821210753\n",
      "l2 norm of weights: 6.009682292271803\n",
      "---------------------\n",
      "Iteration Number: 4016\n",
      "Loss: 40.530484316871046\n",
      "l2 norm of gradients: 0.3056747812044269\n",
      "l2 norm of weights: 6.0095841365278\n",
      "---------------------\n",
      "Iteration Number: 4017\n",
      "Loss: 40.52580933335675\n",
      "l2 norm of gradients: 0.30561301707179817\n",
      "l2 norm of weights: 6.009485992977249\n",
      "---------------------\n",
      "Iteration Number: 4018\n",
      "Loss: 40.52113622078315\n",
      "l2 norm of gradients: 0.3055512896922881\n",
      "l2 norm of weights: 6.009387861615206\n",
      "---------------------\n",
      "Iteration Number: 4019\n",
      "Loss: 40.51646497766414\n",
      "l2 norm of gradients: 0.3054895990350197\n",
      "l2 norm of weights: 6.009289742436737\n",
      "---------------------\n",
      "Iteration Number: 4020\n",
      "Loss: 40.511795602534406\n",
      "l2 norm of gradients: 0.30542794506913973\n",
      "l2 norm of weights: 6.009191635436908\n",
      "---------------------\n",
      "Iteration Number: 4021\n",
      "Loss: 40.50712809394389\n",
      "l2 norm of gradients: 0.30536632776381867\n",
      "l2 norm of weights: 6.009093540610791\n",
      "---------------------\n",
      "Iteration Number: 4022\n",
      "Loss: 40.50246245039124\n",
      "l2 norm of gradients: 0.3053047470882511\n",
      "l2 norm of weights: 6.008995457953464\n",
      "---------------------\n",
      "Iteration Number: 4023\n",
      "Loss: 40.49779867043718\n",
      "l2 norm of gradients: 0.30524320301165503\n",
      "l2 norm of weights: 6.008897387460007\n",
      "---------------------\n",
      "Iteration Number: 4024\n",
      "Loss: 40.49313675261447\n",
      "l2 norm of gradients: 0.30518169550327245\n",
      "l2 norm of weights: 6.008799329125508\n",
      "---------------------\n",
      "Iteration Number: 4025\n",
      "Loss: 40.48847669546392\n",
      "l2 norm of gradients: 0.30512022453236937\n",
      "l2 norm of weights: 6.008701282945055\n",
      "---------------------\n",
      "Iteration Number: 4026\n",
      "Loss: 40.48381849751324\n",
      "l2 norm of gradients: 0.3050587900682351\n",
      "l2 norm of weights: 6.008603248913745\n",
      "---------------------\n",
      "Iteration Number: 4027\n",
      "Loss: 40.47916215731402\n",
      "l2 norm of gradients: 0.30499739208018334\n",
      "l2 norm of weights: 6.008505227026677\n",
      "---------------------\n",
      "Iteration Number: 4028\n",
      "Loss: 40.47450767342025\n",
      "l2 norm of gradients: 0.30493603053755114\n",
      "l2 norm of weights: 6.008407217278955\n",
      "---------------------\n",
      "Iteration Number: 4029\n",
      "Loss: 40.469855044362475\n",
      "l2 norm of gradients: 0.30487470540969974\n",
      "l2 norm of weights: 6.008309219665688\n",
      "---------------------\n",
      "Iteration Number: 4030\n",
      "Loss: 40.46520426868686\n",
      "l2 norm of gradients: 0.3048134166660139\n",
      "l2 norm of weights: 6.0082112341819895\n",
      "---------------------\n",
      "Iteration Number: 4031\n",
      "Loss: 40.460555344949505\n",
      "l2 norm of gradients: 0.30475216427590235\n",
      "l2 norm of weights: 6.008113260822976\n",
      "---------------------\n",
      "Iteration Number: 4032\n",
      "Loss: 40.455908271690866\n",
      "l2 norm of gradients: 0.3046909482087976\n",
      "l2 norm of weights: 6.0080152995837715\n",
      "---------------------\n",
      "Iteration Number: 4033\n",
      "Loss: 40.45126304746669\n",
      "l2 norm of gradients: 0.304629768434156\n",
      "l2 norm of weights: 6.007917350459501\n",
      "---------------------\n",
      "Iteration Number: 4034\n",
      "Loss: 40.446619670838864\n",
      "l2 norm of gradients: 0.30456862492145764\n",
      "l2 norm of weights: 6.007819413445299\n",
      "---------------------\n",
      "Iteration Number: 4035\n",
      "Loss: 40.44197814035526\n",
      "l2 norm of gradients: 0.3045075176402065\n",
      "l2 norm of weights: 6.007721488536298\n",
      "---------------------\n",
      "Iteration Number: 4036\n",
      "Loss: 40.43733845455866\n",
      "l2 norm of gradients: 0.3044464465599304\n",
      "l2 norm of weights: 6.007623575727639\n",
      "---------------------\n",
      "Iteration Number: 4037\n",
      "Loss: 40.43270061200904\n",
      "l2 norm of gradients: 0.30438541165018085\n",
      "l2 norm of weights: 6.0075256750144685\n",
      "---------------------\n",
      "Iteration Number: 4038\n",
      "Loss: 40.42806461129411\n",
      "l2 norm of gradients: 0.30432441288053325\n",
      "l2 norm of weights: 6.0074277863919345\n",
      "---------------------\n",
      "Iteration Number: 4039\n",
      "Loss: 40.42343045094217\n",
      "l2 norm of gradients: 0.30426345022058693\n",
      "l2 norm of weights: 6.007329909855191\n",
      "---------------------\n",
      "Iteration Number: 4040\n",
      "Loss: 40.418798129531226\n",
      "l2 norm of gradients: 0.30420252363996475\n",
      "l2 norm of weights: 6.007232045399396\n",
      "---------------------\n",
      "Iteration Number: 4041\n",
      "Loss: 40.41416764562601\n",
      "l2 norm of gradients: 0.30414163310831377\n",
      "l2 norm of weights: 6.007134193019714\n",
      "---------------------\n",
      "Iteration Number: 4042\n",
      "Loss: 40.40953899777008\n",
      "l2 norm of gradients: 0.30408077859530447\n",
      "l2 norm of weights: 6.00703635271131\n",
      "---------------------\n",
      "Iteration Number: 4043\n",
      "Loss: 40.40491218455224\n",
      "l2 norm of gradients: 0.3040199600706314\n",
      "l2 norm of weights: 6.006938524469357\n",
      "---------------------\n",
      "Iteration Number: 4044\n",
      "Loss: 40.40028720453888\n",
      "l2 norm of gradients: 0.30395917750401275\n",
      "l2 norm of weights: 6.00684070828903\n",
      "---------------------\n",
      "Iteration Number: 4045\n",
      "Loss: 40.39566405628904\n",
      "l2 norm of gradients: 0.3038984308651908\n",
      "l2 norm of weights: 6.006742904165511\n",
      "---------------------\n",
      "Iteration Number: 4046\n",
      "Loss: 40.391042738390695\n",
      "l2 norm of gradients: 0.3038377201239313\n",
      "l2 norm of weights: 6.0066451120939846\n",
      "---------------------\n",
      "Iteration Number: 4047\n",
      "Loss: 40.38642324939595\n",
      "l2 norm of gradients: 0.303777045250024\n",
      "l2 norm of weights: 6.0065473320696405\n",
      "---------------------\n",
      "Iteration Number: 4048\n",
      "Loss: 40.3818055878992\n",
      "l2 norm of gradients: 0.3037164062132824\n",
      "l2 norm of weights: 6.006449564087671\n",
      "---------------------\n",
      "Iteration Number: 4049\n",
      "Loss: 40.37718975245188\n",
      "l2 norm of gradients: 0.3036558029835439\n",
      "l2 norm of weights: 6.006351808143277\n",
      "---------------------\n",
      "Iteration Number: 4050\n",
      "Loss: 40.37257574164179\n",
      "l2 norm of gradients: 0.30359523553066947\n",
      "l2 norm of weights: 6.00625406423166\n",
      "---------------------\n",
      "Iteration Number: 4051\n",
      "Loss: 40.367963554066165\n",
      "l2 norm of gradients: 0.30353470382454417\n",
      "l2 norm of weights: 6.006156332348027\n",
      "---------------------\n",
      "Iteration Number: 4052\n",
      "Loss: 40.36335318828963\n",
      "l2 norm of gradients: 0.30347420783507667\n",
      "l2 norm of weights: 6.006058612487591\n",
      "---------------------\n",
      "Iteration Number: 4053\n",
      "Loss: 40.35874464288248\n",
      "l2 norm of gradients: 0.30341374753219946\n",
      "l2 norm of weights: 6.0059609046455655\n",
      "---------------------\n",
      "Iteration Number: 4054\n",
      "Loss: 40.354137916457276\n",
      "l2 norm of gradients: 0.30335332288586886\n",
      "l2 norm of weights: 6.005863208817173\n",
      "---------------------\n",
      "Iteration Number: 4055\n",
      "Loss: 40.34953300756881\n",
      "l2 norm of gradients: 0.30329293386606504\n",
      "l2 norm of weights: 6.0057655249976385\n",
      "---------------------\n",
      "Iteration Number: 4056\n",
      "Loss: 40.34492991481511\n",
      "l2 norm of gradients: 0.30323258044279183\n",
      "l2 norm of weights: 6.00566785318219\n",
      "---------------------\n",
      "Iteration Number: 4057\n",
      "Loss: 40.340328636796954\n",
      "l2 norm of gradients: 0.303172262586077\n",
      "l2 norm of weights: 6.0055701933660615\n",
      "---------------------\n",
      "Iteration Number: 4058\n",
      "Loss: 40.335729172074856\n",
      "l2 norm of gradients: 0.303111980265972\n",
      "l2 norm of weights: 6.005472545544491\n",
      "---------------------\n",
      "Iteration Number: 4059\n",
      "Loss: 40.331131519267316\n",
      "l2 norm of gradients: 0.30305173345255204\n",
      "l2 norm of weights: 6.005374909712721\n",
      "---------------------\n",
      "Iteration Number: 4060\n",
      "Loss: 40.326535676955636\n",
      "l2 norm of gradients: 0.30299152211591623\n",
      "l2 norm of weights: 6.005277285865998\n",
      "---------------------\n",
      "Iteration Number: 4061\n",
      "Loss: 40.32194164372983\n",
      "l2 norm of gradients: 0.3029313462261873\n",
      "l2 norm of weights: 6.005179673999574\n",
      "---------------------\n",
      "Iteration Number: 4062\n",
      "Loss: 40.31734941818937\n",
      "l2 norm of gradients: 0.30287120575351195\n",
      "l2 norm of weights: 6.005082074108703\n",
      "---------------------\n",
      "Iteration Number: 4063\n",
      "Loss: 40.3127589989447\n",
      "l2 norm of gradients: 0.30281110066806055\n",
      "l2 norm of weights: 6.004984486188646\n",
      "---------------------\n",
      "Iteration Number: 4064\n",
      "Loss: 40.30817038456958\n",
      "l2 norm of gradients: 0.3027510309400271\n",
      "l2 norm of weights: 6.004886910234667\n",
      "---------------------\n",
      "Iteration Number: 4065\n",
      "Loss: 40.30358357367362\n",
      "l2 norm of gradients: 0.3026909965396297\n",
      "l2 norm of weights: 6.004789346242034\n",
      "---------------------\n",
      "Iteration Number: 4066\n",
      "Loss: 40.29899856486363\n",
      "l2 norm of gradients: 0.3026309974371099\n",
      "l2 norm of weights: 6.00469179420602\n",
      "---------------------\n",
      "Iteration Number: 4067\n",
      "Loss: 40.2944153567349\n",
      "l2 norm of gradients: 0.3025710336027333\n",
      "l2 norm of weights: 6.004594254121902\n",
      "---------------------\n",
      "Iteration Number: 4068\n",
      "Loss: 40.28983394790582\n",
      "l2 norm of gradients: 0.3025111050067889\n",
      "l2 norm of weights: 6.004496725984963\n",
      "---------------------\n",
      "Iteration Number: 4069\n",
      "Loss: 40.2852543369658\n",
      "l2 norm of gradients: 0.30245121161958977\n",
      "l2 norm of weights: 6.004399209790487\n",
      "---------------------\n",
      "Iteration Number: 4070\n",
      "Loss: 40.28067652252033\n",
      "l2 norm of gradients: 0.3023913534114726\n",
      "l2 norm of weights: 6.004301705533765\n",
      "---------------------\n",
      "Iteration Number: 4071\n",
      "Loss: 40.27610050319791\n",
      "l2 norm of gradients: 0.30233153035279775\n",
      "l2 norm of weights: 6.004204213210091\n",
      "---------------------\n",
      "Iteration Number: 4072\n",
      "Loss: 40.271526277590155\n",
      "l2 norm of gradients: 0.3022717424139496\n",
      "l2 norm of weights: 6.004106732814764\n",
      "---------------------\n",
      "Iteration Number: 4073\n",
      "Loss: 40.26695384431948\n",
      "l2 norm of gradients: 0.302211989565336\n",
      "l2 norm of weights: 6.004009264343087\n",
      "---------------------\n",
      "Iteration Number: 4074\n",
      "Loss: 40.26238320198917\n",
      "l2 norm of gradients: 0.30215227177738857\n",
      "l2 norm of weights: 6.0039118077903675\n",
      "---------------------\n",
      "Iteration Number: 4075\n",
      "Loss: 40.25781434921996\n",
      "l2 norm of gradients: 0.3020925890205628\n",
      "l2 norm of weights: 6.003814363151917\n",
      "---------------------\n",
      "Iteration Number: 4076\n",
      "Loss: 40.25324728463076\n",
      "l2 norm of gradients: 0.30203294126533775\n",
      "l2 norm of weights: 6.003716930423051\n",
      "---------------------\n",
      "Iteration Number: 4077\n",
      "Loss: 40.248682006834585\n",
      "l2 norm of gradients: 0.3019733284822164\n",
      "l2 norm of weights: 6.0036195095990905\n",
      "---------------------\n",
      "Iteration Number: 4078\n",
      "Loss: 40.24411851444886\n",
      "l2 norm of gradients: 0.30191375064172526\n",
      "l2 norm of weights: 6.003522100675361\n",
      "---------------------\n",
      "Iteration Number: 4079\n",
      "Loss: 40.23955680610686\n",
      "l2 norm of gradients: 0.30185420771441457\n",
      "l2 norm of weights: 6.003424703647189\n",
      "---------------------\n",
      "Iteration Number: 4080\n",
      "Loss: 40.23499688041199\n",
      "l2 norm of gradients: 0.3017946996708585\n",
      "l2 norm of weights: 6.003327318509909\n",
      "---------------------\n",
      "Iteration Number: 4081\n",
      "Loss: 40.23043873599793\n",
      "l2 norm of gradients: 0.30173522648165463\n",
      "l2 norm of weights: 6.003229945258856\n",
      "---------------------\n",
      "Iteration Number: 4082\n",
      "Loss: 40.225882371490975\n",
      "l2 norm of gradients: 0.30167578811742446\n",
      "l2 norm of weights: 6.003132583889376\n",
      "---------------------\n",
      "Iteration Number: 4083\n",
      "Loss: 40.22132778551949\n",
      "l2 norm of gradients: 0.3016163845488132\n",
      "l2 norm of weights: 6.00303523439681\n",
      "---------------------\n",
      "Iteration Number: 4084\n",
      "Loss: 40.21677497670345\n",
      "l2 norm of gradients: 0.3015570157464895\n",
      "l2 norm of weights: 6.002937896776511\n",
      "---------------------\n",
      "Iteration Number: 4085\n",
      "Loss: 40.21222394366937\n",
      "l2 norm of gradients: 0.301497681681146\n",
      "l2 norm of weights: 6.002840571023832\n",
      "---------------------\n",
      "Iteration Number: 4086\n",
      "Loss: 40.207674685061356\n",
      "l2 norm of gradients: 0.3014383823234988\n",
      "l2 norm of weights: 6.0027432571341315\n",
      "---------------------\n",
      "Iteration Number: 4087\n",
      "Loss: 40.20312719949699\n",
      "l2 norm of gradients: 0.30137911764428793\n",
      "l2 norm of weights: 6.002645955102774\n",
      "---------------------\n",
      "Iteration Number: 4088\n",
      "Loss: 40.19858148563098\n",
      "l2 norm of gradients: 0.30131988761427675\n",
      "l2 norm of weights: 6.002548664925124\n",
      "---------------------\n",
      "Iteration Number: 4089\n",
      "Loss: 40.19403754207707\n",
      "l2 norm of gradients: 0.3012606922042527\n",
      "l2 norm of weights: 6.002451386596553\n",
      "---------------------\n",
      "Iteration Number: 4090\n",
      "Loss: 40.18949536748596\n",
      "l2 norm of gradients: 0.3012015313850265\n",
      "l2 norm of weights: 6.002354120112439\n",
      "---------------------\n",
      "Iteration Number: 4091\n",
      "Loss: 40.184954960483154\n",
      "l2 norm of gradients: 0.3011424051274328\n",
      "l2 norm of weights: 6.0022568654681585\n",
      "---------------------\n",
      "Iteration Number: 4092\n",
      "Loss: 40.180416319707035\n",
      "l2 norm of gradients: 0.30108331340232974\n",
      "l2 norm of weights: 6.002159622659096\n",
      "---------------------\n",
      "Iteration Number: 4093\n",
      "Loss: 40.1758794438232\n",
      "l2 norm of gradients: 0.30102425618059914\n",
      "l2 norm of weights: 6.00206239168064\n",
      "---------------------\n",
      "Iteration Number: 4094\n",
      "Loss: 40.171344331444935\n",
      "l2 norm of gradients: 0.30096523343314663\n",
      "l2 norm of weights: 6.001965172528183\n",
      "---------------------\n",
      "Iteration Number: 4095\n",
      "Loss: 40.16681098123723\n",
      "l2 norm of gradients: 0.30090624513090125\n",
      "l2 norm of weights: 6.00186796519712\n",
      "---------------------\n",
      "Iteration Number: 4096\n",
      "Loss: 40.16227939182081\n",
      "l2 norm of gradients: 0.30084729124481585\n",
      "l2 norm of weights: 6.001770769682853\n",
      "---------------------\n",
      "Iteration Number: 4097\n",
      "Loss: 40.15774956187641\n",
      "l2 norm of gradients: 0.30078837174586676\n",
      "l2 norm of weights: 6.001673585980787\n",
      "---------------------\n",
      "Iteration Number: 4098\n",
      "Loss: 40.15322149002008\n",
      "l2 norm of gradients: 0.300729486605054\n",
      "l2 norm of weights: 6.001576414086329\n",
      "---------------------\n",
      "Iteration Number: 4099\n",
      "Loss: 40.14869517491996\n",
      "l2 norm of gradients: 0.30067063579340136\n",
      "l2 norm of weights: 6.0014792539948925\n",
      "---------------------\n",
      "Iteration Number: 4100\n",
      "Loss: 40.14417061521616\n",
      "l2 norm of gradients: 0.3006118192819558\n",
      "l2 norm of weights: 6.001382105701897\n",
      "---------------------\n",
      "Iteration Number: 4101\n",
      "Loss: 40.139647809579245\n",
      "l2 norm of gradients: 0.3005530370417884\n",
      "l2 norm of weights: 6.00128496920276\n",
      "---------------------\n",
      "Iteration Number: 4102\n",
      "Loss: 40.13512675664047\n",
      "l2 norm of gradients: 0.30049428904399356\n",
      "l2 norm of weights: 6.00118784449291\n",
      "---------------------\n",
      "Iteration Number: 4103\n",
      "Loss: 40.13060745507176\n",
      "l2 norm of gradients: 0.3004355752596894\n",
      "l2 norm of weights: 6.001090731567776\n",
      "---------------------\n",
      "Iteration Number: 4104\n",
      "Loss: 40.12608990351342\n",
      "l2 norm of gradients: 0.30037689566001735\n",
      "l2 norm of weights: 6.000993630422791\n",
      "---------------------\n",
      "Iteration Number: 4105\n",
      "Loss: 40.1215741006427\n",
      "l2 norm of gradients: 0.30031825021614267\n",
      "l2 norm of weights: 6.000896541053394\n",
      "---------------------\n",
      "Iteration Number: 4106\n",
      "Loss: 40.11706004510426\n",
      "l2 norm of gradients: 0.30025963889925433\n",
      "l2 norm of weights: 6.000799463455026\n",
      "---------------------\n",
      "Iteration Number: 4107\n",
      "Loss: 40.11254773556265\n",
      "l2 norm of gradients: 0.3002010616805645\n",
      "l2 norm of weights: 6.000702397623134\n",
      "---------------------\n",
      "Iteration Number: 4108\n",
      "Loss: 40.10803717069875\n",
      "l2 norm of gradients: 0.30014251853130913\n",
      "l2 norm of weights: 6.000605343553168\n",
      "---------------------\n",
      "Iteration Number: 4109\n",
      "Loss: 40.10352834914841\n",
      "l2 norm of gradients: 0.3000840094227477\n",
      "l2 norm of weights: 6.000508301240582\n",
      "---------------------\n",
      "Iteration Number: 4110\n",
      "Loss: 40.09902126959091\n",
      "l2 norm of gradients: 0.30002553432616325\n",
      "l2 norm of weights: 6.000411270680836\n",
      "---------------------\n",
      "Iteration Number: 4111\n",
      "Loss: 40.094515930682746\n",
      "l2 norm of gradients: 0.29996709321286225\n",
      "l2 norm of weights: 6.000314251869391\n",
      "---------------------\n",
      "Iteration Number: 4112\n",
      "Loss: 40.09001233109787\n",
      "l2 norm of gradients: 0.2999086860541749\n",
      "l2 norm of weights: 6.000217244801713\n",
      "---------------------\n",
      "Iteration Number: 4113\n",
      "Loss: 40.085510469522056\n",
      "l2 norm of gradients: 0.2998503128214547\n",
      "l2 norm of weights: 6.000120249473275\n",
      "---------------------\n",
      "Iteration Number: 4114\n",
      "Loss: 40.08101034460724\n",
      "l2 norm of gradients: 0.2997919734860789\n",
      "l2 norm of weights: 6.000023265879551\n",
      "---------------------\n",
      "Iteration Number: 4115\n",
      "Loss: 40.076511955026845\n",
      "l2 norm of gradients: 0.299733668019448\n",
      "l2 norm of weights: 5.999926294016021\n",
      "---------------------\n",
      "Iteration Number: 4116\n",
      "Loss: 40.072015299455046\n",
      "l2 norm of gradients: 0.2996753963929863\n",
      "l2 norm of weights: 5.999829333878165\n",
      "---------------------\n",
      "Iteration Number: 4117\n",
      "Loss: 40.067520376586096\n",
      "l2 norm of gradients: 0.29961715857814136\n",
      "l2 norm of weights: 5.999732385461473\n",
      "---------------------\n",
      "Iteration Number: 4118\n",
      "Loss: 40.06302718506017\n",
      "l2 norm of gradients: 0.29955895454638437\n",
      "l2 norm of weights: 5.9996354487614365\n",
      "---------------------\n",
      "Iteration Number: 4119\n",
      "Loss: 40.05853572358671\n",
      "l2 norm of gradients: 0.2995007842692099\n",
      "l2 norm of weights: 5.999538523773549\n",
      "---------------------\n",
      "Iteration Number: 4120\n",
      "Loss: 40.05404599082779\n",
      "l2 norm of gradients: 0.2994426477181361\n",
      "l2 norm of weights: 5.999441610493309\n",
      "---------------------\n",
      "Iteration Number: 4121\n",
      "Loss: 40.049557985475346\n",
      "l2 norm of gradients: 0.2993845448647046\n",
      "l2 norm of weights: 5.999344708916223\n",
      "---------------------\n",
      "Iteration Number: 4122\n",
      "Loss: 40.045071706205526\n",
      "l2 norm of gradients: 0.2993264756804804\n",
      "l2 norm of weights: 5.999247819037797\n",
      "---------------------\n",
      "Iteration Number: 4123\n",
      "Loss: 40.04058715169464\n",
      "l2 norm of gradients: 0.299268440137052\n",
      "l2 norm of weights: 5.999150940853542\n",
      "---------------------\n",
      "Iteration Number: 4124\n",
      "Loss: 40.03610432064319\n",
      "l2 norm of gradients: 0.2992104382060313\n",
      "l2 norm of weights: 5.999054074358973\n",
      "---------------------\n",
      "Iteration Number: 4125\n",
      "Loss: 40.03162321172604\n",
      "l2 norm of gradients: 0.29915246985905375\n",
      "l2 norm of weights: 5.998957219549611\n",
      "---------------------\n",
      "Iteration Number: 4126\n",
      "Loss: 40.02714382363045\n",
      "l2 norm of gradients: 0.2990945350677781\n",
      "l2 norm of weights: 5.99886037642098\n",
      "---------------------\n",
      "Iteration Number: 4127\n",
      "Loss: 40.022666155051624\n",
      "l2 norm of gradients: 0.29903663380388673\n",
      "l2 norm of weights: 5.998763544968605\n",
      "---------------------\n",
      "Iteration Number: 4128\n",
      "Loss: 40.01819020468298\n",
      "l2 norm of gradients: 0.2989787660390851\n",
      "l2 norm of weights: 5.99866672518802\n",
      "---------------------\n",
      "Iteration Number: 4129\n",
      "Loss: 40.01371597120506\n",
      "l2 norm of gradients: 0.2989209317451025\n",
      "l2 norm of weights: 5.99856991707476\n",
      "---------------------\n",
      "Iteration Number: 4130\n",
      "Loss: 40.00924345331774\n",
      "l2 norm of gradients: 0.29886313089369115\n",
      "l2 norm of weights: 5.9984731206243636\n",
      "---------------------\n",
      "Iteration Number: 4131\n",
      "Loss: 40.004772649714724\n",
      "l2 norm of gradients: 0.2988053634566271\n",
      "l2 norm of weights: 5.9983763358323765\n",
      "---------------------\n",
      "Iteration Number: 4132\n",
      "Loss: 40.000303559096686\n",
      "l2 norm of gradients: 0.2987476294057096\n",
      "l2 norm of weights: 5.998279562694345\n",
      "---------------------\n",
      "Iteration Number: 4133\n",
      "Loss: 39.9958361801497\n",
      "l2 norm of gradients: 0.2986899287127612\n",
      "l2 norm of weights: 5.99818280120582\n",
      "---------------------\n",
      "Iteration Number: 4134\n",
      "Loss: 39.99137051158693\n",
      "l2 norm of gradients: 0.2986322613496278\n",
      "l2 norm of weights: 5.9980860513623595\n",
      "---------------------\n",
      "Iteration Number: 4135\n",
      "Loss: 39.98690655209754\n",
      "l2 norm of gradients: 0.29857462728817896\n",
      "l2 norm of weights: 5.997989313159521\n",
      "---------------------\n",
      "Iteration Number: 4136\n",
      "Loss: 39.98244430038423\n",
      "l2 norm of gradients: 0.29851702650030726\n",
      "l2 norm of weights: 5.997892586592869\n",
      "---------------------\n",
      "Iteration Number: 4137\n",
      "Loss: 39.9779837551473\n",
      "l2 norm of gradients: 0.2984594589579288\n",
      "l2 norm of weights: 5.99779587165797\n",
      "---------------------\n",
      "Iteration Number: 4138\n",
      "Loss: 39.97352491509943\n",
      "l2 norm of gradients: 0.29840192463298293\n",
      "l2 norm of weights: 5.9976991683503975\n",
      "---------------------\n",
      "Iteration Number: 4139\n",
      "Loss: 39.96906777894097\n",
      "l2 norm of gradients: 0.2983444234974324\n",
      "l2 norm of weights: 5.997602476665726\n",
      "---------------------\n",
      "Iteration Number: 4140\n",
      "Loss: 39.964612345389035\n",
      "l2 norm of gradients: 0.2982869555232633\n",
      "l2 norm of weights: 5.997505796599535\n",
      "---------------------\n",
      "Iteration Number: 4141\n",
      "Loss: 39.96015861313902\n",
      "l2 norm of gradients: 0.29822952068248476\n",
      "l2 norm of weights: 5.997409128147408\n",
      "---------------------\n",
      "Iteration Number: 4142\n",
      "Loss: 39.95570658090566\n",
      "l2 norm of gradients: 0.2981721189471296\n",
      "l2 norm of weights: 5.997312471304932\n",
      "---------------------\n",
      "Iteration Number: 4143\n",
      "Loss: 39.95125624738436\n",
      "l2 norm of gradients: 0.29811475028925377\n",
      "l2 norm of weights: 5.9972158260677\n",
      "---------------------\n",
      "Iteration Number: 4144\n",
      "Loss: 39.94680761132203\n",
      "l2 norm of gradients: 0.2980574146809364\n",
      "l2 norm of weights: 5.997119192431304\n",
      "---------------------\n",
      "Iteration Number: 4145\n",
      "Loss: 39.94236067138916\n",
      "l2 norm of gradients: 0.29800011209428007\n",
      "l2 norm of weights: 5.997022570391346\n",
      "---------------------\n",
      "Iteration Number: 4146\n",
      "Loss: 39.9379154263387\n",
      "l2 norm of gradients: 0.29794284250141034\n",
      "l2 norm of weights: 5.996925959943428\n",
      "---------------------\n",
      "Iteration Number: 4147\n",
      "Loss: 39.93347187487967\n",
      "l2 norm of gradients: 0.2978856058744765\n",
      "l2 norm of weights: 5.996829361083159\n",
      "---------------------\n",
      "Iteration Number: 4148\n",
      "Loss: 39.929030015709266\n",
      "l2 norm of gradients: 0.29782840218565065\n",
      "l2 norm of weights: 5.996732773806146\n",
      "---------------------\n",
      "Iteration Number: 4149\n",
      "Loss: 39.9245898475693\n",
      "l2 norm of gradients: 0.29777123140712836\n",
      "l2 norm of weights: 5.996636198108008\n",
      "---------------------\n",
      "Iteration Number: 4150\n",
      "Loss: 39.920151369164316\n",
      "l2 norm of gradients: 0.29771409351112826\n",
      "l2 norm of weights: 5.996539633984361\n",
      "---------------------\n",
      "Iteration Number: 4151\n",
      "Loss: 39.915714579229814\n",
      "l2 norm of gradients: 0.2976569884698925\n",
      "l2 norm of weights: 5.996443081430829\n",
      "---------------------\n",
      "Iteration Number: 4152\n",
      "Loss: 39.91127947647547\n",
      "l2 norm of gradients: 0.297599916255686\n",
      "l2 norm of weights: 5.996346540443039\n",
      "---------------------\n",
      "Iteration Number: 4153\n",
      "Loss: 39.906846059626616\n",
      "l2 norm of gradients: 0.29754287684079733\n",
      "l2 norm of weights: 5.99625001101662\n",
      "---------------------\n",
      "Iteration Number: 4154\n",
      "Loss: 39.90241432743092\n",
      "l2 norm of gradients: 0.29748587019753797\n",
      "l2 norm of weights: 5.996153493147209\n",
      "---------------------\n",
      "Iteration Number: 4155\n",
      "Loss: 39.897984278591004\n",
      "l2 norm of gradients: 0.29742889629824254\n",
      "l2 norm of weights: 5.9960569868304425\n",
      "---------------------\n",
      "Iteration Number: 4156\n",
      "Loss: 39.89355591184208\n",
      "l2 norm of gradients: 0.29737195511526926\n",
      "l2 norm of weights: 5.995960492061962\n",
      "---------------------\n",
      "Iteration Number: 4157\n",
      "Loss: 39.889129225919575\n",
      "l2 norm of gradients: 0.297315046620999\n",
      "l2 norm of weights: 5.995864008837414\n",
      "---------------------\n",
      "Iteration Number: 4158\n",
      "Loss: 39.88470421954627\n",
      "l2 norm of gradients: 0.2972581707878359\n",
      "l2 norm of weights: 5.99576753715245\n",
      "---------------------\n",
      "Iteration Number: 4159\n",
      "Loss: 39.88028089146922\n",
      "l2 norm of gradients: 0.29720132758820755\n",
      "l2 norm of weights: 5.995671077002723\n",
      "---------------------\n",
      "Iteration Number: 4160\n",
      "Loss: 39.8758592404106\n",
      "l2 norm of gradients: 0.2971445169945644\n",
      "l2 norm of weights: 5.995574628383891\n",
      "---------------------\n",
      "Iteration Number: 4161\n",
      "Loss: 39.87143926510688\n",
      "l2 norm of gradients: 0.2970877389793802\n",
      "l2 norm of weights: 5.995478191291614\n",
      "---------------------\n",
      "Iteration Number: 4162\n",
      "Loss: 39.86702096428161\n",
      "l2 norm of gradients: 0.2970309935151514\n",
      "l2 norm of weights: 5.995381765721559\n",
      "---------------------\n",
      "Iteration Number: 4163\n",
      "Loss: 39.86260433671263\n",
      "l2 norm of gradients: 0.2969742805743981\n",
      "l2 norm of weights: 5.995285351669394\n",
      "---------------------\n",
      "Iteration Number: 4164\n",
      "Loss: 39.85818938108891\n",
      "l2 norm of gradients: 0.29691760012966323\n",
      "l2 norm of weights: 5.995188949130793\n",
      "---------------------\n",
      "Iteration Number: 4165\n",
      "Loss: 39.85377609618839\n",
      "l2 norm of gradients: 0.2968609521535129\n",
      "l2 norm of weights: 5.995092558101433\n",
      "---------------------\n",
      "Iteration Number: 4166\n",
      "Loss: 39.849364480738735\n",
      "l2 norm of gradients: 0.29680433661853606\n",
      "l2 norm of weights: 5.9949961785769945\n",
      "---------------------\n",
      "Iteration Number: 4167\n",
      "Loss: 39.844954533492334\n",
      "l2 norm of gradients: 0.29674775349734506\n",
      "l2 norm of weights: 5.994899810553162\n",
      "---------------------\n",
      "Iteration Number: 4168\n",
      "Loss: 39.84054625317343\n",
      "l2 norm of gradients: 0.296691202762575\n",
      "l2 norm of weights: 5.994803454025624\n",
      "---------------------\n",
      "Iteration Number: 4169\n",
      "Loss: 39.836139638544815\n",
      "l2 norm of gradients: 0.2966346843868844\n",
      "l2 norm of weights: 5.994707108990073\n",
      "---------------------\n",
      "Iteration Number: 4170\n",
      "Loss: 39.83173468834008\n",
      "l2 norm of gradients: 0.29657819834295435\n",
      "l2 norm of weights: 5.994610775442205\n",
      "---------------------\n",
      "Iteration Number: 4171\n",
      "Loss: 39.82733140133555\n",
      "l2 norm of gradients: 0.2965217446034894\n",
      "l2 norm of weights: 5.994514453377721\n",
      "---------------------\n",
      "Iteration Number: 4172\n",
      "Loss: 39.82292977625201\n",
      "l2 norm of gradients: 0.29646532314121676\n",
      "l2 norm of weights: 5.9944181427923215\n",
      "---------------------\n",
      "Iteration Number: 4173\n",
      "Loss: 39.818529811849025\n",
      "l2 norm of gradients: 0.29640893392888695\n",
      "l2 norm of weights: 5.994321843681717\n",
      "---------------------\n",
      "Iteration Number: 4174\n",
      "Loss: 39.814131506883406\n",
      "l2 norm of gradients: 0.2963525769392733\n",
      "l2 norm of weights: 5.994225556041618\n",
      "---------------------\n",
      "Iteration Number: 4175\n",
      "Loss: 39.80973486011319\n",
      "l2 norm of gradients: 0.296296252145172\n",
      "l2 norm of weights: 5.994129279867739\n",
      "---------------------\n",
      "Iteration Number: 4176\n",
      "Loss: 39.80533987027029\n",
      "l2 norm of gradients: 0.2962399595194027\n",
      "l2 norm of weights: 5.9940330151558\n",
      "---------------------\n",
      "Iteration Number: 4177\n",
      "Loss: 39.800946536150825\n",
      "l2 norm of gradients: 0.29618369903480746\n",
      "l2 norm of weights: 5.993936761901522\n",
      "---------------------\n",
      "Iteration Number: 4178\n",
      "Loss: 39.79655485647044\n",
      "l2 norm of gradients: 0.2961274706642516\n",
      "l2 norm of weights: 5.993840520100634\n",
      "---------------------\n",
      "Iteration Number: 4179\n",
      "Loss: 39.79216483001789\n",
      "l2 norm of gradients: 0.29607127438062325\n",
      "l2 norm of weights: 5.9937442897488635\n",
      "---------------------\n",
      "Iteration Number: 4180\n",
      "Loss: 39.787776455531706\n",
      "l2 norm of gradients: 0.2960151101568337\n",
      "l2 norm of weights: 5.993648070841946\n",
      "---------------------\n",
      "Iteration Number: 4181\n",
      "Loss: 39.78338973179478\n",
      "l2 norm of gradients: 0.2959589779658167\n",
      "l2 norm of weights: 5.993551863375619\n",
      "---------------------\n",
      "Iteration Number: 4182\n",
      "Loss: 39.77900465755927\n",
      "l2 norm of gradients: 0.29590287778052954\n",
      "l2 norm of weights: 5.993455667345625\n",
      "---------------------\n",
      "Iteration Number: 4183\n",
      "Loss: 39.77462123157871\n",
      "l2 norm of gradients: 0.29584680957395193\n",
      "l2 norm of weights: 5.9933594827477075\n",
      "---------------------\n",
      "Iteration Number: 4184\n",
      "Loss: 39.770239452638826\n",
      "l2 norm of gradients: 0.29579077331908654\n",
      "l2 norm of weights: 5.993263309577617\n",
      "---------------------\n",
      "Iteration Number: 4185\n",
      "Loss: 39.765859319490104\n",
      "l2 norm of gradients: 0.2957347689889591\n",
      "l2 norm of weights: 5.993167147831105\n",
      "---------------------\n",
      "Iteration Number: 4186\n",
      "Loss: 39.76148083091552\n",
      "l2 norm of gradients: 0.29567879655661805\n",
      "l2 norm of weights: 5.99307099750393\n",
      "---------------------\n",
      "Iteration Number: 4187\n",
      "Loss: 39.75710398566589\n",
      "l2 norm of gradients: 0.2956228559951348\n",
      "l2 norm of weights: 5.992974858591849\n",
      "---------------------\n",
      "Iteration Number: 4188\n",
      "Loss: 39.75272878253705\n",
      "l2 norm of gradients: 0.2955669472776034\n",
      "l2 norm of weights: 5.99287873109063\n",
      "---------------------\n",
      "Iteration Number: 4189\n",
      "Loss: 39.74835522027297\n",
      "l2 norm of gradients: 0.295511070377141\n",
      "l2 norm of weights: 5.992782614996039\n",
      "---------------------\n",
      "Iteration Number: 4190\n",
      "Loss: 39.743983297664016\n",
      "l2 norm of gradients: 0.2954552252668875\n",
      "l2 norm of weights: 5.992686510303846\n",
      "---------------------\n",
      "Iteration Number: 4191\n",
      "Loss: 39.73961301348467\n",
      "l2 norm of gradients: 0.29539941192000546\n",
      "l2 norm of weights: 5.992590417009828\n",
      "---------------------\n",
      "Iteration Number: 4192\n",
      "Loss: 39.73524436650358\n",
      "l2 norm of gradients: 0.2953436303096803\n",
      "l2 norm of weights: 5.992494335109765\n",
      "---------------------\n",
      "Iteration Number: 4193\n",
      "Loss: 39.73087735551514\n",
      "l2 norm of gradients: 0.2952878804091204\n",
      "l2 norm of weights: 5.992398264599436\n",
      "---------------------\n",
      "Iteration Number: 4194\n",
      "Loss: 39.72651197926486\n",
      "l2 norm of gradients: 0.2952321621915566\n",
      "l2 norm of weights: 5.9923022054746315\n",
      "---------------------\n",
      "Iteration Number: 4195\n",
      "Loss: 39.722148236562276\n",
      "l2 norm of gradients: 0.29517647563024296\n",
      "l2 norm of weights: 5.992206157731138\n",
      "---------------------\n",
      "Iteration Number: 4196\n",
      "Loss: 39.71778612616595\n",
      "l2 norm of gradients: 0.2951208206984559\n",
      "l2 norm of weights: 5.9921101213647505\n",
      "---------------------\n",
      "Iteration Number: 4197\n",
      "Loss: 39.7134256468823\n",
      "l2 norm of gradients: 0.29506519736949466\n",
      "l2 norm of weights: 5.992014096371267\n",
      "---------------------\n",
      "Iteration Number: 4198\n",
      "Loss: 39.709066797476694\n",
      "l2 norm of gradients: 0.2950096056166814\n",
      "l2 norm of weights: 5.991918082746489\n",
      "---------------------\n",
      "Iteration Number: 4199\n",
      "Loss: 39.70470957673857\n",
      "l2 norm of gradients: 0.29495404541336073\n",
      "l2 norm of weights: 5.9918220804862194\n",
      "---------------------\n",
      "Iteration Number: 4200\n",
      "Loss: 39.70035398345197\n",
      "l2 norm of gradients: 0.29489851673290024\n",
      "l2 norm of weights: 5.99172608958627\n",
      "---------------------\n",
      "Iteration Number: 4201\n",
      "Loss: 39.6960000164149\n",
      "l2 norm of gradients: 0.29484301954869\n",
      "l2 norm of weights: 5.991630110042449\n",
      "---------------------\n",
      "Iteration Number: 4202\n",
      "Loss: 39.69164767441113\n",
      "l2 norm of gradients: 0.294787553834143\n",
      "l2 norm of weights: 5.9915341418505745\n",
      "---------------------\n",
      "Iteration Number: 4203\n",
      "Loss: 39.68729695621811\n",
      "l2 norm of gradients: 0.29473211956269457\n",
      "l2 norm of weights: 5.991438185006467\n",
      "---------------------\n",
      "Iteration Number: 4204\n",
      "Loss: 39.68294786063605\n",
      "l2 norm of gradients: 0.2946767167078031\n",
      "l2 norm of weights: 5.991342239505947\n",
      "---------------------\n",
      "Iteration Number: 4205\n",
      "Loss: 39.67860038645752\n",
      "l2 norm of gradients: 0.29462134524294936\n",
      "l2 norm of weights: 5.991246305344842\n",
      "---------------------\n",
      "Iteration Number: 4206\n",
      "Loss: 39.67425453248938\n",
      "l2 norm of gradients: 0.2945660051416368\n",
      "l2 norm of weights: 5.991150382518986\n",
      "---------------------\n",
      "Iteration Number: 4207\n",
      "Loss: 39.669910297493836\n",
      "l2 norm of gradients: 0.2945106963773915\n",
      "l2 norm of weights: 5.991054471024208\n",
      "---------------------\n",
      "Iteration Number: 4208\n",
      "Loss: 39.66556768029357\n",
      "l2 norm of gradients: 0.29445541892376226\n",
      "l2 norm of weights: 5.990958570856349\n",
      "---------------------\n",
      "Iteration Number: 4209\n",
      "Loss: 39.661226679684184\n",
      "l2 norm of gradients: 0.29440017275432045\n",
      "l2 norm of weights: 5.990862682011248\n",
      "---------------------\n",
      "Iteration Number: 4210\n",
      "Loss: 39.656887294457675\n",
      "l2 norm of gradients: 0.2943449578426599\n",
      "l2 norm of weights: 5.990766804484753\n",
      "---------------------\n",
      "Iteration Number: 4211\n",
      "Loss: 39.652549523421314\n",
      "l2 norm of gradients: 0.2942897741623972\n",
      "l2 norm of weights: 5.990670938272712\n",
      "---------------------\n",
      "Iteration Number: 4212\n",
      "Loss: 39.648213365352774\n",
      "l2 norm of gradients: 0.2942346216871714\n",
      "l2 norm of weights: 5.9905750833709766\n",
      "---------------------\n",
      "Iteration Number: 4213\n",
      "Loss: 39.64387881908733\n",
      "l2 norm of gradients: 0.2941795003906442\n",
      "l2 norm of weights: 5.990479239775402\n",
      "---------------------\n",
      "Iteration Number: 4214\n",
      "Loss: 39.63954588340726\n",
      "l2 norm of gradients: 0.29412441024649966\n",
      "l2 norm of weights: 5.990383407481849\n",
      "---------------------\n",
      "Iteration Number: 4215\n",
      "Loss: 39.63521455712184\n",
      "l2 norm of gradients: 0.29406935122844463\n",
      "l2 norm of weights: 5.99028758648618\n",
      "---------------------\n",
      "Iteration Number: 4216\n",
      "Loss: 39.63088483904833\n",
      "l2 norm of gradients: 0.2940143233102083\n",
      "l2 norm of weights: 5.990191776784262\n",
      "---------------------\n",
      "Iteration Number: 4217\n",
      "Loss: 39.6265567279724\n",
      "l2 norm of gradients: 0.29395932646554246\n",
      "l2 norm of weights: 5.990095978371967\n",
      "---------------------\n",
      "Iteration Number: 4218\n",
      "Loss: 39.622230222721655\n",
      "l2 norm of gradients: 0.29390436066822123\n",
      "l2 norm of weights: 5.990000191245166\n",
      "---------------------\n",
      "Iteration Number: 4219\n",
      "Loss: 39.61790532210023\n",
      "l2 norm of gradients: 0.29384942589204144\n",
      "l2 norm of weights: 5.989904415399739\n",
      "---------------------\n",
      "Iteration Number: 4220\n",
      "Loss: 39.61358202491971\n",
      "l2 norm of gradients: 0.2937945221108223\n",
      "l2 norm of weights: 5.989808650831566\n",
      "---------------------\n",
      "Iteration Number: 4221\n",
      "Loss: 39.60926032999402\n",
      "l2 norm of gradients: 0.2937396492984055\n",
      "l2 norm of weights: 5.989712897536533\n",
      "---------------------\n",
      "Iteration Number: 4222\n",
      "Loss: 39.60494023613044\n",
      "l2 norm of gradients: 0.29368480742865516\n",
      "l2 norm of weights: 5.989617155510527\n",
      "---------------------\n",
      "Iteration Number: 4223\n",
      "Loss: 39.600621742145506\n",
      "l2 norm of gradients: 0.29362999647545773\n",
      "l2 norm of weights: 5.989521424749441\n",
      "---------------------\n",
      "Iteration Number: 4224\n",
      "Loss: 39.5963048468595\n",
      "l2 norm of gradients: 0.29357521641272233\n",
      "l2 norm of weights: 5.98942570524917\n",
      "---------------------\n",
      "Iteration Number: 4225\n",
      "Loss: 39.59198954909282\n",
      "l2 norm of gradients: 0.2935204672143802\n",
      "l2 norm of weights: 5.989329997005613\n",
      "---------------------\n",
      "Iteration Number: 4226\n",
      "Loss: 39.58767584764586\n",
      "l2 norm of gradients: 0.2934657488543853\n",
      "l2 norm of weights: 5.9892343000146715\n",
      "---------------------\n",
      "Iteration Number: 4227\n",
      "Loss: 39.58336374136344\n",
      "l2 norm of gradients: 0.2934110613067136\n",
      "l2 norm of weights: 5.989138614272255\n",
      "---------------------\n",
      "Iteration Number: 4228\n",
      "Loss: 39.57905322905406\n",
      "l2 norm of gradients: 0.2933564045453639\n",
      "l2 norm of weights: 5.98904293977427\n",
      "---------------------\n",
      "Iteration Number: 4229\n",
      "Loss: 39.57474430954011\n",
      "l2 norm of gradients: 0.293301778544357\n",
      "l2 norm of weights: 5.988947276516632\n",
      "---------------------\n",
      "Iteration Number: 4230\n",
      "Loss: 39.57043698164317\n",
      "l2 norm of gradients: 0.29324718327773613\n",
      "l2 norm of weights: 5.988851624495257\n",
      "---------------------\n",
      "Iteration Number: 4231\n",
      "Loss: 39.566131244190665\n",
      "l2 norm of gradients: 0.29319261871956687\n",
      "l2 norm of weights: 5.988755983706065\n",
      "---------------------\n",
      "Iteration Number: 4232\n",
      "Loss: 39.56182709600715\n",
      "l2 norm of gradients: 0.2931380848439372\n",
      "l2 norm of weights: 5.988660354144981\n",
      "---------------------\n",
      "Iteration Number: 4233\n",
      "Loss: 39.55752453593414\n",
      "l2 norm of gradients: 0.2930835816249574\n",
      "l2 norm of weights: 5.988564735807933\n",
      "---------------------\n",
      "Iteration Number: 4234\n",
      "Loss: 39.55322356276979\n",
      "l2 norm of gradients: 0.29302910903675994\n",
      "l2 norm of weights: 5.988469128690849\n",
      "---------------------\n",
      "Iteration Number: 4235\n",
      "Loss: 39.54892417537576\n",
      "l2 norm of gradients: 0.29297466705349956\n",
      "l2 norm of weights: 5.988373532789668\n",
      "---------------------\n",
      "Iteration Number: 4236\n",
      "Loss: 39.544626372563556\n",
      "l2 norm of gradients: 0.29292025564935364\n",
      "l2 norm of weights: 5.9882779481003245\n",
      "---------------------\n",
      "Iteration Number: 4237\n",
      "Loss: 39.540330153165165\n",
      "l2 norm of gradients: 0.29286587479852116\n",
      "l2 norm of weights: 5.988182374618762\n",
      "---------------------\n",
      "Iteration Number: 4238\n",
      "Loss: 39.53603551602132\n",
      "l2 norm of gradients: 0.29281152447522407\n",
      "l2 norm of weights: 5.988086812340924\n",
      "---------------------\n",
      "Iteration Number: 4239\n",
      "Loss: 39.53174245996592\n",
      "l2 norm of gradients: 0.292757204653706\n",
      "l2 norm of weights: 5.987991261262761\n",
      "---------------------\n",
      "Iteration Number: 4240\n",
      "Loss: 39.5274509838295\n",
      "l2 norm of gradients: 0.29270291530823306\n",
      "l2 norm of weights: 5.987895721380224\n",
      "---------------------\n",
      "Iteration Number: 4241\n",
      "Loss: 39.52316108645724\n",
      "l2 norm of gradients: 0.2926486564130937\n",
      "l2 norm of weights: 5.98780019268927\n",
      "---------------------\n",
      "Iteration Number: 4242\n",
      "Loss: 39.51887276667702\n",
      "l2 norm of gradients: 0.2925944279425981\n",
      "l2 norm of weights: 5.987704675185856\n",
      "---------------------\n",
      "Iteration Number: 4243\n",
      "Loss: 39.514586023332434\n",
      "l2 norm of gradients: 0.2925402298710791\n",
      "l2 norm of weights: 5.987609168865946\n",
      "---------------------\n",
      "Iteration Number: 4244\n",
      "Loss: 39.51030085527918\n",
      "l2 norm of gradients: 0.2924860621728915\n",
      "l2 norm of weights: 5.987513673725506\n",
      "---------------------\n",
      "Iteration Number: 4245\n",
      "Loss: 39.5060172613297\n",
      "l2 norm of gradients: 0.2924319248224123\n",
      "l2 norm of weights: 5.987418189760506\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 4246\n",
      "Loss: 39.5017352403531\n",
      "l2 norm of gradients: 0.2923778177940406\n",
      "l2 norm of weights: 5.9873227169669185\n",
      "---------------------\n",
      "Iteration Number: 4247\n",
      "Loss: 39.49745479117843\n",
      "l2 norm of gradients: 0.29232374106219766\n",
      "l2 norm of weights: 5.987227255340721\n",
      "---------------------\n",
      "Iteration Number: 4248\n",
      "Loss: 39.493175912647345\n",
      "l2 norm of gradients: 0.2922696946013269\n",
      "l2 norm of weights: 5.987131804877892\n",
      "---------------------\n",
      "Iteration Number: 4249\n",
      "Loss: 39.48889860362282\n",
      "l2 norm of gradients: 0.2922156783858938\n",
      "l2 norm of weights: 5.987036365574417\n",
      "---------------------\n",
      "Iteration Number: 4250\n",
      "Loss: 39.48462286293746\n",
      "l2 norm of gradients: 0.2921616923903859\n",
      "l2 norm of weights: 5.986940937426281\n",
      "---------------------\n",
      "Iteration Number: 4251\n",
      "Loss: 39.48034868945034\n",
      "l2 norm of gradients: 0.29210773658931305\n",
      "l2 norm of weights: 5.986845520429476\n",
      "---------------------\n",
      "Iteration Number: 4252\n",
      "Loss: 39.47607608202186\n",
      "l2 norm of gradients: 0.2920538109572067\n",
      "l2 norm of weights: 5.986750114579995\n",
      "---------------------\n",
      "Iteration Number: 4253\n",
      "Loss: 39.471805039469736\n",
      "l2 norm of gradients: 0.29199991546862086\n",
      "l2 norm of weights: 5.986654719873836\n",
      "---------------------\n",
      "Iteration Number: 4254\n",
      "Loss: 39.467535560676424\n",
      "l2 norm of gradients: 0.2919460500981313\n",
      "l2 norm of weights: 5.986559336307\n",
      "---------------------\n",
      "Iteration Number: 4255\n",
      "Loss: 39.46326764448977\n",
      "l2 norm of gradients: 0.29189221482033584\n",
      "l2 norm of weights: 5.98646396387549\n",
      "---------------------\n",
      "Iteration Number: 4256\n",
      "Loss: 39.45900128974377\n",
      "l2 norm of gradients: 0.29183840960985435\n",
      "l2 norm of weights: 5.986368602575315\n",
      "---------------------\n",
      "Iteration Number: 4257\n",
      "Loss: 39.45473649533507\n",
      "l2 norm of gradients: 0.2917846344413287\n",
      "l2 norm of weights: 5.986273252402485\n",
      "---------------------\n",
      "Iteration Number: 4258\n",
      "Loss: 39.45047326007713\n",
      "l2 norm of gradients: 0.2917308892894228\n",
      "l2 norm of weights: 5.986177913353016\n",
      "---------------------\n",
      "Iteration Number: 4259\n",
      "Loss: 39.44621158285916\n",
      "l2 norm of gradients: 0.2916771741288225\n",
      "l2 norm of weights: 5.986082585422925\n",
      "---------------------\n",
      "Iteration Number: 4260\n",
      "Loss: 39.44195146252869\n",
      "l2 norm of gradients: 0.29162348893423545\n",
      "l2 norm of weights: 5.985987268608234\n",
      "---------------------\n",
      "Iteration Number: 4261\n",
      "Loss: 39.43769289795153\n",
      "l2 norm of gradients: 0.29156983368039163\n",
      "l2 norm of weights: 5.985891962904967\n",
      "---------------------\n",
      "Iteration Number: 4262\n",
      "Loss: 39.433435887985716\n",
      "l2 norm of gradients: 0.2915162083420426\n",
      "l2 norm of weights: 5.9857966683091535\n",
      "---------------------\n",
      "Iteration Number: 4263\n",
      "Loss: 39.429180431497564\n",
      "l2 norm of gradients: 0.29146261289396197\n",
      "l2 norm of weights: 5.985701384816824\n",
      "---------------------\n",
      "Iteration Number: 4264\n",
      "Loss: 39.42492652734127\n",
      "l2 norm of gradients: 0.2914090473109452\n",
      "l2 norm of weights: 5.985606112424015\n",
      "---------------------\n",
      "Iteration Number: 4265\n",
      "Loss: 39.420674174392914\n",
      "l2 norm of gradients: 0.2913555115678097\n",
      "l2 norm of weights: 5.985510851126762\n",
      "---------------------\n",
      "Iteration Number: 4266\n",
      "Loss: 39.416423371521155\n",
      "l2 norm of gradients: 0.29130200563939485\n",
      "l2 norm of weights: 5.98541560092111\n",
      "---------------------\n",
      "Iteration Number: 4267\n",
      "Loss: 39.41217411759276\n",
      "l2 norm of gradients: 0.2912485295005616\n",
      "l2 norm of weights: 5.985320361803103\n",
      "---------------------\n",
      "Iteration Number: 4268\n",
      "Loss: 39.40792641146887\n",
      "l2 norm of gradients: 0.2911950831261933\n",
      "l2 norm of weights: 5.9852251337687905\n",
      "---------------------\n",
      "Iteration Number: 4269\n",
      "Loss: 39.4036802520274\n",
      "l2 norm of gradients: 0.2911416664911945\n",
      "l2 norm of weights: 5.985129916814223\n",
      "---------------------\n",
      "Iteration Number: 4270\n",
      "Loss: 39.39943563813688\n",
      "l2 norm of gradients: 0.29108827957049194\n",
      "l2 norm of weights: 5.985034710935458\n",
      "---------------------\n",
      "Iteration Number: 4271\n",
      "Loss: 39.39519256868119\n",
      "l2 norm of gradients: 0.29103492233903416\n",
      "l2 norm of weights: 5.984939516128552\n",
      "---------------------\n",
      "Iteration Number: 4272\n",
      "Loss: 39.39095104250608\n",
      "l2 norm of gradients: 0.2909815947717914\n",
      "l2 norm of weights: 5.98484433238957\n",
      "---------------------\n",
      "Iteration Number: 4273\n",
      "Loss: 39.386711058506535\n",
      "l2 norm of gradients: 0.2909282968437559\n",
      "l2 norm of weights: 5.984749159714575\n",
      "---------------------\n",
      "Iteration Number: 4274\n",
      "Loss: 39.3824726155607\n",
      "l2 norm of gradients: 0.29087502852994124\n",
      "l2 norm of weights: 5.984653998099637\n",
      "---------------------\n",
      "Iteration Number: 4275\n",
      "Loss: 39.37823571253381\n",
      "l2 norm of gradients: 0.29082178980538326\n",
      "l2 norm of weights: 5.9845588475408285\n",
      "---------------------\n",
      "Iteration Number: 4276\n",
      "Loss: 39.37400034832458\n",
      "l2 norm of gradients: 0.2907685806451391\n",
      "l2 norm of weights: 5.984463708034224\n",
      "---------------------\n",
      "Iteration Number: 4277\n",
      "Loss: 39.36976652179687\n",
      "l2 norm of gradients: 0.29071540102428806\n",
      "l2 norm of weights: 5.984368579575904\n",
      "---------------------\n",
      "Iteration Number: 4278\n",
      "Loss: 39.36553423182798\n",
      "l2 norm of gradients: 0.2906622509179309\n",
      "l2 norm of weights: 5.9842734621619496\n",
      "---------------------\n",
      "Iteration Number: 4279\n",
      "Loss: 39.361303477307224\n",
      "l2 norm of gradients: 0.29060913030119\n",
      "l2 norm of weights: 5.9841783557884485\n",
      "---------------------\n",
      "Iteration Number: 4280\n",
      "Loss: 39.35707425711761\n",
      "l2 norm of gradients: 0.29055603914920963\n",
      "l2 norm of weights: 5.984083260451487\n",
      "---------------------\n",
      "Iteration Number: 4281\n",
      "Loss: 39.352846570143\n",
      "l2 norm of gradients: 0.29050297743715575\n",
      "l2 norm of weights: 5.983988176147159\n",
      "---------------------\n",
      "Iteration Number: 4282\n",
      "Loss: 39.34862041527289\n",
      "l2 norm of gradients: 0.2904499451402157\n",
      "l2 norm of weights: 5.98389310287156\n",
      "---------------------\n",
      "Iteration Number: 4283\n",
      "Loss: 39.34439579138006\n",
      "l2 norm of gradients: 0.29039694223359885\n",
      "l2 norm of weights: 5.98379804062079\n",
      "---------------------\n",
      "Iteration Number: 4284\n",
      "Loss: 39.340172697369255\n",
      "l2 norm of gradients: 0.290343968692536\n",
      "l2 norm of weights: 5.983702989390951\n",
      "---------------------\n",
      "Iteration Number: 4285\n",
      "Loss: 39.335951132120954\n",
      "l2 norm of gradients: 0.29029102449227956\n",
      "l2 norm of weights: 5.983607949178147\n",
      "---------------------\n",
      "Iteration Number: 4286\n",
      "Loss: 39.33173109452099\n",
      "l2 norm of gradients: 0.29023810960810353\n",
      "l2 norm of weights: 5.983512919978488\n",
      "---------------------\n",
      "Iteration Number: 4287\n",
      "Loss: 39.32751258347011\n",
      "l2 norm of gradients: 0.2901852240153036\n",
      "l2 norm of weights: 5.983417901788087\n",
      "---------------------\n",
      "Iteration Number: 4288\n",
      "Loss: 39.323295597854134\n",
      "l2 norm of gradients: 0.29013236768919715\n",
      "l2 norm of weights: 5.983322894603059\n",
      "---------------------\n",
      "Iteration Number: 4289\n",
      "Loss: 39.319080136569184\n",
      "l2 norm of gradients: 0.29007954060512275\n",
      "l2 norm of weights: 5.983227898419524\n",
      "---------------------\n",
      "Iteration Number: 4290\n",
      "Loss: 39.314866198512796\n",
      "l2 norm of gradients: 0.2900267427384408\n",
      "l2 norm of weights: 5.983132913233602\n",
      "---------------------\n",
      "Iteration Number: 4291\n",
      "Loss: 39.31065378257272\n",
      "l2 norm of gradients: 0.2899739740645333\n",
      "l2 norm of weights: 5.983037939041421\n",
      "---------------------\n",
      "Iteration Number: 4292\n",
      "Loss: 39.30644288765192\n",
      "l2 norm of gradients: 0.28992123455880353\n",
      "l2 norm of weights: 5.982942975839109\n",
      "---------------------\n",
      "Iteration Number: 4293\n",
      "Loss: 39.302233512641415\n",
      "l2 norm of gradients: 0.2898685241966765\n",
      "l2 norm of weights: 5.982848023622797\n",
      "---------------------\n",
      "Iteration Number: 4294\n",
      "Loss: 39.298025656453746\n",
      "l2 norm of gradients: 0.28981584295359863\n",
      "l2 norm of weights: 5.982753082388622\n",
      "---------------------\n",
      "Iteration Number: 4295\n",
      "Loss: 39.29381931797436\n",
      "l2 norm of gradients: 0.2897631908050378\n",
      "l2 norm of weights: 5.982658152132722\n",
      "---------------------\n",
      "Iteration Number: 4296\n",
      "Loss: 39.2896144961227\n",
      "l2 norm of gradients: 0.28971056772648335\n",
      "l2 norm of weights: 5.9825632328512395\n",
      "---------------------\n",
      "Iteration Number: 4297\n",
      "Loss: 39.28541118977467\n",
      "l2 norm of gradients: 0.2896579736934462\n",
      "l2 norm of weights: 5.982468324540318\n",
      "---------------------\n",
      "Iteration Number: 4298\n",
      "Loss: 39.281209397861026\n",
      "l2 norm of gradients: 0.28960540868145845\n",
      "l2 norm of weights: 5.982373427196109\n",
      "---------------------\n",
      "Iteration Number: 4299\n",
      "Loss: 39.27700911926698\n",
      "l2 norm of gradients: 0.289552872666074\n",
      "l2 norm of weights: 5.982278540814761\n",
      "---------------------\n",
      "Iteration Number: 4300\n",
      "Loss: 39.27281035291101\n",
      "l2 norm of gradients: 0.28950036562286774\n",
      "l2 norm of weights: 5.982183665392431\n",
      "---------------------\n",
      "Iteration Number: 4301\n",
      "Loss: 39.26861309769621\n",
      "l2 norm of gradients: 0.28944788752743633\n",
      "l2 norm of weights: 5.982088800925277\n",
      "---------------------\n",
      "Iteration Number: 4302\n",
      "Loss: 39.26441735253346\n",
      "l2 norm of gradients: 0.28939543835539766\n",
      "l2 norm of weights: 5.981993947409461\n",
      "---------------------\n",
      "Iteration Number: 4303\n",
      "Loss: 39.26022311631851\n",
      "l2 norm of gradients: 0.2893430180823909\n",
      "l2 norm of weights: 5.981899104841148\n",
      "---------------------\n",
      "Iteration Number: 4304\n",
      "Loss: 39.25603038798537\n",
      "l2 norm of gradients: 0.2892906266840767\n",
      "l2 norm of weights: 5.981804273216505\n",
      "---------------------\n",
      "Iteration Number: 4305\n",
      "Loss: 39.251839166427935\n",
      "l2 norm of gradients: 0.28923826413613707\n",
      "l2 norm of weights: 5.981709452531703\n",
      "---------------------\n",
      "Iteration Number: 4306\n",
      "Loss: 39.24764945056683\n",
      "l2 norm of gradients: 0.28918593041427526\n",
      "l2 norm of weights: 5.981614642782918\n",
      "---------------------\n",
      "Iteration Number: 4307\n",
      "Loss: 39.24346123930412\n",
      "l2 norm of gradients: 0.28913362549421584\n",
      "l2 norm of weights: 5.9815198439663275\n",
      "---------------------\n",
      "Iteration Number: 4308\n",
      "Loss: 39.23927453156141\n",
      "l2 norm of gradients: 0.28908134935170465\n",
      "l2 norm of weights: 5.981425056078112\n",
      "---------------------\n",
      "Iteration Number: 4309\n",
      "Loss: 39.235089326265474\n",
      "l2 norm of gradients: 0.28902910196250914\n",
      "l2 norm of weights: 5.981330279114455\n",
      "---------------------\n",
      "Iteration Number: 4310\n",
      "Loss: 39.23090562232756\n",
      "l2 norm of gradients: 0.28897688330241755\n",
      "l2 norm of weights: 5.981235513071547\n",
      "---------------------\n",
      "Iteration Number: 4311\n",
      "Loss: 39.226723418650124\n",
      "l2 norm of gradients: 0.2889246933472396\n",
      "l2 norm of weights: 5.981140757945575\n",
      "---------------------\n",
      "Iteration Number: 4312\n",
      "Loss: 39.22254271417681\n",
      "l2 norm of gradients: 0.2888725320728063\n",
      "l2 norm of weights: 5.981046013732736\n",
      "---------------------\n",
      "Iteration Number: 4313\n",
      "Loss: 39.218363507817635\n",
      "l2 norm of gradients: 0.28882039945496973\n",
      "l2 norm of weights: 5.980951280429224\n",
      "---------------------\n",
      "Iteration Number: 4314\n",
      "Loss: 39.21418579849619\n",
      "l2 norm of gradients: 0.2887682954696034\n",
      "l2 norm of weights: 5.980856558031241\n",
      "---------------------\n",
      "Iteration Number: 4315\n",
      "Loss: 39.21000958511916\n",
      "l2 norm of gradients: 0.288716220092602\n",
      "l2 norm of weights: 5.980761846534992\n",
      "---------------------\n",
      "Iteration Number: 4316\n",
      "Loss: 39.20583486663326\n",
      "l2 norm of gradients: 0.2886641732998811\n",
      "l2 norm of weights: 5.980667145936682\n",
      "---------------------\n",
      "Iteration Number: 4317\n",
      "Loss: 39.20166164196079\n",
      "l2 norm of gradients: 0.2886121550673778\n",
      "l2 norm of weights: 5.98057245623252\n",
      "---------------------\n",
      "Iteration Number: 4318\n",
      "Loss: 39.19748991001525\n",
      "l2 norm of gradients: 0.2885601653710502\n",
      "l2 norm of weights: 5.980477777418721\n",
      "---------------------\n",
      "Iteration Number: 4319\n",
      "Loss: 39.19331966972088\n",
      "l2 norm of gradients: 0.2885082041868775\n",
      "l2 norm of weights: 5.980383109491501\n",
      "---------------------\n",
      "Iteration Number: 4320\n",
      "Loss: 39.18915092003302\n",
      "l2 norm of gradients: 0.2884562714908601\n",
      "l2 norm of weights: 5.980288452447079\n",
      "---------------------\n",
      "Iteration Number: 4321\n",
      "Loss: 39.18498365985564\n",
      "l2 norm of gradients: 0.2884043672590195\n",
      "l2 norm of weights: 5.980193806281678\n",
      "---------------------\n",
      "Iteration Number: 4322\n",
      "Loss: 39.18081788811952\n",
      "l2 norm of gradients: 0.28835249146739833\n",
      "l2 norm of weights: 5.980099170991523\n",
      "---------------------\n",
      "Iteration Number: 4323\n",
      "Loss: 39.17665360377598\n",
      "l2 norm of gradients: 0.2883006440920603\n",
      "l2 norm of weights: 5.980004546572845\n",
      "---------------------\n",
      "Iteration Number: 4324\n",
      "Loss: 39.172490805734554\n",
      "l2 norm of gradients: 0.2882488251090901\n",
      "l2 norm of weights: 5.979909933021874\n",
      "---------------------\n",
      "Iteration Number: 4325\n",
      "Loss: 39.168329492949105\n",
      "l2 norm of gradients: 0.2881970344945936\n",
      "l2 norm of weights: 5.979815330334847\n",
      "---------------------\n",
      "Iteration Number: 4326\n",
      "Loss: 39.16416966433796\n",
      "l2 norm of gradients: 0.28814527222469766\n",
      "l2 norm of weights: 5.979720738508002\n",
      "---------------------\n",
      "Iteration Number: 4327\n",
      "Loss: 39.16001131884972\n",
      "l2 norm of gradients: 0.28809353827555007\n",
      "l2 norm of weights: 5.979626157537581\n",
      "---------------------\n",
      "Iteration Number: 4328\n",
      "Loss: 39.155854455411486\n",
      "l2 norm of gradients: 0.28804183262331984\n",
      "l2 norm of weights: 5.979531587419827\n",
      "---------------------\n",
      "Iteration Number: 4329\n",
      "Loss: 39.151699072963815\n",
      "l2 norm of gradients: 0.28799015524419685\n",
      "l2 norm of weights: 5.979437028150991\n",
      "---------------------\n",
      "Iteration Number: 4330\n",
      "Loss: 39.14754517046263\n",
      "l2 norm of gradients: 0.2879385061143919\n",
      "l2 norm of weights: 5.979342479727322\n",
      "---------------------\n",
      "Iteration Number: 4331\n",
      "Loss: 39.14339274682554\n",
      "l2 norm of gradients: 0.28788688521013694\n",
      "l2 norm of weights: 5.979247942145076\n",
      "---------------------\n",
      "Iteration Number: 4332\n",
      "Loss: 39.139241801000466\n",
      "l2 norm of gradients: 0.28783529250768475\n",
      "l2 norm of weights: 5.979153415400508\n",
      "---------------------\n",
      "Iteration Number: 4333\n",
      "Loss: 39.135092331926415\n",
      "l2 norm of gradients: 0.28778372798330903\n",
      "l2 norm of weights: 5.979058899489881\n",
      "---------------------\n",
      "Iteration Number: 4334\n",
      "Loss: 39.13094433855933\n",
      "l2 norm of gradients: 0.2877321916133045\n",
      "l2 norm of weights: 5.9789643944094575\n",
      "---------------------\n",
      "Iteration Number: 4335\n",
      "Loss: 39.126797819836746\n",
      "l2 norm of gradients: 0.2876806833739867\n",
      "l2 norm of weights: 5.978869900155504\n",
      "---------------------\n",
      "Iteration Number: 4336\n",
      "Loss: 39.12265277470718\n",
      "l2 norm of gradients: 0.2876292032416922\n",
      "l2 norm of weights: 5.978775416724292\n",
      "---------------------\n",
      "Iteration Number: 4337\n",
      "Loss: 39.118509202113906\n",
      "l2 norm of gradients: 0.2875777511927784\n",
      "l2 norm of weights: 5.978680944112093\n",
      "---------------------\n",
      "Iteration Number: 4338\n",
      "Loss: 39.114367101012135\n",
      "l2 norm of gradients: 0.2875263272036234\n",
      "l2 norm of weights: 5.978586482315184\n",
      "---------------------\n",
      "Iteration Number: 4339\n",
      "Loss: 39.11022647033143\n",
      "l2 norm of gradients: 0.28747493125062645\n",
      "l2 norm of weights: 5.9784920313298455\n",
      "---------------------\n",
      "Iteration Number: 4340\n",
      "Loss: 39.106087309042756\n",
      "l2 norm of gradients: 0.2874235633102073\n",
      "l2 norm of weights: 5.978397591152357\n",
      "---------------------\n",
      "Iteration Number: 4341\n",
      "Loss: 39.10194961609161\n",
      "l2 norm of gradients: 0.28737222335880686\n",
      "l2 norm of weights: 5.978303161779007\n",
      "---------------------\n",
      "Iteration Number: 4342\n",
      "Loss: 39.097813390429586\n",
      "l2 norm of gradients: 0.2873209113728867\n",
      "l2 norm of weights: 5.978208743206081\n",
      "---------------------\n",
      "Iteration Number: 4343\n",
      "Loss: 39.09367863101018\n",
      "l2 norm of gradients: 0.2872696273289291\n",
      "l2 norm of weights: 5.978114335429874\n",
      "---------------------\n",
      "Iteration Number: 4344\n",
      "Loss: 39.0895453367791\n",
      "l2 norm of gradients: 0.2872183712034373\n",
      "l2 norm of weights: 5.978019938446679\n",
      "---------------------\n",
      "Iteration Number: 4345\n",
      "Loss: 39.08541350670992\n",
      "l2 norm of gradients: 0.2871671429729351\n",
      "l2 norm of weights: 5.9779255522527945\n",
      "---------------------\n",
      "Iteration Number: 4346\n",
      "Loss: 39.081283139740876\n",
      "l2 norm of gradients: 0.2871159426139673\n",
      "l2 norm of weights: 5.977831176844521\n",
      "---------------------\n",
      "Iteration Number: 4347\n",
      "Loss: 39.077154234836144\n",
      "l2 norm of gradients: 0.2870647701030992\n",
      "l2 norm of weights: 5.977736812218164\n",
      "---------------------\n",
      "Iteration Number: 4348\n",
      "Loss: 39.07302679096314\n",
      "l2 norm of gradients: 0.28701362541691705\n",
      "l2 norm of weights: 5.977642458370029\n",
      "---------------------\n",
      "Iteration Number: 4349\n",
      "Loss: 39.0689008070777\n",
      "l2 norm of gradients: 0.2869625085320276\n",
      "l2 norm of weights: 5.977548115296427\n",
      "---------------------\n",
      "Iteration Number: 4350\n",
      "Loss: 39.06477628212281\n",
      "l2 norm of gradients: 0.2869114194250584\n",
      "l2 norm of weights: 5.9774537829936705\n",
      "---------------------\n",
      "Iteration Number: 4351\n",
      "Loss: 39.0606532150816\n",
      "l2 norm of gradients: 0.2868603580726577\n",
      "l2 norm of weights: 5.977359461458078\n",
      "---------------------\n",
      "Iteration Number: 4352\n",
      "Loss: 39.05653160491522\n",
      "l2 norm of gradients: 0.2868093244514944\n",
      "l2 norm of weights: 5.977265150685967\n",
      "---------------------\n",
      "Iteration Number: 4353\n",
      "Loss: 39.05241145057449\n",
      "l2 norm of gradients: 0.28675831853825795\n",
      "l2 norm of weights: 5.9771708506736605\n",
      "---------------------\n",
      "Iteration Number: 4354\n",
      "Loss: 39.048292751043014\n",
      "l2 norm of gradients: 0.28670734030965855\n",
      "l2 norm of weights: 5.977076561417485\n",
      "---------------------\n",
      "Iteration Number: 4355\n",
      "Loss: 39.04417550526603\n",
      "l2 norm of gradients: 0.286656389742427\n",
      "l2 norm of weights: 5.976982282913768\n",
      "---------------------\n",
      "Iteration Number: 4356\n",
      "Loss: 39.04005971223065\n",
      "l2 norm of gradients: 0.28660546681331467\n",
      "l2 norm of weights: 5.976888015158843\n",
      "---------------------\n",
      "Iteration Number: 4357\n",
      "Loss: 39.03594537088175\n",
      "l2 norm of gradients: 0.2865545714990936\n",
      "l2 norm of weights: 5.976793758149042\n",
      "---------------------\n",
      "Iteration Number: 4358\n",
      "Loss: 39.031832480223464\n",
      "l2 norm of gradients: 0.2865037037765564\n",
      "l2 norm of weights: 5.976699511880704\n",
      "---------------------\n",
      "Iteration Number: 4359\n",
      "Loss: 39.02772103919531\n",
      "l2 norm of gradients: 0.286452863622516\n",
      "l2 norm of weights: 5.9766052763501705\n",
      "---------------------\n",
      "Iteration Number: 4360\n",
      "Loss: 39.02361104677152\n",
      "l2 norm of gradients: 0.28640205101380634\n",
      "l2 norm of weights: 5.9765110515537865\n",
      "---------------------\n",
      "Iteration Number: 4361\n",
      "Loss: 39.019502501940096\n",
      "l2 norm of gradients: 0.2863512659272814\n",
      "l2 norm of weights: 5.976416837487895\n",
      "---------------------\n",
      "Iteration Number: 4362\n",
      "Loss: 39.0153954036676\n",
      "l2 norm of gradients: 0.286300508339816\n",
      "l2 norm of weights: 5.976322634148849\n",
      "---------------------\n",
      "Iteration Number: 4363\n",
      "Loss: 39.01128975092077\n",
      "l2 norm of gradients: 0.2862497782283054\n",
      "l2 norm of weights: 5.976228441533002\n",
      "---------------------\n",
      "Iteration Number: 4364\n",
      "Loss: 39.00718554268656\n",
      "l2 norm of gradients: 0.2861990755696653\n",
      "l2 norm of weights: 5.976134259636708\n",
      "---------------------\n",
      "Iteration Number: 4365\n",
      "Loss: 39.00308277793341\n",
      "l2 norm of gradients: 0.2861484003408319\n",
      "l2 norm of weights: 5.976040088456327\n",
      "---------------------\n",
      "Iteration Number: 4366\n",
      "Loss: 38.99898145564474\n",
      "l2 norm of gradients: 0.2860977525187618\n",
      "l2 norm of weights: 5.975945927988221\n",
      "---------------------\n",
      "Iteration Number: 4367\n",
      "Loss: 38.99488157479693\n",
      "l2 norm of gradients: 0.2860471320804322\n",
      "l2 norm of weights: 5.975851778228754\n",
      "---------------------\n",
      "Iteration Number: 4368\n",
      "Loss: 38.99078313437057\n",
      "l2 norm of gradients: 0.2859965390028406\n",
      "l2 norm of weights: 5.975757639174296\n",
      "---------------------\n",
      "Iteration Number: 4369\n",
      "Loss: 38.98668613334502\n",
      "l2 norm of gradients: 0.2859459732630049\n",
      "l2 norm of weights: 5.975663510821216\n",
      "---------------------\n",
      "Iteration Number: 4370\n",
      "Loss: 38.982590570707366\n",
      "l2 norm of gradients: 0.28589543483796354\n",
      "l2 norm of weights: 5.97556939316589\n",
      "---------------------\n",
      "Iteration Number: 4371\n",
      "Loss: 38.978496445428064\n",
      "l2 norm of gradients: 0.2858449237047752\n",
      "l2 norm of weights: 5.975475286204694\n",
      "---------------------\n",
      "Iteration Number: 4372\n",
      "Loss: 38.974403756504785\n",
      "l2 norm of gradients: 0.2857944398405191\n",
      "l2 norm of weights: 5.975381189934008\n",
      "---------------------\n",
      "Iteration Number: 4373\n",
      "Loss: 38.97031250292427\n",
      "l2 norm of gradients: 0.28574398322229455\n",
      "l2 norm of weights: 5.975287104350215\n",
      "---------------------\n",
      "Iteration Number: 4374\n",
      "Loss: 38.96622268365353\n",
      "l2 norm of gradients: 0.28569355382722145\n",
      "l2 norm of weights: 5.975193029449701\n",
      "---------------------\n",
      "Iteration Number: 4375\n",
      "Loss: 38.96213429769009\n",
      "l2 norm of gradients: 0.2856431516324398\n",
      "l2 norm of weights: 5.975098965228856\n",
      "---------------------\n",
      "Iteration Number: 4376\n",
      "Loss: 38.9580473440309\n",
      "l2 norm of gradients: 0.2855927766151103\n",
      "l2 norm of weights: 5.975004911684071\n",
      "---------------------\n",
      "Iteration Number: 4377\n",
      "Loss: 38.953961821650466\n",
      "l2 norm of gradients: 0.28554242875241354\n",
      "l2 norm of weights: 5.974910868811741\n",
      "---------------------\n",
      "Iteration Number: 4378\n",
      "Loss: 38.949877729542685\n",
      "l2 norm of gradients: 0.2854921080215505\n",
      "l2 norm of weights: 5.974816836608264\n",
      "---------------------\n",
      "Iteration Number: 4379\n",
      "Loss: 38.94579506671336\n",
      "l2 norm of gradients: 0.2854418143997427\n",
      "l2 norm of weights: 5.9747228150700415\n",
      "---------------------\n",
      "Iteration Number: 4380\n",
      "Loss: 38.94171383214376\n",
      "l2 norm of gradients: 0.28539154786423143\n",
      "l2 norm of weights: 5.974628804193476\n",
      "---------------------\n",
      "Iteration Number: 4381\n",
      "Loss: 38.93763402481685\n",
      "l2 norm of gradients: 0.2853413083922788\n",
      "l2 norm of weights: 5.974534803974976\n",
      "---------------------\n",
      "Iteration Number: 4382\n",
      "Loss: 38.933555643748036\n",
      "l2 norm of gradients: 0.2852910959611664\n",
      "l2 norm of weights: 5.9744408144109515\n",
      "---------------------\n",
      "Iteration Number: 4383\n",
      "Loss: 38.92947868791326\n",
      "l2 norm of gradients: 0.2852409105481969\n",
      "l2 norm of weights: 5.974346835497814\n",
      "---------------------\n",
      "Iteration Number: 4384\n",
      "Loss: 38.925403156316996\n",
      "l2 norm of gradients: 0.28519075213069245\n",
      "l2 norm of weights: 5.974252867231979\n",
      "---------------------\n",
      "Iteration Number: 4385\n",
      "Loss: 38.921329047959205\n",
      "l2 norm of gradients: 0.28514062068599577\n",
      "l2 norm of weights: 5.974158909609865\n",
      "---------------------\n",
      "Iteration Number: 4386\n",
      "Loss: 38.91725636183274\n",
      "l2 norm of gradients: 0.28509051619146963\n",
      "l2 norm of weights: 5.974064962627895\n",
      "---------------------\n",
      "Iteration Number: 4387\n",
      "Loss: 38.91318509694728\n",
      "l2 norm of gradients: 0.2850404386244969\n",
      "l2 norm of weights: 5.973971026282494\n",
      "---------------------\n",
      "Iteration Number: 4388\n",
      "Loss: 38.909115252277125\n",
      "l2 norm of gradients: 0.28499038796248066\n",
      "l2 norm of weights: 5.973877100570087\n",
      "---------------------\n",
      "Iteration Number: 4389\n",
      "Loss: 38.905046826856164\n",
      "l2 norm of gradients: 0.2849403641828442\n",
      "l2 norm of weights: 5.973783185487107\n",
      "---------------------\n",
      "Iteration Number: 4390\n",
      "Loss: 38.90097981967385\n",
      "l2 norm of gradients: 0.28489036726303063\n",
      "l2 norm of weights: 5.973689281029984\n",
      "---------------------\n",
      "Iteration Number: 4391\n",
      "Loss: 38.89691422973446\n",
      "l2 norm of gradients: 0.28484039718050347\n",
      "l2 norm of weights: 5.973595387195158\n",
      "---------------------\n",
      "Iteration Number: 4392\n",
      "Loss: 38.89285005604134\n",
      "l2 norm of gradients: 0.28479045391274604\n",
      "l2 norm of weights: 5.973501503979066\n",
      "---------------------\n",
      "Iteration Number: 4393\n",
      "Loss: 38.88878729759363\n",
      "l2 norm of gradients: 0.284740537437262\n",
      "l2 norm of weights: 5.9734076313781515\n",
      "---------------------\n",
      "Iteration Number: 4394\n",
      "Loss: 38.88472595340546\n",
      "l2 norm of gradients: 0.2846906477315748\n",
      "l2 norm of weights: 5.973313769388858\n",
      "---------------------\n",
      "Iteration Number: 4395\n",
      "Loss: 38.880666022475964\n",
      "l2 norm of gradients: 0.28464078477322813\n",
      "l2 norm of weights: 5.973219918007635\n",
      "---------------------\n",
      "Iteration Number: 4396\n",
      "Loss: 38.876607503832794\n",
      "l2 norm of gradients: 0.2845909485397856\n",
      "l2 norm of weights: 5.973126077230932\n",
      "---------------------\n",
      "Iteration Number: 4397\n",
      "Loss: 38.8725503964668\n",
      "l2 norm of gradients: 0.2845411390088309\n",
      "l2 norm of weights: 5.973032247055202\n",
      "---------------------\n",
      "Iteration Number: 4398\n",
      "Loss: 38.8684946993888\n",
      "l2 norm of gradients: 0.28449135615796733\n",
      "l2 norm of weights: 5.9729384274769055\n",
      "---------------------\n",
      "Iteration Number: 4399\n",
      "Loss: 38.86444041161904\n",
      "l2 norm of gradients: 0.2844415999648188\n",
      "l2 norm of weights: 5.9728446184924975\n",
      "---------------------\n",
      "Iteration Number: 4400\n",
      "Loss: 38.86038753217419\n",
      "l2 norm of gradients: 0.2843918704070288\n",
      "l2 norm of weights: 5.972750820098444\n",
      "---------------------\n",
      "Iteration Number: 4401\n",
      "Loss: 38.85633606005405\n",
      "l2 norm of gradients: 0.2843421674622607\n",
      "l2 norm of weights: 5.972657032291209\n",
      "---------------------\n",
      "Iteration Number: 4402\n",
      "Loss: 38.852285994278574\n",
      "l2 norm of gradients: 0.284292491108198\n",
      "l2 norm of weights: 5.97256325506726\n",
      "---------------------\n",
      "Iteration Number: 4403\n",
      "Loss: 38.8482373338702\n",
      "l2 norm of gradients: 0.284242841322544\n",
      "l2 norm of weights: 5.9724694884230685\n",
      "---------------------\n",
      "Iteration Number: 4404\n",
      "Loss: 38.844190077834156\n",
      "l2 norm of gradients: 0.28419321808302195\n",
      "l2 norm of weights: 5.972375732355109\n",
      "---------------------\n",
      "Iteration Number: 4405\n",
      "Loss: 38.84014422519652\n",
      "l2 norm of gradients: 0.284143621367375\n",
      "l2 norm of weights: 5.972281986859859\n",
      "---------------------\n",
      "Iteration Number: 4406\n",
      "Loss: 38.836099774976894\n",
      "l2 norm of gradients: 0.2840940511533661\n",
      "l2 norm of weights: 5.9721882519337965\n",
      "---------------------\n",
      "Iteration Number: 4407\n",
      "Loss: 38.83205672618984\n",
      "l2 norm of gradients: 0.28404450741877807\n",
      "l2 norm of weights: 5.972094527573406\n",
      "---------------------\n",
      "Iteration Number: 4408\n",
      "Loss: 38.82801507784591\n",
      "l2 norm of gradients: 0.28399499014141366\n",
      "l2 norm of weights: 5.972000813775171\n",
      "---------------------\n",
      "Iteration Number: 4409\n",
      "Loss: 38.82397482898889\n",
      "l2 norm of gradients: 0.28394549929909535\n",
      "l2 norm of weights: 5.971907110535582\n",
      "---------------------\n",
      "Iteration Number: 4410\n",
      "Loss: 38.81993597862654\n",
      "l2 norm of gradients: 0.2838960348696654\n",
      "l2 norm of weights: 5.97181341785113\n",
      "---------------------\n",
      "Iteration Number: 4411\n",
      "Loss: 38.81589852578515\n",
      "l2 norm of gradients: 0.28384659683098595\n",
      "l2 norm of weights: 5.971719735718307\n",
      "---------------------\n",
      "Iteration Number: 4412\n",
      "Loss: 38.8118624694912\n",
      "l2 norm of gradients: 0.283797185160939\n",
      "l2 norm of weights: 5.971626064133613\n",
      "---------------------\n",
      "Iteration Number: 4413\n",
      "Loss: 38.807827808768764\n",
      "l2 norm of gradients: 0.28374779983742604\n",
      "l2 norm of weights: 5.971532403093544\n",
      "---------------------\n",
      "Iteration Number: 4414\n",
      "Loss: 38.8037945426498\n",
      "l2 norm of gradients: 0.2836984408383687\n",
      "l2 norm of weights: 5.971438752594606\n",
      "---------------------\n",
      "Iteration Number: 4415\n",
      "Loss: 38.79976267014238\n",
      "l2 norm of gradients: 0.28364910814170796\n",
      "l2 norm of weights: 5.971345112633305\n",
      "---------------------\n",
      "Iteration Number: 4416\n",
      "Loss: 38.79573219030569\n",
      "l2 norm of gradients: 0.2835998017254047\n",
      "l2 norm of weights: 5.971251483206146\n",
      "---------------------\n",
      "Iteration Number: 4417\n",
      "Loss: 38.791703102137525\n",
      "l2 norm of gradients: 0.28355052156743965\n",
      "l2 norm of weights: 5.971157864309642\n",
      "---------------------\n",
      "Iteration Number: 4418\n",
      "Loss: 38.787675404684606\n",
      "l2 norm of gradients: 0.28350126764581307\n",
      "l2 norm of weights: 5.971064255940307\n",
      "---------------------\n",
      "Iteration Number: 4419\n",
      "Loss: 38.78364909699283\n",
      "l2 norm of gradients: 0.28345203993854473\n",
      "l2 norm of weights: 5.970970658094658\n",
      "---------------------\n",
      "Iteration Number: 4420\n",
      "Loss: 38.779624178066605\n",
      "l2 norm of gradients: 0.2834028384236745\n",
      "l2 norm of weights: 5.970877070769216\n",
      "---------------------\n",
      "Iteration Number: 4421\n",
      "Loss: 38.775600646944866\n",
      "l2 norm of gradients: 0.2833536630792614\n",
      "l2 norm of weights: 5.9707834939605\n",
      "---------------------\n",
      "Iteration Number: 4422\n",
      "Loss: 38.77157850267686\n",
      "l2 norm of gradients: 0.28330451388338446\n",
      "l2 norm of weights: 5.970689927665038\n",
      "---------------------\n",
      "Iteration Number: 4423\n",
      "Loss: 38.767557744285384\n",
      "l2 norm of gradients: 0.2832553908141423\n",
      "l2 norm of weights: 5.970596371879358\n",
      "---------------------\n",
      "Iteration Number: 4424\n",
      "Loss: 38.763538370815176\n",
      "l2 norm of gradients: 0.2832062938496529\n",
      "l2 norm of weights: 5.970502826599989\n",
      "---------------------\n",
      "Iteration Number: 4425\n",
      "Loss: 38.759520381299076\n",
      "l2 norm of gradients: 0.283157222968054\n",
      "l2 norm of weights: 5.970409291823468\n",
      "---------------------\n",
      "Iteration Number: 4426\n",
      "Loss: 38.755503774774766\n",
      "l2 norm of gradients: 0.28310817814750294\n",
      "l2 norm of weights: 5.97031576754633\n",
      "---------------------\n",
      "Iteration Number: 4427\n",
      "Loss: 38.75148855027411\n",
      "l2 norm of gradients: 0.28305915936617654\n",
      "l2 norm of weights: 5.9702222537651135\n",
      "---------------------\n",
      "Iteration Number: 4428\n",
      "Loss: 38.74747470685132\n",
      "l2 norm of gradients: 0.2830101666022712\n",
      "l2 norm of weights: 5.970128750476361\n",
      "---------------------\n",
      "Iteration Number: 4429\n",
      "Loss: 38.74346224354091\n",
      "l2 norm of gradients: 0.28296119983400286\n",
      "l2 norm of weights: 5.970035257676619\n",
      "---------------------\n",
      "Iteration Number: 4430\n",
      "Loss: 38.73945115939326\n",
      "l2 norm of gradients: 0.2829122590396069\n",
      "l2 norm of weights: 5.969941775362433\n",
      "---------------------\n",
      "Iteration Number: 4431\n",
      "Loss: 38.73544145343141\n",
      "l2 norm of gradients: 0.28286334419733833\n",
      "l2 norm of weights: 5.969848303530356\n",
      "---------------------\n",
      "Iteration Number: 4432\n",
      "Loss: 38.73143312472093\n",
      "l2 norm of gradients: 0.28281445528547156\n",
      "l2 norm of weights: 5.96975484217694\n",
      "---------------------\n",
      "Iteration Number: 4433\n",
      "Loss: 38.72742617229013\n",
      "l2 norm of gradients: 0.2827655922823005\n",
      "l2 norm of weights: 5.969661391298741\n",
      "---------------------\n",
      "Iteration Number: 4434\n",
      "Loss: 38.72342059519454\n",
      "l2 norm of gradients: 0.2827167551661385\n",
      "l2 norm of weights: 5.969567950892319\n",
      "---------------------\n",
      "Iteration Number: 4435\n",
      "Loss: 38.7194163924806\n",
      "l2 norm of gradients: 0.28266794391531835\n",
      "l2 norm of weights: 5.969474520954235\n",
      "---------------------\n",
      "Iteration Number: 4436\n",
      "Loss: 38.71541356319388\n",
      "l2 norm of gradients: 0.28261915850819236\n",
      "l2 norm of weights: 5.969381101481053\n",
      "---------------------\n",
      "Iteration Number: 4437\n",
      "Loss: 38.71141210639055\n",
      "l2 norm of gradients: 0.28257039892313207\n",
      "l2 norm of weights: 5.969287692469341\n",
      "---------------------\n",
      "Iteration Number: 4438\n",
      "Loss: 38.70741202111072\n",
      "l2 norm of gradients: 0.2825216651385285\n",
      "l2 norm of weights: 5.9691942939156695\n",
      "---------------------\n",
      "Iteration Number: 4439\n",
      "Loss: 38.703413306405025\n",
      "l2 norm of gradients: 0.2824729571327922\n",
      "l2 norm of weights: 5.96910090581661\n",
      "---------------------\n",
      "Iteration Number: 4440\n",
      "Loss: 38.69941596133054\n",
      "l2 norm of gradients: 0.2824242748843528\n",
      "l2 norm of weights: 5.969007528168739\n",
      "---------------------\n",
      "Iteration Number: 4441\n",
      "Loss: 38.695419984938134\n",
      "l2 norm of gradients: 0.28237561837165953\n",
      "l2 norm of weights: 5.968914160968636\n",
      "---------------------\n",
      "Iteration Number: 4442\n",
      "Loss: 38.69142537628182\n",
      "l2 norm of gradients: 0.28232698757318087\n",
      "l2 norm of weights: 5.9688208042128785\n",
      "---------------------\n",
      "Iteration Number: 4443\n",
      "Loss: 38.68743213441701\n",
      "l2 norm of gradients: 0.2822783824674046\n",
      "l2 norm of weights: 5.968727457898054\n",
      "---------------------\n",
      "Iteration Number: 4444\n",
      "Loss: 38.68344025839253\n",
      "l2 norm of gradients: 0.28222980303283784\n",
      "l2 norm of weights: 5.968634122020747\n",
      "---------------------\n",
      "Iteration Number: 4445\n",
      "Loss: 38.679449747274774\n",
      "l2 norm of gradients: 0.282181249248007\n",
      "l2 norm of weights: 5.968540796577548\n",
      "---------------------\n",
      "Iteration Number: 4446\n",
      "Loss: 38.67546060010836\n",
      "l2 norm of gradients: 0.2821327210914578\n",
      "l2 norm of weights: 5.968447481565048\n",
      "---------------------\n",
      "Iteration Number: 4447\n",
      "Loss: 38.67147281597441\n",
      "l2 norm of gradients: 0.28208421854175514\n",
      "l2 norm of weights: 5.968354176979843\n",
      "---------------------\n",
      "Iteration Number: 4448\n",
      "Loss: 38.66748639391075\n",
      "l2 norm of gradients: 0.2820357415774833\n",
      "l2 norm of weights: 5.968260882818528\n",
      "---------------------\n",
      "Iteration Number: 4449\n",
      "Loss: 38.66350133298183\n",
      "l2 norm of gradients: 0.28198729017724566\n",
      "l2 norm of weights: 5.968167599077708\n",
      "---------------------\n",
      "Iteration Number: 4450\n",
      "Loss: 38.659517632255\n",
      "l2 norm of gradients: 0.28193886431966486\n",
      "l2 norm of weights: 5.968074325753982\n",
      "---------------------\n",
      "Iteration Number: 4451\n",
      "Loss: 38.655535290795136\n",
      "l2 norm of gradients: 0.2818904639833828\n",
      "l2 norm of weights: 5.967981062843957\n",
      "---------------------\n",
      "Iteration Number: 4452\n",
      "Loss: 38.65155430765074\n",
      "l2 norm of gradients: 0.2818420891470605\n",
      "l2 norm of weights: 5.967887810344242\n",
      "---------------------\n",
      "Iteration Number: 4453\n",
      "Loss: 38.64757468190038\n",
      "l2 norm of gradients: 0.28179373978937833\n",
      "l2 norm of weights: 5.967794568251446\n",
      "---------------------\n",
      "Iteration Number: 4454\n",
      "Loss: 38.643596412606435\n",
      "l2 norm of gradients: 0.2817454158890355\n",
      "l2 norm of weights: 5.967701336562187\n",
      "---------------------\n",
      "Iteration Number: 4455\n",
      "Loss: 38.639619498828786\n",
      "l2 norm of gradients: 0.2816971174247507\n",
      "l2 norm of weights: 5.9676081152730776\n",
      "---------------------\n",
      "Iteration Number: 4456\n",
      "Loss: 38.63564393963313\n",
      "l2 norm of gradients: 0.28164884437526155\n",
      "l2 norm of weights: 5.967514904380739\n",
      "---------------------\n",
      "Iteration Number: 4457\n",
      "Loss: 38.63166973410435\n",
      "l2 norm of gradients: 0.28160059671932475\n",
      "l2 norm of weights: 5.967421703881794\n",
      "---------------------\n",
      "Iteration Number: 4458\n",
      "Loss: 38.627696881288415\n",
      "l2 norm of gradients: 0.28155237443571635\n",
      "l2 norm of weights: 5.967328513772865\n",
      "---------------------\n",
      "Iteration Number: 4459\n",
      "Loss: 38.62372538026868\n",
      "l2 norm of gradients: 0.2815041775032313\n",
      "l2 norm of weights: 5.9672353340505815\n",
      "---------------------\n",
      "Iteration Number: 4460\n",
      "Loss: 38.619755230112524\n",
      "l2 norm of gradients: 0.28145600590068376\n",
      "l2 norm of weights: 5.967142164711572\n",
      "---------------------\n",
      "Iteration Number: 4461\n",
      "Loss: 38.61578642988932\n",
      "l2 norm of gradients: 0.28140785960690656\n",
      "l2 norm of weights: 5.9670490057524725\n",
      "---------------------\n",
      "Iteration Number: 4462\n",
      "Loss: 38.61181897867609\n",
      "l2 norm of gradients: 0.2813597386007521\n",
      "l2 norm of weights: 5.966955857169915\n",
      "---------------------\n",
      "Iteration Number: 4463\n",
      "Loss: 38.60785287555053\n",
      "l2 norm of gradients: 0.28131164286109145\n",
      "l2 norm of weights: 5.966862718960539\n",
      "---------------------\n",
      "Iteration Number: 4464\n",
      "Loss: 38.603888119578826\n",
      "l2 norm of gradients: 0.28126357236681493\n",
      "l2 norm of weights: 5.966769591120986\n",
      "---------------------\n",
      "Iteration Number: 4465\n",
      "Loss: 38.59992470983163\n",
      "l2 norm of gradients: 0.28121552709683173\n",
      "l2 norm of weights: 5.966676473647898\n",
      "---------------------\n",
      "Iteration Number: 4466\n",
      "Loss: 38.59596264539185\n",
      "l2 norm of gradients: 0.28116750703007\n",
      "l2 norm of weights: 5.9665833665379235\n",
      "---------------------\n",
      "Iteration Number: 4467\n",
      "Loss: 38.59200192534213\n",
      "l2 norm of gradients: 0.281119512145477\n",
      "l2 norm of weights: 5.966490269787711\n",
      "---------------------\n",
      "Iteration Number: 4468\n",
      "Loss: 38.58804254875199\n",
      "l2 norm of gradients: 0.2810715424220187\n",
      "l2 norm of weights: 5.9663971833939105\n",
      "---------------------\n",
      "Iteration Number: 4469\n",
      "Loss: 38.584084514711115\n",
      "l2 norm of gradients: 0.2810235978386803\n",
      "l2 norm of weights: 5.966304107353177\n",
      "---------------------\n",
      "Iteration Number: 4470\n",
      "Loss: 38.58012782228157\n",
      "l2 norm of gradients: 0.28097567837446574\n",
      "l2 norm of weights: 5.966211041662167\n",
      "---------------------\n",
      "Iteration Number: 4471\n",
      "Loss: 38.57617247056002\n",
      "l2 norm of gradients: 0.2809277840083979\n",
      "l2 norm of weights: 5.966117986317543\n",
      "---------------------\n",
      "Iteration Number: 4472\n",
      "Loss: 38.57221845862811\n",
      "l2 norm of gradients: 0.2808799147195187\n",
      "l2 norm of weights: 5.966024941315964\n",
      "---------------------\n",
      "Iteration Number: 4473\n",
      "Loss: 38.56826578554945\n",
      "l2 norm of gradients: 0.2808320704868887\n",
      "l2 norm of weights: 5.965931906654096\n",
      "---------------------\n",
      "Iteration Number: 4474\n",
      "Loss: 38.5643144504362\n",
      "l2 norm of gradients: 0.28078425128958745\n",
      "l2 norm of weights: 5.965838882328606\n",
      "---------------------\n",
      "Iteration Number: 4475\n",
      "Loss: 38.56036445235165\n",
      "l2 norm of gradients: 0.2807364571067135\n",
      "l2 norm of weights: 5.965745868336167\n",
      "---------------------\n",
      "Iteration Number: 4476\n",
      "Loss: 38.556415790388876\n",
      "l2 norm of gradients: 0.280688687917384\n",
      "l2 norm of weights: 5.965652864673448\n",
      "---------------------\n",
      "Iteration Number: 4477\n",
      "Loss: 38.55246846363819\n",
      "l2 norm of gradients: 0.280640943700735\n",
      "l2 norm of weights: 5.965559871337128\n",
      "---------------------\n",
      "Iteration Number: 4478\n",
      "Loss: 38.54852247117626\n",
      "l2 norm of gradients: 0.2805932244359215\n",
      "l2 norm of weights: 5.965466888323882\n",
      "---------------------\n",
      "Iteration Number: 4479\n",
      "Loss: 38.54457781210168\n",
      "l2 norm of gradients: 0.2805455301021171\n",
      "l2 norm of weights: 5.965373915630393\n",
      "---------------------\n",
      "Iteration Number: 4480\n",
      "Loss: 38.54063448550137\n",
      "l2 norm of gradients: 0.2804978606785143\n",
      "l2 norm of weights: 5.965280953253345\n",
      "---------------------\n",
      "Iteration Number: 4481\n",
      "Loss: 38.53669249045229\n",
      "l2 norm of gradients: 0.2804502161443243\n",
      "l2 norm of weights: 5.965188001189421\n",
      "---------------------\n",
      "Iteration Number: 4482\n",
      "Loss: 38.532751826063574\n",
      "l2 norm of gradients: 0.2804025964787771\n",
      "l2 norm of weights: 5.965095059435313\n",
      "---------------------\n",
      "Iteration Number: 4483\n",
      "Loss: 38.52881249142139\n",
      "l2 norm of gradients: 0.2803550016611213\n",
      "l2 norm of weights: 5.965002127987711\n",
      "---------------------\n",
      "Iteration Number: 4484\n",
      "Loss: 38.524874485614774\n",
      "l2 norm of gradients: 0.28030743167062455\n",
      "l2 norm of weights: 5.964909206843309\n",
      "---------------------\n",
      "Iteration Number: 4485\n",
      "Loss: 38.52093780774676\n",
      "l2 norm of gradients: 0.2802598864865729\n",
      "l2 norm of weights: 5.964816295998803\n",
      "---------------------\n",
      "Iteration Number: 4486\n",
      "Loss: 38.517002456904656\n",
      "l2 norm of gradients: 0.2802123660882713\n",
      "l2 norm of weights: 5.964723395450894\n",
      "---------------------\n",
      "Iteration Number: 4487\n",
      "Loss: 38.51306843217169\n",
      "l2 norm of gradients: 0.280164870455043\n",
      "l2 norm of weights: 5.964630505196282\n",
      "---------------------\n",
      "Iteration Number: 4488\n",
      "Loss: 38.509135732666905\n",
      "l2 norm of gradients: 0.2801173995662305\n",
      "l2 norm of weights: 5.964537625231673\n",
      "---------------------\n",
      "Iteration Number: 4489\n",
      "Loss: 38.5052043574779\n",
      "l2 norm of gradients: 0.2800699534011944\n",
      "l2 norm of weights: 5.964444755553773\n",
      "---------------------\n",
      "Iteration Number: 4490\n",
      "Loss: 38.501274305696654\n",
      "l2 norm of gradients: 0.2800225319393143\n",
      "l2 norm of weights: 5.9643518961592905\n",
      "---------------------\n",
      "Iteration Number: 4491\n",
      "Loss: 38.497345576438164\n",
      "l2 norm of gradients: 0.27997513515998834\n",
      "l2 norm of weights: 5.964259047044941\n",
      "---------------------\n",
      "Iteration Number: 4492\n",
      "Loss: 38.493418168785574\n",
      "l2 norm of gradients: 0.2799277630426331\n",
      "l2 norm of weights: 5.964166208207435\n",
      "---------------------\n",
      "Iteration Number: 4493\n",
      "Loss: 38.48949208185728\n",
      "l2 norm of gradients: 0.27988041556668397\n",
      "l2 norm of weights: 5.964073379643494\n",
      "---------------------\n",
      "Iteration Number: 4494\n",
      "Loss: 38.48556731473836\n",
      "l2 norm of gradients: 0.27983309271159473\n",
      "l2 norm of weights: 5.963980561349837\n",
      "---------------------\n",
      "Iteration Number: 4495\n",
      "Loss: 38.4816438665319\n",
      "l2 norm of gradients: 0.27978579445683793\n",
      "l2 norm of weights: 5.963887753323185\n",
      "---------------------\n",
      "Iteration Number: 4496\n",
      "Loss: 38.47772173634996\n",
      "l2 norm of gradients: 0.2797385207819044\n",
      "l2 norm of weights: 5.963794955560265\n",
      "---------------------\n",
      "Iteration Number: 4497\n",
      "Loss: 38.473800923300495\n",
      "l2 norm of gradients: 0.27969127166630386\n",
      "l2 norm of weights: 5.963702168057804\n",
      "---------------------\n",
      "Iteration Number: 4498\n",
      "Loss: 38.46988142648216\n",
      "l2 norm of gradients: 0.2796440470895642\n",
      "l2 norm of weights: 5.9636093908125325\n",
      "---------------------\n",
      "Iteration Number: 4499\n",
      "Loss: 38.46596324500538\n",
      "l2 norm of gradients: 0.27959684703123183\n",
      "l2 norm of weights: 5.9635166238211825\n",
      "---------------------\n",
      "Iteration Number: 4500\n",
      "Loss: 38.46204637797839\n",
      "l2 norm of gradients: 0.2795496714708722\n",
      "l2 norm of weights: 5.9634238670804915\n",
      "---------------------\n",
      "Iteration Number: 4501\n",
      "Loss: 38.458130824498994\n",
      "l2 norm of gradients: 0.2795025203880684\n",
      "l2 norm of weights: 5.963331120587196\n",
      "---------------------\n",
      "Iteration Number: 4502\n",
      "Loss: 38.454216583677216\n",
      "l2 norm of gradients: 0.2794553937624226\n",
      "l2 norm of weights: 5.963238384338038\n",
      "---------------------\n",
      "Iteration Number: 4503\n",
      "Loss: 38.45030365463333\n",
      "l2 norm of gradients: 0.2794082915735551\n",
      "l2 norm of weights: 5.9631456583297595\n",
      "---------------------\n",
      "Iteration Number: 4504\n",
      "Loss: 38.446392036471856\n",
      "l2 norm of gradients: 0.2793612138011049\n",
      "l2 norm of weights: 5.9630529425591074\n",
      "---------------------\n",
      "Iteration Number: 4505\n",
      "Loss: 38.44248172830573\n",
      "l2 norm of gradients: 0.2793141604247292\n",
      "l2 norm of weights: 5.962960237022829\n",
      "---------------------\n",
      "Iteration Number: 4506\n",
      "Loss: 38.438572729251746\n",
      "l2 norm of gradients: 0.27926713142410364\n",
      "l2 norm of weights: 5.962867541717675\n",
      "---------------------\n",
      "Iteration Number: 4507\n",
      "Loss: 38.434665038423006\n",
      "l2 norm of gradients: 0.2792201267789222\n",
      "l2 norm of weights: 5.962774856640401\n",
      "---------------------\n",
      "Iteration Number: 4508\n",
      "Loss: 38.430758654918755\n",
      "l2 norm of gradients: 0.27917314646889746\n",
      "l2 norm of weights: 5.962682181787761\n",
      "---------------------\n",
      "Iteration Number: 4509\n",
      "Loss: 38.42685357787381\n",
      "l2 norm of gradients: 0.27912619047376014\n",
      "l2 norm of weights: 5.962589517156514\n",
      "---------------------\n",
      "Iteration Number: 4510\n",
      "Loss: 38.42294980638912\n",
      "l2 norm of gradients: 0.27907925877325945\n",
      "l2 norm of weights: 5.962496862743423\n",
      "---------------------\n",
      "Iteration Number: 4511\n",
      "Loss: 38.419047339598684\n",
      "l2 norm of gradients: 0.27903235134716275\n",
      "l2 norm of weights: 5.962404218545248\n",
      "---------------------\n",
      "Iteration Number: 4512\n",
      "Loss: 38.41514617661108\n",
      "l2 norm of gradients: 0.278985468175256\n",
      "l2 norm of weights: 5.9623115845587575\n",
      "---------------------\n",
      "Iteration Number: 4513\n",
      "Loss: 38.41124631653195\n",
      "l2 norm of gradients: 0.278938609237343\n",
      "l2 norm of weights: 5.962218960780721\n",
      "---------------------\n",
      "Iteration Number: 4514\n",
      "Loss: 38.40734775850048\n",
      "l2 norm of gradients: 0.2788917745132463\n",
      "l2 norm of weights: 5.962126347207908\n",
      "---------------------\n",
      "Iteration Number: 4515\n",
      "Loss: 38.40345050162671\n",
      "l2 norm of gradients: 0.2788449639828067\n",
      "l2 norm of weights: 5.962033743837093\n",
      "---------------------\n",
      "Iteration Number: 4516\n",
      "Loss: 38.39955454503837\n",
      "l2 norm of gradients: 0.2787981776258829\n",
      "l2 norm of weights: 5.961941150665052\n",
      "---------------------\n",
      "Iteration Number: 4517\n",
      "Loss: 38.39565988785246\n",
      "l2 norm of gradients: 0.27875141542235204\n",
      "l2 norm of weights: 5.961848567688565\n",
      "---------------------\n",
      "Iteration Number: 4518\n",
      "Loss: 38.39176652920132\n",
      "l2 norm of gradients: 0.2787046773521097\n",
      "l2 norm of weights: 5.961755994904412\n",
      "---------------------\n",
      "Iteration Number: 4519\n",
      "Loss: 38.38787446818833\n",
      "l2 norm of gradients: 0.2786579633950693\n",
      "l2 norm of weights: 5.961663432309378\n",
      "---------------------\n",
      "Iteration Number: 4520\n",
      "Loss: 38.383983703962926\n",
      "l2 norm of gradients: 0.27861127353116283\n",
      "l2 norm of weights: 5.961570879900249\n",
      "---------------------\n",
      "Iteration Number: 4521\n",
      "Loss: 38.38009423563613\n",
      "l2 norm of gradients: 0.27856460774034\n",
      "l2 norm of weights: 5.961478337673813\n",
      "---------------------\n",
      "Iteration Number: 4522\n",
      "Loss: 38.376206062340735\n",
      "l2 norm of gradients: 0.2785179660025692\n",
      "l2 norm of weights: 5.961385805626863\n",
      "---------------------\n",
      "Iteration Number: 4523\n",
      "Loss: 38.37231918319908\n",
      "l2 norm of gradients: 0.2784713482978367\n",
      "l2 norm of weights: 5.961293283756191\n",
      "---------------------\n",
      "Iteration Number: 4524\n",
      "Loss: 38.3684335973453\n",
      "l2 norm of gradients: 0.2784247546061469\n",
      "l2 norm of weights: 5.961200772058596\n",
      "---------------------\n",
      "Iteration Number: 4525\n",
      "Loss: 38.36454930390151\n",
      "l2 norm of gradients: 0.27837818490752253\n",
      "l2 norm of weights: 5.961108270530874\n",
      "---------------------\n",
      "Iteration Number: 4526\n",
      "Loss: 38.36066630199906\n",
      "l2 norm of gradients: 0.27833163918200426\n",
      "l2 norm of weights: 5.961015779169829\n",
      "---------------------\n",
      "Iteration Number: 4527\n",
      "Loss: 38.35678459077245\n",
      "l2 norm of gradients: 0.27828511740965084\n",
      "l2 norm of weights: 5.960923297972265\n",
      "---------------------\n",
      "Iteration Number: 4528\n",
      "Loss: 38.352904169363526\n",
      "l2 norm of gradients: 0.2782386195705392\n",
      "l2 norm of weights: 5.960830826934986\n",
      "---------------------\n",
      "Iteration Number: 4529\n",
      "Loss: 38.349025036885976\n",
      "l2 norm of gradients: 0.27819214564476424\n",
      "l2 norm of weights: 5.960738366054802\n",
      "---------------------\n",
      "Iteration Number: 4530\n",
      "Loss: 38.34514719247834\n",
      "l2 norm of gradients: 0.2781456956124391\n",
      "l2 norm of weights: 5.9606459153285245\n",
      "---------------------\n",
      "Iteration Number: 4531\n",
      "Loss: 38.341270635285426\n",
      "l2 norm of gradients: 0.27809926945369495\n",
      "l2 norm of weights: 5.9605534747529685\n",
      "---------------------\n",
      "Iteration Number: 4532\n",
      "Loss: 38.3373953644289\n",
      "l2 norm of gradients: 0.2780528671486807\n",
      "l2 norm of weights: 5.960461044324949\n",
      "---------------------\n",
      "Iteration Number: 4533\n",
      "Loss: 38.33352137905385\n",
      "l2 norm of gradients: 0.2780064886775636\n",
      "l2 norm of weights: 5.960368624041284\n",
      "---------------------\n",
      "Iteration Number: 4534\n",
      "Loss: 38.32964867828908\n",
      "l2 norm of gradients: 0.2779601340205287\n",
      "l2 norm of weights: 5.960276213898798\n",
      "---------------------\n",
      "Iteration Number: 4535\n",
      "Loss: 38.3257772612819\n",
      "l2 norm of gradients: 0.2779138031577792\n",
      "l2 norm of weights: 5.960183813894311\n",
      "---------------------\n",
      "Iteration Number: 4536\n",
      "Loss: 38.32190712717103\n",
      "l2 norm of gradients: 0.2778674960695361\n",
      "l2 norm of weights: 5.960091424024652\n",
      "---------------------\n",
      "Iteration Number: 4537\n",
      "Loss: 38.318038275091304\n",
      "l2 norm of gradients: 0.27782121273603844\n",
      "l2 norm of weights: 5.959999044286649\n",
      "---------------------\n",
      "Iteration Number: 4538\n",
      "Loss: 38.31417070417482\n",
      "l2 norm of gradients: 0.27777495313754325\n",
      "l2 norm of weights: 5.959906674677133\n",
      "---------------------\n",
      "Iteration Number: 4539\n",
      "Loss: 38.31030441358344\n",
      "l2 norm of gradients: 0.2777287172543254\n",
      "l2 norm of weights: 5.959814315192938\n",
      "---------------------\n",
      "Iteration Number: 4540\n",
      "Loss: 38.30643940244297\n",
      "l2 norm of gradients: 0.27768250506667785\n",
      "l2 norm of weights: 5.959721965830898\n",
      "---------------------\n",
      "Iteration Number: 4541\n",
      "Loss: 38.302575669900996\n",
      "l2 norm of gradients: 0.27763631655491117\n",
      "l2 norm of weights: 5.959629626587855\n",
      "---------------------\n",
      "Iteration Number: 4542\n",
      "Loss: 38.29871321510034\n",
      "l2 norm of gradients: 0.2775901516993541\n",
      "l2 norm of weights: 5.959537297460648\n",
      "---------------------\n",
      "Iteration Number: 4543\n",
      "Loss: 38.29485203718671\n",
      "l2 norm of gradients: 0.27754401048035293\n",
      "l2 norm of weights: 5.95944497844612\n",
      "---------------------\n",
      "Iteration Number: 4544\n",
      "Loss: 38.29099213530463\n",
      "l2 norm of gradients: 0.27749789287827237\n",
      "l2 norm of weights: 5.959352669541118\n",
      "---------------------\n",
      "Iteration Number: 4545\n",
      "Loss: 38.287133508599396\n",
      "l2 norm of gradients: 0.2774517988734943\n",
      "l2 norm of weights: 5.95926037074249\n",
      "---------------------\n",
      "Iteration Number: 4546\n",
      "Loss: 38.28327615622253\n",
      "l2 norm of gradients: 0.277405728446419\n",
      "l2 norm of weights: 5.9591680820470865\n",
      "---------------------\n",
      "Iteration Number: 4547\n",
      "Loss: 38.27942007732477\n",
      "l2 norm of gradients: 0.2773596815774641\n",
      "l2 norm of weights: 5.959075803451761\n",
      "---------------------\n",
      "Iteration Number: 4548\n",
      "Loss: 38.27556527104494\n",
      "l2 norm of gradients: 0.27731365824706533\n",
      "l2 norm of weights: 5.9589835349533695\n",
      "---------------------\n",
      "Iteration Number: 4549\n",
      "Loss: 38.27171173654494\n",
      "l2 norm of gradients: 0.2772676584356761\n",
      "l2 norm of weights: 5.958891276548769\n",
      "---------------------\n",
      "Iteration Number: 4550\n",
      "Loss: 38.26785947295551\n",
      "l2 norm of gradients: 0.2772216821237678\n",
      "l2 norm of weights: 5.958799028234822\n",
      "---------------------\n",
      "Iteration Number: 4551\n",
      "Loss: 38.26400847945018\n",
      "l2 norm of gradients: 0.2771757292918291\n",
      "l2 norm of weights: 5.958706790008389\n",
      "---------------------\n",
      "Iteration Number: 4552\n",
      "Loss: 38.26015875516376\n",
      "l2 norm of gradients: 0.27712979992036696\n",
      "l2 norm of weights: 5.958614561866337\n",
      "---------------------\n",
      "Iteration Number: 4553\n",
      "Loss: 38.256310299258175\n",
      "l2 norm of gradients: 0.2770838939899057\n",
      "l2 norm of weights: 5.958522343805534\n",
      "---------------------\n",
      "Iteration Number: 4554\n",
      "Loss: 38.25246311089302\n",
      "l2 norm of gradients: 0.2770380114809875\n",
      "l2 norm of weights: 5.958430135822848\n",
      "---------------------\n",
      "Iteration Number: 4555\n",
      "Loss: 38.24861718921569\n",
      "l2 norm of gradients: 0.27699215237417224\n",
      "l2 norm of weights: 5.958337937915155\n",
      "---------------------\n",
      "Iteration Number: 4556\n",
      "Loss: 38.244772533383504\n",
      "l2 norm of gradients: 0.27694631665003744\n",
      "l2 norm of weights: 5.9582457500793256\n",
      "---------------------\n",
      "Iteration Number: 4557\n",
      "Loss: 38.24092914254987\n",
      "l2 norm of gradients: 0.2769005042891785\n",
      "l2 norm of weights: 5.958153572312241\n",
      "---------------------\n",
      "Iteration Number: 4558\n",
      "Loss: 38.23708701587362\n",
      "l2 norm of gradients: 0.2768547152722081\n",
      "l2 norm of weights: 5.95806140461078\n",
      "---------------------\n",
      "Iteration Number: 4559\n",
      "Loss: 38.23324615250811\n",
      "l2 norm of gradients: 0.2768089495797569\n",
      "l2 norm of weights: 5.9579692469718255\n",
      "---------------------\n",
      "Iteration Number: 4560\n",
      "Loss: 38.22940655162318\n",
      "l2 norm of gradients: 0.2767632071924729\n",
      "l2 norm of weights: 5.9578770993922605\n",
      "---------------------\n",
      "Iteration Number: 4561\n",
      "Loss: 38.22556821237321\n",
      "l2 norm of gradients: 0.2767174880910222\n",
      "l2 norm of weights: 5.957784961868973\n",
      "---------------------\n",
      "Iteration Number: 4562\n",
      "Loss: 38.221731133917636\n",
      "l2 norm of gradients: 0.2766717922560879\n",
      "l2 norm of weights: 5.957692834398852\n",
      "---------------------\n",
      "Iteration Number: 4563\n",
      "Loss: 38.21789531542461\n",
      "l2 norm of gradients: 0.2766261196683712\n",
      "l2 norm of weights: 5.9576007169787895\n",
      "---------------------\n",
      "Iteration Number: 4564\n",
      "Loss: 38.21406075603882\n",
      "l2 norm of gradients: 0.2765804703085905\n",
      "l2 norm of weights: 5.95750860960568\n",
      "---------------------\n",
      "Iteration Number: 4565\n",
      "Loss: 38.21022745494175\n",
      "l2 norm of gradients: 0.27653484415748203\n",
      "l2 norm of weights: 5.957416512276421\n",
      "---------------------\n",
      "Iteration Number: 4566\n",
      "Loss: 38.20639541129253\n",
      "l2 norm of gradients: 0.2764892411957994\n",
      "l2 norm of weights: 5.95732442498791\n",
      "---------------------\n",
      "Iteration Number: 4567\n",
      "Loss: 38.20256462425346\n",
      "l2 norm of gradients: 0.2764436614043138\n",
      "l2 norm of weights: 5.9572323477370475\n",
      "---------------------\n",
      "Iteration Number: 4568\n",
      "Loss: 38.198735092984876\n",
      "l2 norm of gradients: 0.27639810476381405\n",
      "l2 norm of weights: 5.95714028052074\n",
      "---------------------\n",
      "Iteration Number: 4569\n",
      "Loss: 38.194906816662055\n",
      "l2 norm of gradients: 0.27635257125510626\n",
      "l2 norm of weights: 5.9570482233358915\n",
      "---------------------\n",
      "Iteration Number: 4570\n",
      "Loss: 38.191079794456535\n",
      "l2 norm of gradients: 0.27630706085901424\n",
      "l2 norm of weights: 5.956956176179411\n",
      "---------------------\n",
      "Iteration Number: 4571\n",
      "Loss: 38.18725402552528\n",
      "l2 norm of gradients: 0.27626157355637915\n",
      "l2 norm of weights: 5.95686413904821\n",
      "---------------------\n",
      "Iteration Number: 4572\n",
      "Loss: 38.183429509030496\n",
      "l2 norm of gradients: 0.2762161093280597\n",
      "l2 norm of weights: 5.956772111939202\n",
      "---------------------\n",
      "Iteration Number: 4573\n",
      "Loss: 38.179606244168035\n",
      "l2 norm of gradients: 0.27617066815493196\n",
      "l2 norm of weights: 5.956680094849301\n",
      "---------------------\n",
      "Iteration Number: 4574\n",
      "Loss: 38.17578423008155\n",
      "l2 norm of gradients: 0.2761252500178895\n",
      "l2 norm of weights: 5.956588087775426\n",
      "---------------------\n",
      "Iteration Number: 4575\n",
      "Loss: 38.17196346595465\n",
      "l2 norm of gradients: 0.27607985489784337\n",
      "l2 norm of weights: 5.956496090714497\n",
      "---------------------\n",
      "Iteration Number: 4576\n",
      "Loss: 38.1681439509605\n",
      "l2 norm of gradients: 0.2760344827757218\n",
      "l2 norm of weights: 5.956404103663438\n",
      "---------------------\n",
      "Iteration Number: 4577\n",
      "Loss: 38.164325684270516\n",
      "l2 norm of gradients: 0.2759891336324707\n",
      "l2 norm of weights: 5.956312126619172\n",
      "---------------------\n",
      "Iteration Number: 4578\n",
      "Loss: 38.16050866505903\n",
      "l2 norm of gradients: 0.27594380744905317\n",
      "l2 norm of weights: 5.956220159578627\n",
      "---------------------\n",
      "Iteration Number: 4579\n",
      "Loss: 38.15669289249385\n",
      "l2 norm of gradients: 0.2758985042064497\n",
      "l2 norm of weights: 5.956128202538733\n",
      "---------------------\n",
      "Iteration Number: 4580\n",
      "Loss: 38.15287836575542\n",
      "l2 norm of gradients: 0.27585322388565825\n",
      "l2 norm of weights: 5.956036255496422\n",
      "---------------------\n",
      "Iteration Number: 4581\n",
      "Loss: 38.149065084025246\n",
      "l2 norm of gradients: 0.27580796646769395\n",
      "l2 norm of weights: 5.955944318448629\n",
      "---------------------\n",
      "Iteration Number: 4582\n",
      "Loss: 38.145253046477315\n",
      "l2 norm of gradients: 0.2757627319335894\n",
      "l2 norm of weights: 5.95585239139229\n",
      "---------------------\n",
      "Iteration Number: 4583\n",
      "Loss: 38.14144225228248\n",
      "l2 norm of gradients: 0.2757175202643945\n",
      "l2 norm of weights: 5.955760474324345\n",
      "---------------------\n",
      "Iteration Number: 4584\n",
      "Loss: 38.137632700619534\n",
      "l2 norm of gradients: 0.27567233144117625\n",
      "l2 norm of weights: 5.955668567241734\n",
      "---------------------\n",
      "Iteration Number: 4585\n",
      "Loss: 38.133824390681426\n",
      "l2 norm of gradients: 0.27562716544501914\n",
      "l2 norm of weights: 5.955576670141403\n",
      "---------------------\n",
      "Iteration Number: 4586\n",
      "Loss: 38.130017321639926\n",
      "l2 norm of gradients: 0.2755820222570249\n",
      "l2 norm of weights: 5.955484783020295\n",
      "---------------------\n",
      "Iteration Number: 4587\n",
      "Loss: 38.126211492673335\n",
      "l2 norm of gradients: 0.2755369018583125\n",
      "l2 norm of weights: 5.95539290587536\n",
      "---------------------\n",
      "Iteration Number: 4588\n",
      "Loss: 38.12240690296094\n",
      "l2 norm of gradients: 0.27549180423001807\n",
      "l2 norm of weights: 5.95530103870355\n",
      "---------------------\n",
      "Iteration Number: 4589\n",
      "Loss: 38.118603551695415\n",
      "l2 norm of gradients: 0.2754467293532951\n",
      "l2 norm of weights: 5.955209181501815\n",
      "---------------------\n",
      "Iteration Number: 4590\n",
      "Loss: 38.11480143806102\n",
      "l2 norm of gradients: 0.27540167720931424\n",
      "l2 norm of weights: 5.955117334267114\n",
      "---------------------\n",
      "Iteration Number: 4591\n",
      "Loss: 38.111000561223065\n",
      "l2 norm of gradients: 0.2753566477792632\n",
      "l2 norm of weights: 5.955025496996402\n",
      "---------------------\n",
      "Iteration Number: 4592\n",
      "Loss: 38.10720092038501\n",
      "l2 norm of gradients: 0.27531164104434713\n",
      "l2 norm of weights: 5.954933669686641\n",
      "---------------------\n",
      "Iteration Number: 4593\n",
      "Loss: 38.103402514729\n",
      "l2 norm of gradients: 0.27526665698578817\n",
      "l2 norm of weights: 5.954841852334791\n",
      "---------------------\n",
      "Iteration Number: 4594\n",
      "Loss: 38.0996053434455\n",
      "l2 norm of gradients: 0.2752216955848256\n",
      "l2 norm of weights: 5.9547500449378195\n",
      "---------------------\n",
      "Iteration Number: 4595\n",
      "Loss: 38.09580940570862\n",
      "l2 norm of gradients: 0.27517675682271603\n",
      "l2 norm of weights: 5.95465824749269\n",
      "---------------------\n",
      "Iteration Number: 4596\n",
      "Loss: 38.09201470072234\n",
      "l2 norm of gradients: 0.2751318406807331\n",
      "l2 norm of weights: 5.954566459996374\n",
      "---------------------\n",
      "Iteration Number: 4597\n",
      "Loss: 38.088221227661116\n",
      "l2 norm of gradients: 0.27508694714016746\n",
      "l2 norm of weights: 5.954474682445842\n",
      "---------------------\n",
      "Iteration Number: 4598\n",
      "Loss: 38.084428985723996\n",
      "l2 norm of gradients: 0.27504207618232696\n",
      "l2 norm of weights: 5.95438291483807\n",
      "---------------------\n",
      "Iteration Number: 4599\n",
      "Loss: 38.080637974096454\n",
      "l2 norm of gradients: 0.27499722778853664\n",
      "l2 norm of weights: 5.954291157170032\n",
      "---------------------\n",
      "Iteration Number: 4600\n",
      "Loss: 38.07684819198145\n",
      "l2 norm of gradients: 0.27495240194013837\n",
      "l2 norm of weights: 5.954199409438706\n",
      "---------------------\n",
      "Iteration Number: 4601\n",
      "Loss: 38.07305963855926\n",
      "l2 norm of gradients: 0.2749075986184913\n",
      "l2 norm of weights: 5.954107671641074\n",
      "---------------------\n",
      "Iteration Number: 4602\n",
      "Loss: 38.06927231301709\n",
      "l2 norm of gradients: 0.2748628178049715\n",
      "l2 norm of weights: 5.954015943774119\n",
      "---------------------\n",
      "Iteration Number: 4603\n",
      "Loss: 38.0654862145665\n",
      "l2 norm of gradients: 0.27481805948097227\n",
      "l2 norm of weights: 5.953924225834827\n",
      "---------------------\n",
      "Iteration Number: 4604\n",
      "Loss: 38.06170134239263\n",
      "l2 norm of gradients: 0.27477332362790363\n",
      "l2 norm of weights: 5.953832517820183\n",
      "---------------------\n",
      "Iteration Number: 4605\n",
      "Loss: 38.05791769569162\n",
      "l2 norm of gradients: 0.2747286102271927\n",
      "l2 norm of weights: 5.953740819727179\n",
      "---------------------\n",
      "Iteration Number: 4606\n",
      "Loss: 38.05413527366264\n",
      "l2 norm of gradients: 0.27468391926028385\n",
      "l2 norm of weights: 5.9536491315528055\n",
      "---------------------\n",
      "Iteration Number: 4607\n",
      "Loss: 38.05035407549681\n",
      "l2 norm of gradients: 0.27463925070863804\n",
      "l2 norm of weights: 5.953557453294059\n",
      "---------------------\n",
      "Iteration Number: 4608\n",
      "Loss: 38.04657410039873\n",
      "l2 norm of gradients: 0.2745946045537336\n",
      "l2 norm of weights: 5.953465784947935\n",
      "---------------------\n",
      "Iteration Number: 4609\n",
      "Loss: 38.04279534755849\n",
      "l2 norm of gradients: 0.2745499807770654\n",
      "l2 norm of weights: 5.95337412651143\n",
      "---------------------\n",
      "Iteration Number: 4610\n",
      "Loss: 38.03901781617505\n",
      "l2 norm of gradients: 0.2745053793601455\n",
      "l2 norm of weights: 5.953282477981549\n",
      "---------------------\n",
      "Iteration Number: 4611\n",
      "Loss: 38.03524150546942\n",
      "l2 norm of gradients: 0.27446080028450287\n",
      "l2 norm of weights: 5.953190839355296\n",
      "---------------------\n",
      "Iteration Number: 4612\n",
      "Loss: 38.03146641461189\n",
      "l2 norm of gradients: 0.2744162435316834\n",
      "l2 norm of weights: 5.953099210629671\n",
      "---------------------\n",
      "Iteration Number: 4613\n",
      "Loss: 38.027692542824596\n",
      "l2 norm of gradients: 0.27437170908324976\n",
      "l2 norm of weights: 5.953007591801689\n",
      "---------------------\n",
      "Iteration Number: 4614\n",
      "Loss: 38.0239198893087\n",
      "l2 norm of gradients: 0.2743271969207815\n",
      "l2 norm of weights: 5.952915982868355\n",
      "---------------------\n",
      "Iteration Number: 4615\n",
      "Loss: 38.02014845325876\n",
      "l2 norm of gradients: 0.27428270702587526\n",
      "l2 norm of weights: 5.952824383826684\n",
      "---------------------\n",
      "Iteration Number: 4616\n",
      "Loss: 38.01637823387779\n",
      "l2 norm of gradients: 0.2742382393801443\n",
      "l2 norm of weights: 5.95273279467369\n",
      "---------------------\n",
      "Iteration Number: 4617\n",
      "Loss: 38.01260923038712\n",
      "l2 norm of gradients: 0.27419379396521887\n",
      "l2 norm of weights: 5.9526412154063895\n",
      "---------------------\n",
      "Iteration Number: 4618\n",
      "Loss: 38.00884144197774\n",
      "l2 norm of gradients: 0.2741493707627458\n",
      "l2 norm of weights: 5.9525496460218035\n",
      "---------------------\n",
      "Iteration Number: 4619\n",
      "Loss: 38.00507486785458\n",
      "l2 norm of gradients: 0.27410496975438914\n",
      "l2 norm of weights: 5.95245808651695\n",
      "---------------------\n",
      "Iteration Number: 4620\n",
      "Loss: 38.00130950723513\n",
      "l2 norm of gradients: 0.27406059092182944\n",
      "l2 norm of weights: 5.952366536888857\n",
      "---------------------\n",
      "Iteration Number: 4621\n",
      "Loss: 37.99754535932017\n",
      "l2 norm of gradients: 0.2740162342467641\n",
      "l2 norm of weights: 5.952274997134547\n",
      "---------------------\n",
      "Iteration Number: 4622\n",
      "Loss: 37.99378242331761\n",
      "l2 norm of gradients: 0.27397189971090735\n",
      "l2 norm of weights: 5.952183467251051\n",
      "---------------------\n",
      "Iteration Number: 4623\n",
      "Loss: 37.99002069844206\n",
      "l2 norm of gradients: 0.27392758729599015\n",
      "l2 norm of weights: 5.952091947235397\n",
      "---------------------\n",
      "Iteration Number: 4624\n",
      "Loss: 37.98626018390417\n",
      "l2 norm of gradients: 0.27388329698376024\n",
      "l2 norm of weights: 5.952000437084618\n",
      "---------------------\n",
      "Iteration Number: 4625\n",
      "Loss: 37.98250087891144\n",
      "l2 norm of gradients: 0.273839028755982\n",
      "l2 norm of weights: 5.95190893679575\n",
      "---------------------\n",
      "Iteration Number: 4626\n",
      "Loss: 37.978742782668604\n",
      "l2 norm of gradients: 0.27379478259443657\n",
      "l2 norm of weights: 5.95181744636583\n",
      "---------------------\n",
      "Iteration Number: 4627\n",
      "Loss: 37.97498589440677\n",
      "l2 norm of gradients: 0.2737505584809218\n",
      "l2 norm of weights: 5.951725965791896\n",
      "---------------------\n",
      "Iteration Number: 4628\n",
      "Loss: 37.97123021331884\n",
      "l2 norm of gradients: 0.27370635639725227\n",
      "l2 norm of weights: 5.95163449507099\n",
      "---------------------\n",
      "Iteration Number: 4629\n",
      "Loss: 37.96747573863441\n",
      "l2 norm of gradients: 0.27366217632525924\n",
      "l2 norm of weights: 5.951543034200156\n",
      "---------------------\n",
      "Iteration Number: 4630\n",
      "Loss: 37.96372246955944\n",
      "l2 norm of gradients: 0.2736180182467906\n",
      "l2 norm of weights: 5.95145158317644\n",
      "---------------------\n",
      "Iteration Number: 4631\n",
      "Loss: 37.95997040531056\n",
      "l2 norm of gradients: 0.273573882143711\n",
      "l2 norm of weights: 5.951360141996891\n",
      "---------------------\n",
      "Iteration Number: 4632\n",
      "Loss: 37.95621954511139\n",
      "l2 norm of gradients: 0.27352976799790146\n",
      "l2 norm of weights: 5.951268710658558\n",
      "---------------------\n",
      "Iteration Number: 4633\n",
      "Loss: 37.95246988817098\n",
      "l2 norm of gradients: 0.2734856757912598\n",
      "l2 norm of weights: 5.951177289158493\n",
      "---------------------\n",
      "Iteration Number: 4634\n",
      "Loss: 37.94872143371364\n",
      "l2 norm of gradients: 0.27344160550570057\n",
      "l2 norm of weights: 5.951085877493752\n",
      "---------------------\n",
      "Iteration Number: 4635\n",
      "Loss: 37.944974180938964\n",
      "l2 norm of gradients: 0.2733975571231546\n",
      "l2 norm of weights: 5.950994475661392\n",
      "---------------------\n",
      "Iteration Number: 4636\n",
      "Loss: 37.94122812910111\n",
      "l2 norm of gradients: 0.2733535306255698\n",
      "l2 norm of weights: 5.9509030836584715\n",
      "---------------------\n",
      "Iteration Number: 4637\n",
      "Loss: 37.93748327738869\n",
      "l2 norm of gradients: 0.27330952599491015\n",
      "l2 norm of weights: 5.950811701482054\n",
      "---------------------\n",
      "Iteration Number: 4638\n",
      "Loss: 37.933739625043756\n",
      "l2 norm of gradients: 0.2732655432131564\n",
      "l2 norm of weights: 5.9507203291292\n",
      "---------------------\n",
      "Iteration Number: 4639\n",
      "Loss: 37.92999717126841\n",
      "l2 norm of gradients: 0.27322158226230586\n",
      "l2 norm of weights: 5.950628966596976\n",
      "---------------------\n",
      "Iteration Number: 4640\n",
      "Loss: 37.92625591530938\n",
      "l2 norm of gradients: 0.27317764312437237\n",
      "l2 norm of weights: 5.950537613882451\n",
      "---------------------\n",
      "Iteration Number: 4641\n",
      "Loss: 37.92251585636903\n",
      "l2 norm of gradients: 0.2731337257813862\n",
      "l2 norm of weights: 5.950446270982695\n",
      "---------------------\n",
      "Iteration Number: 4642\n",
      "Loss: 37.91877699367805\n",
      "l2 norm of gradients: 0.27308983021539424\n",
      "l2 norm of weights: 5.9503549378947795\n",
      "---------------------\n",
      "Iteration Number: 4643\n",
      "Loss: 37.91503932646278\n",
      "l2 norm of gradients: 0.2730459564084597\n",
      "l2 norm of weights: 5.95026361461578\n",
      "---------------------\n",
      "Iteration Number: 4644\n",
      "Loss: 37.91130285394731\n",
      "l2 norm of gradients: 0.2730021043426626\n",
      "l2 norm of weights: 5.950172301142772\n",
      "---------------------\n",
      "Iteration Number: 4645\n",
      "Loss: 37.907567575358186\n",
      "l2 norm of gradients: 0.272958274000099\n",
      "l2 norm of weights: 5.950080997472835\n",
      "---------------------\n",
      "Iteration Number: 4646\n",
      "Loss: 37.903833489932765\n",
      "l2 norm of gradients: 0.2729144653628817\n",
      "l2 norm of weights: 5.94998970360305\n",
      "---------------------\n",
      "Iteration Number: 4647\n",
      "Loss: 37.90010059687239\n",
      "l2 norm of gradients: 0.2728706784131399\n",
      "l2 norm of weights: 5.9498984195305\n",
      "---------------------\n",
      "Iteration Number: 4648\n",
      "Loss: 37.89636889543144\n",
      "l2 norm of gradients: 0.272826913133019\n",
      "l2 norm of weights: 5.9498071452522705\n",
      "---------------------\n",
      "Iteration Number: 4649\n",
      "Loss: 37.89263838483075\n",
      "l2 norm of gradients: 0.2727831695046811\n",
      "l2 norm of weights: 5.949715880765448\n",
      "---------------------\n",
      "Iteration Number: 4650\n",
      "Loss: 37.88890906429649\n",
      "l2 norm of gradients: 0.27273944751030466\n",
      "l2 norm of weights: 5.949624626067124\n",
      "---------------------\n",
      "Iteration Number: 4651\n",
      "Loss: 37.885180933059985\n",
      "l2 norm of gradients: 0.27269574713208433\n",
      "l2 norm of weights: 5.9495333811543905\n",
      "---------------------\n",
      "Iteration Number: 4652\n",
      "Loss: 37.881453990361805\n",
      "l2 norm of gradients: 0.2726520683522313\n",
      "l2 norm of weights: 5.94944214602434\n",
      "---------------------\n",
      "Iteration Number: 4653\n",
      "Loss: 37.877728235422786\n",
      "l2 norm of gradients: 0.2726084111529731\n",
      "l2 norm of weights: 5.949350920674068\n",
      "---------------------\n",
      "Iteration Number: 4654\n",
      "Loss: 37.8740036674798\n",
      "l2 norm of gradients: 0.2725647755165534\n",
      "l2 norm of weights: 5.949259705100674\n",
      "---------------------\n",
      "Iteration Number: 4655\n",
      "Loss: 37.87028028576295\n",
      "l2 norm of gradients: 0.27252116142523247\n",
      "l2 norm of weights: 5.949168499301259\n",
      "---------------------\n",
      "Iteration Number: 4656\n",
      "Loss: 37.866558089521575\n",
      "l2 norm of gradients: 0.27247756886128693\n",
      "l2 norm of weights: 5.949077303272925\n",
      "---------------------\n",
      "Iteration Number: 4657\n",
      "Loss: 37.86283707797783\n",
      "l2 norm of gradients: 0.2724339978070092\n",
      "l2 norm of weights: 5.948986117012777\n",
      "---------------------\n",
      "Iteration Number: 4658\n",
      "Loss: 37.85911725036199\n",
      "l2 norm of gradients: 0.2723904482447088\n",
      "l2 norm of weights: 5.948894940517921\n",
      "---------------------\n",
      "Iteration Number: 4659\n",
      "Loss: 37.855398605923476\n",
      "l2 norm of gradients: 0.27234692015671075\n",
      "l2 norm of weights: 5.948803773785466\n",
      "---------------------\n",
      "Iteration Number: 4660\n",
      "Loss: 37.85168114389734\n",
      "l2 norm of gradients: 0.2723034135253567\n",
      "l2 norm of weights: 5.948712616812525\n",
      "---------------------\n",
      "Iteration Number: 4661\n",
      "Loss: 37.84796486351896\n",
      "l2 norm of gradients: 0.27225992833300466\n",
      "l2 norm of weights: 5.948621469596209\n",
      "---------------------\n",
      "Iteration Number: 4662\n",
      "Loss: 37.84424976403025\n",
      "l2 norm of gradients: 0.2722164645620287\n",
      "l2 norm of weights: 5.948530332133635\n",
      "---------------------\n",
      "Iteration Number: 4663\n",
      "Loss: 37.84053584466167\n",
      "l2 norm of gradients: 0.27217302219481904\n",
      "l2 norm of weights: 5.948439204421921\n",
      "---------------------\n",
      "Iteration Number: 4664\n",
      "Loss: 37.83682310466429\n",
      "l2 norm of gradients: 0.27212960121378227\n",
      "l2 norm of weights: 5.948348086458185\n",
      "---------------------\n",
      "Iteration Number: 4665\n",
      "Loss: 37.83311154328045\n",
      "l2 norm of gradients: 0.27208620160134117\n",
      "l2 norm of weights: 5.94825697823955\n",
      "---------------------\n",
      "Iteration Number: 4666\n",
      "Loss: 37.82940115974663\n",
      "l2 norm of gradients: 0.27204282333993457\n",
      "l2 norm of weights: 5.948165879763138\n",
      "---------------------\n",
      "Iteration Number: 4667\n",
      "Loss: 37.82569195330329\n",
      "l2 norm of gradients: 0.27199946641201744\n",
      "l2 norm of weights: 5.9480747910260785\n",
      "---------------------\n",
      "Iteration Number: 4668\n",
      "Loss: 37.821983923195376\n",
      "l2 norm of gradients: 0.27195613080006137\n",
      "l2 norm of weights: 5.947983712025496\n",
      "---------------------\n",
      "Iteration Number: 4669\n",
      "Loss: 37.81827706866958\n",
      "l2 norm of gradients: 0.27191281648655347\n",
      "l2 norm of weights: 5.947892642758524\n",
      "---------------------\n",
      "Iteration Number: 4670\n",
      "Loss: 37.81457138896995\n",
      "l2 norm of gradients: 0.2718695234539974\n",
      "l2 norm of weights: 5.947801583222294\n",
      "---------------------\n",
      "Iteration Number: 4671\n",
      "Loss: 37.81086688334003\n",
      "l2 norm of gradients: 0.2718262516849127\n",
      "l2 norm of weights: 5.947710533413939\n",
      "---------------------\n",
      "Iteration Number: 4672\n",
      "Loss: 37.80716355102478\n",
      "l2 norm of gradients: 0.27178300116183507\n",
      "l2 norm of weights: 5.947619493330597\n",
      "---------------------\n",
      "Iteration Number: 4673\n",
      "Loss: 37.8034613912812\n",
      "l2 norm of gradients: 0.27173977186731646\n",
      "l2 norm of weights: 5.947528462969406\n",
      "---------------------\n",
      "Iteration Number: 4674\n",
      "Loss: 37.79976040334143\n",
      "l2 norm of gradients: 0.27169656378392465\n",
      "l2 norm of weights: 5.947437442327508\n",
      "---------------------\n",
      "Iteration Number: 4675\n",
      "Loss: 37.796060586472976\n",
      "l2 norm of gradients: 0.2716533768942437\n",
      "l2 norm of weights: 5.947346431402044\n",
      "---------------------\n",
      "Iteration Number: 4676\n",
      "Loss: 37.7923619399077\n",
      "l2 norm of gradients: 0.27161021118087364\n",
      "l2 norm of weights: 5.947255430190162\n",
      "---------------------\n",
      "Iteration Number: 4677\n",
      "Loss: 37.78866446290794\n",
      "l2 norm of gradients: 0.2715670666264306\n",
      "l2 norm of weights: 5.947164438689006\n",
      "---------------------\n",
      "Iteration Number: 4678\n",
      "Loss: 37.784968154715855\n",
      "l2 norm of gradients: 0.27152394321354645\n",
      "l2 norm of weights: 5.947073456895727\n",
      "---------------------\n",
      "Iteration Number: 4679\n",
      "Loss: 37.78127301457376\n",
      "l2 norm of gradients: 0.27148084092486946\n",
      "l2 norm of weights: 5.946982484807476\n",
      "---------------------\n",
      "Iteration Number: 4680\n",
      "Loss: 37.777579041764696\n",
      "l2 norm of gradients: 0.27143775974306367\n",
      "l2 norm of weights: 5.946891522421406\n",
      "---------------------\n",
      "Iteration Number: 4681\n",
      "Loss: 37.773886235504946\n",
      "l2 norm of gradients: 0.2713946996508092\n",
      "l2 norm of weights: 5.946800569734673\n",
      "---------------------\n",
      "Iteration Number: 4682\n",
      "Loss: 37.77019459507779\n",
      "l2 norm of gradients: 0.2713516606308021\n",
      "l2 norm of weights: 5.946709626744434\n",
      "---------------------\n",
      "Iteration Number: 4683\n",
      "Loss: 37.76650411972654\n",
      "l2 norm of gradients: 0.27130864266575444\n",
      "l2 norm of weights: 5.94661869344785\n",
      "---------------------\n",
      "Iteration Number: 4684\n",
      "Loss: 37.76281480869905\n",
      "l2 norm of gradients: 0.27126564573839396\n",
      "l2 norm of weights: 5.946527769842081\n",
      "---------------------\n",
      "Iteration Number: 4685\n",
      "Loss: 37.75912666125281\n",
      "l2 norm of gradients: 0.27122266983146487\n",
      "l2 norm of weights: 5.946436855924293\n",
      "---------------------\n",
      "Iteration Number: 4686\n",
      "Loss: 37.755439676651086\n",
      "l2 norm of gradients: 0.2711797149277268\n",
      "l2 norm of weights: 5.94634595169165\n",
      "---------------------\n",
      "Iteration Number: 4687\n",
      "Loss: 37.751753854150365\n",
      "l2 norm of gradients: 0.27113678100995553\n",
      "l2 norm of weights: 5.946255057141319\n",
      "---------------------\n",
      "Iteration Number: 4688\n",
      "Loss: 37.74806919301067\n",
      "l2 norm of gradients: 0.2710938680609426\n",
      "l2 norm of weights: 5.946164172270473\n",
      "---------------------\n",
      "Iteration Number: 4689\n",
      "Loss: 37.74438569247969\n",
      "l2 norm of gradients: 0.27105097606349554\n",
      "l2 norm of weights: 5.946073297076283\n",
      "---------------------\n",
      "Iteration Number: 4690\n",
      "Loss: 37.74070335183061\n",
      "l2 norm of gradients: 0.27100810500043787\n",
      "l2 norm of weights: 5.945982431555922\n",
      "---------------------\n",
      "Iteration Number: 4691\n",
      "Loss: 37.73702217030376\n",
      "l2 norm of gradients: 0.2709652548546087\n",
      "l2 norm of weights: 5.945891575706568\n",
      "---------------------\n",
      "Iteration Number: 4692\n",
      "Loss: 37.73334214718096\n",
      "l2 norm of gradients: 0.27092242560886304\n",
      "l2 norm of weights: 5.9458007295253985\n",
      "---------------------\n",
      "Iteration Number: 4693\n",
      "Loss: 37.7296632817163\n",
      "l2 norm of gradients: 0.27087961724607185\n",
      "l2 norm of weights: 5.945709893009592\n",
      "---------------------\n",
      "Iteration Number: 4694\n",
      "Loss: 37.72598557316336\n",
      "l2 norm of gradients: 0.2708368297491219\n",
      "l2 norm of weights: 5.945619066156334\n",
      "---------------------\n",
      "Iteration Number: 4695\n",
      "Loss: 37.72230902080109\n",
      "l2 norm of gradients: 0.2707940631009156\n",
      "l2 norm of weights: 5.945528248962809\n",
      "---------------------\n",
      "Iteration Number: 4696\n",
      "Loss: 37.718633623879285\n",
      "l2 norm of gradients: 0.27075131728437135\n",
      "l2 norm of weights: 5.945437441426201\n",
      "---------------------\n",
      "Iteration Number: 4697\n",
      "Loss: 37.714959381671406\n",
      "l2 norm of gradients: 0.27070859228242317\n",
      "l2 norm of weights: 5.945346643543699\n",
      "---------------------\n",
      "Iteration Number: 4698\n",
      "Loss: 37.71128629343487\n",
      "l2 norm of gradients: 0.27066588807802094\n",
      "l2 norm of weights: 5.945255855312495\n",
      "---------------------\n",
      "Iteration Number: 4699\n",
      "Loss: 37.70761435844012\n",
      "l2 norm of gradients: 0.2706232046541302\n",
      "l2 norm of weights: 5.945165076729781\n",
      "---------------------\n",
      "Iteration Number: 4700\n",
      "Loss: 37.70394357594596\n",
      "l2 norm of gradients: 0.2705805419937324\n",
      "l2 norm of weights: 5.945074307792752\n",
      "---------------------\n",
      "Iteration Number: 4701\n",
      "Loss: 37.700273945237626\n",
      "l2 norm of gradients: 0.2705379000798244\n",
      "l2 norm of weights: 5.944983548498604\n",
      "---------------------\n",
      "Iteration Number: 4702\n",
      "Loss: 37.69660546556482\n",
      "l2 norm of gradients: 0.27049527889541924\n",
      "l2 norm of weights: 5.944892798844537\n",
      "---------------------\n",
      "Iteration Number: 4703\n",
      "Loss: 37.69293813619843\n",
      "l2 norm of gradients: 0.2704526784235452\n",
      "l2 norm of weights: 5.94480205882775\n",
      "---------------------\n",
      "Iteration Number: 4704\n",
      "Loss: 37.689271956413926\n",
      "l2 norm of gradients: 0.2704100986472465\n",
      "l2 norm of weights: 5.944711328445448\n",
      "---------------------\n",
      "Iteration Number: 4705\n",
      "Loss: 37.685606925479895\n",
      "l2 norm of gradients: 0.27036753954958304\n",
      "l2 norm of weights: 5.9446206076948345\n",
      "---------------------\n",
      "Iteration Number: 4706\n",
      "Loss: 37.68194304266114\n",
      "l2 norm of gradients: 0.2703250011136302\n",
      "l2 norm of weights: 5.9445298965731155\n",
      "---------------------\n",
      "Iteration Number: 4707\n",
      "Loss: 37.67828030724279\n",
      "l2 norm of gradients: 0.2702824833224792\n",
      "l2 norm of weights: 5.944439195077502\n",
      "---------------------\n",
      "Iteration Number: 4708\n",
      "Loss: 37.67461871848552\n",
      "l2 norm of gradients: 0.2702399861592367\n",
      "l2 norm of weights: 5.944348503205204\n",
      "---------------------\n",
      "Iteration Number: 4709\n",
      "Loss: 37.670958275661086\n",
      "l2 norm of gradients: 0.2701975096070253\n",
      "l2 norm of weights: 5.944257820953435\n",
      "---------------------\n",
      "Iteration Number: 4710\n",
      "Loss: 37.667298978044684\n",
      "l2 norm of gradients: 0.27015505364898296\n",
      "l2 norm of weights: 5.944167148319409\n",
      "---------------------\n",
      "Iteration Number: 4711\n",
      "Loss: 37.66364082491448\n",
      "l2 norm of gradients: 0.270112618268263\n",
      "l2 norm of weights: 5.944076485300344\n",
      "---------------------\n",
      "Iteration Number: 4712\n",
      "Loss: 37.65998381554487\n",
      "l2 norm of gradients: 0.27007020344803495\n",
      "l2 norm of weights: 5.943985831893459\n",
      "---------------------\n",
      "Iteration Number: 4713\n",
      "Loss: 37.656327949205206\n",
      "l2 norm of gradients: 0.2700278091714835\n",
      "l2 norm of weights: 5.943895188095974\n",
      "---------------------\n",
      "Iteration Number: 4714\n",
      "Loss: 37.652673225180955\n",
      "l2 norm of gradients: 0.2699854354218088\n",
      "l2 norm of weights: 5.943804553905112\n",
      "---------------------\n",
      "Iteration Number: 4715\n",
      "Loss: 37.64901964273963\n",
      "l2 norm of gradients: 0.26994308218222696\n",
      "l2 norm of weights: 5.9437139293181005\n",
      "---------------------\n",
      "Iteration Number: 4716\n",
      "Loss: 37.645367201169265\n",
      "l2 norm of gradients: 0.2699007494359691\n",
      "l2 norm of weights: 5.9436233143321635\n",
      "---------------------\n",
      "Iteration Number: 4717\n",
      "Loss: 37.64171589973573\n",
      "l2 norm of gradients: 0.26985843716628233\n",
      "l2 norm of weights: 5.943532708944531\n",
      "---------------------\n",
      "Iteration Number: 4718\n",
      "Loss: 37.63806573772859\n",
      "l2 norm of gradients: 0.269816145356429\n",
      "l2 norm of weights: 5.943442113152435\n",
      "---------------------\n",
      "Iteration Number: 4719\n",
      "Loss: 37.63441671441725\n",
      "l2 norm of gradients: 0.26977387398968705\n",
      "l2 norm of weights: 5.943351526953108\n",
      "---------------------\n",
      "Iteration Number: 4720\n",
      "Loss: 37.630768829090556\n",
      "l2 norm of gradients: 0.26973162304934983\n",
      "l2 norm of weights: 5.943260950343785\n",
      "---------------------\n",
      "Iteration Number: 4721\n",
      "Loss: 37.62712208103355\n",
      "l2 norm of gradients: 0.26968939251872615\n",
      "l2 norm of weights: 5.943170383321703\n",
      "---------------------\n",
      "Iteration Number: 4722\n",
      "Loss: 37.62347646951062\n",
      "l2 norm of gradients: 0.26964718238114044\n",
      "l2 norm of weights: 5.9430798258841016\n",
      "---------------------\n",
      "Iteration Number: 4723\n",
      "Loss: 37.61983199382504\n",
      "l2 norm of gradients: 0.26960499261993237\n",
      "l2 norm of weights: 5.942989278028221\n",
      "---------------------\n",
      "Iteration Number: 4724\n",
      "Loss: 37.61618865325092\n",
      "l2 norm of gradients: 0.2695628232184572\n",
      "l2 norm of weights: 5.942898739751304\n",
      "---------------------\n",
      "Iteration Number: 4725\n",
      "Loss: 37.61254644706795\n",
      "l2 norm of gradients: 0.2695206741600855\n",
      "l2 norm of weights: 5.942808211050596\n",
      "---------------------\n",
      "Iteration Number: 4726\n",
      "Loss: 37.60890537456016\n",
      "l2 norm of gradients: 0.2694785454282033\n",
      "l2 norm of weights: 5.942717691923344\n",
      "---------------------\n",
      "Iteration Number: 4727\n",
      "Loss: 37.60526543501891\n",
      "l2 norm of gradients: 0.2694364370062119\n",
      "l2 norm of weights: 5.942627182366799\n",
      "---------------------\n",
      "Iteration Number: 4728\n",
      "Loss: 37.60162662773523\n",
      "l2 norm of gradients: 0.26939434887752817\n",
      "l2 norm of weights: 5.942536682378209\n",
      "---------------------\n",
      "Iteration Number: 4729\n",
      "Loss: 37.597988951973754\n",
      "l2 norm of gradients: 0.26935228102558423\n",
      "l2 norm of weights: 5.942446191954827\n",
      "---------------------\n",
      "Iteration Number: 4730\n",
      "Loss: 37.59435240704697\n",
      "l2 norm of gradients: 0.26931023343382754\n",
      "l2 norm of weights: 5.942355711093909\n",
      "---------------------\n",
      "Iteration Number: 4731\n",
      "Loss: 37.590716992233666\n",
      "l2 norm of gradients: 0.26926820608572094\n",
      "l2 norm of weights: 5.942265239792714\n",
      "---------------------\n",
      "Iteration Number: 4732\n",
      "Loss: 37.58708270681312\n",
      "l2 norm of gradients: 0.26922619896474276\n",
      "l2 norm of weights: 5.9421747780484955\n",
      "---------------------\n",
      "Iteration Number: 4733\n",
      "Loss: 37.58344955008601\n",
      "l2 norm of gradients: 0.2691842120543862\n",
      "l2 norm of weights: 5.942084325858518\n",
      "---------------------\n",
      "Iteration Number: 4734\n",
      "Loss: 37.579817521339145\n",
      "l2 norm of gradients: 0.2691422453381602\n",
      "l2 norm of weights: 5.941993883220044\n",
      "---------------------\n",
      "Iteration Number: 4735\n",
      "Loss: 37.5761866198663\n",
      "l2 norm of gradients: 0.26910029879958886\n",
      "l2 norm of weights: 5.941903450130338\n",
      "---------------------\n",
      "Iteration Number: 4736\n",
      "Loss: 37.57255684495029\n",
      "l2 norm of gradients: 0.26905837242221137\n",
      "l2 norm of weights: 5.941813026586667\n",
      "---------------------\n",
      "Iteration Number: 4737\n",
      "Loss: 37.56892819588681\n",
      "l2 norm of gradients: 0.26901646618958247\n",
      "l2 norm of weights: 5.941722612586298\n",
      "---------------------\n",
      "Iteration Number: 4738\n",
      "Loss: 37.56530067197904\n",
      "l2 norm of gradients: 0.2689745800852721\n",
      "l2 norm of weights: 5.941632208126503\n",
      "---------------------\n",
      "Iteration Number: 4739\n",
      "Loss: 37.561674272500845\n",
      "l2 norm of gradients: 0.2689327140928651\n",
      "l2 norm of weights: 5.941541813204553\n",
      "---------------------\n",
      "Iteration Number: 4740\n",
      "Loss: 37.55804899676612\n",
      "l2 norm of gradients: 0.26889086819596203\n",
      "l2 norm of weights: 5.941451427817725\n",
      "---------------------\n",
      "Iteration Number: 4741\n",
      "Loss: 37.55442484404801\n",
      "l2 norm of gradients: 0.26884904237817836\n",
      "l2 norm of weights: 5.941361051963294\n",
      "---------------------\n",
      "Iteration Number: 4742\n",
      "Loss: 37.55080181366186\n",
      "l2 norm of gradients: 0.2688072366231448\n",
      "l2 norm of weights: 5.941270685638538\n",
      "---------------------\n",
      "Iteration Number: 4743\n",
      "Loss: 37.54717990489996\n",
      "l2 norm of gradients: 0.26876545091450726\n",
      "l2 norm of weights: 5.941180328840739\n",
      "---------------------\n",
      "Iteration Number: 4744\n",
      "Loss: 37.543559117046044\n",
      "l2 norm of gradients: 0.26872368523592693\n",
      "l2 norm of weights: 5.941089981567178\n",
      "---------------------\n",
      "Iteration Number: 4745\n",
      "Loss: 37.53993944941004\n",
      "l2 norm of gradients: 0.26868193957108\n",
      "l2 norm of weights: 5.940999643815139\n",
      "---------------------\n",
      "Iteration Number: 4746\n",
      "Loss: 37.536320901287176\n",
      "l2 norm of gradients: 0.26864021390365794\n",
      "l2 norm of weights: 5.940909315581907\n",
      "---------------------\n",
      "Iteration Number: 4747\n",
      "Loss: 37.53270347196989\n",
      "l2 norm of gradients: 0.26859850821736725\n",
      "l2 norm of weights: 5.940818996864774\n",
      "---------------------\n",
      "Iteration Number: 4748\n",
      "Loss: 37.52908716077256\n",
      "l2 norm of gradients: 0.26855682249592966\n",
      "l2 norm of weights: 5.940728687661026\n",
      "---------------------\n",
      "Iteration Number: 4749\n",
      "Loss: 37.52547196697862\n",
      "l2 norm of gradients: 0.2685151567230819\n",
      "l2 norm of weights: 5.940638387967956\n",
      "---------------------\n",
      "Iteration Number: 4750\n",
      "Loss: 37.521857889898776\n",
      "l2 norm of gradients: 0.268473510882576\n",
      "l2 norm of weights: 5.94054809778286\n",
      "---------------------\n",
      "Iteration Number: 4751\n",
      "Loss: 37.51824492882747\n",
      "l2 norm of gradients: 0.2684318849581788\n",
      "l2 norm of weights: 5.940457817103031\n",
      "---------------------\n",
      "Iteration Number: 4752\n",
      "Loss: 37.51463308308197\n",
      "l2 norm of gradients: 0.2683902789336725\n",
      "l2 norm of weights: 5.94036754592577\n",
      "---------------------\n",
      "Iteration Number: 4753\n",
      "Loss: 37.51102235194551\n",
      "l2 norm of gradients: 0.268348692792854\n",
      "l2 norm of weights: 5.940277284248372\n",
      "---------------------\n",
      "Iteration Number: 4754\n",
      "Loss: 37.50741273473356\n",
      "l2 norm of gradients: 0.26830712651953564\n",
      "l2 norm of weights: 5.940187032068143\n",
      "---------------------\n",
      "Iteration Number: 4755\n",
      "Loss: 37.50380423074341\n",
      "l2 norm of gradients: 0.26826558009754464\n",
      "l2 norm of weights: 5.940096789382384\n",
      "---------------------\n",
      "Iteration Number: 4756\n",
      "Loss: 37.50019683928276\n",
      "l2 norm of gradients: 0.26822405351072315\n",
      "l2 norm of weights: 5.9400065561884015\n",
      "---------------------\n",
      "Iteration Number: 4757\n",
      "Loss: 37.496590559664256\n",
      "l2 norm of gradients: 0.26818254674292835\n",
      "l2 norm of weights: 5.939916332483502\n",
      "---------------------\n",
      "Iteration Number: 4758\n",
      "Loss: 37.49298539118411\n",
      "l2 norm of gradients: 0.26814105977803265\n",
      "l2 norm of weights: 5.939826118264994\n",
      "---------------------\n",
      "Iteration Number: 4759\n",
      "Loss: 37.489381333147385\n",
      "l2 norm of gradients: 0.2680995925999233\n",
      "l2 norm of weights: 5.939735913530192\n",
      "---------------------\n",
      "Iteration Number: 4760\n",
      "Loss: 37.48577838487175\n",
      "l2 norm of gradients: 0.26805814519250226\n",
      "l2 norm of weights: 5.939645718276404\n",
      "---------------------\n",
      "Iteration Number: 4761\n",
      "Loss: 37.48217654565681\n",
      "l2 norm of gradients: 0.26801671753968687\n",
      "l2 norm of weights: 5.93955553250095\n",
      "---------------------\n",
      "Iteration Number: 4762\n",
      "Loss: 37.47857581481697\n",
      "l2 norm of gradients: 0.2679753096254093\n",
      "l2 norm of weights: 5.939465356201144\n",
      "---------------------\n",
      "Iteration Number: 4763\n",
      "Loss: 37.47497619165399\n",
      "l2 norm of gradients: 0.2679339214336165\n",
      "l2 norm of weights: 5.939375189374305\n",
      "---------------------\n",
      "Iteration Number: 4764\n",
      "Loss: 37.47137767548935\n",
      "l2 norm of gradients: 0.2678925529482704\n",
      "l2 norm of weights: 5.939285032017754\n",
      "---------------------\n",
      "Iteration Number: 4765\n",
      "Loss: 37.467780265626764\n",
      "l2 norm of gradients: 0.2678512041533481\n",
      "l2 norm of weights: 5.939194884128813\n",
      "---------------------\n",
      "Iteration Number: 4766\n",
      "Loss: 37.46418396136847\n",
      "l2 norm of gradients: 0.2678098750328412\n",
      "l2 norm of weights: 5.939104745704808\n",
      "---------------------\n",
      "Iteration Number: 4767\n",
      "Loss: 37.46058876203861\n",
      "l2 norm of gradients: 0.26776856557075646\n",
      "l2 norm of weights: 5.939014616743064\n",
      "---------------------\n",
      "Iteration Number: 4768\n",
      "Loss: 37.45699466695746\n",
      "l2 norm of gradients: 0.26772727575111543\n",
      "l2 norm of weights: 5.938924497240909\n",
      "---------------------\n",
      "Iteration Number: 4769\n",
      "Loss: 37.4534016754204\n",
      "l2 norm of gradients: 0.2676860055579545\n",
      "l2 norm of weights: 5.938834387195674\n",
      "---------------------\n",
      "Iteration Number: 4770\n",
      "Loss: 37.44980978674634\n",
      "l2 norm of gradients: 0.267644754975325\n",
      "l2 norm of weights: 5.93874428660469\n",
      "---------------------\n",
      "Iteration Number: 4771\n",
      "Loss: 37.446219000258985\n",
      "l2 norm of gradients: 0.267603523987293\n",
      "l2 norm of weights: 5.938654195465292\n",
      "---------------------\n",
      "Iteration Number: 4772\n",
      "Loss: 37.44262931526228\n",
      "l2 norm of gradients: 0.2675623125779394\n",
      "l2 norm of weights: 5.938564113774815\n",
      "---------------------\n",
      "Iteration Number: 4773\n",
      "Loss: 37.439040731075785\n",
      "l2 norm of gradients: 0.26752112073136003\n",
      "l2 norm of weights: 5.938474041530597\n",
      "---------------------\n",
      "Iteration Number: 4774\n",
      "Loss: 37.43545324701869\n",
      "l2 norm of gradients: 0.2674799484316653\n",
      "l2 norm of weights: 5.938383978729978\n",
      "---------------------\n",
      "Iteration Number: 4775\n",
      "Loss: 37.43186686240985\n",
      "l2 norm of gradients: 0.2674387956629807\n",
      "l2 norm of weights: 5.9382939253702975\n",
      "---------------------\n",
      "Iteration Number: 4776\n",
      "Loss: 37.428281576563165\n",
      "l2 norm of gradients: 0.2673976624094462\n",
      "l2 norm of weights: 5.9382038814489\n",
      "---------------------\n",
      "Iteration Number: 4777\n",
      "Loss: 37.42469738879907\n",
      "l2 norm of gradients: 0.26735654865521674\n",
      "l2 norm of weights: 5.938113846963131\n",
      "---------------------\n",
      "Iteration Number: 4778\n",
      "Loss: 37.421114298435455\n",
      "l2 norm of gradients: 0.267315454384462\n",
      "l2 norm of weights: 5.938023821910337\n",
      "---------------------\n",
      "Iteration Number: 4779\n",
      "Loss: 37.417532304787585\n",
      "l2 norm of gradients: 0.2672743795813663\n",
      "l2 norm of weights: 5.937933806287867\n",
      "---------------------\n",
      "Iteration Number: 4780\n",
      "Loss: 37.41395140718465\n",
      "l2 norm of gradients: 0.26723332423012874\n",
      "l2 norm of weights: 5.937843800093072\n",
      "---------------------\n",
      "Iteration Number: 4781\n",
      "Loss: 37.410371604941936\n",
      "l2 norm of gradients: 0.2671922883149631\n",
      "l2 norm of weights: 5.937753803323304\n",
      "---------------------\n",
      "Iteration Number: 4782\n",
      "Loss: 37.40679289738273\n",
      "l2 norm of gradients: 0.26715127182009785\n",
      "l2 norm of weights: 5.937663815975918\n",
      "---------------------\n",
      "Iteration Number: 4783\n",
      "Loss: 37.403215283830285\n",
      "l2 norm of gradients: 0.2671102747297763\n",
      "l2 norm of weights: 5.937573838048269\n",
      "---------------------\n",
      "Iteration Number: 4784\n",
      "Loss: 37.399638763606625\n",
      "l2 norm of gradients: 0.2670692970282562\n",
      "l2 norm of weights: 5.937483869537718\n",
      "---------------------\n",
      "Iteration Number: 4785\n",
      "Loss: 37.39606333603201\n",
      "l2 norm of gradients: 0.2670283386998102\n",
      "l2 norm of weights: 5.937393910441622\n",
      "---------------------\n",
      "Iteration Number: 4786\n",
      "Loss: 37.39248900044126\n",
      "l2 norm of gradients: 0.2669873997287255\n",
      "l2 norm of weights: 5.937303960757344\n",
      "---------------------\n",
      "Iteration Number: 4787\n",
      "Loss: 37.38891575615457\n",
      "l2 norm of gradients: 0.2669464800993038\n",
      "l2 norm of weights: 5.937214020482247\n",
      "---------------------\n",
      "Iteration Number: 4788\n",
      "Loss: 37.385343602484205\n",
      "l2 norm of gradients: 0.26690557979586166\n",
      "l2 norm of weights: 5.937124089613698\n",
      "---------------------\n",
      "Iteration Number: 4789\n",
      "Loss: 37.381772538779806\n",
      "l2 norm of gradients: 0.26686469880273\n",
      "l2 norm of weights: 5.937034168149064\n",
      "---------------------\n",
      "Iteration Number: 4790\n",
      "Loss: 37.378202564346566\n",
      "l2 norm of gradients: 0.2668238371042548\n",
      "l2 norm of weights: 5.936944256085714\n",
      "---------------------\n",
      "Iteration Number: 4791\n",
      "Loss: 37.37463367852541\n",
      "l2 norm of gradients: 0.2667829946847961\n",
      "l2 norm of weights: 5.936854353421019\n",
      "---------------------\n",
      "Iteration Number: 4792\n",
      "Loss: 37.371065880641204\n",
      "l2 norm of gradients: 0.2667421715287289\n",
      "l2 norm of weights: 5.936764460152352\n",
      "---------------------\n",
      "Iteration Number: 4793\n",
      "Loss: 37.36749917002076\n",
      "l2 norm of gradients: 0.26670136762044255\n",
      "l2 norm of weights: 5.936674576277087\n",
      "---------------------\n",
      "Iteration Number: 4794\n",
      "Loss: 37.363933545993824\n",
      "l2 norm of gradients: 0.266660582944341\n",
      "l2 norm of weights: 5.936584701792602\n",
      "---------------------\n",
      "Iteration Number: 4795\n",
      "Loss: 37.360369007886796\n",
      "l2 norm of gradients: 0.2666198174848427\n",
      "l2 norm of weights: 5.936494836696275\n",
      "---------------------\n",
      "Iteration Number: 4796\n",
      "Loss: 37.35680555504525\n",
      "l2 norm of gradients: 0.26657907122638086\n",
      "l2 norm of weights: 5.936404980985484\n",
      "---------------------\n",
      "Iteration Number: 4797\n",
      "Loss: 37.35324318678585\n",
      "l2 norm of gradients: 0.26653834415340294\n",
      "l2 norm of weights: 5.936315134657615\n",
      "---------------------\n",
      "Iteration Number: 4798\n",
      "Loss: 37.34968190243995\n",
      "l2 norm of gradients: 0.26649763625037104\n",
      "l2 norm of weights: 5.936225297710051\n",
      "---------------------\n",
      "Iteration Number: 4799\n",
      "Loss: 37.34612170135155\n",
      "l2 norm of gradients: 0.2664569475017617\n",
      "l2 norm of weights: 5.936135470140174\n",
      "---------------------\n",
      "Iteration Number: 4800\n",
      "Loss: 37.34256258284302\n",
      "l2 norm of gradients: 0.266416277892066\n",
      "l2 norm of weights: 5.936045651945376\n",
      "---------------------\n",
      "Iteration Number: 4801\n",
      "Loss: 37.33900454625368\n",
      "l2 norm of gradients: 0.26637562740578935\n",
      "l2 norm of weights: 5.935955843123044\n",
      "---------------------\n",
      "Iteration Number: 4802\n",
      "Loss: 37.335447590910896\n",
      "l2 norm of gradients: 0.2663349960274519\n",
      "l2 norm of weights: 5.93586604367057\n",
      "---------------------\n",
      "Iteration Number: 4803\n",
      "Loss: 37.33189171616796\n",
      "l2 norm of gradients: 0.26629438374158804\n",
      "l2 norm of weights: 5.935776253585347\n",
      "---------------------\n",
      "Iteration Number: 4804\n",
      "Loss: 37.328336921339506\n",
      "l2 norm of gradients: 0.26625379053274645\n",
      "l2 norm of weights: 5.93568647286477\n",
      "---------------------\n",
      "Iteration Number: 4805\n",
      "Loss: 37.324783205768426\n",
      "l2 norm of gradients: 0.2662132163854905\n",
      "l2 norm of weights: 5.935596701506234\n",
      "---------------------\n",
      "Iteration Number: 4806\n",
      "Loss: 37.321230568794334\n",
      "l2 norm of gradients: 0.26617266128439787\n",
      "l2 norm of weights: 5.93550693950714\n",
      "---------------------\n",
      "Iteration Number: 4807\n",
      "Loss: 37.317679009757725\n",
      "l2 norm of gradients: 0.2661321252140607\n",
      "l2 norm of weights: 5.935417186864887\n",
      "---------------------\n",
      "Iteration Number: 4808\n",
      "Loss: 37.31412852799145\n",
      "l2 norm of gradients: 0.26609160815908545\n",
      "l2 norm of weights: 5.935327443576877\n",
      "---------------------\n",
      "Iteration Number: 4809\n",
      "Loss: 37.310579122837055\n",
      "l2 norm of gradients: 0.2660511101040928\n",
      "l2 norm of weights: 5.935237709640514\n",
      "---------------------\n",
      "Iteration Number: 4810\n",
      "Loss: 37.30703079362668\n",
      "l2 norm of gradients: 0.266010631033718\n",
      "l2 norm of weights: 5.935147985053202\n",
      "---------------------\n",
      "Iteration Number: 4811\n",
      "Loss: 37.30348353971509\n",
      "l2 norm of gradients: 0.2659701709326107\n",
      "l2 norm of weights: 5.935058269812352\n",
      "---------------------\n",
      "Iteration Number: 4812\n",
      "Loss: 37.299937360436694\n",
      "l2 norm of gradients: 0.26592972978543467\n",
      "l2 norm of weights: 5.934968563915371\n",
      "---------------------\n",
      "Iteration Number: 4813\n",
      "Loss: 37.2963922551182\n",
      "l2 norm of gradients: 0.2658893075768682\n",
      "l2 norm of weights: 5.93487886735967\n",
      "---------------------\n",
      "Iteration Number: 4814\n",
      "Loss: 37.29284822312524\n",
      "l2 norm of gradients: 0.26584890429160357\n",
      "l2 norm of weights: 5.934789180142663\n",
      "---------------------\n",
      "Iteration Number: 4815\n",
      "Loss: 37.28930526378305\n",
      "l2 norm of gradients: 0.2658085199143479\n",
      "l2 norm of weights: 5.934699502261762\n",
      "---------------------\n",
      "Iteration Number: 4816\n",
      "Loss: 37.285763376442645\n",
      "l2 norm of gradients: 0.26576815442982216\n",
      "l2 norm of weights: 5.934609833714386\n",
      "---------------------\n",
      "Iteration Number: 4817\n",
      "Loss: 37.2822225604436\n",
      "l2 norm of gradients: 0.2657278078227616\n",
      "l2 norm of weights: 5.934520174497954\n",
      "---------------------\n",
      "Iteration Number: 4818\n",
      "Loss: 37.2786828151421\n",
      "l2 norm of gradients: 0.2656874800779161\n",
      "l2 norm of weights: 5.934430524609883\n",
      "---------------------\n",
      "Iteration Number: 4819\n",
      "Loss: 37.27514413986046\n",
      "l2 norm of gradients: 0.2656471711800493\n",
      "l2 norm of weights: 5.9343408840475975\n",
      "---------------------\n",
      "Iteration Number: 4820\n",
      "Loss: 37.27160653397146\n",
      "l2 norm of gradients: 0.26560688111393954\n",
      "l2 norm of weights: 5.93425125280852\n",
      "---------------------\n",
      "Iteration Number: 4821\n",
      "Loss: 37.268069996798815\n",
      "l2 norm of gradients: 0.2655666098643791\n",
      "l2 norm of weights: 5.934161630890074\n",
      "---------------------\n",
      "Iteration Number: 4822\n",
      "Loss: 37.264534527699425\n",
      "l2 norm of gradients: 0.26552635741617436\n",
      "l2 norm of weights: 5.93407201828969\n",
      "---------------------\n",
      "Iteration Number: 4823\n",
      "Loss: 37.2610001260206\n",
      "l2 norm of gradients: 0.2654861237541464\n",
      "l2 norm of weights: 5.933982415004794\n",
      "---------------------\n",
      "Iteration Number: 4824\n",
      "Loss: 37.25746679111554\n",
      "l2 norm of gradients: 0.26544590886313\n",
      "l2 norm of weights: 5.93389282103282\n",
      "---------------------\n",
      "Iteration Number: 4825\n",
      "Loss: 37.253934522319796\n",
      "l2 norm of gradients: 0.26540571272797436\n",
      "l2 norm of weights: 5.933803236371196\n",
      "---------------------\n",
      "Iteration Number: 4826\n",
      "Loss: 37.25040331898958\n",
      "l2 norm of gradients: 0.2653655353335428\n",
      "l2 norm of weights: 5.933713661017359\n",
      "---------------------\n",
      "Iteration Number: 4827\n",
      "Loss: 37.24687318046976\n",
      "l2 norm of gradients: 0.26532537666471273\n",
      "l2 norm of weights: 5.933624094968745\n",
      "---------------------\n",
      "Iteration Number: 4828\n",
      "Loss: 37.24334410612473\n",
      "l2 norm of gradients: 0.2652852367063758\n",
      "l2 norm of weights: 5.9335345382227915\n",
      "---------------------\n",
      "Iteration Number: 4829\n",
      "Loss: 37.239816095294834\n",
      "l2 norm of gradients: 0.2652451154434378\n",
      "l2 norm of weights: 5.9334449907769375\n",
      "---------------------\n",
      "Iteration Number: 4830\n",
      "Loss: 37.236289147331334\n",
      "l2 norm of gradients: 0.2652050128608185\n",
      "l2 norm of weights: 5.933355452628625\n",
      "---------------------\n",
      "Iteration Number: 4831\n",
      "Loss: 37.23276326159171\n",
      "l2 norm of gradients: 0.2651649289434519\n",
      "l2 norm of weights: 5.933265923775296\n",
      "---------------------\n",
      "Iteration Number: 4832\n",
      "Loss: 37.22923843742926\n",
      "l2 norm of gradients: 0.26512486367628607\n",
      "l2 norm of weights: 5.933176404214397\n",
      "---------------------\n",
      "Iteration Number: 4833\n",
      "Loss: 37.22571467419526\n",
      "l2 norm of gradients: 0.26508481704428316\n",
      "l2 norm of weights: 5.933086893943372\n",
      "---------------------\n",
      "Iteration Number: 4834\n",
      "Loss: 37.222191971241394\n",
      "l2 norm of gradients: 0.2650447890324194\n",
      "l2 norm of weights: 5.932997392959672\n",
      "---------------------\n",
      "Iteration Number: 4835\n",
      "Loss: 37.218670327925814\n",
      "l2 norm of gradients: 0.265004779625685\n",
      "l2 norm of weights: 5.932907901260745\n",
      "---------------------\n",
      "Iteration Number: 4836\n",
      "Loss: 37.215149743592924\n",
      "l2 norm of gradients: 0.26496478880908436\n",
      "l2 norm of weights: 5.932818418844044\n",
      "---------------------\n",
      "Iteration Number: 4837\n",
      "Loss: 37.21163021761884\n",
      "l2 norm of gradients: 0.26492481656763583\n",
      "l2 norm of weights: 5.932728945707021\n",
      "---------------------\n",
      "Iteration Number: 4838\n",
      "Loss: 37.208111749345235\n",
      "l2 norm of gradients: 0.26488486288637175\n",
      "l2 norm of weights: 5.932639481847134\n",
      "---------------------\n",
      "Iteration Number: 4839\n",
      "Loss: 37.20459433813965\n",
      "l2 norm of gradients: 0.2648449277503386\n",
      "l2 norm of weights: 5.932550027261838\n",
      "---------------------\n",
      "Iteration Number: 4840\n",
      "Loss: 37.20107798334766\n",
      "l2 norm of gradients: 0.2648050111445967\n",
      "l2 norm of weights: 5.932460581948591\n",
      "---------------------\n",
      "Iteration Number: 4841\n",
      "Loss: 37.19756268433434\n",
      "l2 norm of gradients: 0.2647651130542204\n",
      "l2 norm of weights: 5.932371145904856\n",
      "---------------------\n",
      "Iteration Number: 4842\n",
      "Loss: 37.1940484404651\n",
      "l2 norm of gradients: 0.2647252334642982\n",
      "l2 norm of weights: 5.932281719128094\n",
      "---------------------\n",
      "Iteration Number: 4843\n",
      "Loss: 37.19053525108088\n",
      "l2 norm of gradients: 0.2646853723599322\n",
      "l2 norm of weights: 5.9321923016157685\n",
      "---------------------\n",
      "Iteration Number: 4844\n",
      "Loss: 37.18702311556626\n",
      "l2 norm of gradients: 0.264645529726239\n",
      "l2 norm of weights: 5.932102893365345\n",
      "---------------------\n",
      "Iteration Number: 4845\n",
      "Loss: 37.18351203326357\n",
      "l2 norm of gradients: 0.2646057055483486\n",
      "l2 norm of weights: 5.932013494374293\n",
      "---------------------\n",
      "Iteration Number: 4846\n",
      "Loss: 37.180002003539585\n",
      "l2 norm of gradients: 0.2645658998114051\n",
      "l2 norm of weights: 5.931924104640079\n",
      "---------------------\n",
      "Iteration Number: 4847\n",
      "Loss: 37.17649302575994\n",
      "l2 norm of gradients: 0.26452611250056673\n",
      "l2 norm of weights: 5.931834724160176\n",
      "---------------------\n",
      "Iteration Number: 4848\n",
      "Loss: 37.172985099288354\n",
      "l2 norm of gradients: 0.26448634360100537\n",
      "l2 norm of weights: 5.931745352932056\n",
      "---------------------\n",
      "Iteration Number: 4849\n",
      "Loss: 37.16947822347336\n",
      "l2 norm of gradients: 0.26444659309790686\n",
      "l2 norm of weights: 5.9316559909531925\n",
      "---------------------\n",
      "Iteration Number: 4850\n",
      "Loss: 37.165972397695455\n",
      "l2 norm of gradients: 0.26440686097647104\n",
      "l2 norm of weights: 5.931566638221064\n",
      "---------------------\n",
      "Iteration Number: 4851\n",
      "Loss: 37.16246762131405\n",
      "l2 norm of gradients: 0.26436714722191146\n",
      "l2 norm of weights: 5.931477294733145\n",
      "---------------------\n",
      "Iteration Number: 4852\n",
      "Loss: 37.15896389369185\n",
      "l2 norm of gradients: 0.26432745181945555\n",
      "l2 norm of weights: 5.931387960486917\n",
      "---------------------\n",
      "Iteration Number: 4853\n",
      "Loss: 37.15546121419804\n",
      "l2 norm of gradients: 0.26428777475434473\n",
      "l2 norm of weights: 5.93129863547986\n",
      "---------------------\n",
      "Iteration Number: 4854\n",
      "Loss: 37.15195958219224\n",
      "l2 norm of gradients: 0.26424811601183407\n",
      "l2 norm of weights: 5.931209319709458\n",
      "---------------------\n",
      "Iteration Number: 4855\n",
      "Loss: 37.1484589970496\n",
      "l2 norm of gradients: 0.26420847557719257\n",
      "l2 norm of weights: 5.931120013173197\n",
      "---------------------\n",
      "Iteration Number: 4856\n",
      "Loss: 37.14495945813674\n",
      "l2 norm of gradients: 0.264168853435703\n",
      "l2 norm of weights: 5.93103071586856\n",
      "---------------------\n",
      "Iteration Number: 4857\n",
      "Loss: 37.14146096481354\n",
      "l2 norm of gradients: 0.264129249572662\n",
      "l2 norm of weights: 5.930941427793038\n",
      "---------------------\n",
      "Iteration Number: 4858\n",
      "Loss: 37.137963516453496\n",
      "l2 norm of gradients: 0.2640896639733799\n",
      "l2 norm of weights: 5.930852148944119\n",
      "---------------------\n",
      "Iteration Number: 4859\n",
      "Loss: 37.13446711242406\n",
      "l2 norm of gradients: 0.26405009662318085\n",
      "l2 norm of weights: 5.930762879319296\n",
      "---------------------\n",
      "Iteration Number: 4860\n",
      "Loss: 37.13097175209506\n",
      "l2 norm of gradients: 0.2640105475074028\n",
      "l2 norm of weights: 5.930673618916061\n",
      "---------------------\n",
      "Iteration Number: 4861\n",
      "Loss: 37.12747743484149\n",
      "l2 norm of gradients: 0.2639710166113973\n",
      "l2 norm of weights: 5.9305843677319094\n",
      "---------------------\n",
      "Iteration Number: 4862\n",
      "Loss: 37.12398416002849\n",
      "l2 norm of gradients: 0.2639315039205299\n",
      "l2 norm of weights: 5.930495125764337\n",
      "---------------------\n",
      "Iteration Number: 4863\n",
      "Loss: 37.120491927035225\n",
      "l2 norm of gradients: 0.26389200942017965\n",
      "l2 norm of weights: 5.930405893010843\n",
      "---------------------\n",
      "Iteration Number: 4864\n",
      "Loss: 37.11700073522587\n",
      "l2 norm of gradients: 0.2638525330957394\n",
      "l2 norm of weights: 5.930316669468929\n",
      "---------------------\n",
      "Iteration Number: 4865\n",
      "Loss: 37.11351058397878\n",
      "l2 norm of gradients: 0.26381307493261574\n",
      "l2 norm of weights: 5.930227455136093\n",
      "---------------------\n",
      "Iteration Number: 4866\n",
      "Loss: 37.11002147266022\n",
      "l2 norm of gradients: 0.2637736349162289\n",
      "l2 norm of weights: 5.93013825000984\n",
      "---------------------\n",
      "Iteration Number: 4867\n",
      "Loss: 37.106533400652346\n",
      "l2 norm of gradients: 0.2637342130320128\n",
      "l2 norm of weights: 5.930049054087676\n",
      "---------------------\n",
      "Iteration Number: 4868\n",
      "Loss: 37.10304636731435\n",
      "l2 norm of gradients: 0.263694809265415\n",
      "l2 norm of weights: 5.929959867367106\n",
      "---------------------\n",
      "Iteration Number: 4869\n",
      "Loss: 37.09956037204111\n",
      "l2 norm of gradients: 0.2636554236018968\n",
      "l2 norm of weights: 5.92987068984564\n",
      "---------------------\n",
      "Iteration Number: 4870\n",
      "Loss: 37.09607541419341\n",
      "l2 norm of gradients: 0.26361605602693305\n",
      "l2 norm of weights: 5.929781521520788\n",
      "---------------------\n",
      "Iteration Number: 4871\n",
      "Loss: 37.09259149316083\n",
      "l2 norm of gradients: 0.26357670652601245\n",
      "l2 norm of weights: 5.929692362390061\n",
      "---------------------\n",
      "Iteration Number: 4872\n",
      "Loss: 37.08910860830489\n",
      "l2 norm of gradients: 0.263537375084637\n",
      "l2 norm of weights: 5.9296032124509725\n",
      "---------------------\n",
      "Iteration Number: 4873\n",
      "Loss: 37.08562675901689\n",
      "l2 norm of gradients: 0.26349806168832246\n",
      "l2 norm of weights: 5.929514071701039\n",
      "---------------------\n",
      "Iteration Number: 4874\n",
      "Loss: 37.082145944659516\n",
      "l2 norm of gradients: 0.26345876632259835\n",
      "l2 norm of weights: 5.929424940137776\n",
      "---------------------\n",
      "Iteration Number: 4875\n",
      "Loss: 37.07866616462716\n",
      "l2 norm of gradients: 0.2634194889730075\n",
      "l2 norm of weights: 5.929335817758702\n",
      "---------------------\n",
      "Iteration Number: 4876\n",
      "Loss: 37.07518741828986\n",
      "l2 norm of gradients: 0.2633802296251065\n",
      "l2 norm of weights: 5.929246704561337\n",
      "---------------------\n",
      "Iteration Number: 4877\n",
      "Loss: 37.07170970502395\n",
      "l2 norm of gradients: 0.2633409882644654\n",
      "l2 norm of weights: 5.929157600543204\n",
      "---------------------\n",
      "Iteration Number: 4878\n",
      "Loss: 37.06823302421849\n",
      "l2 norm of gradients: 0.26330176487666795\n",
      "l2 norm of weights: 5.929068505701825\n",
      "---------------------\n",
      "Iteration Number: 4879\n",
      "Loss: 37.06475737525507\n",
      "l2 norm of gradients: 0.2632625594473113\n",
      "l2 norm of weights: 5.928979420034728\n",
      "---------------------\n",
      "Iteration Number: 4880\n",
      "Loss: 37.06128275750536\n",
      "l2 norm of gradients: 0.2632233719620061\n",
      "l2 norm of weights: 5.928890343539437\n",
      "---------------------\n",
      "Iteration Number: 4881\n",
      "Loss: 37.05780917036012\n",
      "l2 norm of gradients: 0.26318420240637674\n",
      "l2 norm of weights: 5.9288012762134805\n",
      "---------------------\n",
      "Iteration Number: 4882\n",
      "Loss: 37.05433661319387\n",
      "l2 norm of gradients: 0.2631450507660609\n",
      "l2 norm of weights: 5.928712218054391\n",
      "---------------------\n",
      "Iteration Number: 4883\n",
      "Loss: 37.05086508539294\n",
      "l2 norm of gradients: 0.2631059170267099\n",
      "l2 norm of weights: 5.928623169059698\n",
      "---------------------\n",
      "Iteration Number: 4884\n",
      "Loss: 37.04739458634072\n",
      "l2 norm of gradients: 0.26306680117398823\n",
      "l2 norm of weights: 5.928534129226937\n",
      "---------------------\n",
      "Iteration Number: 4885\n",
      "Loss: 37.04392511542703\n",
      "l2 norm of gradients: 0.2630277031935743\n",
      "l2 norm of weights: 5.928445098553641\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 4886\n",
      "Loss: 37.04045667203122\n",
      "l2 norm of gradients: 0.26298862307115983\n",
      "l2 norm of weights: 5.928356077037348\n",
      "---------------------\n",
      "Iteration Number: 4887\n",
      "Loss: 37.03698925554509\n",
      "l2 norm of gradients: 0.26294956079244985\n",
      "l2 norm of weights: 5.928267064675596\n",
      "---------------------\n",
      "Iteration Number: 4888\n",
      "Loss: 37.03352286533701\n",
      "l2 norm of gradients: 0.26291051634316287\n",
      "l2 norm of weights: 5.9281780614659265\n",
      "---------------------\n",
      "Iteration Number: 4889\n",
      "Loss: 37.03005750081109\n",
      "l2 norm of gradients: 0.2628714897090309\n",
      "l2 norm of weights: 5.92808906740588\n",
      "---------------------\n",
      "Iteration Number: 4890\n",
      "Loss: 37.02659316134768\n",
      "l2 norm of gradients: 0.2628324808757993\n",
      "l2 norm of weights: 5.928000082493\n",
      "---------------------\n",
      "Iteration Number: 4891\n",
      "Loss: 37.02312984633906\n",
      "l2 norm of gradients: 0.26279348982922696\n",
      "l2 norm of weights: 5.927911106724832\n",
      "---------------------\n",
      "Iteration Number: 4892\n",
      "Loss: 37.01966755517\n",
      "l2 norm of gradients: 0.262754516555086\n",
      "l2 norm of weights: 5.927822140098921\n",
      "---------------------\n",
      "Iteration Number: 4893\n",
      "Loss: 37.016206287224435\n",
      "l2 norm of gradients: 0.262715561039162\n",
      "l2 norm of weights: 5.927733182612818\n",
      "---------------------\n",
      "Iteration Number: 4894\n",
      "Loss: 37.01274604189986\n",
      "l2 norm of gradients: 0.2626766232672538\n",
      "l2 norm of weights: 5.927644234264071\n",
      "---------------------\n",
      "Iteration Number: 4895\n",
      "Loss: 37.00928681858715\n",
      "l2 norm of gradients: 0.2626377032251739\n",
      "l2 norm of weights: 5.927555295050232\n",
      "---------------------\n",
      "Iteration Number: 4896\n",
      "Loss: 37.00582861666243\n",
      "l2 norm of gradients: 0.2625988008987477\n",
      "l2 norm of weights: 5.927466364968855\n",
      "---------------------\n",
      "Iteration Number: 4897\n",
      "Loss: 37.00237143553234\n",
      "l2 norm of gradients: 0.2625599162738143\n",
      "l2 norm of weights: 5.927377444017495\n",
      "---------------------\n",
      "Iteration Number: 4898\n",
      "Loss: 36.99891527458486\n",
      "l2 norm of gradients: 0.26252104933622605\n",
      "l2 norm of weights: 5.927288532193706\n",
      "---------------------\n",
      "Iteration Number: 4899\n",
      "Loss: 36.995460133211914\n",
      "l2 norm of gradients: 0.26248220007184836\n",
      "l2 norm of weights: 5.92719962949505\n",
      "---------------------\n",
      "Iteration Number: 4900\n",
      "Loss: 36.99200601079551\n",
      "l2 norm of gradients: 0.2624433684665603\n",
      "l2 norm of weights: 5.927110735919084\n",
      "---------------------\n",
      "Iteration Number: 4901\n",
      "Loss: 36.988552906738114\n",
      "l2 norm of gradients: 0.262404554506254\n",
      "l2 norm of weights: 5.927021851463369\n",
      "---------------------\n",
      "Iteration Number: 4902\n",
      "Loss: 36.985100820435086\n",
      "l2 norm of gradients: 0.26236575817683505\n",
      "l2 norm of weights: 5.926932976125471\n",
      "---------------------\n",
      "Iteration Number: 4903\n",
      "Loss: 36.98164975128431\n",
      "l2 norm of gradients: 0.2623269794642219\n",
      "l2 norm of weights: 5.926844109902952\n",
      "---------------------\n",
      "Iteration Number: 4904\n",
      "Loss: 36.978199698672036\n",
      "l2 norm of gradients: 0.2622882183543468\n",
      "l2 norm of weights: 5.926755252793378\n",
      "---------------------\n",
      "Iteration Number: 4905\n",
      "Loss: 36.974750661998115\n",
      "l2 norm of gradients: 0.26224947483315486\n",
      "l2 norm of weights: 5.92666640479432\n",
      "---------------------\n",
      "Iteration Number: 4906\n",
      "Loss: 36.97130264066265\n",
      "l2 norm of gradients: 0.2622107488866045\n",
      "l2 norm of weights: 5.926577565903345\n",
      "---------------------\n",
      "Iteration Number: 4907\n",
      "Loss: 36.96785563405389\n",
      "l2 norm of gradients: 0.2621720405006675\n",
      "l2 norm of weights: 5.9264887361180225\n",
      "---------------------\n",
      "Iteration Number: 4908\n",
      "Loss: 36.96440964156766\n",
      "l2 norm of gradients: 0.26213334966132873\n",
      "l2 norm of weights: 5.926399915435929\n",
      "---------------------\n",
      "Iteration Number: 4909\n",
      "Loss: 36.960964662607964\n",
      "l2 norm of gradients: 0.26209467635458616\n",
      "l2 norm of weights: 5.926311103854637\n",
      "---------------------\n",
      "Iteration Number: 4910\n",
      "Loss: 36.95752069657373\n",
      "l2 norm of gradients: 0.26205602056645116\n",
      "l2 norm of weights: 5.926222301371722\n",
      "---------------------\n",
      "Iteration Number: 4911\n",
      "Loss: 36.95407774286038\n",
      "l2 norm of gradients: 0.26201738228294813\n",
      "l2 norm of weights: 5.926133507984762\n",
      "---------------------\n",
      "Iteration Number: 4912\n",
      "Loss: 36.950635800869875\n",
      "l2 norm of gradients: 0.26197876149011473\n",
      "l2 norm of weights: 5.926044723691337\n",
      "---------------------\n",
      "Iteration Number: 4913\n",
      "Loss: 36.947194870001645\n",
      "l2 norm of gradients: 0.2619401581740016\n",
      "l2 norm of weights: 5.925955948489028\n",
      "---------------------\n",
      "Iteration Number: 4914\n",
      "Loss: 36.94375494965474\n",
      "l2 norm of gradients: 0.26190157232067274\n",
      "l2 norm of weights: 5.9258671823754145\n",
      "---------------------\n",
      "Iteration Number: 4915\n",
      "Loss: 36.94031603922614\n",
      "l2 norm of gradients: 0.2618630039162051\n",
      "l2 norm of weights: 5.925778425348082\n",
      "---------------------\n",
      "Iteration Number: 4916\n",
      "Loss: 36.93687813812835\n",
      "l2 norm of gradients: 0.2618244529466889\n",
      "l2 norm of weights: 5.925689677404618\n",
      "---------------------\n",
      "Iteration Number: 4917\n",
      "Loss: 36.933441245762026\n",
      "l2 norm of gradients: 0.26178591939822726\n",
      "l2 norm of weights: 5.925600938542607\n",
      "---------------------\n",
      "Iteration Number: 4918\n",
      "Loss: 36.930005361518994\n",
      "l2 norm of gradients: 0.2617474032569366\n",
      "l2 norm of weights: 5.92551220875964\n",
      "---------------------\n",
      "Iteration Number: 4919\n",
      "Loss: 36.92657048481096\n",
      "l2 norm of gradients: 0.26170890450894635\n",
      "l2 norm of weights: 5.925423488053306\n",
      "---------------------\n",
      "Iteration Number: 4920\n",
      "Loss: 36.92313661503663\n",
      "l2 norm of gradients: 0.26167042314039884\n",
      "l2 norm of weights: 5.925334776421197\n",
      "---------------------\n",
      "Iteration Number: 4921\n",
      "Loss: 36.91970375160317\n",
      "l2 norm of gradients: 0.26163195913744985\n",
      "l2 norm of weights: 5.925246073860906\n",
      "---------------------\n",
      "Iteration Number: 4922\n",
      "Loss: 36.91627189392098\n",
      "l2 norm of gradients: 0.2615935124862679\n",
      "l2 norm of weights: 5.925157380370029\n",
      "---------------------\n",
      "Iteration Number: 4923\n",
      "Loss: 36.91284104139068\n",
      "l2 norm of gradients: 0.2615550831730345\n",
      "l2 norm of weights: 5.925068695946163\n",
      "---------------------\n",
      "Iteration Number: 4924\n",
      "Loss: 36.90941119341323\n",
      "l2 norm of gradients: 0.2615166711839445\n",
      "l2 norm of weights: 5.924980020586906\n",
      "---------------------\n",
      "Iteration Number: 4925\n",
      "Loss: 36.905982349402926\n",
      "l2 norm of gradients: 0.26147827650520555\n",
      "l2 norm of weights: 5.924891354289858\n",
      "---------------------\n",
      "Iteration Number: 4926\n",
      "Loss: 36.90255450876367\n",
      "l2 norm of gradients: 0.26143989912303817\n",
      "l2 norm of weights: 5.92480269705262\n",
      "---------------------\n",
      "Iteration Number: 4927\n",
      "Loss: 36.89912767090029\n",
      "l2 norm of gradients: 0.2614015390236761\n",
      "l2 norm of weights: 5.9247140488727945\n",
      "---------------------\n",
      "Iteration Number: 4928\n",
      "Loss: 36.895701835233766\n",
      "l2 norm of gradients: 0.26136319619336607\n",
      "l2 norm of weights: 5.924625409747987\n",
      "---------------------\n",
      "Iteration Number: 4929\n",
      "Loss: 36.89227700114863\n",
      "l2 norm of gradients: 0.2613248706183677\n",
      "l2 norm of weights: 5.924536779675804\n",
      "---------------------\n",
      "Iteration Number: 4930\n",
      "Loss: 36.888853168077915\n",
      "l2 norm of gradients: 0.2612865622849534\n",
      "l2 norm of weights: 5.924448158653853\n",
      "---------------------\n",
      "Iteration Number: 4931\n",
      "Loss: 36.88543033542189\n",
      "l2 norm of gradients: 0.2612482711794089\n",
      "l2 norm of weights: 5.924359546679743\n",
      "---------------------\n",
      "Iteration Number: 4932\n",
      "Loss: 36.882008502590416\n",
      "l2 norm of gradients: 0.26120999728803257\n",
      "l2 norm of weights: 5.924270943751085\n",
      "---------------------\n",
      "Iteration Number: 4933\n",
      "Loss: 36.87858766898857\n",
      "l2 norm of gradients: 0.2611717405971358\n",
      "l2 norm of weights: 5.924182349865492\n",
      "---------------------\n",
      "Iteration Number: 4934\n",
      "Loss: 36.87516783404783\n",
      "l2 norm of gradients: 0.26113350109304295\n",
      "l2 norm of weights: 5.924093765020578\n",
      "---------------------\n",
      "Iteration Number: 4935\n",
      "Loss: 36.8717489971566\n",
      "l2 norm of gradients: 0.2610952787620911\n",
      "l2 norm of weights: 5.924005189213957\n",
      "---------------------\n",
      "Iteration Number: 4936\n",
      "Loss: 36.868331157739284\n",
      "l2 norm of gradients: 0.26105707359063046\n",
      "l2 norm of weights: 5.923916622443248\n",
      "---------------------\n",
      "Iteration Number: 4937\n",
      "Loss: 36.86491431521596\n",
      "l2 norm of gradients: 0.261018885565024\n",
      "l2 norm of weights: 5.923828064706069\n",
      "---------------------\n",
      "Iteration Number: 4938\n",
      "Loss: 36.86149846897804\n",
      "l2 norm of gradients: 0.26098071467164746\n",
      "l2 norm of weights: 5.923739516000041\n",
      "---------------------\n",
      "Iteration Number: 4939\n",
      "Loss: 36.85808361846834\n",
      "l2 norm of gradients: 0.26094256089688966\n",
      "l2 norm of weights: 5.923650976322786\n",
      "---------------------\n",
      "Iteration Number: 4940\n",
      "Loss: 36.854669763075336\n",
      "l2 norm of gradients: 0.26090442422715204\n",
      "l2 norm of weights: 5.9235624456719265\n",
      "---------------------\n",
      "Iteration Number: 4941\n",
      "Loss: 36.851256902228215\n",
      "l2 norm of gradients: 0.2608663046488491\n",
      "l2 norm of weights: 5.923473924045087\n",
      "---------------------\n",
      "Iteration Number: 4942\n",
      "Loss: 36.84784503534378\n",
      "l2 norm of gradients: 0.2608282021484078\n",
      "l2 norm of weights: 5.923385411439896\n",
      "---------------------\n",
      "Iteration Number: 4943\n",
      "Loss: 36.844434161830385\n",
      "l2 norm of gradients: 0.2607901167122684\n",
      "l2 norm of weights: 5.92329690785398\n",
      "---------------------\n",
      "Iteration Number: 4944\n",
      "Loss: 36.84102428111235\n",
      "l2 norm of gradients: 0.2607520483268837\n",
      "l2 norm of weights: 5.923208413284969\n",
      "---------------------\n",
      "Iteration Number: 4945\n",
      "Loss: 36.837615392596426\n",
      "l2 norm of gradients: 0.2607139969787192\n",
      "l2 norm of weights: 5.923119927730494\n",
      "---------------------\n",
      "Iteration Number: 4946\n",
      "Loss: 36.834207495714665\n",
      "l2 norm of gradients: 0.2606759626542534\n",
      "l2 norm of weights: 5.923031451188187\n",
      "---------------------\n",
      "Iteration Number: 4947\n",
      "Loss: 36.83080058987922\n",
      "l2 norm of gradients: 0.2606379453399773\n",
      "l2 norm of weights: 5.922942983655686\n",
      "---------------------\n",
      "Iteration Number: 4948\n",
      "Loss: 36.82739467449876\n",
      "l2 norm of gradients: 0.2605999450223949\n",
      "l2 norm of weights: 5.922854525130622\n",
      "---------------------\n",
      "Iteration Number: 4949\n",
      "Loss: 36.823989749017144\n",
      "l2 norm of gradients: 0.2605619616880228\n",
      "l2 norm of weights: 5.922766075610635\n",
      "---------------------\n",
      "Iteration Number: 4950\n",
      "Loss: 36.82058581283111\n",
      "l2 norm of gradients: 0.2605239953233906\n",
      "l2 norm of weights: 5.922677635093364\n",
      "---------------------\n",
      "Iteration Number: 4951\n",
      "Loss: 36.81718286536759\n",
      "l2 norm of gradients: 0.26048604591504015\n",
      "l2 norm of weights: 5.922589203576448\n",
      "---------------------\n",
      "Iteration Number: 4952\n",
      "Loss: 36.81378090605558\n",
      "l2 norm of gradients: 0.2604481134495263\n",
      "l2 norm of weights: 5.922500781057531\n",
      "---------------------\n",
      "Iteration Number: 4953\n",
      "Loss: 36.81037993431321\n",
      "l2 norm of gradients: 0.26041019791341674\n",
      "l2 norm of weights: 5.922412367534253\n",
      "---------------------\n",
      "Iteration Number: 4954\n",
      "Loss: 36.80697994955636\n",
      "l2 norm of gradients: 0.2603722992932916\n",
      "l2 norm of weights: 5.9223239630042634\n",
      "---------------------\n",
      "Iteration Number: 4955\n",
      "Loss: 36.80358095120392\n",
      "l2 norm of gradients: 0.26033441757574366\n",
      "l2 norm of weights: 5.922235567465207\n",
      "---------------------\n",
      "Iteration Number: 4956\n",
      "Loss: 36.80018293870122\n",
      "l2 norm of gradients: 0.2602965527473787\n",
      "l2 norm of weights: 5.92214718091473\n",
      "---------------------\n",
      "Iteration Number: 4957\n",
      "Loss: 36.79678591144619\n",
      "l2 norm of gradients: 0.2602587047948148\n",
      "l2 norm of weights: 5.922058803350485\n",
      "---------------------\n",
      "Iteration Number: 4958\n",
      "Loss: 36.793389868883416\n",
      "l2 norm of gradients: 0.2602208737046828\n",
      "l2 norm of weights: 5.921970434770121\n",
      "---------------------\n",
      "Iteration Number: 4959\n",
      "Loss: 36.78999481042225\n",
      "l2 norm of gradients: 0.26018305946362624\n",
      "l2 norm of weights: 5.921882075171292\n",
      "---------------------\n",
      "Iteration Number: 4960\n",
      "Loss: 36.78660073550334\n",
      "l2 norm of gradients: 0.2601452620583012\n",
      "l2 norm of weights: 5.921793724551652\n",
      "---------------------\n",
      "Iteration Number: 4961\n",
      "Loss: 36.78320764353719\n",
      "l2 norm of gradients: 0.26010748147537655\n",
      "l2 norm of weights: 5.921705382908857\n",
      "---------------------\n",
      "Iteration Number: 4962\n",
      "Loss: 36.779815533955826\n",
      "l2 norm of gradients: 0.2600697177015335\n",
      "l2 norm of weights: 5.921617050240563\n",
      "---------------------\n",
      "Iteration Number: 4963\n",
      "Loss: 36.77642440619219\n",
      "l2 norm of gradients: 0.26003197072346607\n",
      "l2 norm of weights: 5.921528726544429\n",
      "---------------------\n",
      "Iteration Number: 4964\n",
      "Loss: 36.77303425966396\n",
      "l2 norm of gradients: 0.2599942405278805\n",
      "l2 norm of weights: 5.921440411818116\n",
      "---------------------\n",
      "Iteration Number: 4965\n",
      "Loss: 36.769645093809835\n",
      "l2 norm of gradients: 0.2599565271014962\n",
      "l2 norm of weights: 5.921352106059286\n",
      "---------------------\n",
      "Iteration Number: 4966\n",
      "Loss: 36.766256908048476\n",
      "l2 norm of gradients: 0.25991883043104463\n",
      "l2 norm of weights: 5.921263809265603\n",
      "---------------------\n",
      "Iteration Number: 4967\n",
      "Loss: 36.76286970180546\n",
      "l2 norm of gradients: 0.25988115050326993\n",
      "l2 norm of weights: 5.921175521434729\n",
      "---------------------\n",
      "Iteration Number: 4968\n",
      "Loss: 36.759483474530626\n",
      "l2 norm of gradients: 0.25984348730492896\n",
      "l2 norm of weights: 5.921087242564333\n",
      "---------------------\n",
      "Iteration Number: 4969\n",
      "Loss: 36.75609822563334\n",
      "l2 norm of gradients: 0.2598058408227908\n",
      "l2 norm of weights: 5.920998972652082\n",
      "---------------------\n",
      "Iteration Number: 4970\n",
      "Loss: 36.752713954554665\n",
      "l2 norm of gradients: 0.25976821104363723\n",
      "l2 norm of weights: 5.920910711695646\n",
      "---------------------\n",
      "Iteration Number: 4971\n",
      "Loss: 36.74933066071574\n",
      "l2 norm of gradients: 0.25973059795426257\n",
      "l2 norm of weights: 5.920822459692694\n",
      "---------------------\n",
      "Iteration Number: 4972\n",
      "Loss: 36.74594834356696\n",
      "l2 norm of gradients: 0.25969300154147346\n",
      "l2 norm of weights: 5.9207342166409\n",
      "---------------------\n",
      "Iteration Number: 4973\n",
      "Loss: 36.742567002515315\n",
      "l2 norm of gradients: 0.25965542179208917\n",
      "l2 norm of weights: 5.920645982537937\n",
      "---------------------\n",
      "Iteration Number: 4974\n",
      "Loss: 36.73918663701122\n",
      "l2 norm of gradients: 0.2596178586929413\n",
      "l2 norm of weights: 5.920557757381482\n",
      "---------------------\n",
      "Iteration Number: 4975\n",
      "Loss: 36.735807246486566\n",
      "l2 norm of gradients: 0.2595803122308742\n",
      "l2 norm of weights: 5.9204695411692105\n",
      "---------------------\n",
      "Iteration Number: 4976\n",
      "Loss: 36.73242883036625\n",
      "l2 norm of gradients: 0.25954278239274425\n",
      "l2 norm of weights: 5.9203813338988\n",
      "---------------------\n",
      "Iteration Number: 4977\n",
      "Loss: 36.72905138809141\n",
      "l2 norm of gradients: 0.25950526916542066\n",
      "l2 norm of weights: 5.920293135567931\n",
      "---------------------\n",
      "Iteration Number: 4978\n",
      "Loss: 36.7256749190947\n",
      "l2 norm of gradients: 0.2594677725357847\n",
      "l2 norm of weights: 5.9202049461742865\n",
      "---------------------\n",
      "Iteration Number: 4979\n",
      "Loss: 36.72229942281043\n",
      "l2 norm of gradients: 0.2594302924907303\n",
      "l2 norm of weights: 5.920116765715548\n",
      "---------------------\n",
      "Iteration Number: 4980\n",
      "Loss: 36.71892489867704\n",
      "l2 norm of gradients: 0.25939282901716376\n",
      "l2 norm of weights: 5.920028594189398\n",
      "---------------------\n",
      "Iteration Number: 4981\n",
      "Loss: 36.715551346133076\n",
      "l2 norm of gradients: 0.2593553821020037\n",
      "l2 norm of weights: 5.919940431593526\n",
      "---------------------\n",
      "Iteration Number: 4982\n",
      "Loss: 36.71217876460636\n",
      "l2 norm of gradients: 0.25931795173218136\n",
      "l2 norm of weights: 5.919852277925618\n",
      "---------------------\n",
      "Iteration Number: 4983\n",
      "Loss: 36.7088071535408\n",
      "l2 norm of gradients: 0.25928053789463984\n",
      "l2 norm of weights: 5.919764133183361\n",
      "---------------------\n",
      "Iteration Number: 4984\n",
      "Loss: 36.705436512369964\n",
      "l2 norm of gradients: 0.2592431405763351\n",
      "l2 norm of weights: 5.919675997364448\n",
      "---------------------\n",
      "Iteration Number: 4985\n",
      "Loss: 36.70206684053738\n",
      "l2 norm of gradients: 0.2592057597642352\n",
      "l2 norm of weights: 5.919587870466567\n",
      "---------------------\n",
      "Iteration Number: 4986\n",
      "Loss: 36.69869813747703\n",
      "l2 norm of gradients: 0.2591683954453206\n",
      "l2 norm of weights: 5.919499752487415\n",
      "---------------------\n",
      "Iteration Number: 4987\n",
      "Loss: 36.69533040263474\n",
      "l2 norm of gradients: 0.2591310476065842\n",
      "l2 norm of weights: 5.919411643424685\n",
      "---------------------\n",
      "Iteration Number: 4988\n",
      "Loss: 36.6919636354375\n",
      "l2 norm of gradients: 0.2590937162350309\n",
      "l2 norm of weights: 5.919323543276073\n",
      "---------------------\n",
      "Iteration Number: 4989\n",
      "Loss: 36.68859783534028\n",
      "l2 norm of gradients: 0.2590564013176783\n",
      "l2 norm of weights: 5.9192354520392785\n",
      "---------------------\n",
      "Iteration Number: 4990\n",
      "Loss: 36.685233001770946\n",
      "l2 norm of gradients: 0.259019102841556\n",
      "l2 norm of weights: 5.919147369711998\n",
      "---------------------\n",
      "Iteration Number: 4991\n",
      "Loss: 36.68186913418064\n",
      "l2 norm of gradients: 0.258981820793706\n",
      "l2 norm of weights: 5.919059296291934\n",
      "---------------------\n",
      "Iteration Number: 4992\n",
      "Loss: 36.67850623201012\n",
      "l2 norm of gradients: 0.2589445551611825\n",
      "l2 norm of weights: 5.918971231776788\n",
      "---------------------\n",
      "Iteration Number: 4993\n",
      "Loss: 36.675144294690824\n",
      "l2 norm of gradients: 0.2589073059310521\n",
      "l2 norm of weights: 5.918883176164263\n",
      "---------------------\n",
      "Iteration Number: 4994\n",
      "Loss: 36.67178332168714\n",
      "l2 norm of gradients: 0.25887007309039356\n",
      "l2 norm of weights: 5.918795129452064\n",
      "---------------------\n",
      "Iteration Number: 4995\n",
      "Loss: 36.668423312415044\n",
      "l2 norm of gradients: 0.25883285662629785\n",
      "l2 norm of weights: 5.918707091637899\n",
      "---------------------\n",
      "Iteration Number: 4996\n",
      "Loss: 36.66506426634173\n",
      "l2 norm of gradients: 0.2587956565258683\n",
      "l2 norm of weights: 5.9186190627194755\n",
      "---------------------\n",
      "Iteration Number: 4997\n",
      "Loss: 36.661706182906165\n",
      "l2 norm of gradients: 0.25875847277622027\n",
      "l2 norm of weights: 5.918531042694501\n",
      "---------------------\n",
      "Iteration Number: 4998\n",
      "Loss: 36.65834906153746\n",
      "l2 norm of gradients: 0.2587213053644814\n",
      "l2 norm of weights: 5.918443031560688\n",
      "---------------------\n",
      "Iteration Number: 4999\n",
      "Loss: 36.654992901700034\n",
      "l2 norm of gradients: 0.2586841542777917\n",
      "l2 norm of weights: 5.918355029315749\n",
      "---------------------\n",
      "Iteration Number: 5000\n",
      "Loss: 36.6516377028298\n",
      "l2 norm of gradients: 0.258647019503303\n",
      "l2 norm of weights: 5.918267035957397\n",
      "---------------------\n",
      "Iteration Number: 5001\n",
      "Loss: 36.648283464379425\n",
      "l2 norm of gradients: 0.25860990102817966\n",
      "l2 norm of weights: 5.918179051483348\n",
      "---------------------\n",
      "Iteration Number: 5002\n",
      "Loss: 36.64493018578912\n",
      "l2 norm of gradients: 0.25857279883959805\n",
      "l2 norm of weights: 5.918091075891318\n",
      "---------------------\n",
      "Iteration Number: 5003\n",
      "Loss: 36.64157786650906\n",
      "l2 norm of gradients: 0.25853571292474675\n",
      "l2 norm of weights: 5.9180031091790255\n",
      "---------------------\n",
      "Iteration Number: 5004\n",
      "Loss: 36.63822650598788\n",
      "l2 norm of gradients: 0.2584986432708263\n",
      "l2 norm of weights: 5.91791515134419\n",
      "---------------------\n",
      "Iteration Number: 5005\n",
      "Loss: 36.634876103673335\n",
      "l2 norm of gradients: 0.2584615898650497\n",
      "l2 norm of weights: 5.917827202384532\n",
      "---------------------\n",
      "Iteration Number: 5006\n",
      "Loss: 36.6315266590138\n",
      "l2 norm of gradients: 0.2584245526946417\n",
      "l2 norm of weights: 5.9177392622977765\n",
      "---------------------\n",
      "Iteration Number: 5007\n",
      "Loss: 36.62817817145784\n",
      "l2 norm of gradients: 0.2583875317468395\n",
      "l2 norm of weights: 5.917651331081644\n",
      "---------------------\n",
      "Iteration Number: 5008\n",
      "Loss: 36.62483064045779\n",
      "l2 norm of gradients: 0.2583505270088921\n",
      "l2 norm of weights: 5.917563408733862\n",
      "---------------------\n",
      "Iteration Number: 5009\n",
      "Loss: 36.62148406546596\n",
      "l2 norm of gradients: 0.2583135384680608\n",
      "l2 norm of weights: 5.917475495252157\n",
      "---------------------\n",
      "Iteration Number: 5010\n",
      "Loss: 36.61813844592696\n",
      "l2 norm of gradients: 0.2582765661116189\n",
      "l2 norm of weights: 5.917387590634256\n",
      "---------------------\n",
      "Iteration Number: 5011\n",
      "Loss: 36.614793781290174\n",
      "l2 norm of gradients: 0.2582396099268518\n",
      "l2 norm of weights: 5.91729969487789\n",
      "---------------------\n",
      "Iteration Number: 5012\n",
      "Loss: 36.611450071023754\n",
      "l2 norm of gradients: 0.258202669901057\n",
      "l2 norm of weights: 5.917211807980791\n",
      "---------------------\n",
      "Iteration Number: 5013\n",
      "Loss: 36.608107314565586\n",
      "l2 norm of gradients: 0.2581657460215438\n",
      "l2 norm of weights: 5.91712392994069\n",
      "---------------------\n",
      "Iteration Number: 5014\n",
      "Loss: 36.60476551136184\n",
      "l2 norm of gradients: 0.2581288382756338\n",
      "l2 norm of weights: 5.917036060755321\n",
      "---------------------\n",
      "Iteration Number: 5015\n",
      "Loss: 36.60142466088773\n",
      "l2 norm of gradients: 0.25809194665066054\n",
      "l2 norm of weights: 5.9169482004224205\n",
      "---------------------\n",
      "Iteration Number: 5016\n",
      "Loss: 36.59808476257703\n",
      "l2 norm of gradients: 0.2580550711339697\n",
      "l2 norm of weights: 5.916860348939725\n",
      "---------------------\n",
      "Iteration Number: 5017\n",
      "Loss: 36.594745815885624\n",
      "l2 norm of gradients: 0.2580182117129186\n",
      "l2 norm of weights: 5.916772506304972\n",
      "---------------------\n",
      "Iteration Number: 5018\n",
      "Loss: 36.59140782029083\n",
      "l2 norm of gradients: 0.25798136837487695\n",
      "l2 norm of weights: 5.9166846725159035\n",
      "---------------------\n",
      "Iteration Number: 5019\n",
      "Loss: 36.5880707752184\n",
      "l2 norm of gradients: 0.25794454110722626\n",
      "l2 norm of weights: 5.916596847570257\n",
      "---------------------\n",
      "Iteration Number: 5020\n",
      "Loss: 36.5847346801425\n",
      "l2 norm of gradients: 0.25790772989735994\n",
      "l2 norm of weights: 5.916509031465779\n",
      "---------------------\n",
      "Iteration Number: 5021\n",
      "Loss: 36.58139953451012\n",
      "l2 norm of gradients: 0.2578709347326836\n",
      "l2 norm of weights: 5.916421224200211\n",
      "---------------------\n",
      "Iteration Number: 5022\n",
      "Loss: 36.57806533778642\n",
      "l2 norm of gradients: 0.25783415560061446\n",
      "l2 norm of weights: 5.9163334257713\n",
      "---------------------\n",
      "Iteration Number: 5023\n",
      "Loss: 36.574732089428025\n",
      "l2 norm of gradients: 0.257797392488582\n",
      "l2 norm of weights: 5.916245636176792\n",
      "---------------------\n",
      "Iteration Number: 5024\n",
      "Loss: 36.57139978888161\n",
      "l2 norm of gradients: 0.2577606453840275\n",
      "l2 norm of weights: 5.916157855414435\n",
      "---------------------\n",
      "Iteration Number: 5025\n",
      "Loss: 36.56806843561459\n",
      "l2 norm of gradients: 0.25772391427440405\n",
      "l2 norm of weights: 5.916070083481979\n",
      "---------------------\n",
      "Iteration Number: 5026\n",
      "Loss: 36.56473802908329\n",
      "l2 norm of gradients: 0.25768719914717686\n",
      "l2 norm of weights: 5.915982320377176\n",
      "---------------------\n",
      "Iteration Number: 5027\n",
      "Loss: 36.56140856874367\n",
      "l2 norm of gradients: 0.2576504999898229\n",
      "l2 norm of weights: 5.915894566097777\n",
      "---------------------\n",
      "Iteration Number: 5028\n",
      "Loss: 36.55808005406774\n",
      "l2 norm of gradients: 0.25761381678983103\n",
      "l2 norm of weights: 5.915806820641538\n",
      "---------------------\n",
      "Iteration Number: 5029\n",
      "Loss: 36.55475248449972\n",
      "l2 norm of gradients: 0.25757714953470195\n",
      "l2 norm of weights: 5.915719084006212\n",
      "---------------------\n",
      "Iteration Number: 5030\n",
      "Loss: 36.55142585950978\n",
      "l2 norm of gradients: 0.25754049821194847\n",
      "l2 norm of weights: 5.915631356189557\n",
      "---------------------\n",
      "Iteration Number: 5031\n",
      "Loss: 36.548100178556\n",
      "l2 norm of gradients: 0.2575038628090949\n",
      "l2 norm of weights: 5.915543637189331\n",
      "---------------------\n",
      "Iteration Number: 5032\n",
      "Loss: 36.54477544110097\n",
      "l2 norm of gradients: 0.25746724331367754\n",
      "l2 norm of weights: 5.915455927003294\n",
      "---------------------\n",
      "Iteration Number: 5033\n",
      "Loss: 36.54145164660765\n",
      "l2 norm of gradients: 0.2574306397132446\n",
      "l2 norm of weights: 5.915368225629208\n",
      "---------------------\n",
      "Iteration Number: 5034\n",
      "Loss: 36.53812879453304\n",
      "l2 norm of gradients: 0.25739405199535614\n",
      "l2 norm of weights: 5.915280533064833\n",
      "---------------------\n",
      "Iteration Number: 5035\n",
      "Loss: 36.534806884349734\n",
      "l2 norm of gradients: 0.25735748014758386\n",
      "l2 norm of weights: 5.9151928493079335\n",
      "---------------------\n",
      "Iteration Number: 5036\n",
      "Loss: 36.53148591551422\n",
      "l2 norm of gradients: 0.25732092415751123\n",
      "l2 norm of weights: 5.915105174356276\n",
      "---------------------\n",
      "Iteration Number: 5037\n",
      "Loss: 36.52816588749543\n",
      "l2 norm of gradients: 0.25728438401273385\n",
      "l2 norm of weights: 5.915017508207626\n",
      "---------------------\n",
      "Iteration Number: 5038\n",
      "Loss: 36.52484679974895\n",
      "l2 norm of gradients: 0.25724785970085867\n",
      "l2 norm of weights: 5.914929850859752\n",
      "---------------------\n",
      "Iteration Number: 5039\n",
      "Loss: 36.52152865174579\n",
      "l2 norm of gradients: 0.2572113512095048\n",
      "l2 norm of weights: 5.914842202310424\n",
      "---------------------\n",
      "Iteration Number: 5040\n",
      "Loss: 36.51821144295484\n",
      "l2 norm of gradients: 0.25717485852630284\n",
      "l2 norm of weights: 5.914754562557411\n",
      "---------------------\n",
      "Iteration Number: 5041\n",
      "Loss: 36.51489517283771\n",
      "l2 norm of gradients: 0.2571383816388952\n",
      "l2 norm of weights: 5.914666931598486\n",
      "---------------------\n",
      "Iteration Number: 5042\n",
      "Loss: 36.51157984086151\n",
      "l2 norm of gradients: 0.2571019205349361\n",
      "l2 norm of weights: 5.9145793094314225\n",
      "---------------------\n",
      "Iteration Number: 5043\n",
      "Loss: 36.50826544648779\n",
      "l2 norm of gradients: 0.25706547520209133\n",
      "l2 norm of weights: 5.914491696053997\n",
      "---------------------\n",
      "Iteration Number: 5044\n",
      "Loss: 36.50495198919863\n",
      "l2 norm of gradients: 0.2570290456280386\n",
      "l2 norm of weights: 5.914404091463984\n",
      "---------------------\n",
      "Iteration Number: 5045\n",
      "Loss: 36.50163946844217\n",
      "l2 norm of gradients: 0.2569926318004672\n",
      "l2 norm of weights: 5.914316495659162\n",
      "---------------------\n",
      "Iteration Number: 5046\n",
      "Loss: 36.49832788370496\n",
      "l2 norm of gradients: 0.2569562337070782\n",
      "l2 norm of weights: 5.91422890863731\n",
      "---------------------\n",
      "Iteration Number: 5047\n",
      "Loss: 36.495017234448014\n",
      "l2 norm of gradients: 0.2569198513355842\n",
      "l2 norm of weights: 5.91414133039621\n",
      "---------------------\n",
      "Iteration Number: 5048\n",
      "Loss: 36.491707520135016\n",
      "l2 norm of gradients: 0.25688348467370953\n",
      "l2 norm of weights: 5.9140537609336405\n",
      "---------------------\n",
      "Iteration Number: 5049\n",
      "Loss: 36.48839874024273\n",
      "l2 norm of gradients: 0.2568471337091905\n",
      "l2 norm of weights: 5.913966200247388\n",
      "---------------------\n",
      "Iteration Number: 5050\n",
      "Loss: 36.48509089425032\n",
      "l2 norm of gradients: 0.2568107984297745\n",
      "l2 norm of weights: 5.913878648335235\n",
      "---------------------\n",
      "Iteration Number: 5051\n",
      "Loss: 36.481783981605076\n",
      "l2 norm of gradients: 0.25677447882322096\n",
      "l2 norm of weights: 5.91379110519497\n",
      "---------------------\n",
      "Iteration Number: 5052\n",
      "Loss: 36.478478001795125\n",
      "l2 norm of gradients: 0.25673817487730094\n",
      "l2 norm of weights: 5.913703570824378\n",
      "---------------------\n",
      "Iteration Number: 5053\n",
      "Loss: 36.47517295428819\n",
      "l2 norm of gradients: 0.256701886579797\n",
      "l2 norm of weights: 5.913616045221248\n",
      "---------------------\n",
      "Iteration Number: 5054\n",
      "Loss: 36.47186883855595\n",
      "l2 norm of gradients: 0.2566656139185032\n",
      "l2 norm of weights: 5.913528528383373\n",
      "---------------------\n",
      "Iteration Number: 5055\n",
      "Loss: 36.46856565407341\n",
      "l2 norm of gradients: 0.2566293568812254\n",
      "l2 norm of weights: 5.913441020308541\n",
      "---------------------\n",
      "Iteration Number: 5056\n",
      "Loss: 36.46526340031048\n",
      "l2 norm of gradients: 0.25659311545578106\n",
      "l2 norm of weights: 5.913353520994547\n",
      "---------------------\n",
      "Iteration Number: 5057\n",
      "Loss: 36.46196207674698\n",
      "l2 norm of gradients: 0.2565568896299991\n",
      "l2 norm of weights: 5.913266030439185\n",
      "---------------------\n",
      "Iteration Number: 5058\n",
      "Loss: 36.45866168283909\n",
      "l2 norm of gradients: 0.25652067939172013\n",
      "l2 norm of weights: 5.91317854864025\n",
      "---------------------\n",
      "Iteration Number: 5059\n",
      "Loss: 36.45536221808215\n",
      "l2 norm of gradients: 0.2564844847287962\n",
      "l2 norm of weights: 5.913091075595539\n",
      "---------------------\n",
      "Iteration Number: 5060\n",
      "Loss: 36.45206368194657\n",
      "l2 norm of gradients: 0.2564483056290909\n",
      "l2 norm of weights: 5.913003611302852\n",
      "---------------------\n",
      "Iteration Number: 5061\n",
      "Loss: 36.44876607389591\n",
      "l2 norm of gradients: 0.25641214208047947\n",
      "l2 norm of weights: 5.912916155759986\n",
      "---------------------\n",
      "Iteration Number: 5062\n",
      "Loss: 36.445469393423\n",
      "l2 norm of gradients: 0.2563759940708487\n",
      "l2 norm of weights: 5.912828708964746\n",
      "---------------------\n",
      "Iteration Number: 5063\n",
      "Loss: 36.44217363998667\n",
      "l2 norm of gradients: 0.2563398615880967\n",
      "l2 norm of weights: 5.91274127091493\n",
      "---------------------\n",
      "Iteration Number: 5064\n",
      "Loss: 36.43887881308187\n",
      "l2 norm of gradients: 0.2563037446201332\n",
      "l2 norm of weights: 5.9126538416083445\n",
      "---------------------\n",
      "Iteration Number: 5065\n",
      "Loss: 36.43558491216905\n",
      "l2 norm of gradients: 0.2562676431548796\n",
      "l2 norm of weights: 5.912566421042794\n",
      "---------------------\n",
      "Iteration Number: 5066\n",
      "Loss: 36.43229193672773\n",
      "l2 norm of gradients: 0.25623155718026847\n",
      "l2 norm of weights: 5.912479009216087\n",
      "---------------------\n",
      "Iteration Number: 5067\n",
      "Loss: 36.42899988625185\n",
      "l2 norm of gradients: 0.2561954866842441\n",
      "l2 norm of weights: 5.912391606126028\n",
      "---------------------\n",
      "Iteration Number: 5068\n",
      "Loss: 36.425708760207\n",
      "l2 norm of gradients: 0.25615943165476207\n",
      "l2 norm of weights: 5.9123042117704285\n",
      "---------------------\n",
      "Iteration Number: 5069\n",
      "Loss: 36.422418558079386\n",
      "l2 norm of gradients: 0.25612339207978957\n",
      "l2 norm of weights: 5.912216826147098\n",
      "---------------------\n",
      "Iteration Number: 5070\n",
      "Loss: 36.41912927933851\n",
      "l2 norm of gradients: 0.25608736794730513\n",
      "l2 norm of weights: 5.91212944925385\n",
      "---------------------\n",
      "Iteration Number: 5071\n",
      "Loss: 36.41584092347542\n",
      "l2 norm of gradients: 0.25605135924529876\n",
      "l2 norm of weights: 5.912042081088497\n",
      "---------------------\n",
      "Iteration Number: 5072\n",
      "Loss: 36.412553489961006\n",
      "l2 norm of gradients: 0.25601536596177193\n",
      "l2 norm of weights: 5.911954721648853\n",
      "---------------------\n",
      "Iteration Number: 5073\n",
      "Loss: 36.40926697828465\n",
      "l2 norm of gradients: 0.25597938808473747\n",
      "l2 norm of weights: 5.911867370932735\n",
      "---------------------\n",
      "Iteration Number: 5074\n",
      "Loss: 36.405981387922694\n",
      "l2 norm of gradients: 0.2559434256022195\n",
      "l2 norm of weights: 5.91178002893796\n",
      "---------------------\n",
      "Iteration Number: 5075\n",
      "Loss: 36.40269671835824\n",
      "l2 norm of gradients: 0.25590747850225376\n",
      "l2 norm of weights: 5.911692695662346\n",
      "---------------------\n",
      "Iteration Number: 5076\n",
      "Loss: 36.399412969080025\n",
      "l2 norm of gradients: 0.2558715467728873\n",
      "l2 norm of weights: 5.911605371103714\n",
      "---------------------\n",
      "Iteration Number: 5077\n",
      "Loss: 36.39613013955079\n",
      "l2 norm of gradients: 0.25583563040217844\n",
      "l2 norm of weights: 5.911518055259885\n",
      "---------------------\n",
      "Iteration Number: 5078\n",
      "Loss: 36.392848229278506\n",
      "l2 norm of gradients: 0.25579972937819695\n",
      "l2 norm of weights: 5.911430748128682\n",
      "---------------------\n",
      "Iteration Number: 5079\n",
      "Loss: 36.38956723773519\n",
      "l2 norm of gradients: 0.25576384368902394\n",
      "l2 norm of weights: 5.911343449707928\n",
      "---------------------\n",
      "Iteration Number: 5080\n",
      "Loss: 36.38628716440571\n",
      "l2 norm of gradients: 0.25572797332275193\n",
      "l2 norm of weights: 5.91125615999545\n",
      "---------------------\n",
      "Iteration Number: 5081\n",
      "Loss: 36.383008008779356\n",
      "l2 norm of gradients: 0.2556921182674845\n",
      "l2 norm of weights: 5.9111688789890735\n",
      "---------------------\n",
      "Iteration Number: 5082\n",
      "Loss: 36.3797297703297\n",
      "l2 norm of gradients: 0.255656278511337\n",
      "l2 norm of weights: 5.9110816066866265\n",
      "---------------------\n",
      "Iteration Number: 5083\n",
      "Loss: 36.37645244855599\n",
      "l2 norm of gradients: 0.2556204540424357\n",
      "l2 norm of weights: 5.91099434308594\n",
      "---------------------\n",
      "Iteration Number: 5084\n",
      "Loss: 36.37317604293269\n",
      "l2 norm of gradients: 0.2555846448489184\n",
      "l2 norm of weights: 5.910907088184843\n",
      "---------------------\n",
      "Iteration Number: 5085\n",
      "Loss: 36.36990055294793\n",
      "l2 norm of gradients: 0.2555488509189339\n",
      "l2 norm of weights: 5.910819841981169\n",
      "---------------------\n",
      "Iteration Number: 5086\n",
      "Loss: 36.366625978100636\n",
      "l2 norm of gradients: 0.2555130722406427\n",
      "l2 norm of weights: 5.9107326044727495\n",
      "---------------------\n",
      "Iteration Number: 5087\n",
      "Loss: 36.36335231786679\n",
      "l2 norm of gradients: 0.25547730880221625\n",
      "l2 norm of weights: 5.91064537565742\n",
      "---------------------\n",
      "Iteration Number: 5088\n",
      "Loss: 36.36007957173599\n",
      "l2 norm of gradients: 0.2554415605918374\n",
      "l2 norm of weights: 5.910558155533018\n",
      "---------------------\n",
      "Iteration Number: 5089\n",
      "Loss: 36.356807739199844\n",
      "l2 norm of gradients: 0.2554058275977002\n",
      "l2 norm of weights: 5.91047094409738\n",
      "---------------------\n",
      "Iteration Number: 5090\n",
      "Loss: 36.353536819748356\n",
      "l2 norm of gradients: 0.25537010980800995\n",
      "l2 norm of weights: 5.910383741348343\n",
      "---------------------\n",
      "Iteration Number: 5091\n",
      "Loss: 36.3502668128565\n",
      "l2 norm of gradients: 0.2553344072109831\n",
      "l2 norm of weights: 5.910296547283749\n",
      "---------------------\n",
      "Iteration Number: 5092\n",
      "Loss: 36.34699771803779\n",
      "l2 norm of gradients: 0.2552987197948476\n",
      "l2 norm of weights: 5.910209361901438\n",
      "---------------------\n",
      "Iteration Number: 5093\n",
      "Loss: 36.34372953476163\n",
      "l2 norm of gradients: 0.2552630475478422\n",
      "l2 norm of weights: 5.910122185199254\n",
      "---------------------\n",
      "Iteration Number: 5094\n",
      "Loss: 36.3404622625333\n",
      "l2 norm of gradients: 0.25522739045821713\n",
      "l2 norm of weights: 5.91003501717504\n",
      "---------------------\n",
      "Iteration Number: 5095\n",
      "Loss: 36.33719590083029\n",
      "l2 norm of gradients: 0.2551917485142337\n",
      "l2 norm of weights: 5.909947857826642\n",
      "---------------------\n",
      "Iteration Number: 5096\n",
      "Loss: 36.33393044915304\n",
      "l2 norm of gradients: 0.2551561217041645\n",
      "l2 norm of weights: 5.909860707151905\n",
      "---------------------\n",
      "Iteration Number: 5097\n",
      "Loss: 36.33066590699449\n",
      "l2 norm of gradients: 0.2551205100162932\n",
      "l2 norm of weights: 5.909773565148678\n",
      "---------------------\n",
      "Iteration Number: 5098\n",
      "Loss: 36.327402273845614\n",
      "l2 norm of gradients: 0.25508491343891465\n",
      "l2 norm of weights: 5.90968643181481\n",
      "---------------------\n",
      "Iteration Number: 5099\n",
      "Loss: 36.32413954918821\n",
      "l2 norm of gradients: 0.2550493319603348\n",
      "l2 norm of weights: 5.909599307148153\n",
      "---------------------\n",
      "Iteration Number: 5100\n",
      "Loss: 36.32087773253157\n",
      "l2 norm of gradients: 0.2550137655688708\n",
      "l2 norm of weights: 5.909512191146557\n",
      "---------------------\n",
      "Iteration Number: 5101\n",
      "Loss: 36.31761682336419\n",
      "l2 norm of gradients: 0.2549782142528509\n",
      "l2 norm of weights: 5.909425083807875\n",
      "---------------------\n",
      "Iteration Number: 5102\n",
      "Loss: 36.31435682117654\n",
      "l2 norm of gradients: 0.2549426780006145\n",
      "l2 norm of weights: 5.909337985129962\n",
      "---------------------\n",
      "Iteration Number: 5103\n",
      "Loss: 36.31109772546877\n",
      "l2 norm of gradients: 0.25490715680051207\n",
      "l2 norm of weights: 5.9092508951106755\n",
      "---------------------\n",
      "Iteration Number: 5104\n",
      "Loss: 36.30783953572758\n",
      "l2 norm of gradients: 0.25487165064090517\n",
      "l2 norm of weights: 5.909163813747869\n",
      "---------------------\n",
      "Iteration Number: 5105\n",
      "Loss: 36.30458225145569\n",
      "l2 norm of gradients: 0.2548361595101664\n",
      "l2 norm of weights: 5.909076741039403\n",
      "---------------------\n",
      "Iteration Number: 5106\n",
      "Loss: 36.301325872150144\n",
      "l2 norm of gradients: 0.2548006833966797\n",
      "l2 norm of weights: 5.908989676983136\n",
      "---------------------\n",
      "Iteration Number: 5107\n",
      "Loss: 36.29807039729826\n",
      "l2 norm of gradients: 0.25476522228883974\n",
      "l2 norm of weights: 5.908902621576932\n",
      "---------------------\n",
      "Iteration Number: 5108\n",
      "Loss: 36.294815826404516\n",
      "l2 norm of gradients: 0.2547297761750524\n",
      "l2 norm of weights: 5.908815574818649\n",
      "---------------------\n",
      "Iteration Number: 5109\n",
      "Loss: 36.291562158966556\n",
      "l2 norm of gradients: 0.2546943450437345\n",
      "l2 norm of weights: 5.908728536706152\n",
      "---------------------\n",
      "Iteration Number: 5110\n",
      "Loss: 36.28830939447676\n",
      "l2 norm of gradients: 0.25465892888331415\n",
      "l2 norm of weights: 5.9086415072373075\n",
      "---------------------\n",
      "Iteration Number: 5111\n",
      "Loss: 36.28505753244384\n",
      "l2 norm of gradients: 0.2546235276822302\n",
      "l2 norm of weights: 5.90855448640998\n",
      "---------------------\n",
      "Iteration Number: 5112\n",
      "Loss: 36.28180657234563\n",
      "l2 norm of gradients: 0.25458814142893266\n",
      "l2 norm of weights: 5.908467474222038\n",
      "---------------------\n",
      "Iteration Number: 5113\n",
      "Loss: 36.27855651371015\n",
      "l2 norm of gradients: 0.2545527701118826\n",
      "l2 norm of weights: 5.908380470671348\n",
      "---------------------\n",
      "Iteration Number: 5114\n",
      "Loss: 36.27530735601044\n",
      "l2 norm of gradients: 0.2545174137195519\n",
      "l2 norm of weights: 5.908293475755784\n",
      "---------------------\n",
      "Iteration Number: 5115\n",
      "Loss: 36.27205909876564\n",
      "l2 norm of gradients: 0.2544820722404235\n",
      "l2 norm of weights: 5.908206489473212\n",
      "---------------------\n",
      "Iteration Number: 5116\n",
      "Loss: 36.268811741471026\n",
      "l2 norm of gradients: 0.2544467456629916\n",
      "l2 norm of weights: 5.908119511821509\n",
      "---------------------\n",
      "Iteration Number: 5117\n",
      "Loss: 36.265565283614116\n",
      "l2 norm of gradients: 0.25441143397576077\n",
      "l2 norm of weights: 5.908032542798546\n",
      "---------------------\n",
      "Iteration Number: 5118\n",
      "Loss: 36.26231972470797\n",
      "l2 norm of gradients: 0.25437613716724716\n",
      "l2 norm of weights: 5.9079455824022\n",
      "---------------------\n",
      "Iteration Number: 5119\n",
      "Loss: 36.25907506425774\n",
      "l2 norm of gradients: 0.25434085522597744\n",
      "l2 norm of weights: 5.907858630630346\n",
      "---------------------\n",
      "Iteration Number: 5120\n",
      "Loss: 36.255831301757\n",
      "l2 norm of gradients: 0.25430558814048937\n",
      "l2 norm of weights: 5.907771687480863\n",
      "---------------------\n",
      "Iteration Number: 5121\n",
      "Loss: 36.25258843671609\n",
      "l2 norm of gradients: 0.2542703358993316\n",
      "l2 norm of weights: 5.907684752951629\n",
      "---------------------\n",
      "Iteration Number: 5122\n",
      "Loss: 36.24934646862776\n",
      "l2 norm of gradients: 0.25423509849106385\n",
      "l2 norm of weights: 5.907597827040524\n",
      "---------------------\n",
      "Iteration Number: 5123\n",
      "Loss: 36.24610539700738\n",
      "l2 norm of gradients: 0.25419987590425647\n",
      "l2 norm of weights: 5.907510909745431\n",
      "---------------------\n",
      "Iteration Number: 5124\n",
      "Loss: 36.24286522134996\n",
      "l2 norm of gradients: 0.25416466812749094\n",
      "l2 norm of weights: 5.90742400106423\n",
      "---------------------\n",
      "Iteration Number: 5125\n",
      "Loss: 36.23962594116297\n",
      "l2 norm of gradients: 0.25412947514935946\n",
      "l2 norm of weights: 5.907337100994808\n",
      "---------------------\n",
      "Iteration Number: 5126\n",
      "Loss: 36.236387555952206\n",
      "l2 norm of gradients: 0.2540942969584652\n",
      "l2 norm of weights: 5.907250209535049\n",
      "---------------------\n",
      "Iteration Number: 5127\n",
      "Loss: 36.233150065227385\n",
      "l2 norm of gradients: 0.2540591335434223\n",
      "l2 norm of weights: 5.9071633266828405\n",
      "---------------------\n",
      "Iteration Number: 5128\n",
      "Loss: 36.22991346848314\n",
      "l2 norm of gradients: 0.25402398489285544\n",
      "l2 norm of weights: 5.907076452436069\n",
      "---------------------\n",
      "Iteration Number: 5129\n",
      "Loss: 36.22667776522991\n",
      "l2 norm of gradients: 0.2539888509954004\n",
      "l2 norm of weights: 5.9069895867926245\n",
      "---------------------\n",
      "Iteration Number: 5130\n",
      "Loss: 36.223442954980214\n",
      "l2 norm of gradients: 0.25395373183970377\n",
      "l2 norm of weights: 5.906902729750397\n",
      "---------------------\n",
      "Iteration Number: 5131\n",
      "Loss: 36.22020903723351\n",
      "l2 norm of gradients: 0.2539186274144229\n",
      "l2 norm of weights: 5.906815881307278\n",
      "---------------------\n",
      "Iteration Number: 5132\n",
      "Loss: 36.216976011499185\n",
      "l2 norm of gradients: 0.25388353770822597\n",
      "l2 norm of weights: 5.906729041461161\n",
      "---------------------\n",
      "Iteration Number: 5133\n",
      "Loss: 36.213743877284834\n",
      "l2 norm of gradients: 0.253848462709792\n",
      "l2 norm of weights: 5.906642210209941\n",
      "---------------------\n",
      "Iteration Number: 5134\n",
      "Loss: 36.21051263410057\n",
      "l2 norm of gradients: 0.2538134024078108\n",
      "l2 norm of weights: 5.906555387551511\n",
      "---------------------\n",
      "Iteration Number: 5135\n",
      "Loss: 36.20728228145478\n",
      "l2 norm of gradients: 0.2537783567909829\n",
      "l2 norm of weights: 5.906468573483769\n",
      "---------------------\n",
      "Iteration Number: 5136\n",
      "Loss: 36.204052818861484\n",
      "l2 norm of gradients: 0.2537433258480196\n",
      "l2 norm of weights: 5.906381768004614\n",
      "---------------------\n",
      "Iteration Number: 5137\n",
      "Loss: 36.20082424581137\n",
      "l2 norm of gradients: 0.2537083095676432\n",
      "l2 norm of weights: 5.906294971111944\n",
      "---------------------\n",
      "Iteration Number: 5138\n",
      "Loss: 36.19759656183575\n",
      "l2 norm of gradients: 0.2536733079385864\n",
      "l2 norm of weights: 5.90620818280366\n",
      "---------------------\n",
      "Iteration Number: 5139\n",
      "Loss: 36.19436976644021\n",
      "l2 norm of gradients: 0.2536383209495928\n",
      "l2 norm of weights: 5.9061214030776625\n",
      "---------------------\n",
      "Iteration Number: 5140\n",
      "Loss: 36.1911438591286\n",
      "l2 norm of gradients: 0.2536033485894169\n",
      "l2 norm of weights: 5.906034631931857\n",
      "---------------------\n",
      "Iteration Number: 5141\n",
      "Loss: 36.18791883941654\n",
      "l2 norm of gradients: 0.25356839084682375\n",
      "l2 norm of weights: 5.905947869364145\n",
      "---------------------\n",
      "Iteration Number: 5142\n",
      "Loss: 36.18469470681262\n",
      "l2 norm of gradients: 0.253533447710589\n",
      "l2 norm of weights: 5.905861115372435\n",
      "---------------------\n",
      "Iteration Number: 5143\n",
      "Loss: 36.18147146082822\n",
      "l2 norm of gradients: 0.2534985191694994\n",
      "l2 norm of weights: 5.905774369954631\n",
      "---------------------\n",
      "Iteration Number: 5144\n",
      "Loss: 36.17824910098671\n",
      "l2 norm of gradients: 0.2534636052123519\n",
      "l2 norm of weights: 5.905687633108641\n",
      "---------------------\n",
      "Iteration Number: 5145\n",
      "Loss: 36.17502762678731\n",
      "l2 norm of gradients: 0.2534287058279545\n",
      "l2 norm of weights: 5.905600904832378\n",
      "---------------------\n",
      "Iteration Number: 5146\n",
      "Loss: 36.17180703774923\n",
      "l2 norm of gradients: 0.25339382100512575\n",
      "l2 norm of weights: 5.905514185123747\n",
      "---------------------\n",
      "Iteration Number: 5147\n",
      "Loss: 36.16858733339531\n",
      "l2 norm of gradients: 0.25335895073269493\n",
      "l2 norm of weights: 5.905427473980665\n",
      "---------------------\n",
      "Iteration Number: 5148\n",
      "Loss: 36.16536851322641\n",
      "l2 norm of gradients: 0.2533240949995018\n",
      "l2 norm of weights: 5.90534077140104\n",
      "---------------------\n",
      "Iteration Number: 5149\n",
      "Loss: 36.16215057675534\n",
      "l2 norm of gradients: 0.253289253794397\n",
      "l2 norm of weights: 5.905254077382789\n",
      "---------------------\n",
      "Iteration Number: 5150\n",
      "Loss: 36.15893352350669\n",
      "l2 norm of gradients: 0.2532544271062417\n",
      "l2 norm of weights: 5.905167391923829\n",
      "---------------------\n",
      "Iteration Number: 5151\n",
      "Loss: 36.15571735299831\n",
      "l2 norm of gradients: 0.2532196149239076\n",
      "l2 norm of weights: 5.905080715022073\n",
      "---------------------\n",
      "Iteration Number: 5152\n",
      "Loss: 36.152502064734975\n",
      "l2 norm of gradients: 0.25318481723627717\n",
      "l2 norm of weights: 5.904994046675442\n",
      "---------------------\n",
      "Iteration Number: 5153\n",
      "Loss: 36.14928765823881\n",
      "l2 norm of gradients: 0.2531500340322435\n",
      "l2 norm of weights: 5.904907386881852\n",
      "---------------------\n",
      "Iteration Number: 5154\n",
      "Loss: 36.14607413303163\n",
      "l2 norm of gradients: 0.2531152653007101\n",
      "l2 norm of weights: 5.904820735639227\n",
      "---------------------\n",
      "Iteration Number: 5155\n",
      "Loss: 36.142861488616774\n",
      "l2 norm of gradients: 0.2530805110305912\n",
      "l2 norm of weights: 5.9047340929454855\n",
      "---------------------\n",
      "Iteration Number: 5156\n",
      "Loss: 36.139649724527935\n",
      "l2 norm of gradients: 0.25304577121081157\n",
      "l2 norm of weights: 5.904647458798553\n",
      "---------------------\n",
      "Iteration Number: 5157\n",
      "Loss: 36.1364388402778\n",
      "l2 norm of gradients: 0.25301104583030665\n",
      "l2 norm of weights: 5.904560833196351\n",
      "---------------------\n",
      "Iteration Number: 5158\n",
      "Loss: 36.133228835374304\n",
      "l2 norm of gradients: 0.2529763348780223\n",
      "l2 norm of weights: 5.904474216136806\n",
      "---------------------\n",
      "Iteration Number: 5159\n",
      "Loss: 36.13001970934869\n",
      "l2 norm of gradients: 0.25294163834291494\n",
      "l2 norm of weights: 5.9043876076178465\n",
      "---------------------\n",
      "Iteration Number: 5160\n",
      "Loss: 36.12681146172377\n",
      "l2 norm of gradients: 0.25290695621395154\n",
      "l2 norm of weights: 5.904301007637398\n",
      "---------------------\n",
      "Iteration Number: 5161\n",
      "Loss: 36.1236040919976\n",
      "l2 norm of gradients: 0.25287228848010973\n",
      "l2 norm of weights: 5.904214416193389\n",
      "---------------------\n",
      "Iteration Number: 5162\n",
      "Loss: 36.12039759971691\n",
      "l2 norm of gradients: 0.25283763513037755\n",
      "l2 norm of weights: 5.904127833283751\n",
      "---------------------\n",
      "Iteration Number: 5163\n",
      "Loss: 36.11719198437902\n",
      "l2 norm of gradients: 0.2528029961537535\n",
      "l2 norm of weights: 5.904041258906415\n",
      "---------------------\n",
      "Iteration Number: 5164\n",
      "Loss: 36.11398724552732\n",
      "l2 norm of gradients: 0.2527683715392467\n",
      "l2 norm of weights: 5.903954693059314\n",
      "---------------------\n",
      "Iteration Number: 5165\n",
      "Loss: 36.110783382664465\n",
      "l2 norm of gradients: 0.2527337612758766\n",
      "l2 norm of weights: 5.903868135740382\n",
      "---------------------\n",
      "Iteration Number: 5166\n",
      "Loss: 36.10758039531958\n",
      "l2 norm of gradients: 0.25269916535267334\n",
      "l2 norm of weights: 5.903781586947553\n",
      "---------------------\n",
      "Iteration Number: 5167\n",
      "Loss: 36.104378283019\n",
      "l2 norm of gradients: 0.2526645837586773\n",
      "l2 norm of weights: 5.903695046678766\n",
      "---------------------\n",
      "Iteration Number: 5168\n",
      "Loss: 36.10117704528464\n",
      "l2 norm of gradients: 0.2526300164829395\n",
      "l2 norm of weights: 5.903608514931954\n",
      "---------------------\n",
      "Iteration Number: 5169\n",
      "Loss: 36.09797668162866\n",
      "l2 norm of gradients: 0.25259546351452133\n",
      "l2 norm of weights: 5.90352199170506\n",
      "---------------------\n",
      "Iteration Number: 5170\n",
      "Loss: 36.09477719157914\n",
      "l2 norm of gradients: 0.25256092484249465\n",
      "l2 norm of weights: 5.903435476996021\n",
      "---------------------\n",
      "Iteration Number: 5171\n",
      "Loss: 36.09157857467224\n",
      "l2 norm of gradients: 0.2525264004559418\n",
      "l2 norm of weights: 5.9033489708027815\n",
      "---------------------\n",
      "Iteration Number: 5172\n",
      "Loss: 36.088380830415915\n",
      "l2 norm of gradients: 0.2524918903439554\n",
      "l2 norm of weights: 5.903262473123281\n",
      "---------------------\n",
      "Iteration Number: 5173\n",
      "Loss: 36.08518395834665\n",
      "l2 norm of gradients: 0.25245739449563864\n",
      "l2 norm of weights: 5.903175983955464\n",
      "---------------------\n",
      "Iteration Number: 5174\n",
      "Loss: 36.08198795797946\n",
      "l2 norm of gradients: 0.25242291290010493\n",
      "l2 norm of weights: 5.9030895032972746\n",
      "---------------------\n",
      "Iteration Number: 5175\n",
      "Loss: 36.07879282884764\n",
      "l2 norm of gradients: 0.25238844554647827\n",
      "l2 norm of weights: 5.90300303114666\n",
      "---------------------\n",
      "Iteration Number: 5176\n",
      "Loss: 36.075598570474845\n",
      "l2 norm of gradients: 0.252353992423893\n",
      "l2 norm of weights: 5.902916567501568\n",
      "---------------------\n",
      "Iteration Number: 5177\n",
      "Loss: 36.072405182387065\n",
      "l2 norm of gradients: 0.2523195535214937\n",
      "l2 norm of weights: 5.902830112359945\n",
      "---------------------\n",
      "Iteration Number: 5178\n",
      "Loss: 36.06921266411094\n",
      "l2 norm of gradients: 0.25228512882843546\n",
      "l2 norm of weights: 5.902743665719742\n",
      "---------------------\n",
      "Iteration Number: 5179\n",
      "Loss: 36.066021015175814\n",
      "l2 norm of gradients: 0.2522507183338836\n",
      "l2 norm of weights: 5.902657227578909\n",
      "---------------------\n",
      "Iteration Number: 5180\n",
      "Loss: 36.0628302351077\n",
      "l2 norm of gradients: 0.25221632202701383\n",
      "l2 norm of weights: 5.9025707979353985\n",
      "---------------------\n",
      "Iteration Number: 5181\n",
      "Loss: 36.05964032342568\n",
      "l2 norm of gradients: 0.2521819398970124\n",
      "l2 norm of weights: 5.902484376787164\n",
      "---------------------\n",
      "Iteration Number: 5182\n",
      "Loss: 36.056451279677276\n",
      "l2 norm of gradients: 0.2521475719330755\n",
      "l2 norm of weights: 5.9023979641321604\n",
      "---------------------\n",
      "Iteration Number: 5183\n",
      "Loss: 36.05326310338063\n",
      "l2 norm of gradients: 0.25211321812440995\n",
      "l2 norm of weights: 5.902311559968341\n",
      "---------------------\n",
      "Iteration Number: 5184\n",
      "Loss: 36.05007579406186\n",
      "l2 norm of gradients: 0.2520788784602328\n",
      "l2 norm of weights: 5.902225164293665\n",
      "---------------------\n",
      "Iteration Number: 5185\n",
      "Loss: 36.04688935125119\n",
      "l2 norm of gradients: 0.25204455292977124\n",
      "l2 norm of weights: 5.9021387771060905\n",
      "---------------------\n",
      "Iteration Number: 5186\n",
      "Loss: 36.04370377448188\n",
      "l2 norm of gradients: 0.2520102415222629\n",
      "l2 norm of weights: 5.902052398403574\n",
      "---------------------\n",
      "Iteration Number: 5187\n",
      "Loss: 36.040519063285224\n",
      "l2 norm of gradients: 0.2519759442269557\n",
      "l2 norm of weights: 5.90196602818408\n",
      "---------------------\n",
      "Iteration Number: 5188\n",
      "Loss: 36.037335217190545\n",
      "l2 norm of gradients: 0.25194166103310783\n",
      "l2 norm of weights: 5.9018796664455655\n",
      "---------------------\n",
      "Iteration Number: 5189\n",
      "Loss: 36.03415223573671\n",
      "l2 norm of gradients: 0.2519073919299875\n",
      "l2 norm of weights: 5.901793313185998\n",
      "---------------------\n",
      "Iteration Number: 5190\n",
      "Loss: 36.0309701184398\n",
      "l2 norm of gradients: 0.2518731369068736\n",
      "l2 norm of weights: 5.901706968403338\n",
      "---------------------\n",
      "Iteration Number: 5191\n",
      "Loss: 36.0277888648469\n",
      "l2 norm of gradients: 0.25183889595305486\n",
      "l2 norm of weights: 5.901620632095551\n",
      "---------------------\n",
      "Iteration Number: 5192\n",
      "Loss: 36.02460847447479\n",
      "l2 norm of gradients: 0.2518046690578305\n",
      "l2 norm of weights: 5.901534304260605\n",
      "---------------------\n",
      "Iteration Number: 5193\n",
      "Loss: 36.02142894686562\n",
      "l2 norm of gradients: 0.25177045621050975\n",
      "l2 norm of weights: 5.901447984896467\n",
      "---------------------\n",
      "Iteration Number: 5194\n",
      "Loss: 36.01825028155822\n",
      "l2 norm of gradients: 0.25173625740041217\n",
      "l2 norm of weights: 5.901361674001104\n",
      "---------------------\n",
      "Iteration Number: 5195\n",
      "Loss: 36.01507247807736\n",
      "l2 norm of gradients: 0.2517020726168676\n",
      "l2 norm of weights: 5.901275371572488\n",
      "---------------------\n",
      "Iteration Number: 5196\n",
      "Loss: 36.01189553596009\n",
      "l2 norm of gradients: 0.25166790184921595\n",
      "l2 norm of weights: 5.901189077608589\n",
      "---------------------\n",
      "Iteration Number: 5197\n",
      "Loss: 36.0087194547472\n",
      "l2 norm of gradients: 0.25163374508680725\n",
      "l2 norm of weights: 5.90110279210738\n",
      "---------------------\n",
      "Iteration Number: 5198\n",
      "Loss: 36.00554423395962\n",
      "l2 norm of gradients: 0.25159960231900197\n",
      "l2 norm of weights: 5.901016515066835\n",
      "---------------------\n",
      "Iteration Number: 5199\n",
      "Loss: 36.002369873142555\n",
      "l2 norm of gradients: 0.25156547353517034\n",
      "l2 norm of weights: 5.900930246484926\n",
      "---------------------\n",
      "Iteration Number: 5200\n",
      "Loss: 35.99919637182393\n",
      "l2 norm of gradients: 0.2515313587246931\n",
      "l2 norm of weights: 5.900843986359631\n",
      "---------------------\n",
      "Iteration Number: 5201\n",
      "Loss: 35.9960237295581\n",
      "l2 norm of gradients: 0.25149725787696103\n",
      "l2 norm of weights: 5.900757734688926\n",
      "---------------------\n",
      "Iteration Number: 5202\n",
      "Loss: 35.992851945857296\n",
      "l2 norm of gradients: 0.25146317098137505\n",
      "l2 norm of weights: 5.90067149147079\n",
      "---------------------\n",
      "Iteration Number: 5203\n",
      "Loss: 35.98968102027686\n",
      "l2 norm of gradients: 0.251429098027346\n",
      "l2 norm of weights: 5.900585256703202\n",
      "---------------------\n",
      "Iteration Number: 5204\n",
      "Loss: 35.986510952344865\n",
      "l2 norm of gradients: 0.2513950390042951\n",
      "l2 norm of weights: 5.900499030384142\n",
      "---------------------\n",
      "Iteration Number: 5205\n",
      "Loss: 35.98334174159824\n",
      "l2 norm of gradients: 0.2513609939016537\n",
      "l2 norm of weights: 5.900412812511592\n",
      "---------------------\n",
      "Iteration Number: 5206\n",
      "Loss: 35.98017338758896\n",
      "l2 norm of gradients: 0.2513269627088629\n",
      "l2 norm of weights: 5.900326603083535\n",
      "---------------------\n",
      "Iteration Number: 5207\n",
      "Loss: 35.97700588983647\n",
      "l2 norm of gradients: 0.2512929454153743\n",
      "l2 norm of weights: 5.900240402097953\n",
      "---------------------\n",
      "Iteration Number: 5208\n",
      "Loss: 35.97383924788988\n",
      "l2 norm of gradients: 0.25125894201064924\n",
      "l2 norm of weights: 5.900154209552835\n",
      "---------------------\n",
      "Iteration Number: 5209\n",
      "Loss: 35.97067346128349\n",
      "l2 norm of gradients: 0.2512249524841594\n",
      "l2 norm of weights: 5.900068025446164\n",
      "---------------------\n",
      "Iteration Number: 5210\n",
      "Loss: 35.967508529566544\n",
      "l2 norm of gradients: 0.2511909768253864\n",
      "l2 norm of weights: 5.89998184977593\n",
      "---------------------\n",
      "Iteration Number: 5211\n",
      "Loss: 35.964344452269316\n",
      "l2 norm of gradients: 0.2511570150238218\n",
      "l2 norm of weights: 5.899895682540119\n",
      "---------------------\n",
      "Iteration Number: 5212\n",
      "Loss: 35.96118122893286\n",
      "l2 norm of gradients: 0.25112306706896736\n",
      "l2 norm of weights: 5.899809523736724\n",
      "---------------------\n",
      "Iteration Number: 5213\n",
      "Loss: 35.9580188591057\n",
      "l2 norm of gradients: 0.2510891329503349\n",
      "l2 norm of weights: 5.899723373363732\n",
      "---------------------\n",
      "Iteration Number: 5214\n",
      "Loss: 35.95485734232117\n",
      "l2 norm of gradients: 0.251055212657446\n",
      "l2 norm of weights: 5.899637231419138\n",
      "---------------------\n",
      "Iteration Number: 5215\n",
      "Loss: 35.951696678125856\n",
      "l2 norm of gradients: 0.25102130617983254\n",
      "l2 norm of weights: 5.899551097900934\n",
      "---------------------\n",
      "Iteration Number: 5216\n",
      "Loss: 35.948536866060756\n",
      "l2 norm of gradients: 0.2509874135070363\n",
      "l2 norm of weights: 5.899464972807116\n",
      "---------------------\n",
      "Iteration Number: 5217\n",
      "Loss: 35.94537790566426\n",
      "l2 norm of gradients: 0.250953534628609\n",
      "l2 norm of weights: 5.899378856135678\n",
      "---------------------\n",
      "Iteration Number: 5218\n",
      "Loss: 35.94221979648374\n",
      "l2 norm of gradients: 0.25091966953411243\n",
      "l2 norm of weights: 5.899292747884617\n",
      "---------------------\n",
      "Iteration Number: 5219\n",
      "Loss: 35.93906253805971\n",
      "l2 norm of gradients: 0.25088581821311823\n",
      "l2 norm of weights: 5.899206648051932\n",
      "---------------------\n",
      "Iteration Number: 5220\n",
      "Loss: 35.93590612994322\n",
      "l2 norm of gradients: 0.2508519806552081\n",
      "l2 norm of weights: 5.89912055663562\n",
      "---------------------\n",
      "Iteration Number: 5221\n",
      "Loss: 35.93275057167112\n",
      "l2 norm of gradients: 0.2508181568499737\n",
      "l2 norm of weights: 5.899034473633684\n",
      "---------------------\n",
      "Iteration Number: 5222\n",
      "Loss: 35.929595862784744\n",
      "l2 norm of gradients: 0.25078434678701667\n",
      "l2 norm of weights: 5.898948399044122\n",
      "---------------------\n",
      "Iteration Number: 5223\n",
      "Loss: 35.92644200283582\n",
      "l2 norm of gradients: 0.25075055045594835\n",
      "l2 norm of weights: 5.8988623328649386\n",
      "---------------------\n",
      "Iteration Number: 5224\n",
      "Loss: 35.92328899136779\n",
      "l2 norm of gradients: 0.25071676784639024\n",
      "l2 norm of weights: 5.898776275094137\n",
      "---------------------\n",
      "Iteration Number: 5225\n",
      "Loss: 35.92013682792404\n",
      "l2 norm of gradients: 0.25068299894797386\n",
      "l2 norm of weights: 5.898690225729723\n",
      "---------------------\n",
      "Iteration Number: 5226\n",
      "Loss: 35.916985512051035\n",
      "l2 norm of gradients: 0.25064924375034037\n",
      "l2 norm of weights: 5.8986041847697015\n",
      "---------------------\n",
      "Iteration Number: 5227\n",
      "Loss: 35.91383504329715\n",
      "l2 norm of gradients: 0.2506155022431409\n",
      "l2 norm of weights: 5.898518152212079\n",
      "---------------------\n",
      "Iteration Number: 5228\n",
      "Loss: 35.91068542121639\n",
      "l2 norm of gradients: 0.2505817744160365\n",
      "l2 norm of weights: 5.898432128054865\n",
      "---------------------\n",
      "Iteration Number: 5229\n",
      "Loss: 35.90753664533869\n",
      "l2 norm of gradients: 0.2505480602586982\n",
      "l2 norm of weights: 5.8983461122960685\n",
      "---------------------\n",
      "Iteration Number: 5230\n",
      "Loss: 35.904388715217756\n",
      "l2 norm of gradients: 0.2505143597608067\n",
      "l2 norm of weights: 5.898260104933699\n",
      "---------------------\n",
      "Iteration Number: 5231\n",
      "Loss: 35.90124163041319\n",
      "l2 norm of gradients: 0.2504806729120527\n",
      "l2 norm of weights: 5.8981741059657695\n",
      "---------------------\n",
      "Iteration Number: 5232\n",
      "Loss: 35.898095390456014\n",
      "l2 norm of gradients: 0.25044699970213674\n",
      "l2 norm of weights: 5.8980881153902915\n",
      "---------------------\n",
      "Iteration Number: 5233\n",
      "Loss: 35.894949994907705\n",
      "l2 norm of gradients: 0.2504133401207691\n",
      "l2 norm of weights: 5.8980021332052806\n",
      "---------------------\n",
      "Iteration Number: 5234\n",
      "Loss: 35.89180544331311\n",
      "l2 norm of gradients: 0.25037969415767014\n",
      "l2 norm of weights: 5.897916159408751\n",
      "---------------------\n",
      "Iteration Number: 5235\n",
      "Loss: 35.888661735220516\n",
      "l2 norm of gradients: 0.2503460618025697\n",
      "l2 norm of weights: 5.897830193998717\n",
      "---------------------\n",
      "Iteration Number: 5236\n",
      "Loss: 35.88551887018144\n",
      "l2 norm of gradients: 0.2503124430452077\n",
      "l2 norm of weights: 5.897744236973199\n",
      "---------------------\n",
      "Iteration Number: 5237\n",
      "Loss: 35.882376847744936\n",
      "l2 norm of gradients: 0.2502788378753337\n",
      "l2 norm of weights: 5.897658288330214\n",
      "---------------------\n",
      "Iteration Number: 5238\n",
      "Loss: 35.87923566747068\n",
      "l2 norm of gradients: 0.2502452462827073\n",
      "l2 norm of weights: 5.897572348067781\n",
      "---------------------\n",
      "Iteration Number: 5239\n",
      "Loss: 35.876095328888226\n",
      "l2 norm of gradients: 0.25021166825709756\n",
      "l2 norm of weights: 5.897486416183922\n",
      "---------------------\n",
      "Iteration Number: 5240\n",
      "Loss: 35.87295583156968\n",
      "l2 norm of gradients: 0.25017810378828353\n",
      "l2 norm of weights: 5.897400492676658\n",
      "---------------------\n",
      "Iteration Number: 5241\n",
      "Loss: 35.8698171750585\n",
      "l2 norm of gradients: 0.25014455286605397\n",
      "l2 norm of weights: 5.8973145775440114\n",
      "---------------------\n",
      "Iteration Number: 5242\n",
      "Loss: 35.86667935890307\n",
      "l2 norm of gradients: 0.2501110154802074\n",
      "l2 norm of weights: 5.897228670784008\n",
      "---------------------\n",
      "Iteration Number: 5243\n",
      "Loss: 35.86354238266603\n",
      "l2 norm of gradients: 0.25007749162055215\n",
      "l2 norm of weights: 5.897142772394671\n",
      "---------------------\n",
      "Iteration Number: 5244\n",
      "Loss: 35.86040624588652\n",
      "l2 norm of gradients: 0.25004398127690614\n",
      "l2 norm of weights: 5.897056882374029\n",
      "---------------------\n",
      "Iteration Number: 5245\n",
      "Loss: 35.85727094813135\n",
      "l2 norm of gradients: 0.2500104844390972\n",
      "l2 norm of weights: 5.896971000720108\n",
      "---------------------\n",
      "Iteration Number: 5246\n",
      "Loss: 35.85413648895593\n",
      "l2 norm of gradients: 0.24997700109696266\n",
      "l2 norm of weights: 5.896885127430936\n",
      "---------------------\n",
      "Iteration Number: 5247\n",
      "Loss: 35.851002867893925\n",
      "l2 norm of gradients: 0.24994353124034988\n",
      "l2 norm of weights: 5.896799262504544\n",
      "---------------------\n",
      "Iteration Number: 5248\n",
      "Loss: 35.84787008452056\n",
      "l2 norm of gradients: 0.2499100748591156\n",
      "l2 norm of weights: 5.8967134059389625\n",
      "---------------------\n",
      "Iteration Number: 5249\n",
      "Loss: 35.8447381383847\n",
      "l2 norm of gradients: 0.2498766319431265\n",
      "l2 norm of weights: 5.896627557732223\n",
      "---------------------\n",
      "Iteration Number: 5250\n",
      "Loss: 35.84160702903607\n",
      "l2 norm of gradients: 0.24984320248225886\n",
      "l2 norm of weights: 5.896541717882358\n",
      "---------------------\n",
      "Iteration Number: 5251\n",
      "Loss: 35.83847675603551\n",
      "l2 norm of gradients: 0.24980978646639848\n",
      "l2 norm of weights: 5.896455886387403\n",
      "---------------------\n",
      "Iteration Number: 5252\n",
      "Loss: 35.83534731893498\n",
      "l2 norm of gradients: 0.24977638388544113\n",
      "l2 norm of weights: 5.896370063245392\n",
      "---------------------\n",
      "Iteration Number: 5253\n",
      "Loss: 35.83221871729713\n",
      "l2 norm of gradients: 0.24974299472929196\n",
      "l2 norm of weights: 5.8962842484543625\n",
      "---------------------\n",
      "Iteration Number: 5254\n",
      "Loss: 35.82909095067462\n",
      "l2 norm of gradients: 0.24970961898786592\n",
      "l2 norm of weights: 5.896198442012352\n",
      "---------------------\n",
      "Iteration Number: 5255\n",
      "Loss: 35.825964018627744\n",
      "l2 norm of gradients: 0.24967625665108753\n",
      "l2 norm of weights: 5.896112643917399\n",
      "---------------------\n",
      "Iteration Number: 5256\n",
      "Loss: 35.82283792070168\n",
      "l2 norm of gradients: 0.24964290770889108\n",
      "l2 norm of weights: 5.896026854167542\n",
      "---------------------\n",
      "Iteration Number: 5257\n",
      "Loss: 35.81971265647268\n",
      "l2 norm of gradients: 0.2496095721512203\n",
      "l2 norm of weights: 5.895941072760824\n",
      "---------------------\n",
      "Iteration Number: 5258\n",
      "Loss: 35.816588225487465\n",
      "l2 norm of gradients: 0.24957624996802855\n",
      "l2 norm of weights: 5.895855299695284\n",
      "---------------------\n",
      "Iteration Number: 5259\n",
      "Loss: 35.81346462730888\n",
      "l2 norm of gradients: 0.24954294114927889\n",
      "l2 norm of weights: 5.8957695349689665\n",
      "---------------------\n",
      "Iteration Number: 5260\n",
      "Loss: 35.81034186149585\n",
      "l2 norm of gradients: 0.24950964568494405\n",
      "l2 norm of weights: 5.895683778579917\n",
      "---------------------\n",
      "Iteration Number: 5261\n",
      "Loss: 35.80721992760426\n",
      "l2 norm of gradients: 0.24947636356500613\n",
      "l2 norm of weights: 5.895598030526179\n",
      "---------------------\n",
      "Iteration Number: 5262\n",
      "Loss: 35.804098825196476\n",
      "l2 norm of gradients: 0.24944309477945684\n",
      "l2 norm of weights: 5.895512290805799\n",
      "---------------------\n",
      "Iteration Number: 5263\n",
      "Loss: 35.80097855383017\n",
      "l2 norm of gradients: 0.24940983931829758\n",
      "l2 norm of weights: 5.895426559416825\n",
      "---------------------\n",
      "Iteration Number: 5264\n",
      "Loss: 35.79785911306883\n",
      "l2 norm of gradients: 0.24937659717153926\n",
      "l2 norm of weights: 5.895340836357307\n",
      "---------------------\n",
      "Iteration Number: 5265\n",
      "Loss: 35.79474050247329\n",
      "l2 norm of gradients: 0.24934336832920231\n",
      "l2 norm of weights: 5.895255121625292\n",
      "---------------------\n",
      "Iteration Number: 5266\n",
      "Loss: 35.79162272160046\n",
      "l2 norm of gradients: 0.2493101527813167\n",
      "l2 norm of weights: 5.895169415218831\n",
      "---------------------\n",
      "Iteration Number: 5267\n",
      "Loss: 35.788505770014496\n",
      "l2 norm of gradients: 0.249276950517922\n",
      "l2 norm of weights: 5.895083717135978\n",
      "---------------------\n",
      "Iteration Number: 5268\n",
      "Loss: 35.785389647284276\n",
      "l2 norm of gradients: 0.2492437615290672\n",
      "l2 norm of weights: 5.8949980273747835\n",
      "---------------------\n",
      "Iteration Number: 5269\n",
      "Loss: 35.7822743529644\n",
      "l2 norm of gradients: 0.2492105858048108\n",
      "l2 norm of weights: 5.894912345933302\n",
      "---------------------\n",
      "Iteration Number: 5270\n",
      "Loss: 35.77915988661275\n",
      "l2 norm of gradients: 0.24917742333522103\n",
      "l2 norm of weights: 5.89482667280959\n",
      "---------------------\n",
      "Iteration Number: 5271\n",
      "Loss: 35.77604624780415\n",
      "l2 norm of gradients: 0.24914427411037518\n",
      "l2 norm of weights: 5.894741008001703\n",
      "---------------------\n",
      "Iteration Number: 5272\n",
      "Loss: 35.77293343609539\n",
      "l2 norm of gradients: 0.24911113812036048\n",
      "l2 norm of weights: 5.894655351507699\n",
      "---------------------\n",
      "Iteration Number: 5273\n",
      "Loss: 35.7698214510539\n",
      "l2 norm of gradients: 0.2490780153552733\n",
      "l2 norm of weights: 5.894569703325635\n",
      "---------------------\n",
      "Iteration Number: 5274\n",
      "Loss: 35.76671029223848\n",
      "l2 norm of gradients: 0.24904490580521968\n",
      "l2 norm of weights: 5.894484063453572\n",
      "---------------------\n",
      "Iteration Number: 5275\n",
      "Loss: 35.76359995922143\n",
      "l2 norm of gradients: 0.249011809460315\n",
      "l2 norm of weights: 5.8943984318895675\n",
      "---------------------\n",
      "Iteration Number: 5276\n",
      "Loss: 35.76049045156011\n",
      "l2 norm of gradients: 0.24897872631068424\n",
      "l2 norm of weights: 5.894312808631686\n",
      "---------------------\n",
      "Iteration Number: 5277\n",
      "Loss: 35.75738176882413\n",
      "l2 norm of gradients: 0.24894565634646162\n",
      "l2 norm of weights: 5.894227193677991\n",
      "---------------------\n",
      "Iteration Number: 5278\n",
      "Loss: 35.75427391057095\n",
      "l2 norm of gradients: 0.24891259955779088\n",
      "l2 norm of weights: 5.894141587026544\n",
      "---------------------\n",
      "Iteration Number: 5279\n",
      "Loss: 35.751166876381404\n",
      "l2 norm of gradients: 0.24887955593482533\n",
      "l2 norm of weights: 5.894055988675412\n",
      "---------------------\n",
      "Iteration Number: 5280\n",
      "Loss: 35.74806066581078\n",
      "l2 norm of gradients: 0.24884652546772731\n",
      "l2 norm of weights: 5.893970398622658\n",
      "---------------------\n",
      "Iteration Number: 5281\n",
      "Loss: 35.74495527843101\n",
      "l2 norm of gradients: 0.24881350814666905\n",
      "l2 norm of weights: 5.893884816866352\n",
      "---------------------\n",
      "Iteration Number: 5282\n",
      "Loss: 35.74185071380381\n",
      "l2 norm of gradients: 0.24878050396183168\n",
      "l2 norm of weights: 5.893799243404561\n",
      "---------------------\n",
      "Iteration Number: 5283\n",
      "Loss: 35.73874697150175\n",
      "l2 norm of gradients: 0.2487475129034062\n",
      "l2 norm of weights: 5.893713678235355\n",
      "---------------------\n",
      "Iteration Number: 5284\n",
      "Loss: 35.73564405109144\n",
      "l2 norm of gradients: 0.2487145349615926\n",
      "l2 norm of weights: 5.8936281213568025\n",
      "---------------------\n",
      "Iteration Number: 5285\n",
      "Loss: 35.73254195213889\n",
      "l2 norm of gradients: 0.24868157012660053\n",
      "l2 norm of weights: 5.893542572766977\n",
      "---------------------\n",
      "Iteration Number: 5286\n",
      "Loss: 35.72944067421791\n",
      "l2 norm of gradients: 0.24864861838864866\n",
      "l2 norm of weights: 5.8934570324639495\n",
      "---------------------\n",
      "Iteration Number: 5287\n",
      "Loss: 35.72634021689252\n",
      "l2 norm of gradients: 0.2486156797379653\n",
      "l2 norm of weights: 5.893371500445794\n",
      "---------------------\n",
      "Iteration Number: 5288\n",
      "Loss: 35.72324057973256\n",
      "l2 norm of gradients: 0.24858275416478812\n",
      "l2 norm of weights: 5.893285976710585\n",
      "---------------------\n",
      "Iteration Number: 5289\n",
      "Loss: 35.72014176230917\n",
      "l2 norm of gradients: 0.2485498416593638\n",
      "l2 norm of weights: 5.893200461256399\n",
      "---------------------\n",
      "Iteration Number: 5290\n",
      "Loss: 35.717043764194536\n",
      "l2 norm of gradients: 0.24851694221194873\n",
      "l2 norm of weights: 5.8931149540813115\n",
      "---------------------\n",
      "Iteration Number: 5291\n",
      "Loss: 35.713946584957164\n",
      "l2 norm of gradients: 0.24848405581280833\n",
      "l2 norm of weights: 5.8930294551834015\n",
      "---------------------\n",
      "Iteration Number: 5292\n",
      "Loss: 35.71085022416935\n",
      "l2 norm of gradients: 0.24845118245221753\n",
      "l2 norm of weights: 5.892943964560747\n",
      "---------------------\n",
      "Iteration Number: 5293\n",
      "Loss: 35.7077546813946\n",
      "l2 norm of gradients: 0.24841832212046033\n",
      "l2 norm of weights: 5.892858482211429\n",
      "---------------------\n",
      "Iteration Number: 5294\n",
      "Loss: 35.70465995620977\n",
      "l2 norm of gradients: 0.24838547480783035\n",
      "l2 norm of weights: 5.892773008133528\n",
      "---------------------\n",
      "Iteration Number: 5295\n",
      "Loss: 35.701566048194756\n",
      "l2 norm of gradients: 0.24835264050463\n",
      "l2 norm of weights: 5.892687542325124\n",
      "---------------------\n",
      "Iteration Number: 5296\n",
      "Loss: 35.698472956901554\n",
      "l2 norm of gradients: 0.2483198192011715\n",
      "l2 norm of weights: 5.892602084784303\n",
      "---------------------\n",
      "Iteration Number: 5297\n",
      "Loss: 35.695380681929436\n",
      "l2 norm of gradients: 0.2482870108877759\n",
      "l2 norm of weights: 5.892516635509148\n",
      "---------------------\n",
      "Iteration Number: 5298\n",
      "Loss: 35.692289222828364\n",
      "l2 norm of gradients: 0.24825421555477378\n",
      "l2 norm of weights: 5.892431194497746\n",
      "---------------------\n",
      "Iteration Number: 5299\n",
      "Loss: 35.68919857918203\n",
      "l2 norm of gradients: 0.24822143319250478\n",
      "l2 norm of weights: 5.892345761748182\n",
      "---------------------\n",
      "Iteration Number: 5300\n",
      "Loss: 35.68610875056221\n",
      "l2 norm of gradients: 0.24818866379131793\n",
      "l2 norm of weights: 5.892260337258541\n",
      "---------------------\n",
      "Iteration Number: 5301\n",
      "Loss: 35.68301973654958\n",
      "l2 norm of gradients: 0.24815590734157136\n",
      "l2 norm of weights: 5.892174921026916\n",
      "---------------------\n",
      "Iteration Number: 5302\n",
      "Loss: 35.67993153670571\n",
      "l2 norm of gradients: 0.24812316383363253\n",
      "l2 norm of weights: 5.892089513051394\n",
      "---------------------\n",
      "Iteration Number: 5303\n",
      "Loss: 35.67684415061891\n",
      "l2 norm of gradients: 0.24809043325787788\n",
      "l2 norm of weights: 5.892004113330065\n",
      "---------------------\n",
      "Iteration Number: 5304\n",
      "Loss: 35.67375757784798\n",
      "l2 norm of gradients: 0.2480577156046934\n",
      "l2 norm of weights: 5.891918721861023\n",
      "---------------------\n",
      "Iteration Number: 5305\n",
      "Loss: 35.67067181798311\n",
      "l2 norm of gradients: 0.24802501086447398\n",
      "l2 norm of weights: 5.891833338642358\n",
      "---------------------\n",
      "Iteration Number: 5306\n",
      "Loss: 35.66758687058518\n",
      "l2 norm of gradients: 0.24799231902762378\n",
      "l2 norm of weights: 5.891747963672166\n",
      "---------------------\n",
      "Iteration Number: 5307\n",
      "Loss: 35.66450273525192\n",
      "l2 norm of gradients: 0.2479596400845562\n",
      "l2 norm of weights: 5.89166259694854\n",
      "---------------------\n",
      "Iteration Number: 5308\n",
      "Loss: 35.661419411544315\n",
      "l2 norm of gradients: 0.24792697402569377\n",
      "l2 norm of weights: 5.891577238469576\n",
      "---------------------\n",
      "Iteration Number: 5309\n",
      "Loss: 35.65833689904256\n",
      "l2 norm of gradients: 0.24789432084146806\n",
      "l2 norm of weights: 5.891491888233372\n",
      "---------------------\n",
      "Iteration Number: 5310\n",
      "Loss: 35.65525519732389\n",
      "l2 norm of gradients: 0.24786168052231988\n",
      "l2 norm of weights: 5.891406546238026\n",
      "---------------------\n",
      "Iteration Number: 5311\n",
      "Loss: 35.652174305970874\n",
      "l2 norm of gradients: 0.24782905305869923\n",
      "l2 norm of weights: 5.891321212481638\n",
      "---------------------\n",
      "Iteration Number: 5312\n",
      "Loss: 35.64909422454766\n",
      "l2 norm of gradients: 0.24779643844106516\n",
      "l2 norm of weights: 5.8912358869623045\n",
      "---------------------\n",
      "Iteration Number: 5313\n",
      "Loss: 35.64601495265054\n",
      "l2 norm of gradients: 0.24776383665988597\n",
      "l2 norm of weights: 5.891150569678129\n",
      "---------------------\n",
      "Iteration Number: 5314\n",
      "Loss: 35.642936489843\n",
      "l2 norm of gradients: 0.24773124770563876\n",
      "l2 norm of weights: 5.891065260627213\n",
      "---------------------\n",
      "Iteration Number: 5315\n",
      "Loss: 35.639858835712495\n",
      "l2 norm of gradients: 0.24769867156881006\n",
      "l2 norm of weights: 5.890979959807661\n",
      "---------------------\n",
      "Iteration Number: 5316\n",
      "Loss: 35.63678198983588\n",
      "l2 norm of gradients: 0.24766610823989532\n",
      "l2 norm of weights: 5.890894667217576\n",
      "---------------------\n",
      "Iteration Number: 5317\n",
      "Loss: 35.633705951796514\n",
      "l2 norm of gradients: 0.24763355770939918\n",
      "l2 norm of weights: 5.890809382855062\n",
      "---------------------\n",
      "Iteration Number: 5318\n",
      "Loss: 35.63063072116624\n",
      "l2 norm of gradients: 0.2476010199678352\n",
      "l2 norm of weights: 5.8907241067182285\n",
      "---------------------\n",
      "Iteration Number: 5319\n",
      "Loss: 35.627556297529786\n",
      "l2 norm of gradients: 0.24756849500572625\n",
      "l2 norm of weights: 5.89063883880518\n",
      "---------------------\n",
      "Iteration Number: 5320\n",
      "Loss: 35.624482680474394\n",
      "l2 norm of gradients: 0.2475359828136039\n",
      "l2 norm of weights: 5.890553579114027\n",
      "---------------------\n",
      "Iteration Number: 5321\n",
      "Loss: 35.621409869572624\n",
      "l2 norm of gradients: 0.24750348338200917\n",
      "l2 norm of weights: 5.8904683276428775\n",
      "---------------------\n",
      "Iteration Number: 5322\n",
      "Loss: 35.61833786440948\n",
      "l2 norm of gradients: 0.2474709967014919\n",
      "l2 norm of weights: 5.890383084389843\n",
      "---------------------\n",
      "Iteration Number: 5323\n",
      "Loss: 35.61526666456377\n",
      "l2 norm of gradients: 0.24743852276261097\n",
      "l2 norm of weights: 5.890297849353033\n",
      "---------------------\n",
      "Iteration Number: 5324\n",
      "Loss: 35.61219626962462\n",
      "l2 norm of gradients: 0.24740606155593428\n",
      "l2 norm of weights: 5.890212622530562\n",
      "---------------------\n",
      "Iteration Number: 5325\n",
      "Loss: 35.60912667916745\n",
      "l2 norm of gradients: 0.2473736130720388\n",
      "l2 norm of weights: 5.890127403920544\n",
      "---------------------\n",
      "Iteration Number: 5326\n",
      "Loss: 35.6060578927762\n",
      "l2 norm of gradients: 0.2473411773015105\n",
      "l2 norm of weights: 5.890042193521091\n",
      "---------------------\n",
      "Iteration Number: 5327\n",
      "Loss: 35.6029899100342\n",
      "l2 norm of gradients: 0.2473087542349443\n",
      "l2 norm of weights: 5.889956991330321\n",
      "---------------------\n",
      "Iteration Number: 5328\n",
      "Loss: 35.59992273053218\n",
      "l2 norm of gradients: 0.24727634386294417\n",
      "l2 norm of weights: 5.889871797346349\n",
      "---------------------\n",
      "Iteration Number: 5329\n",
      "Loss: 35.59685635384638\n",
      "l2 norm of gradients: 0.24724394617612291\n",
      "l2 norm of weights: 5.889786611567295\n",
      "---------------------\n",
      "Iteration Number: 5330\n",
      "Loss: 35.59379077955843\n",
      "l2 norm of gradients: 0.24721156116510257\n",
      "l2 norm of weights: 5.889701433991275\n",
      "---------------------\n",
      "Iteration Number: 5331\n",
      "Loss: 35.590726007260706\n",
      "l2 norm of gradients: 0.24717918882051387\n",
      "l2 norm of weights: 5.889616264616412\n",
      "---------------------\n",
      "Iteration Number: 5332\n",
      "Loss: 35.587662036539086\n",
      "l2 norm of gradients: 0.24714682913299665\n",
      "l2 norm of weights: 5.889531103440823\n",
      "---------------------\n",
      "Iteration Number: 5333\n",
      "Loss: 35.58459886696511\n",
      "l2 norm of gradients: 0.2471144820931997\n",
      "l2 norm of weights: 5.8894459504626315\n",
      "---------------------\n",
      "Iteration Number: 5334\n",
      "Loss: 35.581536498143365\n",
      "l2 norm of gradients: 0.24708214769178058\n",
      "l2 norm of weights: 5.889360805679961\n",
      "---------------------\n",
      "Iteration Number: 5335\n",
      "Loss: 35.57847492965003\n",
      "l2 norm of gradients: 0.2470498259194061\n",
      "l2 norm of weights: 5.889275669090935\n",
      "---------------------\n",
      "Iteration Number: 5336\n",
      "Loss: 35.57541416106639\n",
      "l2 norm of gradients: 0.24701751676675163\n",
      "l2 norm of weights: 5.8891905406936775\n",
      "---------------------\n",
      "Iteration Number: 5337\n",
      "Loss: 35.57235419198553\n",
      "l2 norm of gradients: 0.24698522022450167\n",
      "l2 norm of weights: 5.889105420486315\n",
      "---------------------\n",
      "Iteration Number: 5338\n",
      "Loss: 35.569295022001796\n",
      "l2 norm of gradients: 0.24695293628334955\n",
      "l2 norm of weights: 5.889020308466975\n",
      "---------------------\n",
      "Iteration Number: 5339\n",
      "Loss: 35.566236650687316\n",
      "l2 norm of gradients: 0.24692066493399756\n",
      "l2 norm of weights: 5.888935204633784\n",
      "---------------------\n",
      "Iteration Number: 5340\n",
      "Loss: 35.563179077637784\n",
      "l2 norm of gradients: 0.2468884061671567\n",
      "l2 norm of weights: 5.8888501089848715\n",
      "---------------------\n",
      "Iteration Number: 5341\n",
      "Loss: 35.560122302443716\n",
      "l2 norm of gradients: 0.24685615997354712\n",
      "l2 norm of weights: 5.888765021518369\n",
      "---------------------\n",
      "Iteration Number: 5342\n",
      "Loss: 35.55706632468544\n",
      "l2 norm of gradients: 0.24682392634389763\n",
      "l2 norm of weights: 5.888679942232405\n",
      "---------------------\n",
      "Iteration Number: 5343\n",
      "Loss: 35.55401114396114\n",
      "l2 norm of gradients: 0.2467917052689459\n",
      "l2 norm of weights: 5.888594871125113\n",
      "---------------------\n",
      "Iteration Number: 5344\n",
      "Loss: 35.550956759853456\n",
      "l2 norm of gradients: 0.24675949673943856\n",
      "l2 norm of weights: 5.888509808194627\n",
      "---------------------\n",
      "Iteration Number: 5345\n",
      "Loss: 35.54790317195003\n",
      "l2 norm of gradients: 0.24672730074613106\n",
      "l2 norm of weights: 5.888424753439078\n",
      "---------------------\n",
      "Iteration Number: 5346\n",
      "Loss: 35.54485037985289\n",
      "l2 norm of gradients: 0.24669511727978757\n",
      "l2 norm of weights: 5.888339706856603\n",
      "---------------------\n",
      "Iteration Number: 5347\n",
      "Loss: 35.541798383138264\n",
      "l2 norm of gradients: 0.24666294633118122\n",
      "l2 norm of weights: 5.888254668445338\n",
      "---------------------\n",
      "Iteration Number: 5348\n",
      "Loss: 35.538747181403345\n",
      "l2 norm of gradients: 0.24663078789109386\n",
      "l2 norm of weights: 5.888169638203419\n",
      "---------------------\n",
      "Iteration Number: 5349\n",
      "Loss: 35.535696774243284\n",
      "l2 norm of gradients: 0.24659864195031628\n",
      "l2 norm of weights: 5.888084616128987\n",
      "---------------------\n",
      "Iteration Number: 5350\n",
      "Loss: 35.53264716123047\n",
      "l2 norm of gradients: 0.2465665084996479\n",
      "l2 norm of weights: 5.8879996022201775\n",
      "---------------------\n",
      "Iteration Number: 5351\n",
      "Loss: 35.52959834197801\n",
      "l2 norm of gradients: 0.24653438752989704\n",
      "l2 norm of weights: 5.887914596475132\n",
      "---------------------\n",
      "Iteration Number: 5352\n",
      "Loss: 35.526550316070285\n",
      "l2 norm of gradients: 0.24650227903188077\n",
      "l2 norm of weights: 5.887829598891992\n",
      "---------------------\n",
      "Iteration Number: 5353\n",
      "Loss: 35.52350308309594\n",
      "l2 norm of gradients: 0.24647018299642498\n",
      "l2 norm of weights: 5.887744609468898\n",
      "---------------------\n",
      "Iteration Number: 5354\n",
      "Loss: 35.52045664265529\n",
      "l2 norm of gradients: 0.24643809941436423\n",
      "l2 norm of weights: 5.887659628203996\n",
      "---------------------\n",
      "Iteration Number: 5355\n",
      "Loss: 35.51741099432574\n",
      "l2 norm of gradients: 0.2464060282765419\n",
      "l2 norm of weights: 5.887574655095427\n",
      "---------------------\n",
      "Iteration Number: 5356\n",
      "Loss: 35.51436613772152\n",
      "l2 norm of gradients: 0.24637396957381014\n",
      "l2 norm of weights: 5.887489690141338\n",
      "---------------------\n",
      "Iteration Number: 5357\n",
      "Loss: 35.511322072416704\n",
      "l2 norm of gradients: 0.24634192329702978\n",
      "l2 norm of weights: 5.887404733339875\n",
      "---------------------\n",
      "Iteration Number: 5358\n",
      "Loss: 35.50827879801939\n",
      "l2 norm of gradients: 0.24630988943707033\n",
      "l2 norm of weights: 5.887319784689184\n",
      "---------------------\n",
      "Iteration Number: 5359\n",
      "Loss: 35.505236314118015\n",
      "l2 norm of gradients: 0.2462778679848103\n",
      "l2 norm of weights: 5.887234844187414\n",
      "---------------------\n",
      "Iteration Number: 5360\n",
      "Loss: 35.50219462030882\n",
      "l2 norm of gradients: 0.24624585893113662\n",
      "l2 norm of weights: 5.887149911832712\n",
      "---------------------\n",
      "Iteration Number: 5361\n",
      "Loss: 35.49915371617727\n",
      "l2 norm of gradients: 0.24621386226694494\n",
      "l2 norm of weights: 5.887064987623233\n",
      "---------------------\n",
      "Iteration Number: 5362\n",
      "Loss: 35.49611360133073\n",
      "l2 norm of gradients: 0.24618187798313984\n",
      "l2 norm of weights: 5.886980071557122\n",
      "---------------------\n",
      "Iteration Number: 5363\n",
      "Loss: 35.49307427536138\n",
      "l2 norm of gradients: 0.24614990607063428\n",
      "l2 norm of weights: 5.886895163632536\n",
      "---------------------\n",
      "Iteration Number: 5364\n",
      "Loss: 35.490035737870116\n",
      "l2 norm of gradients: 0.24611794652035018\n",
      "l2 norm of weights: 5.886810263847626\n",
      "---------------------\n",
      "Iteration Number: 5365\n",
      "Loss: 35.48699798843667\n",
      "l2 norm of gradients: 0.246085999323218\n",
      "l2 norm of weights: 5.886725372200544\n",
      "---------------------\n",
      "Iteration Number: 5366\n",
      "Loss: 35.48396102667214\n",
      "l2 norm of gradients: 0.24605406447017683\n",
      "l2 norm of weights: 5.88664048868945\n",
      "---------------------\n",
      "Iteration Number: 5367\n",
      "Loss: 35.48092485216744\n",
      "l2 norm of gradients: 0.24602214195217448\n",
      "l2 norm of weights: 5.886555613312496\n",
      "---------------------\n",
      "Iteration Number: 5368\n",
      "Loss: 35.4778894645259\n",
      "l2 norm of gradients: 0.24599023176016738\n",
      "l2 norm of weights: 5.88647074606784\n",
      "---------------------\n",
      "Iteration Number: 5369\n",
      "Loss: 35.4748548633427\n",
      "l2 norm of gradients: 0.2459583338851207\n",
      "l2 norm of weights: 5.88638588695364\n",
      "---------------------\n",
      "Iteration Number: 5370\n",
      "Loss: 35.47182104821023\n",
      "l2 norm of gradients: 0.24592644831800806\n",
      "l2 norm of weights: 5.8863010359680565\n",
      "---------------------\n",
      "Iteration Number: 5371\n",
      "Loss: 35.46878801872581\n",
      "l2 norm of gradients: 0.24589457504981171\n",
      "l2 norm of weights: 5.886216193109248\n",
      "---------------------\n",
      "Iteration Number: 5372\n",
      "Loss: 35.465755774496486\n",
      "l2 norm of gradients: 0.2458627140715228\n",
      "l2 norm of weights: 5.8861313583753745\n",
      "---------------------\n",
      "Iteration Number: 5373\n",
      "Loss: 35.46272431512571\n",
      "l2 norm of gradients: 0.24583086537414067\n",
      "l2 norm of weights: 5.886046531764601\n",
      "---------------------\n",
      "Iteration Number: 5374\n",
      "Loss: 35.45969364019241\n",
      "l2 norm of gradients: 0.24579902894867361\n",
      "l2 norm of weights: 5.885961713275087\n",
      "---------------------\n",
      "Iteration Number: 5375\n",
      "Loss: 35.456663749315126\n",
      "l2 norm of gradients: 0.24576720478613825\n",
      "l2 norm of weights: 5.885876902904998\n",
      "---------------------\n",
      "Iteration Number: 5376\n",
      "Loss: 35.453634642081695\n",
      "l2 norm of gradients: 0.24573539287756\n",
      "l2 norm of weights: 5.885792100652501\n",
      "---------------------\n",
      "Iteration Number: 5377\n",
      "Loss: 35.45060631810388\n",
      "l2 norm of gradients: 0.24570359321397267\n",
      "l2 norm of weights: 5.885707306515759\n",
      "---------------------\n",
      "Iteration Number: 5378\n",
      "Loss: 35.447578776968996\n",
      "l2 norm of gradients: 0.2456718057864188\n",
      "l2 norm of weights: 5.885622520492939\n",
      "---------------------\n",
      "Iteration Number: 5379\n",
      "Loss: 35.44455201828564\n",
      "l2 norm of gradients: 0.2456400305859493\n",
      "l2 norm of weights: 5.88553774258221\n",
      "---------------------\n",
      "Iteration Number: 5380\n",
      "Loss: 35.44152604165994\n",
      "l2 norm of gradients: 0.24560826760362378\n",
      "l2 norm of weights: 5.885452972781741\n",
      "---------------------\n",
      "Iteration Number: 5381\n",
      "Loss: 35.43850084668191\n",
      "l2 norm of gradients: 0.24557651683051024\n",
      "l2 norm of weights: 5.885368211089701\n",
      "---------------------\n",
      "Iteration Number: 5382\n",
      "Loss: 35.435476432956186\n",
      "l2 norm of gradients: 0.24554477825768545\n",
      "l2 norm of weights: 5.885283457504261\n",
      "---------------------\n",
      "Iteration Number: 5383\n",
      "Loss: 35.4324528000929\n",
      "l2 norm of gradients: 0.24551305187623437\n",
      "l2 norm of weights: 5.885198712023593\n",
      "---------------------\n",
      "Iteration Number: 5384\n",
      "Loss: 35.42942994768915\n",
      "l2 norm of gradients: 0.24548133767725075\n",
      "l2 norm of weights: 5.885113974645869\n",
      "---------------------\n",
      "Iteration Number: 5385\n",
      "Loss: 35.426407875348566\n",
      "l2 norm of gradients: 0.24544963565183683\n",
      "l2 norm of weights: 5.885029245369265\n",
      "---------------------\n",
      "Iteration Number: 5386\n",
      "Loss: 35.42338658266912\n",
      "l2 norm of gradients: 0.24541794579110307\n",
      "l2 norm of weights: 5.884944524191952\n",
      "---------------------\n",
      "Iteration Number: 5387\n",
      "Loss: 35.42036606925852\n",
      "l2 norm of gradients: 0.24538626808616876\n",
      "l2 norm of weights: 5.884859811112109\n",
      "---------------------\n",
      "Iteration Number: 5388\n",
      "Loss: 35.417346334729025\n",
      "l2 norm of gradients: 0.24535460252816146\n",
      "l2 norm of weights: 5.884775106127909\n",
      "---------------------\n",
      "Iteration Number: 5389\n",
      "Loss: 35.414327378671516\n",
      "l2 norm of gradients: 0.2453229491082173\n",
      "l2 norm of weights: 5.884690409237533\n",
      "---------------------\n",
      "Iteration Number: 5390\n",
      "Loss: 35.41130920069032\n",
      "l2 norm of gradients: 0.24529130781748082\n",
      "l2 norm of weights: 5.884605720439159\n",
      "---------------------\n",
      "Iteration Number: 5391\n",
      "Loss: 35.40829180040366\n",
      "l2 norm of gradients: 0.24525967864710507\n",
      "l2 norm of weights: 5.8845210397309655\n",
      "---------------------\n",
      "Iteration Number: 5392\n",
      "Loss: 35.40527517740697\n",
      "l2 norm of gradients: 0.24522806158825144\n",
      "l2 norm of weights: 5.8844363671111335\n",
      "---------------------\n",
      "Iteration Number: 5393\n",
      "Loss: 35.40225933130507\n",
      "l2 norm of gradients: 0.2451964566320899\n",
      "l2 norm of weights: 5.8843517025778445\n",
      "---------------------\n",
      "Iteration Number: 5394\n",
      "Loss: 35.39924426170273\n",
      "l2 norm of gradients: 0.24516486376979876\n",
      "l2 norm of weights: 5.884267046129279\n",
      "---------------------\n",
      "Iteration Number: 5395\n",
      "Loss: 35.396229968219\n",
      "l2 norm of gradients: 0.24513328299256473\n",
      "l2 norm of weights: 5.884182397763624\n",
      "---------------------\n",
      "Iteration Number: 5396\n",
      "Loss: 35.39321645043857\n",
      "l2 norm of gradients: 0.24510171429158303\n",
      "l2 norm of weights: 5.884097757479061\n",
      "---------------------\n",
      "Iteration Number: 5397\n",
      "Loss: 35.390203707987105\n",
      "l2 norm of gradients: 0.24507015765805723\n",
      "l2 norm of weights: 5.884013125273777\n",
      "---------------------\n",
      "Iteration Number: 5398\n",
      "Loss: 35.387191740463074\n",
      "l2 norm of gradients: 0.24503861308319932\n",
      "l2 norm of weights: 5.883928501145957\n",
      "---------------------\n",
      "Iteration Number: 5399\n",
      "Loss: 35.38418054748108\n",
      "l2 norm of gradients: 0.2450070805582296\n",
      "l2 norm of weights: 5.883843885093789\n",
      "---------------------\n",
      "Iteration Number: 5400\n",
      "Loss: 35.381170128634515\n",
      "l2 norm of gradients: 0.2449755600743768\n",
      "l2 norm of weights: 5.88375927711546\n",
      "---------------------\n",
      "Iteration Number: 5401\n",
      "Loss: 35.378160483547305\n",
      "l2 norm of gradients: 0.2449440516228781\n",
      "l2 norm of weights: 5.883674677209161\n",
      "---------------------\n",
      "Iteration Number: 5402\n",
      "Loss: 35.37515161181872\n",
      "l2 norm of gradients: 0.24491255519497904\n",
      "l2 norm of weights: 5.883590085373081\n",
      "---------------------\n",
      "Iteration Number: 5403\n",
      "Loss: 35.37214351305897\n",
      "l2 norm of gradients: 0.24488107078193325\n",
      "l2 norm of weights: 5.883505501605411\n",
      "---------------------\n",
      "Iteration Number: 5404\n",
      "Loss: 35.36913618687808\n",
      "l2 norm of gradients: 0.24484959837500317\n",
      "l2 norm of weights: 5.8834209259043435\n",
      "---------------------\n",
      "Iteration Number: 5405\n",
      "Loss: 35.36612963288093\n",
      "l2 norm of gradients: 0.24481813796545915\n",
      "l2 norm of weights: 5.883336358268071\n",
      "---------------------\n",
      "Iteration Number: 5406\n",
      "Loss: 35.36312385068392\n",
      "l2 norm of gradients: 0.24478668954458013\n",
      "l2 norm of weights: 5.883251798694788\n",
      "---------------------\n",
      "Iteration Number: 5407\n",
      "Loss: 35.36011883989164\n",
      "l2 norm of gradients: 0.24475525310365323\n",
      "l2 norm of weights: 5.883167247182688\n",
      "---------------------\n",
      "Iteration Number: 5408\n",
      "Loss: 35.3571146001236\n",
      "l2 norm of gradients: 0.24472382863397402\n",
      "l2 norm of weights: 5.8830827037299676\n",
      "---------------------\n",
      "Iteration Number: 5409\n",
      "Loss: 35.35411113097445\n",
      "l2 norm of gradients: 0.24469241612684622\n",
      "l2 norm of weights: 5.882998168334825\n",
      "---------------------\n",
      "Iteration Number: 5410\n",
      "Loss: 35.351108432066475\n",
      "l2 norm of gradients: 0.24466101557358208\n",
      "l2 norm of weights: 5.882913640995456\n",
      "---------------------\n",
      "Iteration Number: 5411\n",
      "Loss: 35.348106503015345\n",
      "l2 norm of gradients: 0.2446296269655018\n",
      "l2 norm of weights: 5.882829121710061\n",
      "---------------------\n",
      "Iteration Number: 5412\n",
      "Loss: 35.3451053434163\n",
      "l2 norm of gradients: 0.24459825029393428\n",
      "l2 norm of weights: 5.882744610476837\n",
      "---------------------\n",
      "Iteration Number: 5413\n",
      "Loss: 35.3421049529004\n",
      "l2 norm of gradients: 0.24456688555021627\n",
      "l2 norm of weights: 5.882660107293987\n",
      "---------------------\n",
      "Iteration Number: 5414\n",
      "Loss: 35.33910533106494\n",
      "l2 norm of gradients: 0.24453553272569312\n",
      "l2 norm of weights: 5.882575612159711\n",
      "---------------------\n",
      "Iteration Number: 5415\n",
      "Loss: 35.33610647752439\n",
      "l2 norm of gradients: 0.24450419181171834\n",
      "l2 norm of weights: 5.882491125072211\n",
      "---------------------\n",
      "Iteration Number: 5416\n",
      "Loss: 35.3331083919052\n",
      "l2 norm of gradients: 0.2444728627996536\n",
      "l2 norm of weights: 5.882406646029693\n",
      "---------------------\n",
      "Iteration Number: 5417\n",
      "Loss: 35.330111073805035\n",
      "l2 norm of gradients: 0.24444154568086887\n",
      "l2 norm of weights: 5.882322175030359\n",
      "---------------------\n",
      "Iteration Number: 5418\n",
      "Loss: 35.32711452283859\n",
      "l2 norm of gradients: 0.24441024044674242\n",
      "l2 norm of weights: 5.882237712072415\n",
      "---------------------\n",
      "Iteration Number: 5419\n",
      "Loss: 35.32411873862838\n",
      "l2 norm of gradients: 0.24437894708866065\n",
      "l2 norm of weights: 5.882153257154068\n",
      "---------------------\n",
      "Iteration Number: 5420\n",
      "Loss: 35.32112372078055\n",
      "l2 norm of gradients: 0.24434766559801824\n",
      "l2 norm of weights: 5.882068810273524\n",
      "---------------------\n",
      "Iteration Number: 5421\n",
      "Loss: 35.318129468914776\n",
      "l2 norm of gradients: 0.2443163959662181\n",
      "l2 norm of weights: 5.881984371428992\n",
      "---------------------\n",
      "Iteration Number: 5422\n",
      "Loss: 35.31513598264212\n",
      "l2 norm of gradients: 0.24428513818467118\n",
      "l2 norm of weights: 5.88189994061868\n",
      "---------------------\n",
      "Iteration Number: 5423\n",
      "Loss: 35.31214326158256\n",
      "l2 norm of gradients: 0.24425389224479682\n",
      "l2 norm of weights: 5.881815517840798\n",
      "---------------------\n",
      "Iteration Number: 5424\n",
      "Loss: 35.309151305344066\n",
      "l2 norm of gradients: 0.2442226581380225\n",
      "l2 norm of weights: 5.881731103093558\n",
      "---------------------\n",
      "Iteration Number: 5425\n",
      "Loss: 35.30616011355228\n",
      "l2 norm of gradients: 0.24419143585578382\n",
      "l2 norm of weights: 5.881646696375172\n",
      "---------------------\n",
      "Iteration Number: 5426\n",
      "Loss: 35.30316968580743\n",
      "l2 norm of gradients: 0.24416022538952467\n",
      "l2 norm of weights: 5.881562297683851\n",
      "---------------------\n",
      "Iteration Number: 5427\n",
      "Loss: 35.300180021743806\n",
      "l2 norm of gradients: 0.2441290267306969\n",
      "l2 norm of weights: 5.88147790701781\n",
      "---------------------\n",
      "Iteration Number: 5428\n",
      "Loss: 35.29719112096604\n",
      "l2 norm of gradients: 0.24409783987076064\n",
      "l2 norm of weights: 5.881393524375263\n",
      "---------------------\n",
      "Iteration Number: 5429\n",
      "Loss: 35.29420298309675\n",
      "l2 norm of gradients: 0.2440666648011843\n",
      "l2 norm of weights: 5.881309149754426\n",
      "---------------------\n",
      "Iteration Number: 5430\n",
      "Loss: 35.29121560775302\n",
      "l2 norm of gradients: 0.24403550151344405\n",
      "l2 norm of weights: 5.881224783153515\n",
      "---------------------\n",
      "Iteration Number: 5431\n",
      "Loss: 35.28822899454413\n",
      "l2 norm of gradients: 0.24400434999902465\n",
      "l2 norm of weights: 5.881140424570749\n",
      "---------------------\n",
      "Iteration Number: 5432\n",
      "Loss: 35.28524314309605\n",
      "l2 norm of gradients: 0.24397321024941865\n",
      "l2 norm of weights: 5.881056074004344\n",
      "---------------------\n",
      "Iteration Number: 5433\n",
      "Loss: 35.28225805302752\n",
      "l2 norm of gradients: 0.24394208225612687\n",
      "l2 norm of weights: 5.880971731452521\n",
      "---------------------\n",
      "Iteration Number: 5434\n",
      "Loss: 35.2792737239546\n",
      "l2 norm of gradients: 0.2439109660106581\n",
      "l2 norm of weights: 5.880887396913501\n",
      "---------------------\n",
      "Iteration Number: 5435\n",
      "Loss: 35.27629015549548\n",
      "l2 norm of gradients: 0.24387986150452945\n",
      "l2 norm of weights: 5.880803070385502\n",
      "---------------------\n",
      "Iteration Number: 5436\n",
      "Loss: 35.273307347272606\n",
      "l2 norm of gradients: 0.2438487687292659\n",
      "l2 norm of weights: 5.880718751866749\n",
      "---------------------\n",
      "Iteration Number: 5437\n",
      "Loss: 35.270325298899806\n",
      "l2 norm of gradients: 0.24381768767640064\n",
      "l2 norm of weights: 5.880634441355464\n",
      "---------------------\n",
      "Iteration Number: 5438\n",
      "Loss: 35.267344009998176\n",
      "l2 norm of gradients: 0.2437866183374749\n",
      "l2 norm of weights: 5.88055013884987\n",
      "---------------------\n",
      "Iteration Number: 5439\n",
      "Loss: 35.264363480192266\n",
      "l2 norm of gradients: 0.24375556070403795\n",
      "l2 norm of weights: 5.880465844348193\n",
      "---------------------\n",
      "Iteration Number: 5440\n",
      "Loss: 35.261383709097686\n",
      "l2 norm of gradients: 0.24372451476764714\n",
      "l2 norm of weights: 5.880381557848659\n",
      "---------------------\n",
      "Iteration Number: 5441\n",
      "Loss: 35.25840469633763\n",
      "l2 norm of gradients: 0.24369348051986797\n",
      "l2 norm of weights: 5.880297279349493\n",
      "---------------------\n",
      "Iteration Number: 5442\n",
      "Loss: 35.25542644153092\n",
      "l2 norm of gradients: 0.24366245795227376\n",
      "l2 norm of weights: 5.880213008848925\n",
      "---------------------\n",
      "Iteration Number: 5443\n",
      "Loss: 35.25244894430011\n",
      "l2 norm of gradients: 0.24363144705644607\n",
      "l2 norm of weights: 5.880128746345182\n",
      "---------------------\n",
      "Iteration Number: 5444\n",
      "Loss: 35.249472204271314\n",
      "l2 norm of gradients: 0.2436004478239744\n",
      "l2 norm of weights: 5.880044491836493\n",
      "---------------------\n",
      "Iteration Number: 5445\n",
      "Loss: 35.246496221059985\n",
      "l2 norm of gradients: 0.24356946024645626\n",
      "l2 norm of weights: 5.879960245321088\n",
      "---------------------\n",
      "Iteration Number: 5446\n",
      "Loss: 35.24352099428765\n",
      "l2 norm of gradients: 0.24353848431549724\n",
      "l2 norm of weights: 5.879876006797201\n",
      "---------------------\n",
      "Iteration Number: 5447\n",
      "Loss: 35.240546523577805\n",
      "l2 norm of gradients: 0.24350752002271084\n",
      "l2 norm of weights: 5.879791776263063\n",
      "---------------------\n",
      "Iteration Number: 5448\n",
      "Loss: 35.23757280856009\n",
      "l2 norm of gradients: 0.2434765673597186\n",
      "l2 norm of weights: 5.879707553716905\n",
      "---------------------\n",
      "Iteration Number: 5449\n",
      "Loss: 35.23459984884891\n",
      "l2 norm of gradients: 0.24344562631815012\n",
      "l2 norm of weights: 5.8796233391569634\n",
      "---------------------\n",
      "Iteration Number: 5450\n",
      "Loss: 35.231627644068354\n",
      "l2 norm of gradients: 0.24341469688964282\n",
      "l2 norm of weights: 5.879539132581471\n",
      "---------------------\n",
      "Iteration Number: 5451\n",
      "Loss: 35.22865619385023\n",
      "l2 norm of gradients: 0.2433837790658423\n",
      "l2 norm of weights: 5.879454933988666\n",
      "---------------------\n",
      "Iteration Number: 5452\n",
      "Loss: 35.2256854978054\n",
      "l2 norm of gradients: 0.2433528728384018\n",
      "l2 norm of weights: 5.879370743376783\n",
      "---------------------\n",
      "Iteration Number: 5453\n",
      "Loss: 35.22271555557125\n",
      "l2 norm of gradients: 0.24332197819898294\n",
      "l2 norm of weights: 5.879286560744062\n",
      "---------------------\n",
      "Iteration Number: 5454\n",
      "Loss: 35.21974636677135\n",
      "l2 norm of gradients: 0.24329109513925498\n",
      "l2 norm of weights: 5.87920238608874\n",
      "---------------------\n",
      "Iteration Number: 5455\n",
      "Loss: 35.21677793102045\n",
      "l2 norm of gradients: 0.24326022365089517\n",
      "l2 norm of weights: 5.879118219409056\n",
      "---------------------\n",
      "Iteration Number: 5456\n",
      "Loss: 35.213810247949034\n",
      "l2 norm of gradients: 0.24322936372558882\n",
      "l2 norm of weights: 5.879034060703252\n",
      "---------------------\n",
      "Iteration Number: 5457\n",
      "Loss: 35.21084331718343\n",
      "l2 norm of gradients: 0.24319851535502904\n",
      "l2 norm of weights: 5.878949909969568\n",
      "---------------------\n",
      "Iteration Number: 5458\n",
      "Loss: 35.20787713834814\n",
      "l2 norm of gradients: 0.24316767853091698\n",
      "l2 norm of weights: 5.878865767206246\n",
      "---------------------\n",
      "Iteration Number: 5459\n",
      "Loss: 35.20491171107454\n",
      "l2 norm of gradients: 0.24313685324496145\n",
      "l2 norm of weights: 5.87878163241153\n",
      "---------------------\n",
      "Iteration Number: 5460\n",
      "Loss: 35.20194703498247\n",
      "l2 norm of gradients: 0.2431060394888795\n",
      "l2 norm of weights: 5.8786975055836646\n",
      "---------------------\n",
      "Iteration Number: 5461\n",
      "Loss: 35.19898310969634\n",
      "l2 norm of gradients: 0.24307523725439578\n",
      "l2 norm of weights: 5.878613386720893\n",
      "---------------------\n",
      "Iteration Number: 5462\n",
      "Loss: 35.19601993485652\n",
      "l2 norm of gradients: 0.24304444653324303\n",
      "l2 norm of weights: 5.878529275821462\n",
      "---------------------\n",
      "Iteration Number: 5463\n",
      "Loss: 35.1930575100694\n",
      "l2 norm of gradients: 0.24301366731716179\n",
      "l2 norm of weights: 5.878445172883618\n",
      "---------------------\n",
      "Iteration Number: 5464\n",
      "Loss: 35.19009583498609\n",
      "l2 norm of gradients: 0.24298289959790043\n",
      "l2 norm of weights: 5.878361077905608\n",
      "---------------------\n",
      "Iteration Number: 5465\n",
      "Loss: 35.18713490922151\n",
      "l2 norm of gradients: 0.24295214336721527\n",
      "l2 norm of weights: 5.878276990885682\n",
      "---------------------\n",
      "Iteration Number: 5466\n",
      "Loss: 35.184174732408295\n",
      "l2 norm of gradients: 0.24292139861687034\n",
      "l2 norm of weights: 5.8781929118220875\n",
      "---------------------\n",
      "Iteration Number: 5467\n",
      "Loss: 35.18121530416401\n",
      "l2 norm of gradients: 0.24289066533863773\n",
      "l2 norm of weights: 5.878108840713077\n",
      "---------------------\n",
      "Iteration Number: 5468\n",
      "Loss: 35.17825662413122\n",
      "l2 norm of gradients: 0.24285994352429724\n",
      "l2 norm of weights: 5.878024777556901\n",
      "---------------------\n",
      "Iteration Number: 5469\n",
      "Loss: 35.17529869193354\n",
      "l2 norm of gradients: 0.24282923316563643\n",
      "l2 norm of weights: 5.877940722351809\n",
      "---------------------\n",
      "Iteration Number: 5470\n",
      "Loss: 35.17234150719974\n",
      "l2 norm of gradients: 0.24279853425445083\n",
      "l2 norm of weights: 5.877856675096058\n",
      "---------------------\n",
      "Iteration Number: 5471\n",
      "Loss: 35.169385069558565\n",
      "l2 norm of gradients: 0.24276784678254373\n",
      "l2 norm of weights: 5.8777726357879\n",
      "---------------------\n",
      "Iteration Number: 5472\n",
      "Loss: 35.16642937864623\n",
      "l2 norm of gradients: 0.24273717074172624\n",
      "l2 norm of weights: 5.877688604425589\n",
      "---------------------\n",
      "Iteration Number: 5473\n",
      "Loss: 35.16347443407959\n",
      "l2 norm of gradients: 0.24270650612381728\n",
      "l2 norm of weights: 5.877604581007382\n",
      "---------------------\n",
      "Iteration Number: 5474\n",
      "Loss: 35.16052023551236\n",
      "l2 norm of gradients: 0.24267585292064342\n",
      "l2 norm of weights: 5.877520565531534\n",
      "---------------------\n",
      "Iteration Number: 5475\n",
      "Loss: 35.157566782551534\n",
      "l2 norm of gradients: 0.24264521112403933\n",
      "l2 norm of weights: 5.877436557996304\n",
      "---------------------\n",
      "Iteration Number: 5476\n",
      "Loss: 35.1546140748423\n",
      "l2 norm of gradients: 0.24261458072584707\n",
      "l2 norm of weights: 5.8773525583999495\n",
      "---------------------\n",
      "Iteration Number: 5477\n",
      "Loss: 35.15166211201228\n",
      "l2 norm of gradients: 0.2425839617179168\n",
      "l2 norm of weights: 5.877268566740731\n",
      "---------------------\n",
      "Iteration Number: 5478\n",
      "Loss: 35.14871089368775\n",
      "l2 norm of gradients: 0.24255335409210627\n",
      "l2 norm of weights: 5.8771845830169065\n",
      "---------------------\n",
      "Iteration Number: 5479\n",
      "Loss: 35.14576041950754\n",
      "l2 norm of gradients: 0.242522757840281\n",
      "l2 norm of weights: 5.877100607226739\n",
      "---------------------\n",
      "Iteration Number: 5480\n",
      "Loss: 35.14281068911214\n",
      "l2 norm of gradients: 0.24249217295431424\n",
      "l2 norm of weights: 5.877016639368489\n",
      "---------------------\n",
      "Iteration Number: 5481\n",
      "Loss: 35.13986170211654\n",
      "l2 norm of gradients: 0.2424615994260872\n",
      "l2 norm of weights: 5.8769326794404195\n",
      "---------------------\n",
      "Iteration Number: 5482\n",
      "Loss: 35.13691345816026\n",
      "l2 norm of gradients: 0.24243103724748843\n",
      "l2 norm of weights: 5.8768487274407955\n",
      "---------------------\n",
      "Iteration Number: 5483\n",
      "Loss: 35.133965956886044\n",
      "l2 norm of gradients: 0.24240048641041456\n",
      "l2 norm of weights: 5.876764783367879\n",
      "---------------------\n",
      "Iteration Number: 5484\n",
      "Loss: 35.131019197915386\n",
      "l2 norm of gradients: 0.24236994690676975\n",
      "l2 norm of weights: 5.876680847219938\n",
      "---------------------\n",
      "Iteration Number: 5485\n",
      "Loss: 35.12807318088298\n",
      "l2 norm of gradients: 0.2423394187284659\n",
      "l2 norm of weights: 5.8765969189952365\n",
      "---------------------\n",
      "Iteration Number: 5486\n",
      "Loss: 35.125127905433715\n",
      "l2 norm of gradients: 0.2423089018674226\n",
      "l2 norm of weights: 5.876512998692045\n",
      "---------------------\n",
      "Iteration Number: 5487\n",
      "Loss: 35.12218337119045\n",
      "l2 norm of gradients: 0.24227839631556716\n",
      "l2 norm of weights: 5.876429086308627\n",
      "---------------------\n",
      "Iteration Number: 5488\n",
      "Loss: 35.119239577796414\n",
      "l2 norm of gradients: 0.2422479020648347\n",
      "l2 norm of weights: 5.876345181843255\n",
      "---------------------\n",
      "Iteration Number: 5489\n",
      "Loss: 35.116296524881434\n",
      "l2 norm of gradients: 0.24221741910716774\n",
      "l2 norm of weights: 5.876261285294198\n",
      "---------------------\n",
      "Iteration Number: 5490\n",
      "Loss: 35.11335421207785\n",
      "l2 norm of gradients: 0.24218694743451666\n",
      "l2 norm of weights: 5.876177396659725\n",
      "---------------------\n",
      "Iteration Number: 5491\n",
      "Loss: 35.11041263903068\n",
      "l2 norm of gradients: 0.24215648703883952\n",
      "l2 norm of weights: 5.87609351593811\n",
      "---------------------\n",
      "Iteration Number: 5492\n",
      "Loss: 35.10747180537254\n",
      "l2 norm of gradients: 0.24212603791210194\n",
      "l2 norm of weights: 5.876009643127624\n",
      "---------------------\n",
      "Iteration Number: 5493\n",
      "Loss: 35.10453171073315\n",
      "l2 norm of gradients: 0.24209560004627723\n",
      "l2 norm of weights: 5.875925778226541\n",
      "---------------------\n",
      "Iteration Number: 5494\n",
      "Loss: 35.10159235475648\n",
      "l2 norm of gradients: 0.24206517343334635\n",
      "l2 norm of weights: 5.875841921233135\n",
      "---------------------\n",
      "Iteration Number: 5495\n",
      "Loss: 35.09865373707427\n",
      "l2 norm of gradients: 0.24203475806529792\n",
      "l2 norm of weights: 5.875758072145682\n",
      "---------------------\n",
      "Iteration Number: 5496\n",
      "Loss: 35.09571585732592\n",
      "l2 norm of gradients: 0.24200435393412806\n",
      "l2 norm of weights: 5.8756742309624554\n",
      "---------------------\n",
      "Iteration Number: 5497\n",
      "Loss: 35.0927787151529\n",
      "l2 norm of gradients: 0.24197396103184077\n",
      "l2 norm of weights: 5.8755903976817345\n",
      "---------------------\n",
      "Iteration Number: 5498\n",
      "Loss: 35.08984231018978\n",
      "l2 norm of gradients: 0.24194357935044725\n",
      "l2 norm of weights: 5.875506572301795\n",
      "---------------------\n",
      "Iteration Number: 5499\n",
      "Loss: 35.0869066420611\n",
      "l2 norm of gradients: 0.24191320888196677\n",
      "l2 norm of weights: 5.875422754820918\n",
      "---------------------\n",
      "Iteration Number: 5500\n",
      "Loss: 35.08397171042821\n",
      "l2 norm of gradients: 0.2418828496184258\n",
      "l2 norm of weights: 5.87533894523738\n",
      "---------------------\n",
      "Iteration Number: 5501\n",
      "Loss: 35.081037514920055\n",
      "l2 norm of gradients: 0.24185250155185864\n",
      "l2 norm of weights: 5.875255143549464\n",
      "---------------------\n",
      "Iteration Number: 5502\n",
      "Loss: 35.07810405517243\n",
      "l2 norm of gradients: 0.24182216467430714\n",
      "l2 norm of weights: 5.875171349755449\n",
      "---------------------\n",
      "Iteration Number: 5503\n",
      "Loss: 35.0751713308258\n",
      "l2 norm of gradients: 0.24179183897782058\n",
      "l2 norm of weights: 5.875087563853618\n",
      "---------------------\n",
      "Iteration Number: 5504\n",
      "Loss: 35.07223934152172\n",
      "l2 norm of gradients: 0.24176152445445603\n",
      "l2 norm of weights: 5.875003785842254\n",
      "---------------------\n",
      "Iteration Number: 5505\n",
      "Loss: 35.06930808689389\n",
      "l2 norm of gradients: 0.24173122109627782\n",
      "l2 norm of weights: 5.874920015719641\n",
      "---------------------\n",
      "Iteration Number: 5506\n",
      "Loss: 35.066377566593374\n",
      "l2 norm of gradients: 0.24170092889535816\n",
      "l2 norm of weights: 5.874836253484063\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 5507\n",
      "Loss: 35.06344778025239\n",
      "l2 norm of gradients: 0.24167064784377656\n",
      "l2 norm of weights: 5.874752499133805\n",
      "---------------------\n",
      "Iteration Number: 5508\n",
      "Loss: 35.060518727509276\n",
      "l2 norm of gradients: 0.24164037793362014\n",
      "l2 norm of weights: 5.874668752667155\n",
      "---------------------\n",
      "Iteration Number: 5509\n",
      "Loss: 35.057590408015535\n",
      "l2 norm of gradients: 0.2416101191569837\n",
      "l2 norm of weights: 5.874585014082399\n",
      "---------------------\n",
      "Iteration Number: 5510\n",
      "Loss: 35.054662821402665\n",
      "l2 norm of gradients: 0.2415798715059692\n",
      "l2 norm of weights: 5.874501283377826\n",
      "---------------------\n",
      "Iteration Number: 5511\n",
      "Loss: 35.05173596731159\n",
      "l2 norm of gradients: 0.2415496349726865\n",
      "l2 norm of weights: 5.874417560551723\n",
      "---------------------\n",
      "Iteration Number: 5512\n",
      "Loss: 35.048809845393166\n",
      "l2 norm of gradients: 0.24151940954925272\n",
      "l2 norm of weights: 5.874333845602382\n",
      "---------------------\n",
      "Iteration Number: 5513\n",
      "Loss: 35.045884455276806\n",
      "l2 norm of gradients: 0.24148919522779264\n",
      "l2 norm of weights: 5.874250138528092\n",
      "---------------------\n",
      "Iteration Number: 5514\n",
      "Loss: 35.0429597966153\n",
      "l2 norm of gradients: 0.2414589920004383\n",
      "l2 norm of weights: 5.874166439327145\n",
      "---------------------\n",
      "Iteration Number: 5515\n",
      "Loss: 35.04003586904854\n",
      "l2 norm of gradients: 0.24142879985932944\n",
      "l2 norm of weights: 5.874082747997835\n",
      "---------------------\n",
      "Iteration Number: 5516\n",
      "Loss: 35.03711267221253\n",
      "l2 norm of gradients: 0.2413986187966133\n",
      "l2 norm of weights: 5.873999064538453\n",
      "---------------------\n",
      "Iteration Number: 5517\n",
      "Loss: 35.0341902057651\n",
      "l2 norm of gradients: 0.24136844880444439\n",
      "l2 norm of weights: 5.873915388947294\n",
      "---------------------\n",
      "Iteration Number: 5518\n",
      "Loss: 35.03126846933453\n",
      "l2 norm of gradients: 0.24133828987498485\n",
      "l2 norm of weights: 5.873831721222653\n",
      "---------------------\n",
      "Iteration Number: 5519\n",
      "Loss: 35.02834746257125\n",
      "l2 norm of gradients: 0.24130814200040412\n",
      "l2 norm of weights: 5.873748061362825\n",
      "---------------------\n",
      "Iteration Number: 5520\n",
      "Loss: 35.02542718511892\n",
      "l2 norm of gradients: 0.24127800517287917\n",
      "l2 norm of weights: 5.873664409366109\n",
      "---------------------\n",
      "Iteration Number: 5521\n",
      "Loss: 35.02250763662634\n",
      "l2 norm of gradients: 0.24124787938459458\n",
      "l2 norm of weights: 5.873580765230799\n",
      "---------------------\n",
      "Iteration Number: 5522\n",
      "Loss: 35.0195888167288\n",
      "l2 norm of gradients: 0.241217764627742\n",
      "l2 norm of weights: 5.873497128955195\n",
      "---------------------\n",
      "Iteration Number: 5523\n",
      "Loss: 35.01667072507036\n",
      "l2 norm of gradients: 0.2411876608945208\n",
      "l2 norm of weights: 5.873413500537597\n",
      "---------------------\n",
      "Iteration Number: 5524\n",
      "Loss: 35.013753361313924\n",
      "l2 norm of gradients: 0.24115756817713763\n",
      "l2 norm of weights: 5.873329879976305\n",
      "---------------------\n",
      "Iteration Number: 5525\n",
      "Loss: 35.0108367250851\n",
      "l2 norm of gradients: 0.24112748646780663\n",
      "l2 norm of weights: 5.873246267269619\n",
      "---------------------\n",
      "Iteration Number: 5526\n",
      "Loss: 35.00792081603673\n",
      "l2 norm of gradients: 0.24109741575874924\n",
      "l2 norm of weights: 5.87316266241584\n",
      "---------------------\n",
      "Iteration Number: 5527\n",
      "Loss: 35.005005633817916\n",
      "l2 norm of gradients: 0.24106735604219434\n",
      "l2 norm of weights: 5.873079065413272\n",
      "---------------------\n",
      "Iteration Number: 5528\n",
      "Loss: 35.00209117807\n",
      "l2 norm of gradients: 0.24103730731037826\n",
      "l2 norm of weights: 5.872995476260218\n",
      "---------------------\n",
      "Iteration Number: 5529\n",
      "Loss: 34.99917744844242\n",
      "l2 norm of gradients: 0.24100726955554472\n",
      "l2 norm of weights: 5.872911894954982\n",
      "---------------------\n",
      "Iteration Number: 5530\n",
      "Loss: 34.99626444457829\n",
      "l2 norm of gradients: 0.24097724276994456\n",
      "l2 norm of weights: 5.87282832149587\n",
      "---------------------\n",
      "Iteration Number: 5531\n",
      "Loss: 34.993352166128275\n",
      "l2 norm of gradients: 0.24094722694583634\n",
      "l2 norm of weights: 5.872744755881187\n",
      "---------------------\n",
      "Iteration Number: 5532\n",
      "Loss: 34.990440612738965\n",
      "l2 norm of gradients: 0.24091722207548574\n",
      "l2 norm of weights: 5.87266119810924\n",
      "---------------------\n",
      "Iteration Number: 5533\n",
      "Loss: 34.987529784062886\n",
      "l2 norm of gradients: 0.24088722815116595\n",
      "l2 norm of weights: 5.872577648178338\n",
      "---------------------\n",
      "Iteration Number: 5534\n",
      "Loss: 34.984619679735005\n",
      "l2 norm of gradients: 0.24085724516515725\n",
      "l2 norm of weights: 5.872494106086787\n",
      "---------------------\n",
      "Iteration Number: 5535\n",
      "Loss: 34.981710299416974\n",
      "l2 norm of gradients: 0.24082727310974755\n",
      "l2 norm of weights: 5.872410571832898\n",
      "---------------------\n",
      "Iteration Number: 5536\n",
      "Loss: 34.97880164274551\n",
      "l2 norm of gradients: 0.24079731197723198\n",
      "l2 norm of weights: 5.8723270454149805\n",
      "---------------------\n",
      "Iteration Number: 5537\n",
      "Loss: 34.97589370938276\n",
      "l2 norm of gradients: 0.2407673617599129\n",
      "l2 norm of weights: 5.872243526831346\n",
      "---------------------\n",
      "Iteration Number: 5538\n",
      "Loss: 34.97298649897012\n",
      "l2 norm of gradients: 0.24073742245010005\n",
      "l2 norm of weights: 5.872160016080306\n",
      "---------------------\n",
      "Iteration Number: 5539\n",
      "Loss: 34.97008001115677\n",
      "l2 norm of gradients: 0.24070749404011055\n",
      "l2 norm of weights: 5.872076513160173\n",
      "---------------------\n",
      "Iteration Number: 5540\n",
      "Loss: 34.96717424558594\n",
      "l2 norm of gradients: 0.24067757652226876\n",
      "l2 norm of weights: 5.871993018069261\n",
      "---------------------\n",
      "Iteration Number: 5541\n",
      "Loss: 34.96426920192161\n",
      "l2 norm of gradients: 0.24064766988890618\n",
      "l2 norm of weights: 5.871909530805885\n",
      "---------------------\n",
      "Iteration Number: 5542\n",
      "Loss: 34.96136487980731\n",
      "l2 norm of gradients: 0.24061777413236193\n",
      "l2 norm of weights: 5.871826051368359\n",
      "---------------------\n",
      "Iteration Number: 5543\n",
      "Loss: 34.958461278891996\n",
      "l2 norm of gradients: 0.24058788924498212\n",
      "l2 norm of weights: 5.871742579754999\n",
      "---------------------\n",
      "Iteration Number: 5544\n",
      "Loss: 34.95555839882961\n",
      "l2 norm of gradients: 0.2405580152191203\n",
      "l2 norm of weights: 5.871659115964123\n",
      "---------------------\n",
      "Iteration Number: 5545\n",
      "Loss: 34.952656239258495\n",
      "l2 norm of gradients: 0.24052815204713704\n",
      "l2 norm of weights: 5.871575659994047\n",
      "---------------------\n",
      "Iteration Number: 5546\n",
      "Loss: 34.94975479984708\n",
      "l2 norm of gradients: 0.24049829972140055\n",
      "l2 norm of weights: 5.8714922118430914\n",
      "---------------------\n",
      "Iteration Number: 5547\n",
      "Loss: 34.946854080248215\n",
      "l2 norm of gradients: 0.24046845823428592\n",
      "l2 norm of weights: 5.871408771509574\n",
      "---------------------\n",
      "Iteration Number: 5548\n",
      "Loss: 34.94395408009163\n",
      "l2 norm of gradients: 0.24043862757817572\n",
      "l2 norm of weights: 5.8713253389918165\n",
      "---------------------\n",
      "Iteration Number: 5549\n",
      "Loss: 34.9410547990495\n",
      "l2 norm of gradients: 0.24040880774545964\n",
      "l2 norm of weights: 5.871241914288139\n",
      "---------------------\n",
      "Iteration Number: 5550\n",
      "Loss: 34.938156236770794\n",
      "l2 norm of gradients: 0.24037899872853463\n",
      "l2 norm of weights: 5.871158497396863\n",
      "---------------------\n",
      "Iteration Number: 5551\n",
      "Loss: 34.93525839290584\n",
      "l2 norm of gradients: 0.24034920051980488\n",
      "l2 norm of weights: 5.871075088316311\n",
      "---------------------\n",
      "Iteration Number: 5552\n",
      "Loss: 34.932361267102216\n",
      "l2 norm of gradients: 0.24031941311168176\n",
      "l2 norm of weights: 5.870991687044808\n",
      "---------------------\n",
      "Iteration Number: 5553\n",
      "Loss: 34.929464859020925\n",
      "l2 norm of gradients: 0.2402896364965838\n",
      "l2 norm of weights: 5.870908293580677\n",
      "---------------------\n",
      "Iteration Number: 5554\n",
      "Loss: 34.92656916831209\n",
      "l2 norm of gradients: 0.24025987066693694\n",
      "l2 norm of weights: 5.870824907922244\n",
      "---------------------\n",
      "Iteration Number: 5555\n",
      "Loss: 34.92367419462751\n",
      "l2 norm of gradients: 0.240230115615174\n",
      "l2 norm of weights: 5.870741530067835\n",
      "---------------------\n",
      "Iteration Number: 5556\n",
      "Loss: 34.92077993762343\n",
      "l2 norm of gradients: 0.24020037133373517\n",
      "l2 norm of weights: 5.8706581600157755\n",
      "---------------------\n",
      "Iteration Number: 5557\n",
      "Loss: 34.91788639695345\n",
      "l2 norm of gradients: 0.2401706378150679\n",
      "l2 norm of weights: 5.870574797764394\n",
      "---------------------\n",
      "Iteration Number: 5558\n",
      "Loss: 34.91499357227999\n",
      "l2 norm of gradients: 0.24014091505162657\n",
      "l2 norm of weights: 5.8704914433120186\n",
      "---------------------\n",
      "Iteration Number: 5559\n",
      "Loss: 34.9121014632389\n",
      "l2 norm of gradients: 0.24011120303587294\n",
      "l2 norm of weights: 5.87040809665698\n",
      "---------------------\n",
      "Iteration Number: 5560\n",
      "Loss: 34.909210069507765\n",
      "l2 norm of gradients: 0.2400815017602757\n",
      "l2 norm of weights: 5.870324757797606\n",
      "---------------------\n",
      "Iteration Number: 5561\n",
      "Loss: 34.906319390726615\n",
      "l2 norm of gradients: 0.24005181121731098\n",
      "l2 norm of weights: 5.870241426732229\n",
      "---------------------\n",
      "Iteration Number: 5562\n",
      "Loss: 34.90342942655661\n",
      "l2 norm of gradients: 0.24002213139946182\n",
      "l2 norm of weights: 5.870158103459181\n",
      "---------------------\n",
      "Iteration Number: 5563\n",
      "Loss: 34.900540176652086\n",
      "l2 norm of gradients: 0.2399924622992185\n",
      "l2 norm of weights: 5.870074787976792\n",
      "---------------------\n",
      "Iteration Number: 5564\n",
      "Loss: 34.89765164067204\n",
      "l2 norm of gradients: 0.23996280390907834\n",
      "l2 norm of weights: 5.8699914802833995\n",
      "---------------------\n",
      "Iteration Number: 5565\n",
      "Loss: 34.894763818267165\n",
      "l2 norm of gradients: 0.23993315622154596\n",
      "l2 norm of weights: 5.869908180377334\n",
      "---------------------\n",
      "Iteration Number: 5566\n",
      "Loss: 34.89187670909874\n",
      "l2 norm of gradients: 0.23990351922913283\n",
      "l2 norm of weights: 5.869824888256933\n",
      "---------------------\n",
      "Iteration Number: 5567\n",
      "Loss: 34.888990312823296\n",
      "l2 norm of gradients: 0.23987389292435768\n",
      "l2 norm of weights: 5.869741603920531\n",
      "---------------------\n",
      "Iteration Number: 5568\n",
      "Loss: 34.886104629099584\n",
      "l2 norm of gradients: 0.2398442772997464\n",
      "l2 norm of weights: 5.869658327366465\n",
      "---------------------\n",
      "Iteration Number: 5569\n",
      "Loss: 34.88321965757989\n",
      "l2 norm of gradients: 0.2398146723478318\n",
      "l2 norm of weights: 5.869575058593072\n",
      "---------------------\n",
      "Iteration Number: 5570\n",
      "Loss: 34.88033539792815\n",
      "l2 norm of gradients: 0.23978507806115398\n",
      "l2 norm of weights: 5.869491797598691\n",
      "---------------------\n",
      "Iteration Number: 5571\n",
      "Loss: 34.87745184979759\n",
      "l2 norm of gradients: 0.23975549443225994\n",
      "l2 norm of weights: 5.869408544381661\n",
      "---------------------\n",
      "Iteration Number: 5572\n",
      "Loss: 34.87456901284387\n",
      "l2 norm of gradients: 0.23972592145370378\n",
      "l2 norm of weights: 5.86932529894032\n",
      "---------------------\n",
      "Iteration Number: 5573\n",
      "Loss: 34.87168688673746\n",
      "l2 norm of gradients: 0.23969635911804674\n",
      "l2 norm of weights: 5.869242061273011\n",
      "---------------------\n",
      "Iteration Number: 5574\n",
      "Loss: 34.868805471126876\n",
      "l2 norm of gradients: 0.23966680741785715\n",
      "l2 norm of weights: 5.869158831378076\n",
      "---------------------\n",
      "Iteration Number: 5575\n",
      "Loss: 34.86592476567424\n",
      "l2 norm of gradients: 0.23963726634571014\n",
      "l2 norm of weights: 5.869075609253855\n",
      "---------------------\n",
      "Iteration Number: 5576\n",
      "Loss: 34.86304477003608\n",
      "l2 norm of gradients: 0.2396077358941882\n",
      "l2 norm of weights: 5.868992394898693\n",
      "---------------------\n",
      "Iteration Number: 5577\n",
      "Loss: 34.86016548388041\n",
      "l2 norm of gradients: 0.23957821605588067\n",
      "l2 norm of weights: 5.868909188310932\n",
      "---------------------\n",
      "Iteration Number: 5578\n",
      "Loss: 34.857286906857524\n",
      "l2 norm of gradients: 0.23954870682338394\n",
      "l2 norm of weights: 5.868825989488918\n",
      "---------------------\n",
      "Iteration Number: 5579\n",
      "Loss: 34.85440903862972\n",
      "l2 norm of gradients: 0.2395192081893015\n",
      "l2 norm of weights: 5.868742798430997\n",
      "---------------------\n",
      "Iteration Number: 5580\n",
      "Loss: 34.851531878862865\n",
      "l2 norm of gradients: 0.23948972014624376\n",
      "l2 norm of weights: 5.868659615135514\n",
      "---------------------\n",
      "Iteration Number: 5581\n",
      "Loss: 34.848655427211675\n",
      "l2 norm of gradients: 0.23946024268682808\n",
      "l2 norm of weights: 5.868576439600817\n",
      "---------------------\n",
      "Iteration Number: 5582\n",
      "Loss: 34.845779683340055\n",
      "l2 norm of gradients: 0.23943077580367905\n",
      "l2 norm of weights: 5.868493271825255\n",
      "---------------------\n",
      "Iteration Number: 5583\n",
      "Loss: 34.842904646915876\n",
      "l2 norm of gradients: 0.239401319489428\n",
      "l2 norm of weights: 5.868410111807174\n",
      "---------------------\n",
      "Iteration Number: 5584\n",
      "Loss: 34.84003031758368\n",
      "l2 norm of gradients: 0.2393718737367135\n",
      "l2 norm of weights: 5.868326959544926\n",
      "---------------------\n",
      "Iteration Number: 5585\n",
      "Loss: 34.83715669502465\n",
      "l2 norm of gradients: 0.23934243853818077\n",
      "l2 norm of weights: 5.868243815036861\n",
      "---------------------\n",
      "Iteration Number: 5586\n",
      "Loss: 34.834283778878394\n",
      "l2 norm of gradients: 0.23931301388648243\n",
      "l2 norm of weights: 5.868160678281329\n",
      "---------------------\n",
      "Iteration Number: 5587\n",
      "Loss: 34.83141156883312\n",
      "l2 norm of gradients: 0.23928359977427757\n",
      "l2 norm of weights: 5.868077549276682\n",
      "---------------------\n",
      "Iteration Number: 5588\n",
      "Loss: 34.82854006452794\n",
      "l2 norm of gradients: 0.23925419619423266\n",
      "l2 norm of weights: 5.867994428021274\n",
      "---------------------\n",
      "Iteration Number: 5589\n",
      "Loss: 34.825669265646866\n",
      "l2 norm of gradients: 0.2392248031390209\n",
      "l2 norm of weights: 5.867911314513457\n",
      "---------------------\n",
      "Iteration Number: 5590\n",
      "Loss: 34.8227991718313\n",
      "l2 norm of gradients: 0.23919542060132248\n",
      "l2 norm of weights: 5.867828208751589\n",
      "---------------------\n",
      "Iteration Number: 5591\n",
      "Loss: 34.819929782766934\n",
      "l2 norm of gradients: 0.2391660485738246\n",
      "l2 norm of weights: 5.8677451107340195\n",
      "---------------------\n",
      "Iteration Number: 5592\n",
      "Loss: 34.81706109809878\n",
      "l2 norm of gradients: 0.23913668704922122\n",
      "l2 norm of weights: 5.867662020459109\n",
      "---------------------\n",
      "Iteration Number: 5593\n",
      "Loss: 34.814193117499954\n",
      "l2 norm of gradients: 0.23910733602021342\n",
      "l2 norm of weights: 5.867578937925213\n",
      "---------------------\n",
      "Iteration Number: 5594\n",
      "Loss: 34.8113258406356\n",
      "l2 norm of gradients: 0.239077995479509\n",
      "l2 norm of weights: 5.867495863130688\n",
      "---------------------\n",
      "Iteration Number: 5595\n",
      "Loss: 34.80845926716103\n",
      "l2 norm of gradients: 0.23904866541982295\n",
      "l2 norm of weights: 5.867412796073893\n",
      "---------------------\n",
      "Iteration Number: 5596\n",
      "Loss: 34.805593396752954\n",
      "l2 norm of gradients: 0.23901934583387682\n",
      "l2 norm of weights: 5.867329736753186\n",
      "---------------------\n",
      "Iteration Number: 5597\n",
      "Loss: 34.80272822906652\n",
      "l2 norm of gradients: 0.23899003671439936\n",
      "l2 norm of weights: 5.86724668516693\n",
      "---------------------\n",
      "Iteration Number: 5598\n",
      "Loss: 34.79986376376972\n",
      "l2 norm of gradients: 0.238960738054126\n",
      "l2 norm of weights: 5.867163641313482\n",
      "---------------------\n",
      "Iteration Number: 5599\n",
      "Loss: 34.79700000053319\n",
      "l2 norm of gradients: 0.23893144984579906\n",
      "l2 norm of weights: 5.867080605191207\n",
      "---------------------\n",
      "Iteration Number: 5600\n",
      "Loss: 34.794136939019815\n",
      "l2 norm of gradients: 0.23890217208216782\n",
      "l2 norm of weights: 5.8669975767984655\n",
      "---------------------\n",
      "Iteration Number: 5601\n",
      "Loss: 34.79127457889324\n",
      "l2 norm of gradients: 0.23887290475598855\n",
      "l2 norm of weights: 5.86691455613362\n",
      "---------------------\n",
      "Iteration Number: 5602\n",
      "Loss: 34.7884129198169\n",
      "l2 norm of gradients: 0.23884364786002407\n",
      "l2 norm of weights: 5.866831543195035\n",
      "---------------------\n",
      "Iteration Number: 5603\n",
      "Loss: 34.78555196146547\n",
      "l2 norm of gradients: 0.2388144013870443\n",
      "l2 norm of weights: 5.866748537981076\n",
      "---------------------\n",
      "Iteration Number: 5604\n",
      "Loss: 34.782691703498074\n",
      "l2 norm of gradients: 0.23878516532982594\n",
      "l2 norm of weights: 5.866665540490107\n",
      "---------------------\n",
      "Iteration Number: 5605\n",
      "Loss: 34.77983214558596\n",
      "l2 norm of gradients: 0.23875593968115252\n",
      "l2 norm of weights: 5.866582550720495\n",
      "---------------------\n",
      "Iteration Number: 5606\n",
      "Loss: 34.776973287398825\n",
      "l2 norm of gradients: 0.23872672443381435\n",
      "l2 norm of weights: 5.866499568670608\n",
      "---------------------\n",
      "Iteration Number: 5607\n",
      "Loss: 34.77411512859661\n",
      "l2 norm of gradients: 0.23869751958060872\n",
      "l2 norm of weights: 5.866416594338811\n",
      "---------------------\n",
      "Iteration Number: 5608\n",
      "Loss: 34.77125766885924\n",
      "l2 norm of gradients: 0.23866832511433955\n",
      "l2 norm of weights: 5.866333627723476\n",
      "---------------------\n",
      "Iteration Number: 5609\n",
      "Loss: 34.76840090784176\n",
      "l2 norm of gradients: 0.23863914102781764\n",
      "l2 norm of weights: 5.86625066882297\n",
      "---------------------\n",
      "Iteration Number: 5610\n",
      "Loss: 34.765544845217654\n",
      "l2 norm of gradients: 0.23860996731386072\n",
      "l2 norm of weights: 5.866167717635664\n",
      "---------------------\n",
      "Iteration Number: 5611\n",
      "Loss: 34.76268948065327\n",
      "l2 norm of gradients: 0.23858080396529316\n",
      "l2 norm of weights: 5.866084774159929\n",
      "---------------------\n",
      "Iteration Number: 5612\n",
      "Loss: 34.759834813822145\n",
      "l2 norm of gradients: 0.23855165097494624\n",
      "l2 norm of weights: 5.866001838394136\n",
      "---------------------\n",
      "Iteration Number: 5613\n",
      "Loss: 34.756980844391336\n",
      "l2 norm of gradients: 0.23852250833565788\n",
      "l2 norm of weights: 5.865918910336659\n",
      "---------------------\n",
      "Iteration Number: 5614\n",
      "Loss: 34.75412757203047\n",
      "l2 norm of gradients: 0.2384933760402729\n",
      "l2 norm of weights: 5.86583598998587\n",
      "---------------------\n",
      "Iteration Number: 5615\n",
      "Loss: 34.75127499640336\n",
      "l2 norm of gradients: 0.2384642540816429\n",
      "l2 norm of weights: 5.865753077340144\n",
      "---------------------\n",
      "Iteration Number: 5616\n",
      "Loss: 34.74842311719179\n",
      "l2 norm of gradients: 0.2384351424526262\n",
      "l2 norm of weights: 5.865670172397855\n",
      "---------------------\n",
      "Iteration Number: 5617\n",
      "Loss: 34.74557193405465\n",
      "l2 norm of gradients: 0.23840604114608785\n",
      "l2 norm of weights: 5.865587275157378\n",
      "---------------------\n",
      "Iteration Number: 5618\n",
      "Loss: 34.74272144666504\n",
      "l2 norm of gradients: 0.23837695015489968\n",
      "l2 norm of weights: 5.865504385617092\n",
      "---------------------\n",
      "Iteration Number: 5619\n",
      "Loss: 34.739871654698035\n",
      "l2 norm of gradients: 0.2383478694719403\n",
      "l2 norm of weights: 5.865421503775371\n",
      "---------------------\n",
      "Iteration Number: 5620\n",
      "Loss: 34.73702255782432\n",
      "l2 norm of gradients: 0.2383187990900951\n",
      "l2 norm of weights: 5.865338629630595\n",
      "---------------------\n",
      "Iteration Number: 5621\n",
      "Loss: 34.734174155704785\n",
      "l2 norm of gradients: 0.238289739002256\n",
      "l2 norm of weights: 5.865255763181143\n",
      "---------------------\n",
      "Iteration Number: 5622\n",
      "Loss: 34.73132644802408\n",
      "l2 norm of gradients: 0.23826068920132185\n",
      "l2 norm of weights: 5.865172904425393\n",
      "---------------------\n",
      "Iteration Number: 5623\n",
      "Loss: 34.72847943444994\n",
      "l2 norm of gradients: 0.23823164968019817\n",
      "l2 norm of weights: 5.865090053361726\n",
      "---------------------\n",
      "Iteration Number: 5624\n",
      "Loss: 34.72563311464746\n",
      "l2 norm of gradients: 0.23820262043179713\n",
      "l2 norm of weights: 5.865007209988523\n",
      "---------------------\n",
      "Iteration Number: 5625\n",
      "Loss: 34.72278748829747\n",
      "l2 norm of gradients: 0.23817360144903765\n",
      "l2 norm of weights: 5.864924374304167\n",
      "---------------------\n",
      "Iteration Number: 5626\n",
      "Loss: 34.719942555068116\n",
      "l2 norm of gradients: 0.2381445927248453\n",
      "l2 norm of weights: 5.864841546307038\n",
      "---------------------\n",
      "Iteration Number: 5627\n",
      "Loss: 34.71709831463149\n",
      "l2 norm of gradients: 0.23811559425215245\n",
      "l2 norm of weights: 5.864758725995522\n",
      "---------------------\n",
      "Iteration Number: 5628\n",
      "Loss: 34.7142547666623\n",
      "l2 norm of gradients: 0.2380866060238981\n",
      "l2 norm of weights: 5.864675913368001\n",
      "---------------------\n",
      "Iteration Number: 5629\n",
      "Loss: 34.71141191083257\n",
      "l2 norm of gradients: 0.23805762803302785\n",
      "l2 norm of weights: 5.864593108422861\n",
      "---------------------\n",
      "Iteration Number: 5630\n",
      "Loss: 34.70856974681512\n",
      "l2 norm of gradients: 0.238028660272494\n",
      "l2 norm of weights: 5.864510311158488\n",
      "---------------------\n",
      "Iteration Number: 5631\n",
      "Loss: 34.70572827428468\n",
      "l2 norm of gradients: 0.2379997027352556\n",
      "l2 norm of weights: 5.864427521573269\n",
      "---------------------\n",
      "Iteration Number: 5632\n",
      "Loss: 34.70288749292028\n",
      "l2 norm of gradients: 0.23797075541427845\n",
      "l2 norm of weights: 5.864344739665589\n",
      "---------------------\n",
      "Iteration Number: 5633\n",
      "Loss: 34.700047402385245\n",
      "l2 norm of gradients: 0.23794181830253464\n",
      "l2 norm of weights: 5.864261965433838\n",
      "---------------------\n",
      "Iteration Number: 5634\n",
      "Loss: 34.69720800236135\n",
      "l2 norm of gradients: 0.23791289139300323\n",
      "l2 norm of weights: 5.864179198876403\n",
      "---------------------\n",
      "Iteration Number: 5635\n",
      "Loss: 34.69436929252091\n",
      "l2 norm of gradients: 0.23788397467866979\n",
      "l2 norm of weights: 5.864096439991676\n",
      "---------------------\n",
      "Iteration Number: 5636\n",
      "Loss: 34.691531272538015\n",
      "l2 norm of gradients: 0.23785506815252652\n",
      "l2 norm of weights: 5.864013688778044\n",
      "---------------------\n",
      "Iteration Number: 5637\n",
      "Loss: 34.688693942091156\n",
      "l2 norm of gradients: 0.23782617180757232\n",
      "l2 norm of weights: 5.863930945233901\n",
      "---------------------\n",
      "Iteration Number: 5638\n",
      "Loss: 34.68585730085383\n",
      "l2 norm of gradients: 0.23779728563681263\n",
      "l2 norm of weights: 5.863848209357638\n",
      "---------------------\n",
      "Iteration Number: 5639\n",
      "Loss: 34.683021348499885\n",
      "l2 norm of gradients: 0.2377684096332595\n",
      "l2 norm of weights: 5.863765481147647\n",
      "---------------------\n",
      "Iteration Number: 5640\n",
      "Loss: 34.680186084712275\n",
      "l2 norm of gradients: 0.23773954378993165\n",
      "l2 norm of weights: 5.863682760602321\n",
      "---------------------\n",
      "Iteration Number: 5641\n",
      "Loss: 34.677351509158086\n",
      "l2 norm of gradients: 0.23771068809985432\n",
      "l2 norm of weights: 5.863600047720055\n",
      "---------------------\n",
      "Iteration Number: 5642\n",
      "Loss: 34.67451762152098\n",
      "l2 norm of gradients: 0.23768184255605948\n",
      "l2 norm of weights: 5.863517342499243\n",
      "---------------------\n",
      "Iteration Number: 5643\n",
      "Loss: 34.671684421470914\n",
      "l2 norm of gradients: 0.2376530071515855\n",
      "l2 norm of weights: 5.863434644938282\n",
      "---------------------\n",
      "Iteration Number: 5644\n",
      "Loss: 34.66885190868973\n",
      "l2 norm of gradients: 0.23762418187947737\n",
      "l2 norm of weights: 5.863351955035567\n",
      "---------------------\n",
      "Iteration Number: 5645\n",
      "Loss: 34.66602008285102\n",
      "l2 norm of gradients: 0.23759536673278678\n",
      "l2 norm of weights: 5.863269272789496\n",
      "---------------------\n",
      "Iteration Number: 5646\n",
      "Loss: 34.6631889436341\n",
      "l2 norm of gradients: 0.23756656170457183\n",
      "l2 norm of weights: 5.863186598198466\n",
      "---------------------\n",
      "Iteration Number: 5647\n",
      "Loss: 34.66035849072011\n",
      "l2 norm of gradients: 0.23753776678789734\n",
      "l2 norm of weights: 5.863103931260876\n",
      "---------------------\n",
      "Iteration Number: 5648\n",
      "Loss: 34.65752872378006\n",
      "l2 norm of gradients: 0.23750898197583448\n",
      "l2 norm of weights: 5.863021271975126\n",
      "---------------------\n",
      "Iteration Number: 5649\n",
      "Loss: 34.65469964249867\n",
      "l2 norm of gradients: 0.23748020726146124\n",
      "l2 norm of weights: 5.862938620339615\n",
      "---------------------\n",
      "Iteration Number: 5650\n",
      "Loss: 34.65187124655055\n",
      "l2 norm of gradients: 0.2374514426378618\n",
      "l2 norm of weights: 5.862855976352745\n",
      "---------------------\n",
      "Iteration Number: 5651\n",
      "Loss: 34.64904353561444\n",
      "l2 norm of gradients: 0.23742268809812717\n",
      "l2 norm of weights: 5.862773340012918\n",
      "---------------------\n",
      "Iteration Number: 5652\n",
      "Loss: 34.646216509368806\n",
      "l2 norm of gradients: 0.23739394363535468\n",
      "l2 norm of weights: 5.8626907113185345\n",
      "---------------------\n",
      "Iteration Number: 5653\n",
      "Loss: 34.64339016749691\n",
      "l2 norm of gradients: 0.23736520924264837\n",
      "l2 norm of weights: 5.862608090268\n",
      "---------------------\n",
      "Iteration Number: 5654\n",
      "Loss: 34.64056450966628\n",
      "l2 norm of gradients: 0.23733648491311862\n",
      "l2 norm of weights: 5.862525476859716\n",
      "---------------------\n",
      "Iteration Number: 5655\n",
      "Loss: 34.63773953557592\n",
      "l2 norm of gradients: 0.23730777063988237\n",
      "l2 norm of weights: 5.86244287109209\n",
      "---------------------\n",
      "Iteration Number: 5656\n",
      "Loss: 34.63491524489193\n",
      "l2 norm of gradients: 0.2372790664160632\n",
      "l2 norm of weights: 5.8623602729635245\n",
      "---------------------\n",
      "Iteration Number: 5657\n",
      "Loss: 34.63209163729411\n",
      "l2 norm of gradients: 0.23725037223479095\n",
      "l2 norm of weights: 5.8622776824724285\n",
      "---------------------\n",
      "Iteration Number: 5658\n",
      "Loss: 34.62926871247535\n",
      "l2 norm of gradients: 0.23722168808920208\n",
      "l2 norm of weights: 5.862195099617206\n",
      "---------------------\n",
      "Iteration Number: 5659\n",
      "Loss: 34.626446470100106\n",
      "l2 norm of gradients: 0.2371930139724395\n",
      "l2 norm of weights: 5.862112524396267\n",
      "---------------------\n",
      "Iteration Number: 5660\n",
      "Loss: 34.623624909856446\n",
      "l2 norm of gradients: 0.23716434987765259\n",
      "l2 norm of weights: 5.862029956808019\n",
      "---------------------\n",
      "Iteration Number: 5661\n",
      "Loss: 34.62080403142866\n",
      "l2 norm of gradients: 0.23713569579799723\n",
      "l2 norm of weights: 5.861947396850871\n",
      "---------------------\n",
      "Iteration Number: 5662\n",
      "Loss: 34.61798383449326\n",
      "l2 norm of gradients: 0.23710705172663576\n",
      "l2 norm of weights: 5.861864844523233\n",
      "---------------------\n",
      "Iteration Number: 5663\n",
      "Loss: 34.61516431872771\n",
      "l2 norm of gradients: 0.2370784176567369\n",
      "l2 norm of weights: 5.861782299823517\n",
      "---------------------\n",
      "Iteration Number: 5664\n",
      "Loss: 34.61234548382662\n",
      "l2 norm of gradients: 0.2370497935814759\n",
      "l2 norm of weights: 5.861699762750132\n",
      "---------------------\n",
      "Iteration Number: 5665\n",
      "Loss: 34.60952732946742\n",
      "l2 norm of gradients: 0.23702117949403437\n",
      "l2 norm of weights: 5.861617233301493\n",
      "---------------------\n",
      "Iteration Number: 5666\n",
      "Loss: 34.606709855323416\n",
      "l2 norm of gradients: 0.23699257538760043\n",
      "l2 norm of weights: 5.8615347114760095\n",
      "---------------------\n",
      "Iteration Number: 5667\n",
      "Loss: 34.60389306108617\n",
      "l2 norm of gradients: 0.2369639812553686\n",
      "l2 norm of weights: 5.861452197272098\n",
      "---------------------\n",
      "Iteration Number: 5668\n",
      "Loss: 34.60107694643306\n",
      "l2 norm of gradients: 0.23693539709053985\n",
      "l2 norm of weights: 5.861369690688172\n",
      "---------------------\n",
      "Iteration Number: 5669\n",
      "Loss: 34.59826151105585\n",
      "l2 norm of gradients: 0.23690682288632142\n",
      "l2 norm of weights: 5.861287191722646\n",
      "---------------------\n",
      "Iteration Number: 5670\n",
      "Loss: 34.59544675463385\n",
      "l2 norm of gradients: 0.23687825863592718\n",
      "l2 norm of weights: 5.8612047003739365\n",
      "---------------------\n",
      "Iteration Number: 5671\n",
      "Loss: 34.59263267684403\n",
      "l2 norm of gradients: 0.23684970433257732\n",
      "l2 norm of weights: 5.86112221664046\n",
      "---------------------\n",
      "Iteration Number: 5672\n",
      "Loss: 34.589819277378346\n",
      "l2 norm of gradients: 0.23682115996949832\n",
      "l2 norm of weights: 5.861039740520633\n",
      "---------------------\n",
      "Iteration Number: 5673\n",
      "Loss: 34.58700655591534\n",
      "l2 norm of gradients: 0.23679262553992317\n",
      "l2 norm of weights: 5.860957272012876\n",
      "---------------------\n",
      "Iteration Number: 5674\n",
      "Loss: 34.584194512141174\n",
      "l2 norm of gradients: 0.23676410103709114\n",
      "l2 norm of weights: 5.860874811115605\n",
      "---------------------\n",
      "Iteration Number: 5675\n",
      "Loss: 34.58138314573782\n",
      "l2 norm of gradients: 0.236735586454248\n",
      "l2 norm of weights: 5.860792357827241\n",
      "---------------------\n",
      "Iteration Number: 5676\n",
      "Loss: 34.57857245639539\n",
      "l2 norm of gradients: 0.23670708178464583\n",
      "l2 norm of weights: 5.860709912146205\n",
      "---------------------\n",
      "Iteration Number: 5677\n",
      "Loss: 34.57576244379714\n",
      "l2 norm of gradients: 0.236678587021543\n",
      "l2 norm of weights: 5.860627474070916\n",
      "---------------------\n",
      "Iteration Number: 5678\n",
      "Loss: 34.57295310762572\n",
      "l2 norm of gradients: 0.23665010215820442\n",
      "l2 norm of weights: 5.860545043599797\n",
      "---------------------\n",
      "Iteration Number: 5679\n",
      "Loss: 34.57014444756918\n",
      "l2 norm of gradients: 0.23662162718790114\n",
      "l2 norm of weights: 5.860462620731271\n",
      "---------------------\n",
      "Iteration Number: 5680\n",
      "Loss: 34.567336463315314\n",
      "l2 norm of gradients: 0.23659316210391074\n",
      "l2 norm of weights: 5.86038020546376\n",
      "---------------------\n",
      "Iteration Number: 5681\n",
      "Loss: 34.56452915454493\n",
      "l2 norm of gradients: 0.23656470689951695\n",
      "l2 norm of weights: 5.860297797795688\n",
      "---------------------\n",
      "Iteration Number: 5682\n",
      "Loss: 34.56172252094877\n",
      "l2 norm of gradients: 0.23653626156800991\n",
      "l2 norm of weights: 5.860215397725482\n",
      "---------------------\n",
      "Iteration Number: 5683\n",
      "Loss: 34.558916562204466\n",
      "l2 norm of gradients: 0.23650782610268628\n",
      "l2 norm of weights: 5.860133005251565\n",
      "---------------------\n",
      "Iteration Number: 5684\n",
      "Loss: 34.55611127800624\n",
      "l2 norm of gradients: 0.2364794004968487\n",
      "l2 norm of weights: 5.8600506203723635\n",
      "---------------------\n",
      "Iteration Number: 5685\n",
      "Loss: 34.55330666804308\n",
      "l2 norm of gradients: 0.23645098474380633\n",
      "l2 norm of weights: 5.859968243086305\n",
      "---------------------\n",
      "Iteration Number: 5686\n",
      "Loss: 34.550502731996794\n",
      "l2 norm of gradients: 0.23642257883687465\n",
      "l2 norm of weights: 5.859885873391819\n",
      "---------------------\n",
      "Iteration Number: 5687\n",
      "Loss: 34.547699469560136\n",
      "l2 norm of gradients: 0.23639418276937532\n",
      "l2 norm of weights: 5.859803511287329\n",
      "---------------------\n",
      "Iteration Number: 5688\n",
      "Loss: 34.54489688041598\n",
      "l2 norm of gradients: 0.23636579653463638\n",
      "l2 norm of weights: 5.859721156771268\n",
      "---------------------\n",
      "Iteration Number: 5689\n",
      "Loss: 34.54209496425242\n",
      "l2 norm of gradients: 0.2363374201259921\n",
      "l2 norm of weights: 5.859638809842065\n",
      "---------------------\n",
      "Iteration Number: 5690\n",
      "Loss: 34.53929372075738\n",
      "l2 norm of gradients: 0.23630905353678308\n",
      "l2 norm of weights: 5.859556470498151\n",
      "---------------------\n",
      "Iteration Number: 5691\n",
      "Loss: 34.53649314962134\n",
      "l2 norm of gradients: 0.23628069676035615\n",
      "l2 norm of weights: 5.859474138737955\n",
      "---------------------\n",
      "Iteration Number: 5692\n",
      "Loss: 34.53369325053184\n",
      "l2 norm of gradients: 0.23625234979006451\n",
      "l2 norm of weights: 5.859391814559912\n",
      "---------------------\n",
      "Iteration Number: 5693\n",
      "Loss: 34.53089402317849\n",
      "l2 norm of gradients: 0.23622401261926748\n",
      "l2 norm of weights: 5.859309497962453\n",
      "---------------------\n",
      "Iteration Number: 5694\n",
      "Loss: 34.52809546725065\n",
      "l2 norm of gradients: 0.23619568524133072\n",
      "l2 norm of weights: 5.859227188944012\n",
      "---------------------\n",
      "Iteration Number: 5695\n",
      "Loss: 34.52529758243669\n",
      "l2 norm of gradients: 0.23616736764962612\n",
      "l2 norm of weights: 5.859144887503023\n",
      "---------------------\n",
      "Iteration Number: 5696\n",
      "Loss: 34.522500368424026\n",
      "l2 norm of gradients: 0.23613905983753178\n",
      "l2 norm of weights: 5.859062593637921\n",
      "---------------------\n",
      "Iteration Number: 5697\n",
      "Loss: 34.51970382490527\n",
      "l2 norm of gradients: 0.23611076179843216\n",
      "l2 norm of weights: 5.858980307347141\n",
      "---------------------\n",
      "Iteration Number: 5698\n",
      "Loss: 34.51690795156875\n",
      "l2 norm of gradients: 0.23608247352571776\n",
      "l2 norm of weights: 5.85889802862912\n",
      "---------------------\n",
      "Iteration Number: 5699\n",
      "Loss: 34.514112748106754\n",
      "l2 norm of gradients: 0.2360541950127854\n",
      "l2 norm of weights: 5.858815757482296\n",
      "---------------------\n",
      "Iteration Number: 5700\n",
      "Loss: 34.511318214206355\n",
      "l2 norm of gradients: 0.2360259262530382\n",
      "l2 norm of weights: 5.858733493905105\n",
      "---------------------\n",
      "Iteration Number: 5701\n",
      "Loss: 34.508524349562435\n",
      "l2 norm of gradients: 0.23599766723988533\n",
      "l2 norm of weights: 5.8586512378959865\n",
      "---------------------\n",
      "Iteration Number: 5702\n",
      "Loss: 34.50573115386237\n",
      "l2 norm of gradients: 0.23596941796674228\n",
      "l2 norm of weights: 5.85856898945338\n",
      "---------------------\n",
      "Iteration Number: 5703\n",
      "Loss: 34.502938626798134\n",
      "l2 norm of gradients: 0.23594117842703075\n",
      "l2 norm of weights: 5.858486748575724\n",
      "---------------------\n",
      "Iteration Number: 5704\n",
      "Loss: 34.50014676806462\n",
      "l2 norm of gradients: 0.23591294861417847\n",
      "l2 norm of weights: 5.858404515261461\n",
      "---------------------\n",
      "Iteration Number: 5705\n",
      "Loss: 34.49735557734889\n",
      "l2 norm of gradients: 0.2358847285216196\n",
      "l2 norm of weights: 5.858322289509031\n",
      "---------------------\n",
      "Iteration Number: 5706\n",
      "Loss: 34.49456505434325\n",
      "l2 norm of gradients: 0.2358565181427943\n",
      "l2 norm of weights: 5.858240071316877\n",
      "---------------------\n",
      "Iteration Number: 5707\n",
      "Loss: 34.49177519873687\n",
      "l2 norm of gradients: 0.23582831747114893\n",
      "l2 norm of weights: 5.858157860683442\n",
      "---------------------\n",
      "Iteration Number: 5708\n",
      "Loss: 34.488986010232274\n",
      "l2 norm of gradients: 0.23580012650013604\n",
      "l2 norm of weights: 5.858075657607168\n",
      "---------------------\n",
      "Iteration Number: 5709\n",
      "Loss: 34.48619748851149\n",
      "l2 norm of gradients: 0.2357719452232144\n",
      "l2 norm of weights: 5.857993462086501\n",
      "---------------------\n",
      "Iteration Number: 5710\n",
      "Loss: 34.48340963327415\n",
      "l2 norm of gradients: 0.2357437736338488\n",
      "l2 norm of weights: 5.8579112741198855\n",
      "---------------------\n",
      "Iteration Number: 5711\n",
      "Loss: 34.48062244421497\n",
      "l2 norm of gradients: 0.23571561172551037\n",
      "l2 norm of weights: 5.857829093705767\n",
      "---------------------\n",
      "Iteration Number: 5712\n",
      "Loss: 34.47783592101981\n",
      "l2 norm of gradients: 0.23568745949167616\n",
      "l2 norm of weights: 5.857746920842592\n",
      "---------------------\n",
      "Iteration Number: 5713\n",
      "Loss: 34.47505006338018\n",
      "l2 norm of gradients: 0.2356593169258295\n",
      "l2 norm of weights: 5.857664755528807\n",
      "---------------------\n",
      "Iteration Number: 5714\n",
      "Loss: 34.47226487100049\n",
      "l2 norm of gradients: 0.23563118402145983\n",
      "l2 norm of weights: 5.857582597762861\n",
      "---------------------\n",
      "Iteration Number: 5715\n",
      "Loss: 34.46948034356776\n",
      "l2 norm of gradients: 0.23560306077206275\n",
      "l2 norm of weights: 5.8575004475432015\n",
      "---------------------\n",
      "Iteration Number: 5716\n",
      "Loss: 34.46669648077124\n",
      "l2 norm of gradients: 0.23557494717113983\n",
      "l2 norm of weights: 5.857418304868279\n",
      "---------------------\n",
      "Iteration Number: 5717\n",
      "Loss: 34.46391328231752\n",
      "l2 norm of gradients: 0.23554684321219888\n",
      "l2 norm of weights: 5.857336169736541\n",
      "---------------------\n",
      "Iteration Number: 5718\n",
      "Loss: 34.46113074789365\n",
      "l2 norm of gradients: 0.23551874888875393\n",
      "l2 norm of weights: 5.857254042146442\n",
      "---------------------\n",
      "Iteration Number: 5719\n",
      "Loss: 34.458348877197366\n",
      "l2 norm of gradients: 0.23549066419432474\n",
      "l2 norm of weights: 5.8571719220964304\n",
      "---------------------\n",
      "Iteration Number: 5720\n",
      "Loss: 34.45556766992199\n",
      "l2 norm of gradients: 0.23546258912243748\n",
      "l2 norm of weights: 5.857089809584958\n",
      "---------------------\n",
      "Iteration Number: 5721\n",
      "Loss: 34.45278712575849\n",
      "l2 norm of gradients: 0.23543452366662437\n",
      "l2 norm of weights: 5.85700770461048\n",
      "---------------------\n",
      "Iteration Number: 5722\n",
      "Loss: 34.4500072444156\n",
      "l2 norm of gradients: 0.23540646782042357\n",
      "l2 norm of weights: 5.856925607171448\n",
      "---------------------\n",
      "Iteration Number: 5723\n",
      "Loss: 34.44722802557339\n",
      "l2 norm of gradients: 0.23537842157737943\n",
      "l2 norm of weights: 5.8568435172663165\n",
      "---------------------\n",
      "Iteration Number: 5724\n",
      "Loss: 34.444449468937776\n",
      "l2 norm of gradients: 0.2353503849310424\n",
      "l2 norm of weights: 5.856761434893542\n",
      "---------------------\n",
      "Iteration Number: 5725\n",
      "Loss: 34.44167157420665\n",
      "l2 norm of gradients: 0.23532235787496886\n",
      "l2 norm of weights: 5.856679360051578\n",
      "---------------------\n",
      "Iteration Number: 5726\n",
      "Loss: 34.43889434106691\n",
      "l2 norm of gradients: 0.23529434040272135\n",
      "l2 norm of weights: 5.856597292738882\n",
      "---------------------\n",
      "Iteration Number: 5727\n",
      "Loss: 34.43611776922161\n",
      "l2 norm of gradients: 0.23526633250786838\n",
      "l2 norm of weights: 5.85651523295391\n",
      "---------------------\n",
      "Iteration Number: 5728\n",
      "Loss: 34.433341858366475\n",
      "l2 norm of gradients: 0.23523833418398465\n",
      "l2 norm of weights: 5.856433180695121\n",
      "---------------------\n",
      "Iteration Number: 5729\n",
      "Loss: 34.430566608194866\n",
      "l2 norm of gradients: 0.23521034542465075\n",
      "l2 norm of weights: 5.856351135960973\n",
      "---------------------\n",
      "Iteration Number: 5730\n",
      "Loss: 34.427792018410415\n",
      "l2 norm of gradients: 0.2351823662234533\n",
      "l2 norm of weights: 5.856269098749926\n",
      "---------------------\n",
      "Iteration Number: 5731\n",
      "Loss: 34.42501808871352\n",
      "l2 norm of gradients: 0.23515439657398501\n",
      "l2 norm of weights: 5.856187069060438\n",
      "---------------------\n",
      "Iteration Number: 5732\n",
      "Loss: 34.422244818790986\n",
      "l2 norm of gradients: 0.23512643646984469\n",
      "l2 norm of weights: 5.85610504689097\n",
      "---------------------\n",
      "Iteration Number: 5733\n",
      "Loss: 34.41947220834748\n",
      "l2 norm of gradients: 0.23509848590463694\n",
      "l2 norm of weights: 5.856023032239984\n",
      "---------------------\n",
      "Iteration Number: 5734\n",
      "Loss: 34.41670025708302\n",
      "l2 norm of gradients: 0.23507054487197268\n",
      "l2 norm of weights: 5.855941025105943\n",
      "---------------------\n",
      "Iteration Number: 5735\n",
      "Loss: 34.41392896468985\n",
      "l2 norm of gradients: 0.23504261336546847\n",
      "l2 norm of weights: 5.855859025487307\n",
      "---------------------\n",
      "Iteration Number: 5736\n",
      "Loss: 34.411158330870066\n",
      "l2 norm of gradients: 0.2350146913787472\n",
      "l2 norm of weights: 5.855777033382542\n",
      "---------------------\n",
      "Iteration Number: 5737\n",
      "Loss: 34.40838835532782\n",
      "l2 norm of gradients: 0.23498677890543743\n",
      "l2 norm of weights: 5.855695048790109\n",
      "---------------------\n",
      "Iteration Number: 5738\n",
      "Loss: 34.40561903775614\n",
      "l2 norm of gradients: 0.23495887593917397\n",
      "l2 norm of weights: 5.855613071708476\n",
      "---------------------\n",
      "Iteration Number: 5739\n",
      "Loss: 34.4028503778536\n",
      "l2 norm of gradients: 0.23493098247359745\n",
      "l2 norm of weights: 5.855531102136106\n",
      "---------------------\n",
      "Iteration Number: 5740\n",
      "Loss: 34.40008237532288\n",
      "l2 norm of gradients: 0.23490309850235463\n",
      "l2 norm of weights: 5.855449140071466\n",
      "---------------------\n",
      "Iteration Number: 5741\n",
      "Loss: 34.39731502985857\n",
      "l2 norm of gradients: 0.23487522401909802\n",
      "l2 norm of weights: 5.855367185513023\n",
      "---------------------\n",
      "Iteration Number: 5742\n",
      "Loss: 34.39454834117134\n",
      "l2 norm of gradients: 0.23484735901748632\n",
      "l2 norm of weights: 5.855285238459245\n",
      "---------------------\n",
      "Iteration Number: 5743\n",
      "Loss: 34.39178230895054\n",
      "l2 norm of gradients: 0.2348195034911839\n",
      "l2 norm of weights: 5.855203298908599\n",
      "---------------------\n",
      "Iteration Number: 5744\n",
      "Loss: 34.38901693290097\n",
      "l2 norm of gradients: 0.23479165743386132\n",
      "l2 norm of weights: 5.855121366859555\n",
      "---------------------\n",
      "Iteration Number: 5745\n",
      "Loss: 34.386252212730156\n",
      "l2 norm of gradients: 0.23476382083919514\n",
      "l2 norm of weights: 5.855039442310582\n",
      "---------------------\n",
      "Iteration Number: 5746\n",
      "Loss: 34.38348814812527\n",
      "l2 norm of gradients: 0.23473599370086742\n",
      "l2 norm of weights: 5.85495752526015\n",
      "---------------------\n",
      "Iteration Number: 5747\n",
      "Loss: 34.38072473880402\n",
      "l2 norm of gradients: 0.23470817601256672\n",
      "l2 norm of weights: 5.854875615706731\n",
      "---------------------\n",
      "Iteration Number: 5748\n",
      "Loss: 34.37796198444846\n",
      "l2 norm of gradients: 0.23468036776798717\n",
      "l2 norm of weights: 5.854793713648795\n",
      "---------------------\n",
      "Iteration Number: 5749\n",
      "Loss: 34.37519988477583\n",
      "l2 norm of gradients: 0.2346525689608288\n",
      "l2 norm of weights: 5.854711819084817\n",
      "---------------------\n",
      "Iteration Number: 5750\n",
      "Loss: 34.37243843948482\n",
      "l2 norm of gradients: 0.23462477958479783\n",
      "l2 norm of weights: 5.854629932013268\n",
      "---------------------\n",
      "Iteration Number: 5751\n",
      "Loss: 34.36967764827061\n",
      "l2 norm of gradients: 0.23459699963360597\n",
      "l2 norm of weights: 5.854548052432623\n",
      "---------------------\n",
      "Iteration Number: 5752\n",
      "Loss: 34.366917510836984\n",
      "l2 norm of gradients: 0.23456922910097122\n",
      "l2 norm of weights: 5.854466180341355\n",
      "---------------------\n",
      "Iteration Number: 5753\n",
      "Loss: 34.36415802689546\n",
      "l2 norm of gradients: 0.23454146798061729\n",
      "l2 norm of weights: 5.854384315737939\n",
      "---------------------\n",
      "Iteration Number: 5754\n",
      "Loss: 34.36139919614344\n",
      "l2 norm of gradients: 0.23451371626627376\n",
      "l2 norm of weights: 5.854302458620853\n",
      "---------------------\n",
      "Iteration Number: 5755\n",
      "Loss: 34.358641018282704\n",
      "l2 norm of gradients: 0.2344859739516762\n",
      "l2 norm of weights: 5.854220608988573\n",
      "---------------------\n",
      "Iteration Number: 5756\n",
      "Loss: 34.355883493015654\n",
      "l2 norm of gradients: 0.23445824103056595\n",
      "l2 norm of weights: 5.854138766839574\n",
      "---------------------\n",
      "Iteration Number: 5757\n",
      "Loss: 34.35312662005038\n",
      "l2 norm of gradients: 0.2344305174966902\n",
      "l2 norm of weights: 5.854056932172337\n",
      "---------------------\n",
      "Iteration Number: 5758\n",
      "Loss: 34.35037039908947\n",
      "l2 norm of gradients: 0.23440280334380217\n",
      "l2 norm of weights: 5.853975104985337\n",
      "---------------------\n",
      "Iteration Number: 5759\n",
      "Loss: 34.34761482983113\n",
      "l2 norm of gradients: 0.23437509856566077\n",
      "l2 norm of weights: 5.8538932852770555\n",
      "---------------------\n",
      "Iteration Number: 5760\n",
      "Loss: 34.34485991197984\n",
      "l2 norm of gradients: 0.2343474031560309\n",
      "l2 norm of weights: 5.853811473045973\n",
      "---------------------\n",
      "Iteration Number: 5761\n",
      "Loss: 34.342105645246654\n",
      "l2 norm of gradients: 0.23431971710868307\n",
      "l2 norm of weights: 5.853729668290568\n",
      "---------------------\n",
      "Iteration Number: 5762\n",
      "Loss: 34.33935202933613\n",
      "l2 norm of gradients: 0.2342920404173939\n",
      "l2 norm of weights: 5.853647871009324\n",
      "---------------------\n",
      "Iteration Number: 5763\n",
      "Loss: 34.33659906394478\n",
      "l2 norm of gradients: 0.23426437307594575\n",
      "l2 norm of weights: 5.8535660812007215\n",
      "---------------------\n",
      "Iteration Number: 5764\n",
      "Loss: 34.33384674878417\n",
      "l2 norm of gradients: 0.23423671507812666\n",
      "l2 norm of weights: 5.853484298863243\n",
      "---------------------\n",
      "Iteration Number: 5765\n",
      "Loss: 34.33109508355503\n",
      "l2 norm of gradients: 0.2342090664177307\n",
      "l2 norm of weights: 5.853402523995373\n",
      "---------------------\n",
      "Iteration Number: 5766\n",
      "Loss: 34.32834406796682\n",
      "l2 norm of gradients: 0.2341814270885577\n",
      "l2 norm of weights: 5.853320756595595\n",
      "---------------------\n",
      "Iteration Number: 5767\n",
      "Loss: 34.32559370172771\n",
      "l2 norm of gradients: 0.23415379708441328\n",
      "l2 norm of weights: 5.8532389966623946\n",
      "---------------------\n",
      "Iteration Number: 5768\n",
      "Loss: 34.322843984534416\n",
      "l2 norm of gradients: 0.2341261763991087\n",
      "l2 norm of weights: 5.853157244194255\n",
      "---------------------\n",
      "Iteration Number: 5769\n",
      "Loss: 34.32009491610208\n",
      "l2 norm of gradients: 0.23409856502646137\n",
      "l2 norm of weights: 5.853075499189664\n",
      "---------------------\n",
      "Iteration Number: 5770\n",
      "Loss: 34.31734649613089\n",
      "l2 norm of gradients: 0.23407096296029425\n",
      "l2 norm of weights: 5.852993761647109\n",
      "---------------------\n",
      "Iteration Number: 5771\n",
      "Loss: 34.31459872432802\n",
      "l2 norm of gradients: 0.23404337019443613\n",
      "l2 norm of weights: 5.852912031565075\n",
      "---------------------\n",
      "Iteration Number: 5772\n",
      "Loss: 34.311851600407095\n",
      "l2 norm of gradients: 0.23401578672272152\n",
      "l2 norm of weights: 5.8528303089420515\n",
      "---------------------\n",
      "Iteration Number: 5773\n",
      "Loss: 34.30910512406725\n",
      "l2 norm of gradients: 0.23398821253899094\n",
      "l2 norm of weights: 5.852748593776528\n",
      "---------------------\n",
      "Iteration Number: 5774\n",
      "Loss: 34.30635929501284\n",
      "l2 norm of gradients: 0.23396064763709026\n",
      "l2 norm of weights: 5.8526668860669915\n",
      "---------------------\n",
      "Iteration Number: 5775\n",
      "Loss: 34.30361411296339\n",
      "l2 norm of gradients: 0.23393309201087156\n",
      "l2 norm of weights: 5.852585185811934\n",
      "---------------------\n",
      "Iteration Number: 5776\n",
      "Loss: 34.30086957761271\n",
      "l2 norm of gradients: 0.23390554565419241\n",
      "l2 norm of weights: 5.852503493009847\n",
      "---------------------\n",
      "Iteration Number: 5777\n",
      "Loss: 34.298125688681345\n",
      "l2 norm of gradients: 0.2338780085609163\n",
      "l2 norm of weights: 5.85242180765922\n",
      "---------------------\n",
      "Iteration Number: 5778\n",
      "Loss: 34.29538244587186\n",
      "l2 norm of gradients: 0.2338504807249123\n",
      "l2 norm of weights: 5.852340129758547\n",
      "---------------------\n",
      "Iteration Number: 5779\n",
      "Loss: 34.29263984888952\n",
      "l2 norm of gradients: 0.23382296214005516\n",
      "l2 norm of weights: 5.85225845930632\n",
      "---------------------\n",
      "Iteration Number: 5780\n",
      "Loss: 34.28989789744326\n",
      "l2 norm of gradients: 0.2337954528002257\n",
      "l2 norm of weights: 5.852176796301032\n",
      "---------------------\n",
      "Iteration Number: 5781\n",
      "Loss: 34.287156591248284\n",
      "l2 norm of gradients: 0.2337679526993102\n",
      "l2 norm of weights: 5.852095140741178\n",
      "---------------------\n",
      "Iteration Number: 5782\n",
      "Loss: 34.28441593000547\n",
      "l2 norm of gradients: 0.23374046183120065\n",
      "l2 norm of weights: 5.852013492625252\n",
      "---------------------\n",
      "Iteration Number: 5783\n",
      "Loss: 34.281675913426696\n",
      "l2 norm of gradients: 0.2337129801897949\n",
      "l2 norm of weights: 5.85193185195175\n",
      "---------------------\n",
      "Iteration Number: 5784\n",
      "Loss: 34.27893654122288\n",
      "l2 norm of gradients: 0.23368550776899644\n",
      "l2 norm of weights: 5.851850218719168\n",
      "---------------------\n",
      "Iteration Number: 5785\n",
      "Loss: 34.276197813106066\n",
      "l2 norm of gradients: 0.23365804456271444\n",
      "l2 norm of weights: 5.851768592926004\n",
      "---------------------\n",
      "Iteration Number: 5786\n",
      "Loss: 34.273459728781816\n",
      "l2 norm of gradients: 0.2336305905648638\n",
      "l2 norm of weights: 5.851686974570753\n",
      "---------------------\n",
      "Iteration Number: 5787\n",
      "Loss: 34.27072228795819\n",
      "l2 norm of gradients: 0.2336031457693651\n",
      "l2 norm of weights: 5.851605363651916\n",
      "---------------------\n",
      "Iteration Number: 5788\n",
      "Loss: 34.2679854903465\n",
      "l2 norm of gradients: 0.2335757101701447\n",
      "l2 norm of weights: 5.851523760167989\n",
      "---------------------\n",
      "Iteration Number: 5789\n",
      "Loss: 34.26524933566312\n",
      "l2 norm of gradients: 0.2335482837611344\n",
      "l2 norm of weights: 5.851442164117475\n",
      "---------------------\n",
      "Iteration Number: 5790\n",
      "Loss: 34.26251382361079\n",
      "l2 norm of gradients: 0.23352086653627202\n",
      "l2 norm of weights: 5.8513605754988705\n",
      "---------------------\n",
      "Iteration Number: 5791\n",
      "Loss: 34.25977895391023\n",
      "l2 norm of gradients: 0.23349345848950076\n",
      "l2 norm of weights: 5.851278994310679\n",
      "---------------------\n",
      "Iteration Number: 5792\n",
      "Loss: 34.257044726259494\n",
      "l2 norm of gradients: 0.23346605961476963\n",
      "l2 norm of weights: 5.8511974205514\n",
      "---------------------\n",
      "Iteration Number: 5793\n",
      "Loss: 34.25431114037283\n",
      "l2 norm of gradients: 0.23343866990603315\n",
      "l2 norm of weights: 5.851115854219537\n",
      "---------------------\n",
      "Iteration Number: 5794\n",
      "Loss: 34.25157819597078\n",
      "l2 norm of gradients: 0.23341128935725175\n",
      "l2 norm of weights: 5.851034295313593\n",
      "---------------------\n",
      "Iteration Number: 5795\n",
      "Loss: 34.24884589275943\n",
      "l2 norm of gradients: 0.2333839179623913\n",
      "l2 norm of weights: 5.850952743832071\n",
      "---------------------\n",
      "Iteration Number: 5796\n",
      "Loss: 34.246114230450736\n",
      "l2 norm of gradients: 0.23335655571542344\n",
      "l2 norm of weights: 5.850871199773475\n",
      "---------------------\n",
      "Iteration Number: 5797\n",
      "Loss: 34.243383208756256\n",
      "l2 norm of gradients: 0.23332920261032525\n",
      "l2 norm of weights: 5.85078966313631\n",
      "---------------------\n",
      "Iteration Number: 5798\n",
      "Loss: 34.24065282738771\n",
      "l2 norm of gradients: 0.23330185864107963\n",
      "l2 norm of weights: 5.850708133919082\n",
      "---------------------\n",
      "Iteration Number: 5799\n",
      "Loss: 34.23792308606\n",
      "l2 norm of gradients: 0.23327452380167513\n",
      "l2 norm of weights: 5.850626612120296\n",
      "---------------------\n",
      "Iteration Number: 5800\n",
      "Loss: 34.23519398448556\n",
      "l2 norm of gradients: 0.23324719808610567\n",
      "l2 norm of weights: 5.850545097738459\n",
      "---------------------\n",
      "Iteration Number: 5801\n",
      "Loss: 34.23246552237018\n",
      "l2 norm of gradients: 0.23321988148837103\n",
      "l2 norm of weights: 5.850463590772081\n",
      "---------------------\n",
      "Iteration Number: 5802\n",
      "Loss: 34.229737699440065\n",
      "l2 norm of gradients: 0.23319257400247653\n",
      "l2 norm of weights: 5.850382091219667\n",
      "---------------------\n",
      "Iteration Number: 5803\n",
      "Loss: 34.22701051540024\n",
      "l2 norm of gradients: 0.23316527562243305\n",
      "l2 norm of weights: 5.850300599079726\n",
      "---------------------\n",
      "Iteration Number: 5804\n",
      "Loss: 34.22428396996199\n",
      "l2 norm of gradients: 0.23313798634225708\n",
      "l2 norm of weights: 5.850219114350769\n",
      "---------------------\n",
      "Iteration Number: 5805\n",
      "Loss: 34.22155806284595\n",
      "l2 norm of gradients: 0.23311070615597076\n",
      "l2 norm of weights: 5.850137637031305\n",
      "---------------------\n",
      "Iteration Number: 5806\n",
      "Loss: 34.21883279376813\n",
      "l2 norm of gradients: 0.23308343505760176\n",
      "l2 norm of weights: 5.850056167119846\n",
      "---------------------\n",
      "Iteration Number: 5807\n",
      "Loss: 34.2161081624273\n",
      "l2 norm of gradients: 0.23305617304118326\n",
      "l2 norm of weights: 5.849974704614902\n",
      "---------------------\n",
      "Iteration Number: 5808\n",
      "Loss: 34.21338416855777\n",
      "l2 norm of gradients: 0.23302892010075416\n",
      "l2 norm of weights: 5.849893249514986\n",
      "---------------------\n",
      "Iteration Number: 5809\n",
      "Loss: 34.21066081185527\n",
      "l2 norm of gradients: 0.23300167623035883\n",
      "l2 norm of weights: 5.8498118018186105\n",
      "---------------------\n",
      "Iteration Number: 5810\n",
      "Loss: 34.20793809204652\n",
      "l2 norm of gradients: 0.2329744414240472\n",
      "l2 norm of weights: 5.849730361524288\n",
      "---------------------\n",
      "Iteration Number: 5811\n",
      "Loss: 34.20521600884516\n",
      "l2 norm of gradients: 0.23294721567587487\n",
      "l2 norm of weights: 5.849648928630534\n",
      "---------------------\n",
      "Iteration Number: 5812\n",
      "Loss: 34.202494561967846\n",
      "l2 norm of gradients: 0.23291999897990281\n",
      "l2 norm of weights: 5.849567503135862\n",
      "---------------------\n",
      "Iteration Number: 5813\n",
      "Loss: 34.199773751121676\n",
      "l2 norm of gradients: 0.23289279133019772\n",
      "l2 norm of weights: 5.849486085038788\n",
      "---------------------\n",
      "Iteration Number: 5814\n",
      "Loss: 34.19705357603302\n",
      "l2 norm of gradients: 0.2328655927208317\n",
      "l2 norm of weights: 5.849404674337829\n",
      "---------------------\n",
      "Iteration Number: 5815\n",
      "Loss: 34.19433403641211\n",
      "l2 norm of gradients: 0.2328384031458824\n",
      "l2 norm of weights: 5.8493232710314995\n",
      "---------------------\n",
      "Iteration Number: 5816\n",
      "Loss: 34.191615131975354\n",
      "l2 norm of gradients: 0.23281122259943313\n",
      "l2 norm of weights: 5.8492418751183175\n",
      "---------------------\n",
      "Iteration Number: 5817\n",
      "Loss: 34.18889686244292\n",
      "l2 norm of gradients: 0.2327840510755725\n",
      "l2 norm of weights: 5.849160486596801\n",
      "---------------------\n",
      "Iteration Number: 5818\n",
      "Loss: 34.186179227522594\n",
      "l2 norm of gradients: 0.23275688856839488\n",
      "l2 norm of weights: 5.849079105465471\n",
      "---------------------\n",
      "Iteration Number: 5819\n",
      "Loss: 34.1834622269362\n",
      "l2 norm of gradients: 0.23272973507199998\n",
      "l2 norm of weights: 5.848997731722843\n",
      "---------------------\n",
      "Iteration Number: 5820\n",
      "Loss: 34.18074586040202\n",
      "l2 norm of gradients: 0.2327025905804931\n",
      "l2 norm of weights: 5.848916365367439\n",
      "---------------------\n",
      "Iteration Number: 5821\n",
      "Loss: 34.17803012763556\n",
      "l2 norm of gradients: 0.23267545508798498\n",
      "l2 norm of weights: 5.848835006397779\n",
      "---------------------\n",
      "Iteration Number: 5822\n",
      "Loss: 34.17531502835451\n",
      "l2 norm of gradients: 0.23264832858859189\n",
      "l2 norm of weights: 5.848753654812385\n",
      "---------------------\n",
      "Iteration Number: 5823\n",
      "Loss: 34.17260056227696\n",
      "l2 norm of gradients: 0.23262121107643563\n",
      "l2 norm of weights: 5.848672310609777\n",
      "---------------------\n",
      "Iteration Number: 5824\n",
      "Loss: 34.16988672912108\n",
      "l2 norm of gradients: 0.23259410254564328\n",
      "l2 norm of weights: 5.848590973788478\n",
      "---------------------\n",
      "Iteration Number: 5825\n",
      "Loss: 34.16717352860482\n",
      "l2 norm of gradients: 0.23256700299034777\n",
      "l2 norm of weights: 5.848509644347014\n",
      "---------------------\n",
      "Iteration Number: 5826\n",
      "Loss: 34.16446096044492\n",
      "l2 norm of gradients: 0.23253991240468713\n",
      "l2 norm of weights: 5.848428322283905\n",
      "---------------------\n",
      "Iteration Number: 5827\n",
      "Loss: 34.16174902435662\n",
      "l2 norm of gradients: 0.23251283078280507\n",
      "l2 norm of weights: 5.848347007597678\n",
      "---------------------\n",
      "Iteration Number: 5828\n",
      "Loss: 34.15903772006583\n",
      "l2 norm of gradients: 0.23248575811885078\n",
      "l2 norm of weights: 5.848265700286856\n",
      "---------------------\n",
      "Iteration Number: 5829\n",
      "Loss: 34.156327047286936\n",
      "l2 norm of gradients: 0.23245869440697875\n",
      "l2 norm of weights: 5.8481844003499655\n",
      "---------------------\n",
      "Iteration Number: 5830\n",
      "Loss: 34.153617005738305\n",
      "l2 norm of gradients: 0.23243163964134894\n",
      "l2 norm of weights: 5.848103107785534\n",
      "---------------------\n",
      "Iteration Number: 5831\n",
      "Loss: 34.15090759513961\n",
      "l2 norm of gradients: 0.23240459381612694\n",
      "l2 norm of weights: 5.848021822592088\n",
      "---------------------\n",
      "Iteration Number: 5832\n",
      "Loss: 34.14819881521654\n",
      "l2 norm of gradients: 0.2323775569254836\n",
      "l2 norm of weights: 5.847940544768154\n",
      "---------------------\n",
      "Iteration Number: 5833\n",
      "Loss: 34.145490665678395\n",
      "l2 norm of gradients: 0.23235052896359526\n",
      "l2 norm of weights: 5.8478592743122615\n",
      "---------------------\n",
      "Iteration Number: 5834\n",
      "Loss: 34.14278314624643\n",
      "l2 norm of gradients: 0.23232350992464373\n",
      "l2 norm of weights: 5.847778011222939\n",
      "---------------------\n",
      "Iteration Number: 5835\n",
      "Loss: 34.14007625664735\n",
      "l2 norm of gradients: 0.2322964998028161\n",
      "l2 norm of weights: 5.847696755498716\n",
      "---------------------\n",
      "Iteration Number: 5836\n",
      "Loss: 34.13736999660701\n",
      "l2 norm of gradients: 0.23226949859230508\n",
      "l2 norm of weights: 5.847615507138123\n",
      "---------------------\n",
      "Iteration Number: 5837\n",
      "Loss: 34.13466436582745\n",
      "l2 norm of gradients: 0.23224250628730866\n",
      "l2 norm of weights: 5.84753426613969\n",
      "---------------------\n",
      "Iteration Number: 5838\n",
      "Loss: 34.131959364038806\n",
      "l2 norm of gradients: 0.23221552288203023\n",
      "l2 norm of weights: 5.84745303250195\n",
      "---------------------\n",
      "Iteration Number: 5839\n",
      "Loss: 34.12925499096491\n",
      "l2 norm of gradients: 0.23218854837067862\n",
      "l2 norm of weights: 5.8473718062234346\n",
      "---------------------\n",
      "Iteration Number: 5840\n",
      "Loss: 34.12655124632011\n",
      "l2 norm of gradients: 0.2321615827474681\n",
      "l2 norm of weights: 5.847290587302677\n",
      "---------------------\n",
      "Iteration Number: 5841\n",
      "Loss: 34.12384812982795\n",
      "l2 norm of gradients: 0.23213462600661822\n",
      "l2 norm of weights: 5.847209375738208\n",
      "---------------------\n",
      "Iteration Number: 5842\n",
      "Loss: 34.12114564121964\n",
      "l2 norm of gradients: 0.23210767814235403\n",
      "l2 norm of weights: 5.847128171528564\n",
      "---------------------\n",
      "Iteration Number: 5843\n",
      "Loss: 34.118443780202064\n",
      "l2 norm of gradients: 0.23208073914890587\n",
      "l2 norm of weights: 5.84704697467228\n",
      "---------------------\n",
      "Iteration Number: 5844\n",
      "Loss: 34.11574254650479\n",
      "l2 norm of gradients: 0.2320538090205094\n",
      "l2 norm of weights: 5.84696578516789\n",
      "---------------------\n",
      "Iteration Number: 5845\n",
      "Loss: 34.11304193984699\n",
      "l2 norm of gradients: 0.23202688775140587\n",
      "l2 norm of weights: 5.8468846030139305\n",
      "---------------------\n",
      "Iteration Number: 5846\n",
      "Loss: 34.110341959955754\n",
      "l2 norm of gradients: 0.23199997533584177\n",
      "l2 norm of weights: 5.846803428208939\n",
      "---------------------\n",
      "Iteration Number: 5847\n",
      "Loss: 34.107642606546925\n",
      "l2 norm of gradients: 0.2319730717680688\n",
      "l2 norm of weights: 5.84672226075145\n",
      "---------------------\n",
      "Iteration Number: 5848\n",
      "Loss: 34.10494387934833\n",
      "l2 norm of gradients: 0.23194617704234421\n",
      "l2 norm of weights: 5.846641100640005\n",
      "---------------------\n",
      "Iteration Number: 5849\n",
      "Loss: 34.10224577807457\n",
      "l2 norm of gradients: 0.23191929115293053\n",
      "l2 norm of weights: 5.84655994787314\n",
      "---------------------\n",
      "Iteration Number: 5850\n",
      "Loss: 34.09954830246169\n",
      "l2 norm of gradients: 0.23189241409409558\n",
      "l2 norm of weights: 5.846478802449395\n",
      "---------------------\n",
      "Iteration Number: 5851\n",
      "Loss: 34.096851452226026\n",
      "l2 norm of gradients: 0.23186554586011265\n",
      "l2 norm of weights: 5.846397664367308\n",
      "---------------------\n",
      "Iteration Number: 5852\n",
      "Loss: 34.094155227086574\n",
      "l2 norm of gradients: 0.23183868644526026\n",
      "l2 norm of weights: 5.846316533625424\n",
      "---------------------\n",
      "Iteration Number: 5853\n",
      "Loss: 34.0914596267793\n",
      "l2 norm of gradients: 0.23181183584382217\n",
      "l2 norm of weights: 5.846235410222279\n",
      "---------------------\n",
      "Iteration Number: 5854\n",
      "Loss: 34.08876465101386\n",
      "l2 norm of gradients: 0.23178499405008768\n",
      "l2 norm of weights: 5.846154294156417\n",
      "---------------------\n",
      "Iteration Number: 5855\n",
      "Loss: 34.08607029952345\n",
      "l2 norm of gradients: 0.2317581610583512\n",
      "l2 norm of weights: 5.84607318542638\n",
      "---------------------\n",
      "Iteration Number: 5856\n",
      "Loss: 34.08337657202923\n",
      "l2 norm of gradients: 0.23173133686291253\n",
      "l2 norm of weights: 5.845992084030711\n",
      "---------------------\n",
      "Iteration Number: 5857\n",
      "Loss: 34.08068346825753\n",
      "l2 norm of gradients: 0.23170452145807677\n",
      "l2 norm of weights: 5.845910989967953\n",
      "---------------------\n",
      "Iteration Number: 5858\n",
      "Loss: 34.07799098793022\n",
      "l2 norm of gradients: 0.23167771483815436\n",
      "l2 norm of weights: 5.845829903236651\n",
      "---------------------\n",
      "Iteration Number: 5859\n",
      "Loss: 34.07529913077142\n",
      "l2 norm of gradients: 0.2316509169974609\n",
      "l2 norm of weights: 5.845748823835349\n",
      "---------------------\n",
      "Iteration Number: 5860\n",
      "Loss: 34.07260789651453\n",
      "l2 norm of gradients: 0.23162412793031747\n",
      "l2 norm of weights: 5.845667751762592\n",
      "---------------------\n",
      "Iteration Number: 5861\n",
      "Loss: 34.06991728487269\n",
      "l2 norm of gradients: 0.23159734763105028\n",
      "l2 norm of weights: 5.845586687016929\n",
      "---------------------\n",
      "Iteration Number: 5862\n",
      "Loss: 34.06722729557771\n",
      "l2 norm of gradients: 0.2315705760939908\n",
      "l2 norm of weights: 5.845505629596903\n",
      "---------------------\n",
      "Iteration Number: 5863\n",
      "Loss: 34.064537928356266\n",
      "l2 norm of gradients: 0.2315438133134759\n",
      "l2 norm of weights: 5.845424579501063\n",
      "---------------------\n",
      "Iteration Number: 5864\n",
      "Loss: 34.061849182931994\n",
      "l2 norm of gradients: 0.23151705928384766\n",
      "l2 norm of weights: 5.845343536727956\n",
      "---------------------\n",
      "Iteration Number: 5865\n",
      "Loss: 34.05916105903139\n",
      "l2 norm of gradients: 0.23149031399945333\n",
      "l2 norm of weights: 5.845262501276133\n",
      "---------------------\n",
      "Iteration Number: 5866\n",
      "Loss: 34.05647355638292\n",
      "l2 norm of gradients: 0.2314635774546456\n",
      "l2 norm of weights: 5.845181473144139\n",
      "---------------------\n",
      "Iteration Number: 5867\n",
      "Loss: 34.05378667471226\n",
      "l2 norm of gradients: 0.23143684964378217\n",
      "l2 norm of weights: 5.845100452330528\n",
      "---------------------\n",
      "Iteration Number: 5868\n",
      "Loss: 34.05110041373743\n",
      "l2 norm of gradients: 0.2314101305612262\n",
      "l2 norm of weights: 5.845019438833848\n",
      "---------------------\n",
      "Iteration Number: 5869\n",
      "Loss: 34.04841477320125\n",
      "l2 norm of gradients: 0.23138342020134592\n",
      "l2 norm of weights: 5.8449384326526514\n",
      "---------------------\n",
      "Iteration Number: 5870\n",
      "Loss: 34.04572975281847\n",
      "l2 norm of gradients: 0.23135671855851495\n",
      "l2 norm of weights: 5.8448574337854895\n",
      "---------------------\n",
      "Iteration Number: 5871\n",
      "Loss: 34.04304535232031\n",
      "l2 norm of gradients: 0.23133002562711202\n",
      "l2 norm of weights: 5.844776442230914\n",
      "---------------------\n",
      "Iteration Number: 5872\n",
      "Loss: 34.040361571433905\n",
      "l2 norm of gradients: 0.23130334140152103\n",
      "l2 norm of weights: 5.844695457987478\n",
      "---------------------\n",
      "Iteration Number: 5873\n",
      "Loss: 34.03767840988693\n",
      "l2 norm of gradients: 0.23127666587613135\n",
      "l2 norm of weights: 5.844614481053735\n",
      "---------------------\n",
      "Iteration Number: 5874\n",
      "Loss: 34.034995867407346\n",
      "l2 norm of gradients: 0.23124999904533733\n",
      "l2 norm of weights: 5.8445335114282395\n",
      "---------------------\n",
      "Iteration Number: 5875\n",
      "Loss: 34.03231394372539\n",
      "l2 norm of gradients: 0.23122334090353852\n",
      "l2 norm of weights: 5.844452549109546\n",
      "---------------------\n",
      "Iteration Number: 5876\n",
      "Loss: 34.02963263856462\n",
      "l2 norm of gradients: 0.23119669144513985\n",
      "l2 norm of weights: 5.844371594096209\n",
      "---------------------\n",
      "Iteration Number: 5877\n",
      "Loss: 34.026951951652656\n",
      "l2 norm of gradients: 0.23117005066455126\n",
      "l2 norm of weights: 5.844290646386788\n",
      "---------------------\n",
      "Iteration Number: 5878\n",
      "Loss: 34.02427188272689\n",
      "l2 norm of gradients: 0.23114341855618803\n",
      "l2 norm of weights: 5.844209705979836\n",
      "---------------------\n",
      "Iteration Number: 5879\n",
      "Loss: 34.021592431508715\n",
      "l2 norm of gradients: 0.23111679511447056\n",
      "l2 norm of weights: 5.84412877287391\n",
      "---------------------\n",
      "Iteration Number: 5880\n",
      "Loss: 34.0189135977252\n",
      "l2 norm of gradients: 0.23109018033382434\n",
      "l2 norm of weights: 5.8440478470675705\n",
      "---------------------\n",
      "Iteration Number: 5881\n",
      "Loss: 34.01623538111243\n",
      "l2 norm of gradients: 0.23106357420868007\n",
      "l2 norm of weights: 5.843966928559375\n",
      "---------------------\n",
      "Iteration Number: 5882\n",
      "Loss: 34.01355778139773\n",
      "l2 norm of gradients: 0.23103697673347384\n",
      "l2 norm of weights: 5.843886017347883\n",
      "---------------------\n",
      "Iteration Number: 5883\n",
      "Loss: 34.01088079830859\n",
      "l2 norm of gradients: 0.23101038790264664\n",
      "l2 norm of weights: 5.843805113431651\n",
      "---------------------\n",
      "Iteration Number: 5884\n",
      "Loss: 34.008204431575734\n",
      "l2 norm of gradients: 0.23098380771064467\n",
      "l2 norm of weights: 5.843724216809244\n",
      "---------------------\n",
      "Iteration Number: 5885\n",
      "Loss: 34.00552868092994\n",
      "l2 norm of gradients: 0.23095723615191935\n",
      "l2 norm of weights: 5.843643327479221\n",
      "---------------------\n",
      "Iteration Number: 5886\n",
      "Loss: 34.002853546098265\n",
      "l2 norm of gradients: 0.23093067322092728\n",
      "l2 norm of weights: 5.843562445440142\n",
      "---------------------\n",
      "Iteration Number: 5887\n",
      "Loss: 34.000179026812546\n",
      "l2 norm of gradients: 0.23090411891213\n",
      "l2 norm of weights: 5.843481570690571\n",
      "---------------------\n",
      "Iteration Number: 5888\n",
      "Loss: 33.997505122809265\n",
      "l2 norm of gradients: 0.2308775732199945\n",
      "l2 norm of weights: 5.843400703229071\n",
      "---------------------\n",
      "Iteration Number: 5889\n",
      "Loss: 33.99483183380844\n",
      "l2 norm of gradients: 0.23085103613899258\n",
      "l2 norm of weights: 5.843319843054205\n",
      "---------------------\n",
      "Iteration Number: 5890\n",
      "Loss: 33.992159159547974\n",
      "l2 norm of gradients: 0.2308245076636014\n",
      "l2 norm of weights: 5.843238990164536\n",
      "---------------------\n",
      "Iteration Number: 5891\n",
      "Loss: 33.98948709976316\n",
      "l2 norm of gradients: 0.23079798778830318\n",
      "l2 norm of weights: 5.843158144558629\n",
      "---------------------\n",
      "Iteration Number: 5892\n",
      "Loss: 33.986815654175714\n",
      "l2 norm of gradients: 0.23077147650758514\n",
      "l2 norm of weights: 5.84307730623505\n",
      "---------------------\n",
      "Iteration Number: 5893\n",
      "Loss: 33.984144822517585\n",
      "l2 norm of gradients: 0.23074497381593984\n",
      "l2 norm of weights: 5.842996475192366\n",
      "---------------------\n",
      "Iteration Number: 5894\n",
      "Loss: 33.98147460453166\n",
      "l2 norm of gradients: 0.2307184797078647\n",
      "l2 norm of weights: 5.84291565142914\n",
      "---------------------\n",
      "Iteration Number: 5895\n",
      "Loss: 33.9788049999408\n",
      "l2 norm of gradients: 0.23069199417786235\n",
      "l2 norm of weights: 5.842834834943941\n",
      "---------------------\n",
      "Iteration Number: 5896\n",
      "Loss: 33.976136008476516\n",
      "l2 norm of gradients: 0.23066551722044057\n",
      "l2 norm of weights: 5.842754025735338\n",
      "---------------------\n",
      "Iteration Number: 5897\n",
      "Loss: 33.97346762986948\n",
      "l2 norm of gradients: 0.23063904883011221\n",
      "l2 norm of weights: 5.842673223801897\n",
      "---------------------\n",
      "Iteration Number: 5898\n",
      "Loss: 33.9707998638583\n",
      "l2 norm of gradients: 0.23061258900139506\n",
      "l2 norm of weights: 5.842592429142187\n",
      "---------------------\n",
      "Iteration Number: 5899\n",
      "Loss: 33.968132710179255\n",
      "l2 norm of gradients: 0.2305861377288122\n",
      "l2 norm of weights: 5.842511641754779\n",
      "---------------------\n",
      "Iteration Number: 5900\n",
      "Loss: 33.965466168556226\n",
      "l2 norm of gradients: 0.23055969500689164\n",
      "l2 norm of weights: 5.842430861638243\n",
      "---------------------\n",
      "Iteration Number: 5901\n",
      "Loss: 33.96280023872508\n",
      "l2 norm of gradients: 0.2305332608301665\n",
      "l2 norm of weights: 5.8423500887911475\n",
      "---------------------\n",
      "Iteration Number: 5902\n",
      "Loss: 33.9601349204181\n",
      "l2 norm of gradients: 0.23050683519317508\n",
      "l2 norm of weights: 5.842269323212068\n",
      "---------------------\n",
      "Iteration Number: 5903\n",
      "Loss: 33.957470213366115\n",
      "l2 norm of gradients: 0.23048041809046046\n",
      "l2 norm of weights: 5.842188564899572\n",
      "---------------------\n",
      "Iteration Number: 5904\n",
      "Loss: 33.95480611731038\n",
      "l2 norm of gradients: 0.23045400951657113\n",
      "l2 norm of weights: 5.842107813852234\n",
      "---------------------\n",
      "Iteration Number: 5905\n",
      "Loss: 33.95214263198396\n",
      "l2 norm of gradients: 0.23042760946606028\n",
      "l2 norm of weights: 5.842027070068626\n",
      "---------------------\n",
      "Iteration Number: 5906\n",
      "Loss: 33.94947975711411\n",
      "l2 norm of gradients: 0.23040121793348642\n",
      "l2 norm of weights: 5.841946333547325\n",
      "---------------------\n",
      "Iteration Number: 5907\n",
      "Loss: 33.94681749243867\n",
      "l2 norm of gradients: 0.2303748349134129\n",
      "l2 norm of weights: 5.841865604286902\n",
      "---------------------\n",
      "Iteration Number: 5908\n",
      "Loss: 33.94415583769755\n",
      "l2 norm of gradients: 0.2303484604004082\n",
      "l2 norm of weights: 5.841784882285933\n",
      "---------------------\n",
      "Iteration Number: 5909\n",
      "Loss: 33.941494792611934\n",
      "l2 norm of gradients: 0.2303220943890459\n",
      "l2 norm of weights: 5.841704167542993\n",
      "---------------------\n",
      "Iteration Number: 5910\n",
      "Loss: 33.93883435692619\n",
      "l2 norm of gradients: 0.23029573687390448\n",
      "l2 norm of weights: 5.841623460056659\n",
      "---------------------\n",
      "Iteration Number: 5911\n",
      "Loss: 33.936174530375006\n",
      "l2 norm of gradients: 0.23026938784956744\n",
      "l2 norm of weights: 5.841542759825508\n",
      "---------------------\n",
      "Iteration Number: 5912\n",
      "Loss: 33.93351531269217\n",
      "l2 norm of gradients: 0.23024304731062334\n",
      "l2 norm of weights: 5.841462066848116\n",
      "---------------------\n",
      "Iteration Number: 5913\n",
      "Loss: 33.930856703616136\n",
      "l2 norm of gradients: 0.23021671525166573\n",
      "l2 norm of weights: 5.841381381123061\n",
      "---------------------\n",
      "Iteration Number: 5914\n",
      "Loss: 33.928198702878646\n",
      "l2 norm of gradients: 0.2301903916672933\n",
      "l2 norm of weights: 5.841300702648924\n",
      "---------------------\n",
      "Iteration Number: 5915\n",
      "Loss: 33.925541310212616\n",
      "l2 norm of gradients: 0.23016407655210938\n",
      "l2 norm of weights: 5.841220031424282\n",
      "---------------------\n",
      "Iteration Number: 5916\n",
      "Loss: 33.9228845253599\n",
      "l2 norm of gradients: 0.23013776990072277\n",
      "l2 norm of weights: 5.841139367447714\n",
      "---------------------\n",
      "Iteration Number: 5917\n",
      "Loss: 33.920228348054096\n",
      "l2 norm of gradients: 0.23011147170774682\n",
      "l2 norm of weights: 5.841058710717802\n",
      "---------------------\n",
      "Iteration Number: 5918\n",
      "Loss: 33.91757277803126\n",
      "l2 norm of gradients: 0.23008518196780012\n",
      "l2 norm of weights: 5.840978061233127\n",
      "---------------------\n",
      "Iteration Number: 5919\n",
      "Loss: 33.91491781502897\n",
      "l2 norm of gradients: 0.2300589006755061\n",
      "l2 norm of weights: 5.840897418992269\n",
      "---------------------\n",
      "Iteration Number: 5920\n",
      "Loss: 33.91226345877811\n",
      "l2 norm of gradients: 0.23003262782549333\n",
      "l2 norm of weights: 5.840816783993812\n",
      "---------------------\n",
      "Iteration Number: 5921\n",
      "Loss: 33.909609709028636\n",
      "l2 norm of gradients: 0.2300063634123953\n",
      "l2 norm of weights: 5.840736156236337\n",
      "---------------------\n",
      "Iteration Number: 5922\n",
      "Loss: 33.90695656550218\n",
      "l2 norm of gradients: 0.22998010743085026\n",
      "l2 norm of weights: 5.840655535718429\n",
      "---------------------\n",
      "Iteration Number: 5923\n",
      "Loss: 33.90430402794688\n",
      "l2 norm of gradients: 0.2299538598755016\n",
      "l2 norm of weights: 5.8405749224386705\n",
      "---------------------\n",
      "Iteration Number: 5924\n",
      "Loss: 33.901652096093734\n",
      "l2 norm of gradients: 0.22992762074099765\n",
      "l2 norm of weights: 5.840494316395646\n",
      "---------------------\n",
      "Iteration Number: 5925\n",
      "Loss: 33.899000769686545\n",
      "l2 norm of gradients: 0.22990139002199167\n",
      "l2 norm of weights: 5.840413717587943\n",
      "---------------------\n",
      "Iteration Number: 5926\n",
      "Loss: 33.89635004845666\n",
      "l2 norm of gradients: 0.2298751677131418\n",
      "l2 norm of weights: 5.840333126014143\n",
      "---------------------\n",
      "Iteration Number: 5927\n",
      "Loss: 33.89369993214525\n",
      "l2 norm of gradients: 0.22984895380911127\n",
      "l2 norm of weights: 5.840252541672837\n",
      "---------------------\n",
      "Iteration Number: 5928\n",
      "Loss: 33.8910504204924\n",
      "l2 norm of gradients: 0.22982274830456806\n",
      "l2 norm of weights: 5.840171964562608\n",
      "---------------------\n",
      "Iteration Number: 5929\n",
      "Loss: 33.88840151323358\n",
      "l2 norm of gradients: 0.2297965511941852\n",
      "l2 norm of weights: 5.840091394682045\n",
      "---------------------\n",
      "Iteration Number: 5930\n",
      "Loss: 33.8857532101057\n",
      "l2 norm of gradients: 0.22977036247264054\n",
      "l2 norm of weights: 5.840010832029738\n",
      "---------------------\n",
      "Iteration Number: 5931\n",
      "Loss: 33.883105510851074\n",
      "l2 norm of gradients: 0.22974418213461695\n",
      "l2 norm of weights: 5.839930276604272\n",
      "---------------------\n",
      "Iteration Number: 5932\n",
      "Loss: 33.88045841520704\n",
      "l2 norm of gradients: 0.22971801017480223\n",
      "l2 norm of weights: 5.839849728404238\n",
      "---------------------\n",
      "Iteration Number: 5933\n",
      "Loss: 33.877811922913764\n",
      "l2 norm of gradients: 0.22969184658788885\n",
      "l2 norm of weights: 5.839769187428227\n",
      "---------------------\n",
      "Iteration Number: 5934\n",
      "Loss: 33.875166033708425\n",
      "l2 norm of gradients: 0.2296656913685746\n",
      "l2 norm of weights: 5.839688653674828\n",
      "---------------------\n",
      "Iteration Number: 5935\n",
      "Loss: 33.87252074733479\n",
      "l2 norm of gradients: 0.22963954451156168\n",
      "l2 norm of weights: 5.839608127142632\n",
      "---------------------\n",
      "Iteration Number: 5936\n",
      "Loss: 33.86987606352432\n",
      "l2 norm of gradients: 0.22961340601155755\n",
      "l2 norm of weights: 5.839527607830231\n",
      "---------------------\n",
      "Iteration Number: 5937\n",
      "Loss: 33.86723198202549\n",
      "l2 norm of gradients: 0.2295872758632744\n",
      "l2 norm of weights: 5.839447095736217\n",
      "---------------------\n",
      "Iteration Number: 5938\n",
      "Loss: 33.86458850257321\n",
      "l2 norm of gradients: 0.2295611540614293\n",
      "l2 norm of weights: 5.839366590859183\n",
      "---------------------\n",
      "Iteration Number: 5939\n",
      "Loss: 33.86194562490884\n",
      "l2 norm of gradients: 0.22953504060074428\n",
      "l2 norm of weights: 5.839286093197723\n",
      "---------------------\n",
      "Iteration Number: 5940\n",
      "Loss: 33.85930334877625\n",
      "l2 norm of gradients: 0.22950893547594617\n",
      "l2 norm of weights: 5.83920560275043\n",
      "---------------------\n",
      "Iteration Number: 5941\n",
      "Loss: 33.85666167391058\n",
      "l2 norm of gradients: 0.22948283868176664\n",
      "l2 norm of weights: 5.839125119515899\n",
      "---------------------\n",
      "Iteration Number: 5942\n",
      "Loss: 33.854020600057986\n",
      "l2 norm of gradients: 0.22945675021294232\n",
      "l2 norm of weights: 5.839044643492724\n",
      "---------------------\n",
      "Iteration Number: 5943\n",
      "Loss: 33.851380126949365\n",
      "l2 norm of gradients: 0.22943067006421458\n",
      "l2 norm of weights: 5.838964174679503\n",
      "---------------------\n",
      "Iteration Number: 5944\n",
      "Loss: 33.84874025433405\n",
      "l2 norm of gradients: 0.22940459823032977\n",
      "l2 norm of weights: 5.83888371307483\n",
      "---------------------\n",
      "Iteration Number: 5945\n",
      "Loss: 33.84610098195667\n",
      "l2 norm of gradients: 0.22937853470603894\n",
      "l2 norm of weights: 5.838803258677304\n",
      "---------------------\n",
      "Iteration Number: 5946\n",
      "Loss: 33.84346230955026\n",
      "l2 norm of gradients: 0.22935247948609808\n",
      "l2 norm of weights: 5.838722811485522\n",
      "---------------------\n",
      "Iteration Number: 5947\n",
      "Loss: 33.840824236862744\n",
      "l2 norm of gradients: 0.22932643256526802\n",
      "l2 norm of weights: 5.83864237149808\n",
      "---------------------\n",
      "Iteration Number: 5948\n",
      "Loss: 33.8381867636317\n",
      "l2 norm of gradients: 0.22930039393831442\n",
      "l2 norm of weights: 5.838561938713579\n",
      "---------------------\n",
      "Iteration Number: 5949\n",
      "Loss: 33.83554988960442\n",
      "l2 norm of gradients: 0.22927436360000772\n",
      "l2 norm of weights: 5.838481513130618\n",
      "---------------------\n",
      "Iteration Number: 5950\n",
      "Loss: 33.83291361451438\n",
      "l2 norm of gradients: 0.22924834154512308\n",
      "l2 norm of weights: 5.838401094747795\n",
      "---------------------\n",
      "Iteration Number: 5951\n",
      "Loss: 33.83027793811491\n",
      "l2 norm of gradients: 0.22922232776844073\n",
      "l2 norm of weights: 5.838320683563712\n",
      "---------------------\n",
      "Iteration Number: 5952\n",
      "Loss: 33.82764286014002\n",
      "l2 norm of gradients: 0.2291963222647456\n",
      "l2 norm of weights: 5.8382402795769694\n",
      "---------------------\n",
      "Iteration Number: 5953\n",
      "Loss: 33.8250083803325\n",
      "l2 norm of gradients: 0.22917032502882728\n",
      "l2 norm of weights: 5.83815988278617\n",
      "---------------------\n",
      "Iteration Number: 5954\n",
      "Loss: 33.82237449843706\n",
      "l2 norm of gradients: 0.22914433605548046\n",
      "l2 norm of weights: 5.838079493189913\n",
      "---------------------\n",
      "Iteration Number: 5955\n",
      "Loss: 33.81974121420459\n",
      "l2 norm of gradients: 0.2291183553395043\n",
      "l2 norm of weights: 5.837999110786805\n",
      "---------------------\n",
      "Iteration Number: 5956\n",
      "Loss: 33.817108527366265\n",
      "l2 norm of gradients: 0.22909238287570297\n",
      "l2 norm of weights: 5.837918735575445\n",
      "---------------------\n",
      "Iteration Number: 5957\n",
      "Loss: 33.81447643767241\n",
      "l2 norm of gradients: 0.2290664186588854\n",
      "l2 norm of weights: 5.837838367554441\n",
      "---------------------\n",
      "Iteration Number: 5958\n",
      "Loss: 33.81184494486569\n",
      "l2 norm of gradients: 0.22904046268386521\n",
      "l2 norm of weights: 5.837758006722394\n",
      "---------------------\n",
      "Iteration Number: 5959\n",
      "Loss: 33.80921404869178\n",
      "l2 norm of gradients: 0.2290145149454609\n",
      "l2 norm of weights: 5.837677653077913\n",
      "---------------------\n",
      "Iteration Number: 5960\n",
      "Loss: 33.80658374888728\n",
      "l2 norm of gradients: 0.22898857543849557\n",
      "l2 norm of weights: 5.8375973066196\n",
      "---------------------\n",
      "Iteration Number: 5961\n",
      "Loss: 33.80395404520396\n",
      "l2 norm of gradients: 0.2289626441577974\n",
      "l2 norm of weights: 5.837516967346062\n",
      "---------------------\n",
      "Iteration Number: 5962\n",
      "Loss: 33.80132493738221\n",
      "l2 norm of gradients: 0.228936721098199\n",
      "l2 norm of weights: 5.837436635255908\n",
      "---------------------\n",
      "Iteration Number: 5963\n",
      "Loss: 33.79869642516867\n",
      "l2 norm of gradients: 0.22891080625453789\n",
      "l2 norm of weights: 5.837356310347742\n",
      "---------------------\n",
      "Iteration Number: 5964\n",
      "Loss: 33.79606850830741\n",
      "l2 norm of gradients: 0.22888489962165642\n",
      "l2 norm of weights: 5.8372759926201745\n",
      "---------------------\n",
      "Iteration Number: 5965\n",
      "Loss: 33.793441186546985\n",
      "l2 norm of gradients: 0.22885900119440153\n",
      "l2 norm of weights: 5.837195682071812\n",
      "---------------------\n",
      "Iteration Number: 5966\n",
      "Loss: 33.790814459627214\n",
      "l2 norm of gradients: 0.22883311096762507\n",
      "l2 norm of weights: 5.837115378701265\n",
      "---------------------\n",
      "Iteration Number: 5967\n",
      "Loss: 33.788188327293206\n",
      "l2 norm of gradients: 0.22880722893618338\n",
      "l2 norm of weights: 5.837035082507142\n",
      "---------------------\n",
      "Iteration Number: 5968\n",
      "Loss: 33.78556278929281\n",
      "l2 norm of gradients: 0.2287813550949378\n",
      "l2 norm of weights: 5.836954793488055\n",
      "---------------------\n",
      "Iteration Number: 5969\n",
      "Loss: 33.78293784536945\n",
      "l2 norm of gradients: 0.22875548943875423\n",
      "l2 norm of weights: 5.8368745116426135\n",
      "---------------------\n",
      "Iteration Number: 5970\n",
      "Loss: 33.78031349527038\n",
      "l2 norm of gradients: 0.22872963196250334\n",
      "l2 norm of weights: 5.836794236969429\n",
      "---------------------\n",
      "Iteration Number: 5971\n",
      "Loss: 33.777689738746986\n",
      "l2 norm of gradients: 0.22870378266106062\n",
      "l2 norm of weights: 5.836713969467113\n",
      "---------------------\n",
      "Iteration Number: 5972\n",
      "Loss: 33.7750665755375\n",
      "l2 norm of gradients: 0.22867794152930607\n",
      "l2 norm of weights: 5.836633709134279\n",
      "---------------------\n",
      "Iteration Number: 5973\n",
      "Loss: 33.772444005389595\n",
      "l2 norm of gradients: 0.2286521085621246\n",
      "l2 norm of weights: 5.83655345596954\n",
      "---------------------\n",
      "Iteration Number: 5974\n",
      "Loss: 33.76982202805246\n",
      "l2 norm of gradients: 0.22862628375440558\n",
      "l2 norm of weights: 5.836473209971508\n",
      "---------------------\n",
      "Iteration Number: 5975\n",
      "Loss: 33.767200643270556\n",
      "l2 norm of gradients: 0.2286004671010434\n",
      "l2 norm of weights: 5.8363929711388\n",
      "---------------------\n",
      "Iteration Number: 5976\n",
      "Loss: 33.76457985079207\n",
      "l2 norm of gradients: 0.22857465859693693\n",
      "l2 norm of weights: 5.836312739470028\n",
      "---------------------\n",
      "Iteration Number: 5977\n",
      "Loss: 33.761959650363586\n",
      "l2 norm of gradients: 0.2285488582369897\n",
      "l2 norm of weights: 5.83623251496381\n",
      "---------------------\n",
      "Iteration Number: 5978\n",
      "Loss: 33.759340041733005\n",
      "l2 norm of gradients: 0.22852306601611003\n",
      "l2 norm of weights: 5.836152297618759\n",
      "---------------------\n",
      "Iteration Number: 5979\n",
      "Loss: 33.75672102464837\n",
      "l2 norm of gradients: 0.22849728192921093\n",
      "l2 norm of weights: 5.836072087433495\n",
      "---------------------\n",
      "Iteration Number: 5980\n",
      "Loss: 33.75410259885109\n",
      "l2 norm of gradients: 0.22847150597121005\n",
      "l2 norm of weights: 5.835991884406631\n",
      "---------------------\n",
      "Iteration Number: 5981\n",
      "Loss: 33.751484764099615\n",
      "l2 norm of gradients: 0.2284457381370296\n",
      "l2 norm of weights: 5.835911688536788\n",
      "---------------------\n",
      "Iteration Number: 5982\n",
      "Loss: 33.74886752013273\n",
      "l2 norm of gradients: 0.22841997842159675\n",
      "l2 norm of weights: 5.835831499822583\n",
      "---------------------\n",
      "Iteration Number: 5983\n",
      "Loss: 33.74625086670266\n",
      "l2 norm of gradients: 0.22839422681984287\n",
      "l2 norm of weights: 5.835751318262634\n",
      "---------------------\n",
      "Iteration Number: 5984\n",
      "Loss: 33.74363480356122\n",
      "l2 norm of gradients: 0.22836848332670448\n",
      "l2 norm of weights: 5.835671143855562\n",
      "---------------------\n",
      "Iteration Number: 5985\n",
      "Loss: 33.74101933044856\n",
      "l2 norm of gradients: 0.22834274793712242\n",
      "l2 norm of weights: 5.8355909765999865\n",
      "---------------------\n",
      "Iteration Number: 5986\n",
      "Loss: 33.73840444711582\n",
      "l2 norm of gradients: 0.2283170206460424\n",
      "l2 norm of weights: 5.835510816494526\n",
      "---------------------\n",
      "Iteration Number: 5987\n",
      "Loss: 33.73579015331757\n",
      "l2 norm of gradients: 0.2282913014484145\n",
      "l2 norm of weights: 5.835430663537804\n",
      "---------------------\n",
      "Iteration Number: 5988\n",
      "Loss: 33.733176448796705\n",
      "l2 norm of gradients: 0.22826559033919375\n",
      "l2 norm of weights: 5.835350517728441\n",
      "---------------------\n",
      "Iteration Number: 5989\n",
      "Loss: 33.73056333330672\n",
      "l2 norm of gradients: 0.2282398873133395\n",
      "l2 norm of weights: 5.835270379065058\n",
      "---------------------\n",
      "Iteration Number: 5990\n",
      "Loss: 33.72795080659258\n",
      "l2 norm of gradients: 0.22821419236581592\n",
      "l2 norm of weights: 5.83519024754628\n",
      "---------------------\n",
      "Iteration Number: 5991\n",
      "Loss: 33.72533886840628\n",
      "l2 norm of gradients: 0.22818850549159186\n",
      "l2 norm of weights: 5.83511012317073\n",
      "---------------------\n",
      "Iteration Number: 5992\n",
      "Loss: 33.722727518493436\n",
      "l2 norm of gradients: 0.22816282668564056\n",
      "l2 norm of weights: 5.83503000593703\n",
      "---------------------\n",
      "Iteration Number: 5993\n",
      "Loss: 33.72011675661193\n",
      "l2 norm of gradients: 0.22813715594294007\n",
      "l2 norm of weights: 5.834949895843806\n",
      "---------------------\n",
      "Iteration Number: 5994\n",
      "Loss: 33.71750658250961\n",
      "l2 norm of gradients: 0.228111493258473\n",
      "l2 norm of weights: 5.834869792889682\n",
      "---------------------\n",
      "Iteration Number: 5995\n",
      "Loss: 33.71489699593065\n",
      "l2 norm of gradients: 0.2280858386272265\n",
      "l2 norm of weights: 5.834789697073284\n",
      "---------------------\n",
      "Iteration Number: 5996\n",
      "Loss: 33.71228799663006\n",
      "l2 norm of gradients: 0.22806019204419237\n",
      "l2 norm of weights: 5.834709608393239\n",
      "---------------------\n",
      "Iteration Number: 5997\n",
      "Loss: 33.709679584357154\n",
      "l2 norm of gradients: 0.22803455350436702\n",
      "l2 norm of weights: 5.834629526848174\n",
      "---------------------\n",
      "Iteration Number: 5998\n",
      "Loss: 33.707071758866626\n",
      "l2 norm of gradients: 0.22800892300275138\n",
      "l2 norm of weights: 5.834549452436713\n",
      "---------------------\n",
      "Iteration Number: 5999\n",
      "Loss: 33.70446451990341\n",
      "l2 norm of gradients: 0.22798330053435106\n",
      "l2 norm of weights: 5.834469385157487\n",
      "---------------------\n",
      "Iteration Number: 6000\n",
      "Loss: 33.70185786721767\n",
      "l2 norm of gradients: 0.22795768609417613\n",
      "l2 norm of weights: 5.8343893250091226\n",
      "---------------------\n",
      "Iteration Number: 6001\n",
      "Loss: 33.6992518005699\n",
      "l2 norm of gradients: 0.2279320796772414\n",
      "l2 norm of weights: 5.83430927199025\n",
      "---------------------\n",
      "Iteration Number: 6002\n",
      "Loss: 33.69664631970417\n",
      "l2 norm of gradients: 0.22790648127856614\n",
      "l2 norm of weights: 5.834229226099497\n",
      "---------------------\n",
      "Iteration Number: 6003\n",
      "Loss: 33.694041424368514\n",
      "l2 norm of gradients: 0.22788089089317415\n",
      "l2 norm of weights: 5.834149187335495\n",
      "---------------------\n",
      "Iteration Number: 6004\n",
      "Loss: 33.691437114327414\n",
      "l2 norm of gradients: 0.22785530851609384\n",
      "l2 norm of weights: 5.8340691556968745\n",
      "---------------------\n",
      "Iteration Number: 6005\n",
      "Loss: 33.688833389320266\n",
      "l2 norm of gradients: 0.2278297341423582\n",
      "l2 norm of weights: 5.833989131182267\n",
      "---------------------\n",
      "Iteration Number: 6006\n",
      "Loss: 33.686230249107666\n",
      "l2 norm of gradients: 0.2278041677670047\n",
      "l2 norm of weights: 5.833909113790301\n",
      "---------------------\n",
      "Iteration Number: 6007\n",
      "Loss: 33.68362769343704\n",
      "l2 norm of gradients: 0.22777860938507552\n",
      "l2 norm of weights: 5.833829103519613\n",
      "---------------------\n",
      "Iteration Number: 6008\n",
      "Loss: 33.68102572206345\n",
      "l2 norm of gradients: 0.22775305899161724\n",
      "l2 norm of weights: 5.833749100368834\n",
      "---------------------\n",
      "Iteration Number: 6009\n",
      "Loss: 33.67842433473771\n",
      "l2 norm of gradients: 0.2277275165816809\n",
      "l2 norm of weights: 5.833669104336598\n",
      "---------------------\n",
      "Iteration Number: 6010\n",
      "Loss: 33.675823531211364\n",
      "l2 norm of gradients: 0.22770198215032225\n",
      "l2 norm of weights: 5.833589115421537\n",
      "---------------------\n",
      "Iteration Number: 6011\n",
      "Loss: 33.67322331124587\n",
      "l2 norm of gradients: 0.22767645569260156\n",
      "l2 norm of weights: 5.833509133622287\n",
      "---------------------\n",
      "Iteration Number: 6012\n",
      "Loss: 33.670623674577776\n",
      "l2 norm of gradients: 0.22765093720358337\n",
      "l2 norm of weights: 5.833429158937483\n",
      "---------------------\n",
      "Iteration Number: 6013\n",
      "Loss: 33.66802462097567\n",
      "l2 norm of gradients: 0.22762542667833716\n",
      "l2 norm of weights: 5.833349191365761\n",
      "---------------------\n",
      "Iteration Number: 6014\n",
      "Loss: 33.66542615018784\n",
      "l2 norm of gradients: 0.22759992411193644\n",
      "l2 norm of weights: 5.833269230905755\n",
      "---------------------\n",
      "Iteration Number: 6015\n",
      "Loss: 33.66282826196904\n",
      "l2 norm of gradients: 0.22757442949945975\n",
      "l2 norm of weights: 5.833189277556105\n",
      "---------------------\n",
      "Iteration Number: 6016\n",
      "Loss: 33.66023095607039\n",
      "l2 norm of gradients: 0.2275489428359897\n",
      "l2 norm of weights: 5.833109331315446\n",
      "---------------------\n",
      "Iteration Number: 6017\n",
      "Loss: 33.65763423225169\n",
      "l2 norm of gradients: 0.22752346411661364\n",
      "l2 norm of weights: 5.833029392182415\n",
      "---------------------\n",
      "Iteration Number: 6018\n",
      "Loss: 33.655038090259225\n",
      "l2 norm of gradients: 0.2274979933364232\n",
      "l2 norm of weights: 5.832949460155653\n",
      "---------------------\n",
      "Iteration Number: 6019\n",
      "Loss: 33.652442529851996\n",
      "l2 norm of gradients: 0.2274725304905148\n",
      "l2 norm of weights: 5.832869535233797\n",
      "---------------------\n",
      "Iteration Number: 6020\n",
      "Loss: 33.649847550780244\n",
      "l2 norm of gradients: 0.22744707557398916\n",
      "l2 norm of weights: 5.832789617415487\n",
      "---------------------\n",
      "Iteration Number: 6021\n",
      "Loss: 33.64725315280858\n",
      "l2 norm of gradients: 0.22742162858195142\n",
      "l2 norm of weights: 5.8327097066993625\n",
      "---------------------\n",
      "Iteration Number: 6022\n",
      "Loss: 33.644659335681176\n",
      "l2 norm of gradients: 0.2273961895095114\n",
      "l2 norm of weights: 5.832629803084064\n",
      "---------------------\n",
      "Iteration Number: 6023\n",
      "Loss: 33.64206609915424\n",
      "l2 norm of gradients: 0.22737075835178325\n",
      "l2 norm of weights: 5.832549906568235\n",
      "---------------------\n",
      "Iteration Number: 6024\n",
      "Loss: 33.63947344299257\n",
      "l2 norm of gradients: 0.22734533510388558\n",
      "l2 norm of weights: 5.832470017150512\n",
      "---------------------\n",
      "Iteration Number: 6025\n",
      "Loss: 33.63688136694192\n",
      "l2 norm of gradients: 0.2273199197609415\n",
      "l2 norm of weights: 5.832390134829542\n",
      "---------------------\n",
      "Iteration Number: 6026\n",
      "Loss: 33.63428987076044\n",
      "l2 norm of gradients: 0.22729451231807862\n",
      "l2 norm of weights: 5.832310259603966\n",
      "---------------------\n",
      "Iteration Number: 6027\n",
      "Loss: 33.63169895420259\n",
      "l2 norm of gradients: 0.22726911277042888\n",
      "l2 norm of weights: 5.832230391472428\n",
      "---------------------\n",
      "Iteration Number: 6028\n",
      "Loss: 33.62910861702953\n",
      "l2 norm of gradients: 0.22724372111312888\n",
      "l2 norm of weights: 5.83215053043357\n",
      "---------------------\n",
      "Iteration Number: 6029\n",
      "Loss: 33.62651885899562\n",
      "l2 norm of gradients: 0.22721833734131944\n",
      "l2 norm of weights: 5.832070676486039\n",
      "---------------------\n",
      "Iteration Number: 6030\n",
      "Loss: 33.62392967984818\n",
      "l2 norm of gradients: 0.2271929614501459\n",
      "l2 norm of weights: 5.831990829628476\n",
      "---------------------\n",
      "Iteration Number: 6031\n",
      "Loss: 33.6213410793551\n",
      "l2 norm of gradients: 0.22716759343475815\n",
      "l2 norm of weights: 5.831910989859532\n",
      "---------------------\n",
      "Iteration Number: 6032\n",
      "Loss: 33.61875305726445\n",
      "l2 norm of gradients: 0.22714223329031033\n",
      "l2 norm of weights: 5.831831157177849\n",
      "---------------------\n",
      "Iteration Number: 6033\n",
      "Loss: 33.61616561334437\n",
      "l2 norm of gradients: 0.22711688101196106\n",
      "l2 norm of weights: 5.8317513315820735\n",
      "---------------------\n",
      "Iteration Number: 6034\n",
      "Loss: 33.6135787473395\n",
      "l2 norm of gradients: 0.22709153659487355\n",
      "l2 norm of weights: 5.831671513070854\n",
      "---------------------\n",
      "Iteration Number: 6035\n",
      "Loss: 33.61099245901006\n",
      "l2 norm of gradients: 0.22706620003421518\n",
      "l2 norm of weights: 5.8315917016428385\n",
      "---------------------\n",
      "Iteration Number: 6036\n",
      "Loss: 33.608406748116735\n",
      "l2 norm of gradients: 0.22704087132515785\n",
      "l2 norm of weights: 5.831511897296673\n",
      "---------------------\n",
      "Iteration Number: 6037\n",
      "Loss: 33.60582161441396\n",
      "l2 norm of gradients: 0.22701555046287786\n",
      "l2 norm of weights: 5.83143210003101\n",
      "---------------------\n",
      "Iteration Number: 6038\n",
      "Loss: 33.60323705766406\n",
      "l2 norm of gradients: 0.22699023744255603\n",
      "l2 norm of weights: 5.831352309844494\n",
      "---------------------\n",
      "Iteration Number: 6039\n",
      "Loss: 33.60065307761456\n",
      "l2 norm of gradients: 0.22696493225937733\n",
      "l2 norm of weights: 5.831272526735778\n",
      "---------------------\n",
      "Iteration Number: 6040\n",
      "Loss: 33.598069674036786\n",
      "l2 norm of gradients: 0.2269396349085313\n",
      "l2 norm of weights: 5.831192750703512\n",
      "---------------------\n",
      "Iteration Number: 6041\n",
      "Loss: 33.595486846675485\n",
      "l2 norm of gradients: 0.2269143453852119\n",
      "l2 norm of weights: 5.831112981746347\n",
      "---------------------\n",
      "Iteration Number: 6042\n",
      "Loss: 33.59290459530133\n",
      "l2 norm of gradients: 0.22688906368461736\n",
      "l2 norm of weights: 5.831033219862934\n",
      "---------------------\n",
      "Iteration Number: 6043\n",
      "Loss: 33.59032291966269\n",
      "l2 norm of gradients: 0.2268637898019504\n",
      "l2 norm of weights: 5.830953465051924\n",
      "---------------------\n",
      "Iteration Number: 6044\n",
      "Loss: 33.5877418195229\n",
      "l2 norm of gradients: 0.22683852373241795\n",
      "l2 norm of weights: 5.83087371731197\n",
      "---------------------\n",
      "Iteration Number: 6045\n",
      "Loss: 33.585161294638766\n",
      "l2 norm of gradients: 0.22681326547123146\n",
      "l2 norm of weights: 5.830793976641728\n",
      "---------------------\n",
      "Iteration Number: 6046\n",
      "Loss: 33.58258134477377\n",
      "l2 norm of gradients: 0.22678801501360682\n",
      "l2 norm of weights: 5.830714243039848\n",
      "---------------------\n",
      "Iteration Number: 6047\n",
      "Loss: 33.580001969682435\n",
      "l2 norm of gradients: 0.22676277235476397\n",
      "l2 norm of weights: 5.830634516504984\n",
      "---------------------\n",
      "Iteration Number: 6048\n",
      "Loss: 33.57742316912165\n",
      "l2 norm of gradients: 0.2267375374899275\n",
      "l2 norm of weights: 5.830554797035794\n",
      "---------------------\n",
      "Iteration Number: 6049\n",
      "Loss: 33.57484494285412\n",
      "l2 norm of gradients: 0.22671231041432632\n",
      "l2 norm of weights: 5.830475084630931\n",
      "---------------------\n",
      "Iteration Number: 6050\n",
      "Loss: 33.57226729064226\n",
      "l2 norm of gradients: 0.2266870911231935\n",
      "l2 norm of weights: 5.83039537928905\n",
      "---------------------\n",
      "Iteration Number: 6051\n",
      "Loss: 33.569690212243955\n",
      "l2 norm of gradients: 0.22666187961176665\n",
      "l2 norm of weights: 5.830315681008808\n",
      "---------------------\n",
      "Iteration Number: 6052\n",
      "Loss: 33.56711370741658\n",
      "l2 norm of gradients: 0.2266366758752877\n",
      "l2 norm of weights: 5.830235989788863\n",
      "---------------------\n",
      "Iteration Number: 6053\n",
      "Loss: 33.564537775920364\n",
      "l2 norm of gradients: 0.22661147990900288\n",
      "l2 norm of weights: 5.830156305627871\n",
      "---------------------\n",
      "Iteration Number: 6054\n",
      "Loss: 33.56196241751698\n",
      "l2 norm of gradients: 0.2265862917081626\n",
      "l2 norm of weights: 5.83007662852449\n",
      "---------------------\n",
      "Iteration Number: 6055\n",
      "Loss: 33.55938763196513\n",
      "l2 norm of gradients: 0.2265611112680219\n",
      "l2 norm of weights: 5.8299969584773805\n",
      "---------------------\n",
      "Iteration Number: 6056\n",
      "Loss: 33.55681341903017\n",
      "l2 norm of gradients: 0.2265359385838398\n",
      "l2 norm of weights: 5.829917295485199\n",
      "---------------------\n",
      "Iteration Number: 6057\n",
      "Loss: 33.554239778469615\n",
      "l2 norm of gradients: 0.22651077365088002\n",
      "l2 norm of weights: 5.829837639546605\n",
      "---------------------\n",
      "Iteration Number: 6058\n",
      "Loss: 33.551666710042\n",
      "l2 norm of gradients: 0.22648561646441023\n",
      "l2 norm of weights: 5.82975799066026\n",
      "---------------------\n",
      "Iteration Number: 6059\n",
      "Loss: 33.54909421350999\n",
      "l2 norm of gradients: 0.22646046701970263\n",
      "l2 norm of weights: 5.829678348824825\n",
      "---------------------\n",
      "Iteration Number: 6060\n",
      "Loss: 33.546522288633994\n",
      "l2 norm of gradients: 0.22643532531203373\n",
      "l2 norm of weights: 5.8295987140389585\n",
      "---------------------\n",
      "Iteration Number: 6061\n",
      "Loss: 33.54395093518208\n",
      "l2 norm of gradients: 0.2264101913366841\n",
      "l2 norm of weights: 5.829519086301325\n",
      "---------------------\n",
      "Iteration Number: 6062\n",
      "Loss: 33.54138015290965\n",
      "l2 norm of gradients: 0.22638506508893894\n",
      "l2 norm of weights: 5.829439465610585\n",
      "---------------------\n",
      "Iteration Number: 6063\n",
      "Loss: 33.53880994157432\n",
      "l2 norm of gradients: 0.2263599465640875\n",
      "l2 norm of weights: 5.829359851965401\n",
      "---------------------\n",
      "Iteration Number: 6064\n",
      "Loss: 33.53624030094525\n",
      "l2 norm of gradients: 0.22633483575742344\n",
      "l2 norm of weights: 5.829280245364437\n",
      "---------------------\n",
      "Iteration Number: 6065\n",
      "Loss: 33.533671230781046\n",
      "l2 norm of gradients: 0.22630973266424462\n",
      "l2 norm of weights: 5.829200645806357\n",
      "---------------------\n",
      "Iteration Number: 6066\n",
      "Loss: 33.531102730848545\n",
      "l2 norm of gradients: 0.22628463727985323\n",
      "l2 norm of weights: 5.829121053289825\n",
      "---------------------\n",
      "Iteration Number: 6067\n",
      "Loss: 33.528534800902285\n",
      "l2 norm of gradients: 0.22625954959955566\n",
      "l2 norm of weights: 5.829041467813505\n",
      "---------------------\n",
      "Iteration Number: 6068\n",
      "Loss: 33.52596744071053\n",
      "l2 norm of gradients: 0.2262344696186627\n",
      "l2 norm of weights: 5.828961889376064\n",
      "---------------------\n",
      "Iteration Number: 6069\n",
      "Loss: 33.52340065003285\n",
      "l2 norm of gradients: 0.22620939733248932\n",
      "l2 norm of weights: 5.828882317976167\n",
      "---------------------\n",
      "Iteration Number: 6070\n",
      "Loss: 33.52083442863251\n",
      "l2 norm of gradients: 0.22618433273635477\n",
      "l2 norm of weights: 5.828802753612481\n",
      "---------------------\n",
      "Iteration Number: 6071\n",
      "Loss: 33.51826877627536\n",
      "l2 norm of gradients: 0.22615927582558254\n",
      "l2 norm of weights: 5.828723196283673\n",
      "---------------------\n",
      "Iteration Number: 6072\n",
      "Loss: 33.51570369271888\n",
      "l2 norm of gradients: 0.2261342265955003\n",
      "l2 norm of weights: 5.828643645988408\n",
      "---------------------\n",
      "Iteration Number: 6073\n",
      "Loss: 33.51313917773833\n",
      "l2 norm of gradients: 0.22610918504144017\n",
      "l2 norm of weights: 5.828564102725357\n",
      "---------------------\n",
      "Iteration Number: 6074\n",
      "Loss: 33.51057523108013\n",
      "l2 norm of gradients: 0.22608415115873828\n",
      "l2 norm of weights: 5.828484566493187\n",
      "---------------------\n",
      "Iteration Number: 6075\n",
      "Loss: 33.50801185252296\n",
      "l2 norm of gradients: 0.22605912494273517\n",
      "l2 norm of weights: 5.828405037290568\n",
      "---------------------\n",
      "Iteration Number: 6076\n",
      "Loss: 33.50544904182124\n",
      "l2 norm of gradients: 0.22603410638877544\n",
      "l2 norm of weights: 5.828325515116169\n",
      "---------------------\n",
      "Iteration Number: 6077\n",
      "Loss: 33.50288679874007\n",
      "l2 norm of gradients: 0.22600909549220818\n",
      "l2 norm of weights: 5.82824599996866\n",
      "---------------------\n",
      "Iteration Number: 6078\n",
      "Loss: 33.500325123045144\n",
      "l2 norm of gradients: 0.22598409224838645\n",
      "l2 norm of weights: 5.828166491846712\n",
      "---------------------\n",
      "Iteration Number: 6079\n",
      "Loss: 33.49776401450645\n",
      "l2 norm of gradients: 0.2259590966526677\n",
      "l2 norm of weights: 5.8280869907489965\n",
      "---------------------\n",
      "Iteration Number: 6080\n",
      "Loss: 33.49520347287903\n",
      "l2 norm of gradients: 0.2259341087004134\n",
      "l2 norm of weights: 5.828007496674186\n",
      "---------------------\n",
      "Iteration Number: 6081\n",
      "Loss: 33.49264349793316\n",
      "l2 norm of gradients: 0.22590912838698946\n",
      "l2 norm of weights: 5.82792800962095\n",
      "---------------------\n",
      "Iteration Number: 6082\n",
      "Loss: 33.49008408943025\n",
      "l2 norm of gradients: 0.2258841557077659\n",
      "l2 norm of weights: 5.827848529587963\n",
      "---------------------\n",
      "Iteration Number: 6083\n",
      "Loss: 33.4875252471359\n",
      "l2 norm of gradients: 0.22585919065811688\n",
      "l2 norm of weights: 5.827769056573898\n",
      "---------------------\n",
      "Iteration Number: 6084\n",
      "Loss: 33.484966970818846\n",
      "l2 norm of gradients: 0.2258342332334209\n",
      "l2 norm of weights: 5.82768959057743\n",
      "---------------------\n",
      "Iteration Number: 6085\n",
      "Loss: 33.48240926024018\n",
      "l2 norm of gradients: 0.22580928342906045\n",
      "l2 norm of weights: 5.827610131597231\n",
      "---------------------\n",
      "Iteration Number: 6086\n",
      "Loss: 33.479852115164555\n",
      "l2 norm of gradients: 0.2257843412404224\n",
      "l2 norm of weights: 5.827530679631978\n",
      "---------------------\n",
      "Iteration Number: 6087\n",
      "Loss: 33.47729553536109\n",
      "l2 norm of gradients: 0.22575940666289773\n",
      "l2 norm of weights: 5.827451234680344\n",
      "---------------------\n",
      "Iteration Number: 6088\n",
      "Loss: 33.474739520598106\n",
      "l2 norm of gradients: 0.2257344796918816\n",
      "l2 norm of weights: 5.827371796741008\n",
      "---------------------\n",
      "Iteration Number: 6089\n",
      "Loss: 33.4721840706319\n",
      "l2 norm of gradients: 0.22570956032277337\n",
      "l2 norm of weights: 5.827292365812643\n",
      "---------------------\n",
      "Iteration Number: 6090\n",
      "Loss: 33.46962918523829\n",
      "l2 norm of gradients: 0.2256846485509765\n",
      "l2 norm of weights: 5.827212941893929\n",
      "---------------------\n",
      "Iteration Number: 6091\n",
      "Loss: 33.4670748641763\n",
      "l2 norm of gradients: 0.2256597443718987\n",
      "l2 norm of weights: 5.827133524983543\n",
      "---------------------\n",
      "Iteration Number: 6092\n",
      "Loss: 33.464521107217436\n",
      "l2 norm of gradients: 0.22563484778095186\n",
      "l2 norm of weights: 5.827054115080161\n",
      "---------------------\n",
      "Iteration Number: 6093\n",
      "Loss: 33.46196791412249\n",
      "l2 norm of gradients: 0.22560995877355194\n",
      "l2 norm of weights: 5.826974712182461\n",
      "---------------------\n",
      "Iteration Number: 6094\n",
      "Loss: 33.45941528466246\n",
      "l2 norm of gradients: 0.2255850773451191\n",
      "l2 norm of weights: 5.826895316289126\n",
      "---------------------\n",
      "Iteration Number: 6095\n",
      "Loss: 33.45686321860649\n",
      "l2 norm of gradients: 0.22556020349107764\n",
      "l2 norm of weights: 5.826815927398832\n",
      "---------------------\n",
      "Iteration Number: 6096\n",
      "Loss: 33.45431171571766\n",
      "l2 norm of gradients: 0.22553533720685606\n",
      "l2 norm of weights: 5.82673654551026\n",
      "---------------------\n",
      "Iteration Number: 6097\n",
      "Loss: 33.45176077575803\n",
      "l2 norm of gradients: 0.22551047848788683\n",
      "l2 norm of weights: 5.8266571706220915\n",
      "---------------------\n",
      "Iteration Number: 6098\n",
      "Loss: 33.44921039850937\n",
      "l2 norm of gradients: 0.22548562732960686\n",
      "l2 norm of weights: 5.8265778027330075\n",
      "---------------------\n",
      "Iteration Number: 6099\n",
      "Loss: 33.44666058372488\n",
      "l2 norm of gradients: 0.2254607837274569\n",
      "l2 norm of weights: 5.826498441841688\n",
      "---------------------\n",
      "Iteration Number: 6100\n",
      "Loss: 33.44411133117687\n",
      "l2 norm of gradients: 0.22543594767688202\n",
      "l2 norm of weights: 5.8264190879468165\n",
      "---------------------\n",
      "Iteration Number: 6101\n",
      "Loss: 33.441562640634174\n",
      "l2 norm of gradients: 0.22541111917333123\n",
      "l2 norm of weights: 5.826339741047075\n",
      "---------------------\n",
      "Iteration Number: 6102\n",
      "Loss: 33.43901451186831\n",
      "l2 norm of gradients: 0.22538629821225792\n",
      "l2 norm of weights: 5.8262604011411465\n",
      "---------------------\n",
      "Iteration Number: 6103\n",
      "Loss: 33.43646694463969\n",
      "l2 norm of gradients: 0.22536148478911935\n",
      "l2 norm of weights: 5.826181068227716\n",
      "---------------------\n",
      "Iteration Number: 6104\n",
      "Loss: 33.4339199387233\n",
      "l2 norm of gradients: 0.22533667889937706\n",
      "l2 norm of weights: 5.826101742305467\n",
      "---------------------\n",
      "Iteration Number: 6105\n",
      "Loss: 33.43137349388068\n",
      "l2 norm of gradients: 0.22531188053849652\n",
      "l2 norm of weights: 5.826022423373084\n",
      "---------------------\n",
      "Iteration Number: 6106\n",
      "Loss: 33.42882760988844\n",
      "l2 norm of gradients: 0.22528708970194747\n",
      "l2 norm of weights: 5.825943111429251\n",
      "---------------------\n",
      "Iteration Number: 6107\n",
      "Loss: 33.42628228650601\n",
      "l2 norm of gradients: 0.22526230638520373\n",
      "l2 norm of weights: 5.825863806472657\n",
      "---------------------\n",
      "Iteration Number: 6108\n",
      "Loss: 33.42373752351168\n",
      "l2 norm of gradients: 0.22523753058374316\n",
      "l2 norm of weights: 5.825784508501984\n",
      "---------------------\n",
      "Iteration Number: 6109\n",
      "Loss: 33.42119332066653\n",
      "l2 norm of gradients: 0.2252127622930476\n",
      "l2 norm of weights: 5.825705217515923\n",
      "---------------------\n",
      "Iteration Number: 6110\n",
      "Loss: 33.41864967774985\n",
      "l2 norm of gradients: 0.2251880015086033\n",
      "l2 norm of weights: 5.825625933513158\n",
      "---------------------\n",
      "Iteration Number: 6111\n",
      "Loss: 33.4161065945161\n",
      "l2 norm of gradients: 0.22516324822590025\n",
      "l2 norm of weights: 5.825546656492379\n",
      "---------------------\n",
      "Iteration Number: 6112\n",
      "Loss: 33.41356407074834\n",
      "l2 norm of gradients: 0.2251385024404327\n",
      "l2 norm of weights: 5.825467386452273\n",
      "---------------------\n",
      "Iteration Number: 6113\n",
      "Loss: 33.41102210620922\n",
      "l2 norm of gradients: 0.22511376414769899\n",
      "l2 norm of weights: 5.825388123391528\n",
      "---------------------\n",
      "Iteration Number: 6114\n",
      "Loss: 33.40848070067013\n",
      "l2 norm of gradients: 0.22508903334320135\n",
      "l2 norm of weights: 5.825308867308836\n",
      "---------------------\n",
      "Iteration Number: 6115\n",
      "Loss: 33.40593985389935\n",
      "l2 norm of gradients: 0.2250643100224463\n",
      "l2 norm of weights: 5.8252296182028855\n",
      "---------------------\n",
      "Iteration Number: 6116\n",
      "Loss: 33.403399565675244\n",
      "l2 norm of gradients: 0.22503959418094432\n",
      "l2 norm of weights: 5.825150376072366\n",
      "---------------------\n",
      "Iteration Number: 6117\n",
      "Loss: 33.40085983575657\n",
      "l2 norm of gradients: 0.22501488581420986\n",
      "l2 norm of weights: 5.82507114091597\n",
      "---------------------\n",
      "Iteration Number: 6118\n",
      "Loss: 33.398320663920536\n",
      "l2 norm of gradients: 0.22499018491776154\n",
      "l2 norm of weights: 5.824991912732387\n",
      "---------------------\n",
      "Iteration Number: 6119\n",
      "Loss: 33.395782049935335\n",
      "l2 norm of gradients: 0.22496549148712208\n",
      "l2 norm of weights: 5.8249126915203115\n",
      "---------------------\n",
      "Iteration Number: 6120\n",
      "Loss: 33.3932439935698\n",
      "l2 norm of gradients: 0.22494080551781806\n",
      "l2 norm of weights: 5.824833477278433\n",
      "---------------------\n",
      "Iteration Number: 6121\n",
      "Loss: 33.39070649460134\n",
      "l2 norm of gradients: 0.22491612700538022\n",
      "l2 norm of weights: 5.824754270005446\n",
      "---------------------\n",
      "Iteration Number: 6122\n",
      "Loss: 33.388169552795596\n",
      "l2 norm of gradients: 0.2248914559453433\n",
      "l2 norm of weights: 5.824675069700044\n",
      "---------------------\n",
      "Iteration Number: 6123\n",
      "Loss: 33.38563316792156\n",
      "l2 norm of gradients: 0.22486679233324613\n",
      "l2 norm of weights: 5.82459587636092\n",
      "---------------------\n",
      "Iteration Number: 6124\n",
      "Loss: 33.383097339758685\n",
      "l2 norm of gradients: 0.22484213616463153\n",
      "l2 norm of weights: 5.824516689986769\n",
      "---------------------\n",
      "Iteration Number: 6125\n",
      "Loss: 33.38056206807161\n",
      "l2 norm of gradients: 0.22481748743504626\n",
      "l2 norm of weights: 5.824437510576287\n",
      "---------------------\n",
      "Iteration Number: 6126\n",
      "Loss: 33.378027352638156\n",
      "l2 norm of gradients: 0.2247928461400412\n",
      "l2 norm of weights: 5.824358338128167\n",
      "---------------------\n",
      "Iteration Number: 6127\n",
      "Loss: 33.37549319322181\n",
      "l2 norm of gradients: 0.22476821227517124\n",
      "l2 norm of weights: 5.824279172641107\n",
      "---------------------\n",
      "Iteration Number: 6128\n",
      "Loss: 33.37295958960017\n",
      "l2 norm of gradients: 0.22474358583599524\n",
      "l2 norm of weights: 5.824200014113803\n",
      "---------------------\n",
      "Iteration Number: 6129\n",
      "Loss: 33.37042654154429\n",
      "l2 norm of gradients: 0.22471896681807613\n",
      "l2 norm of weights: 5.824120862544951\n",
      "---------------------\n",
      "Iteration Number: 6130\n",
      "Loss: 33.36789404882515\n",
      "l2 norm of gradients: 0.2246943552169807\n",
      "l2 norm of weights: 5.82404171793325\n",
      "---------------------\n",
      "Iteration Number: 6131\n",
      "Loss: 33.365362111220264\n",
      "l2 norm of gradients: 0.2246697510282799\n",
      "l2 norm of weights: 5.823962580277397\n",
      "---------------------\n",
      "Iteration Number: 6132\n",
      "Loss: 33.36283072849241\n",
      "l2 norm of gradients: 0.2246451542475486\n",
      "l2 norm of weights: 5.8238834495760905\n",
      "---------------------\n",
      "Iteration Number: 6133\n",
      "Loss: 33.36029990042482\n",
      "l2 norm of gradients: 0.22462056487036566\n",
      "l2 norm of weights: 5.823804325828029\n",
      "---------------------\n",
      "Iteration Number: 6134\n",
      "Loss: 33.35776962678386\n",
      "l2 norm of gradients: 0.22459598289231386\n",
      "l2 norm of weights: 5.823725209031914\n",
      "---------------------\n",
      "Iteration Number: 6135\n",
      "Loss: 33.355239907343226\n",
      "l2 norm of gradients: 0.2245714083089801\n",
      "l2 norm of weights: 5.823646099186444\n",
      "---------------------\n",
      "Iteration Number: 6136\n",
      "Loss: 33.35271074187972\n",
      "l2 norm of gradients: 0.22454684111595521\n",
      "l2 norm of weights: 5.82356699629032\n",
      "---------------------\n",
      "Iteration Number: 6137\n",
      "Loss: 33.350182130158124\n",
      "l2 norm of gradients: 0.22452228130883395\n",
      "l2 norm of weights: 5.823487900342241\n",
      "---------------------\n",
      "Iteration Number: 6138\n",
      "Loss: 33.34765407196514\n",
      "l2 norm of gradients: 0.224497728883215\n",
      "l2 norm of weights: 5.8234088113409115\n",
      "---------------------\n",
      "Iteration Number: 6139\n",
      "Loss: 33.345126567064625\n",
      "l2 norm of gradients: 0.22447318383470113\n",
      "l2 norm of weights: 5.823329729285033\n",
      "---------------------\n",
      "Iteration Number: 6140\n",
      "Loss: 33.34259961523133\n",
      "l2 norm of gradients: 0.22444864615889903\n",
      "l2 norm of weights: 5.823250654173306\n",
      "---------------------\n",
      "Iteration Number: 6141\n",
      "Loss: 33.34007321624279\n",
      "l2 norm of gradients: 0.22442411585141928\n",
      "l2 norm of weights: 5.823171586004435\n",
      "---------------------\n",
      "Iteration Number: 6142\n",
      "Loss: 33.33754736987165\n",
      "l2 norm of gradients: 0.2243995929078765\n",
      "l2 norm of weights: 5.823092524777123\n",
      "---------------------\n",
      "Iteration Number: 6143\n",
      "Loss: 33.33502207589018\n",
      "l2 norm of gradients: 0.22437507732388917\n",
      "l2 norm of weights: 5.823013470490075\n",
      "---------------------\n",
      "Iteration Number: 6144\n",
      "Loss: 33.33249733407438\n",
      "l2 norm of gradients: 0.22435056909507978\n",
      "l2 norm of weights: 5.822934423141994\n",
      "---------------------\n",
      "Iteration Number: 6145\n",
      "Loss: 33.32997314419639\n",
      "l2 norm of gradients: 0.22432606821707476\n",
      "l2 norm of weights: 5.8228553827315865\n",
      "---------------------\n",
      "Iteration Number: 6146\n",
      "Loss: 33.327449506036785\n",
      "l2 norm of gradients: 0.22430157468550443\n",
      "l2 norm of weights: 5.822776349257557\n",
      "---------------------\n",
      "Iteration Number: 6147\n",
      "Loss: 33.32492641936282\n",
      "l2 norm of gradients: 0.22427708849600309\n",
      "l2 norm of weights: 5.8226973227186125\n",
      "---------------------\n",
      "Iteration Number: 6148\n",
      "Loss: 33.32240388395546\n",
      "l2 norm of gradients: 0.22425260964420893\n",
      "l2 norm of weights: 5.822618303113458\n",
      "---------------------\n",
      "Iteration Number: 6149\n",
      "Loss: 33.31988189958956\n",
      "l2 norm of gradients: 0.22422813812576406\n",
      "l2 norm of weights: 5.822539290440802\n",
      "---------------------\n",
      "Iteration Number: 6150\n",
      "Loss: 33.31736046603552\n",
      "l2 norm of gradients: 0.22420367393631452\n",
      "l2 norm of weights: 5.822460284699352\n",
      "---------------------\n",
      "Iteration Number: 6151\n",
      "Loss: 33.31483958307428\n",
      "l2 norm of gradients: 0.22417921707151028\n",
      "l2 norm of weights: 5.822381285887814\n",
      "---------------------\n",
      "Iteration Number: 6152\n",
      "Loss: 33.31231925047548\n",
      "l2 norm of gradients: 0.22415476752700528\n",
      "l2 norm of weights: 5.822302294004899\n",
      "---------------------\n",
      "Iteration Number: 6153\n",
      "Loss: 33.30979946802225\n",
      "l2 norm of gradients: 0.22413032529845717\n",
      "l2 norm of weights: 5.822223309049314\n",
      "---------------------\n",
      "Iteration Number: 6154\n",
      "Loss: 33.30728023548405\n",
      "l2 norm of gradients: 0.22410589038152767\n",
      "l2 norm of weights: 5.8221443310197705\n",
      "---------------------\n",
      "Iteration Number: 6155\n",
      "Loss: 33.304761552639\n",
      "l2 norm of gradients: 0.22408146277188243\n",
      "l2 norm of weights: 5.822065359914976\n",
      "---------------------\n",
      "Iteration Number: 6156\n",
      "Loss: 33.302243419264165\n",
      "l2 norm of gradients: 0.2240570424651908\n",
      "l2 norm of weights: 5.821986395733644\n",
      "---------------------\n",
      "Iteration Number: 6157\n",
      "Loss: 33.29972583513178\n",
      "l2 norm of gradients: 0.22403262945712624\n",
      "l2 norm of weights: 5.821907438474483\n",
      "---------------------\n",
      "Iteration Number: 6158\n",
      "Loss: 33.29720880002673\n",
      "l2 norm of gradients: 0.22400822374336601\n",
      "l2 norm of weights: 5.821828488136206\n",
      "---------------------\n",
      "Iteration Number: 6159\n",
      "Loss: 33.29469231372228\n",
      "l2 norm of gradients: 0.22398382531959116\n",
      "l2 norm of weights: 5.821749544717524\n",
      "---------------------\n",
      "Iteration Number: 6160\n",
      "Loss: 33.29217637599016\n",
      "l2 norm of gradients: 0.2239594341814867\n",
      "l2 norm of weights: 5.82167060821715\n",
      "---------------------\n",
      "Iteration Number: 6161\n",
      "Loss: 33.28966098661296\n",
      "l2 norm of gradients: 0.22393505032474154\n",
      "l2 norm of weights: 5.821591678633798\n",
      "---------------------\n",
      "Iteration Number: 6162\n",
      "Loss: 33.28714614536484\n",
      "l2 norm of gradients: 0.22391067374504842\n",
      "l2 norm of weights: 5.821512755966179\n",
      "---------------------\n",
      "Iteration Number: 6163\n",
      "Loss: 33.28463185202263\n",
      "l2 norm of gradients: 0.22388630443810398\n",
      "l2 norm of weights: 5.821433840213008\n",
      "---------------------\n",
      "Iteration Number: 6164\n",
      "Loss: 33.2821181063645\n",
      "l2 norm of gradients: 0.2238619423996087\n",
      "l2 norm of weights: 5.821354931373\n",
      "---------------------\n",
      "Iteration Number: 6165\n",
      "Loss: 33.27960490817146\n",
      "l2 norm of gradients: 0.22383758762526684\n",
      "l2 norm of weights: 5.821276029444871\n",
      "---------------------\n",
      "Iteration Number: 6166\n",
      "Loss: 33.27709225721861\n",
      "l2 norm of gradients: 0.22381324011078668\n",
      "l2 norm of weights: 5.821197134427334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Iteration Number: 6167\n",
      "Loss: 33.274580153276275\n",
      "l2 norm of gradients: 0.22378889985188022\n",
      "l2 norm of weights: 5.821118246319107\n",
      "---------------------\n",
      "Iteration Number: 6168\n",
      "Loss: 33.272068596136606\n",
      "l2 norm of gradients: 0.22376456684426332\n",
      "l2 norm of weights: 5.821039365118904\n",
      "---------------------\n",
      "Iteration Number: 6169\n",
      "Loss: 33.269557585568336\n",
      "l2 norm of gradients: 0.2237402410836557\n",
      "l2 norm of weights: 5.820960490825444\n",
      "---------------------\n",
      "Iteration Number: 6170\n",
      "Loss: 33.267047121354466\n",
      "l2 norm of gradients: 0.22371592256578102\n",
      "l2 norm of weights: 5.820881623437445\n",
      "---------------------\n",
      "Iteration Number: 6171\n",
      "Loss: 33.26453720326547\n",
      "l2 norm of gradients: 0.22369161128636655\n",
      "l2 norm of weights: 5.820802762953622\n",
      "---------------------\n",
      "Iteration Number: 6172\n",
      "Loss: 33.26202783108881\n",
      "l2 norm of gradients: 0.2236673072411436\n",
      "l2 norm of weights: 5.820723909372696\n",
      "---------------------\n",
      "Iteration Number: 6173\n",
      "Loss: 33.25951900459875\n",
      "l2 norm of gradients: 0.22364301042584714\n",
      "l2 norm of weights: 5.8206450626933846\n",
      "---------------------\n",
      "Iteration Number: 6174\n",
      "Loss: 33.257010723574375\n",
      "l2 norm of gradients: 0.22361872083621617\n",
      "l2 norm of weights: 5.820566222914408\n",
      "---------------------\n",
      "Iteration Number: 6175\n",
      "Loss: 33.25450298779788\n",
      "l2 norm of gradients: 0.22359443846799323\n",
      "l2 norm of weights: 5.820487390034486\n",
      "---------------------\n",
      "Iteration Number: 6176\n",
      "Loss: 33.25199579703997\n",
      "l2 norm of gradients: 0.22357016331692486\n",
      "l2 norm of weights: 5.820408564052337\n",
      "---------------------\n",
      "Iteration Number: 6177\n",
      "Loss: 33.24948915109366\n",
      "l2 norm of gradients: 0.22354589537876146\n",
      "l2 norm of weights: 5.820329744966685\n",
      "---------------------\n",
      "Iteration Number: 6178\n",
      "Loss: 33.24698304972395\n",
      "l2 norm of gradients: 0.22352163464925706\n",
      "l2 norm of weights: 5.820250932776249\n",
      "---------------------\n",
      "Iteration Number: 6179\n",
      "Loss: 33.244477492720144\n",
      "l2 norm of gradients: 0.22349738112416961\n",
      "l2 norm of weights: 5.8201721274797515\n",
      "---------------------\n",
      "Iteration Number: 6180\n",
      "Loss: 33.24197247986126\n",
      "l2 norm of gradients: 0.22347313479926084\n",
      "l2 norm of weights: 5.820093329075915\n",
      "---------------------\n",
      "Iteration Number: 6181\n",
      "Loss: 33.239468010921186\n",
      "l2 norm of gradients: 0.2234488956702962\n",
      "l2 norm of weights: 5.820014537563462\n",
      "---------------------\n",
      "Iteration Number: 6182\n",
      "Loss: 33.2369640856815\n",
      "l2 norm of gradients: 0.22342466373304498\n",
      "l2 norm of weights: 5.819935752941116\n",
      "---------------------\n",
      "Iteration Number: 6183\n",
      "Loss: 33.23446070392711\n",
      "l2 norm of gradients: 0.22340043898328038\n",
      "l2 norm of weights: 5.8198569752076\n",
      "---------------------\n",
      "Iteration Number: 6184\n",
      "Loss: 33.2319578654337\n",
      "l2 norm of gradients: 0.22337622141677912\n",
      "l2 norm of weights: 5.81977820436164\n",
      "---------------------\n",
      "Iteration Number: 6185\n",
      "Loss: 33.22945556998677\n",
      "l2 norm of gradients: 0.2233520110293219\n",
      "l2 norm of weights: 5.819699440401959\n",
      "---------------------\n",
      "Iteration Number: 6186\n",
      "Loss: 33.22695381736121\n",
      "l2 norm of gradients: 0.22332780781669317\n",
      "l2 norm of weights: 5.819620683327283\n",
      "---------------------\n",
      "Iteration Number: 6187\n",
      "Loss: 33.22445260734392\n",
      "l2 norm of gradients: 0.22330361177468108\n",
      "l2 norm of weights: 5.819541933136338\n",
      "---------------------\n",
      "Iteration Number: 6188\n",
      "Loss: 33.22195193970727\n",
      "l2 norm of gradients: 0.22327942289907754\n",
      "l2 norm of weights: 5.819463189827848\n",
      "---------------------\n",
      "Iteration Number: 6189\n",
      "Loss: 33.219451814235775\n",
      "l2 norm of gradients: 0.22325524118567827\n",
      "l2 norm of weights: 5.819384453400544\n",
      "---------------------\n",
      "Iteration Number: 6190\n",
      "Loss: 33.21695223071852\n",
      "l2 norm of gradients: 0.22323106663028283\n",
      "l2 norm of weights: 5.819305723853148\n",
      "---------------------\n",
      "Iteration Number: 6191\n",
      "Loss: 33.214453188924\n",
      "l2 norm of gradients: 0.22320689922869436\n",
      "l2 norm of weights: 5.819227001184391\n",
      "---------------------\n",
      "Iteration Number: 6192\n",
      "Loss: 33.21195468864125\n",
      "l2 norm of gradients: 0.22318273897671984\n",
      "l2 norm of weights: 5.819148285393002\n",
      "---------------------\n",
      "Iteration Number: 6193\n",
      "Loss: 33.209456729652985\n",
      "l2 norm of gradients: 0.22315858587016987\n",
      "l2 norm of weights: 5.819069576477706\n",
      "---------------------\n",
      "Iteration Number: 6194\n",
      "Loss: 33.206959311737144\n",
      "l2 norm of gradients: 0.22313443990485912\n",
      "l2 norm of weights: 5.818990874437235\n",
      "---------------------\n",
      "Iteration Number: 6195\n",
      "Loss: 33.20446243467675\n",
      "l2 norm of gradients: 0.22311030107660573\n",
      "l2 norm of weights: 5.818912179270317\n",
      "---------------------\n",
      "Iteration Number: 6196\n",
      "Loss: 33.20196609825445\n",
      "l2 norm of gradients: 0.22308616938123152\n",
      "l2 norm of weights: 5.818833490975683\n",
      "---------------------\n",
      "Iteration Number: 6197\n",
      "Loss: 33.199470302249246\n",
      "l2 norm of gradients: 0.22306204481456227\n",
      "l2 norm of weights: 5.818754809552063\n",
      "---------------------\n",
      "Iteration Number: 6198\n",
      "Loss: 33.19697504645048\n",
      "l2 norm of gradients: 0.2230379273724272\n",
      "l2 norm of weights: 5.818676134998188\n",
      "---------------------\n",
      "Iteration Number: 6199\n",
      "Loss: 33.194480330634704\n",
      "l2 norm of gradients: 0.22301381705065965\n",
      "l2 norm of weights: 5.81859746731279\n",
      "---------------------\n",
      "Iteration Number: 6200\n",
      "Loss: 33.19198615458707\n",
      "l2 norm of gradients: 0.22298971384509625\n",
      "l2 norm of weights: 5.818518806494601\n",
      "---------------------\n",
      "Iteration Number: 6201\n",
      "Loss: 33.18949251808879\n",
      "l2 norm of gradients: 0.22296561775157756\n",
      "l2 norm of weights: 5.818440152542352\n",
      "---------------------\n",
      "Iteration Number: 6202\n",
      "Loss: 33.18699942092256\n",
      "l2 norm of gradients: 0.22294152876594792\n",
      "l2 norm of weights: 5.818361505454777\n",
      "---------------------\n",
      "Iteration Number: 6203\n",
      "Loss: 33.18450686287207\n",
      "l2 norm of gradients: 0.22291744688405524\n",
      "l2 norm of weights: 5.818282865230611\n",
      "---------------------\n",
      "Iteration Number: 6204\n",
      "Loss: 33.182014843720324\n",
      "l2 norm of gradients: 0.22289337210175117\n",
      "l2 norm of weights: 5.8182042318685845\n",
      "---------------------\n",
      "Iteration Number: 6205\n",
      "Loss: 33.17952336325454\n",
      "l2 norm of gradients: 0.22286930441489103\n",
      "l2 norm of weights: 5.818125605367435\n",
      "---------------------\n",
      "Iteration Number: 6206\n",
      "Loss: 33.17703242124712\n",
      "l2 norm of gradients: 0.22284524381933396\n",
      "l2 norm of weights: 5.818046985725895\n",
      "---------------------\n",
      "Iteration Number: 6207\n",
      "Loss: 33.17454201749767\n",
      "l2 norm of gradients: 0.2228211903109426\n",
      "l2 norm of weights: 5.817968372942702\n",
      "---------------------\n",
      "Iteration Number: 6208\n",
      "Loss: 33.17205215177835\n",
      "l2 norm of gradients: 0.22279714388558342\n",
      "l2 norm of weights: 5.81788976701659\n",
      "---------------------\n",
      "Iteration Number: 6209\n",
      "Loss: 33.169562823873434\n",
      "l2 norm of gradients: 0.22277310453912652\n",
      "l2 norm of weights: 5.817811167946296\n",
      "---------------------\n",
      "Iteration Number: 6210\n",
      "Loss: 33.16707403357136\n",
      "l2 norm of gradients: 0.22274907226744575\n",
      "l2 norm of weights: 5.817732575730557\n",
      "---------------------\n",
      "Iteration Number: 6211\n",
      "Loss: 33.16458578065697\n",
      "l2 norm of gradients: 0.22272504706641852\n",
      "l2 norm of weights: 5.817653990368109\n",
      "---------------------\n",
      "Iteration Number: 6212\n",
      "Loss: 33.16209806491095\n",
      "l2 norm of gradients: 0.2227010289319259\n",
      "l2 norm of weights: 5.817575411857693\n",
      "---------------------\n",
      "Iteration Number: 6213\n",
      "Loss: 33.159610886116184\n",
      "l2 norm of gradients: 0.2226770178598528\n",
      "l2 norm of weights: 5.817496840198043\n",
      "---------------------\n",
      "Iteration Number: 6214\n",
      "Loss: 33.15712424406187\n",
      "l2 norm of gradients: 0.22265301384608763\n",
      "l2 norm of weights: 5.8174182753879\n",
      "---------------------\n",
      "Iteration Number: 6215\n",
      "Loss: 33.15463813853219\n",
      "l2 norm of gradients: 0.2226290168865225\n",
      "l2 norm of weights: 5.817339717426003\n",
      "---------------------\n",
      "Iteration Number: 6216\n",
      "Loss: 33.15215256930853\n",
      "l2 norm of gradients: 0.22260502697705328\n",
      "l2 norm of weights: 5.817261166311091\n",
      "---------------------\n",
      "Iteration Number: 6217\n",
      "Loss: 33.14966753617788\n",
      "l2 norm of gradients: 0.22258104411357932\n",
      "l2 norm of weights: 5.817182622041904\n",
      "---------------------\n",
      "Iteration Number: 6218\n",
      "Loss: 33.14718303892969\n",
      "l2 norm of gradients: 0.2225570682920037\n",
      "l2 norm of weights: 5.817104084617184\n",
      "---------------------\n",
      "Iteration Number: 6219\n",
      "Loss: 33.144699077342246\n",
      "l2 norm of gradients: 0.22253309950823313\n",
      "l2 norm of weights: 5.81702555403567\n",
      "---------------------\n",
      "Iteration Number: 6220\n",
      "Loss: 33.14221565120234\n",
      "l2 norm of gradients: 0.22250913775817804\n",
      "l2 norm of weights: 5.816947030296105\n",
      "---------------------\n",
      "Iteration Number: 6221\n",
      "Loss: 33.13973276030224\n",
      "l2 norm of gradients: 0.2224851830377524\n",
      "l2 norm of weights: 5.81686851339723\n",
      "---------------------\n",
      "Iteration Number: 6222\n",
      "Loss: 33.13725040441867\n",
      "l2 norm of gradients: 0.2224612353428738\n",
      "l2 norm of weights: 5.816790003337789\n",
      "---------------------\n",
      "Iteration Number: 6223\n",
      "Loss: 33.1347685833395\n",
      "l2 norm of gradients: 0.22243729466946358\n",
      "l2 norm of weights: 5.816711500116523\n",
      "---------------------\n",
      "Iteration Number: 6224\n",
      "Loss: 33.13228729685889\n",
      "l2 norm of gradients: 0.22241336101344655\n",
      "l2 norm of weights: 5.816633003732177\n",
      "---------------------\n",
      "Iteration Number: 6225\n",
      "Loss: 33.129806544753876\n",
      "l2 norm of gradients: 0.22238943437075126\n",
      "l2 norm of weights: 5.816554514183493\n",
      "---------------------\n",
      "Iteration Number: 6226\n",
      "Loss: 33.1273263268153\n",
      "l2 norm of gradients: 0.22236551473730987\n",
      "l2 norm of weights: 5.816476031469218\n",
      "---------------------\n",
      "Iteration Number: 6227\n",
      "Loss: 33.12484664282477\n",
      "l2 norm of gradients: 0.222341602109058\n",
      "l2 norm of weights: 5.816397555588094\n",
      "---------------------\n",
      "Iteration Number: 6228\n",
      "Loss: 33.12236749257351\n",
      "l2 norm of gradients: 0.2223176964819351\n",
      "l2 norm of weights: 5.816319086538868\n",
      "---------------------\n",
      "Iteration Number: 6229\n",
      "Loss: 33.11988887584304\n",
      "l2 norm of gradients: 0.22229379785188416\n",
      "l2 norm of weights: 5.816240624320286\n",
      "---------------------\n",
      "Iteration Number: 6230\n",
      "Loss: 33.117410792433105\n",
      "l2 norm of gradients: 0.22226990621485163\n",
      "l2 norm of weights: 5.8161621689310925\n",
      "---------------------\n",
      "Iteration Number: 6231\n",
      "Loss: 33.114933242115455\n",
      "l2 norm of gradients: 0.2222460215667877\n",
      "l2 norm of weights: 5.816083720370036\n",
      "---------------------\n",
      "Iteration Number: 6232\n",
      "Loss: 33.112456224680734\n",
      "l2 norm of gradients: 0.22222214390364606\n",
      "l2 norm of weights: 5.816005278635863\n",
      "---------------------\n",
      "Iteration Number: 6233\n",
      "Loss: 33.10997973992045\n",
      "l2 norm of gradients: 0.22219827322138416\n",
      "l2 norm of weights: 5.815926843727321\n",
      "---------------------\n",
      "Iteration Number: 6234\n",
      "Loss: 33.10750378762311\n",
      "l2 norm of gradients: 0.22217440951596287\n",
      "l2 norm of weights: 5.815848415643159\n",
      "---------------------\n",
      "Iteration Number: 6235\n",
      "Loss: 33.105028367568316\n",
      "l2 norm of gradients: 0.2221505527833467\n",
      "l2 norm of weights: 5.815769994382124\n",
      "---------------------\n",
      "Iteration Number: 6236\n",
      "Loss: 33.10255347955201\n",
      "l2 norm of gradients: 0.22212670301950369\n",
      "l2 norm of weights: 5.815691579942966\n",
      "---------------------\n",
      "Iteration Number: 6237\n",
      "Loss: 33.10007912335757\n",
      "l2 norm of gradients: 0.22210286022040557\n",
      "l2 norm of weights: 5.815613172324435\n",
      "---------------------\n",
      "Iteration Number: 6238\n",
      "Loss: 33.09760529877681\n",
      "l2 norm of gradients: 0.2220790243820276\n",
      "l2 norm of weights: 5.81553477152528\n",
      "---------------------\n",
      "Iteration Number: 6239\n",
      "Loss: 33.09513200559326\n",
      "l2 norm of gradients: 0.2220551955003484\n",
      "l2 norm of weights: 5.81545637754425\n",
      "---------------------\n",
      "Iteration Number: 6240\n",
      "Loss: 33.092659243597936\n",
      "l2 norm of gradients: 0.2220313735713506\n",
      "l2 norm of weights: 5.8153779903800995\n",
      "---------------------\n",
      "Iteration Number: 6241\n",
      "Loss: 33.09018701257789\n",
      "l2 norm of gradients: 0.22200755859101992\n",
      "l2 norm of weights: 5.815299610031578\n",
      "---------------------\n",
      "Iteration Number: 6242\n",
      "Loss: 33.08771531231681\n",
      "l2 norm of gradients: 0.22198375055534594\n",
      "l2 norm of weights: 5.815221236497437\n",
      "---------------------\n",
      "Iteration Number: 6243\n",
      "Loss: 33.08524414261497\n",
      "l2 norm of gradients: 0.2219599494603216\n",
      "l2 norm of weights: 5.815142869776429\n",
      "---------------------\n",
      "Iteration Number: 6244\n",
      "Loss: 33.08277350325364\n",
      "l2 norm of gradients: 0.2219361553019436\n",
      "l2 norm of weights: 5.815064509867309\n",
      "---------------------\n",
      "Iteration Number: 6245\n",
      "Loss: 33.08030339402296\n",
      "l2 norm of gradients: 0.22191236807621195\n",
      "l2 norm of weights: 5.814986156768826\n",
      "---------------------\n",
      "Iteration Number: 6246\n",
      "Loss: 33.07783381470903\n",
      "l2 norm of gradients: 0.22188858777913043\n",
      "l2 norm of weights: 5.814907810479736\n",
      "---------------------\n",
      "Iteration Number: 6247\n",
      "Loss: 33.07536476510638\n",
      "l2 norm of gradients: 0.2218648144067061\n",
      "l2 norm of weights: 5.814829470998794\n",
      "---------------------\n",
      "Iteration Number: 6248\n",
      "Loss: 33.07289624500153\n",
      "l2 norm of gradients: 0.22184104795494985\n",
      "l2 norm of weights: 5.814751138324754\n",
      "---------------------\n",
      "Iteration Number: 6249\n",
      "Loss: 33.07042825418039\n",
      "l2 norm of gradients: 0.22181728841987589\n",
      "l2 norm of weights: 5.8146728124563705\n",
      "---------------------\n",
      "Iteration Number: 6250\n",
      "Loss: 33.067960792438626\n",
      "l2 norm of gradients: 0.22179353579750197\n",
      "l2 norm of weights: 5.814594493392399\n",
      "---------------------\n",
      "Iteration Number: 6251\n",
      "Loss: 33.06549385956477\n",
      "l2 norm of gradients: 0.22176979008384948\n",
      "l2 norm of weights: 5.814516181131597\n",
      "---------------------\n",
      "Iteration Number: 6252\n",
      "Loss: 33.06302745534912\n",
      "l2 norm of gradients: 0.22174605127494323\n",
      "l2 norm of weights: 5.814437875672719\n",
      "---------------------\n",
      "Iteration Number: 6253\n",
      "Loss: 33.060561579574845\n",
      "l2 norm of gradients: 0.2217223193668115\n",
      "l2 norm of weights: 5.814359577014523\n",
      "---------------------\n",
      "Iteration Number: 6254\n",
      "Loss: 33.05809623203894\n",
      "l2 norm of gradients: 0.22169859435548628\n",
      "l2 norm of weights: 5.814281285155767\n",
      "---------------------\n",
      "Iteration Number: 6255\n",
      "Loss: 33.05563141253425\n",
      "l2 norm of gradients: 0.2216748762370028\n",
      "l2 norm of weights: 5.814203000095207\n",
      "---------------------\n",
      "Iteration Number: 6256\n",
      "Loss: 33.05316712084306\n",
      "l2 norm of gradients: 0.2216511650074001\n",
      "l2 norm of weights: 5.814124721831604\n",
      "---------------------\n",
      "Iteration Number: 6257\n",
      "Loss: 33.050703356759335\n",
      "l2 norm of gradients: 0.22162746066272043\n",
      "l2 norm of weights: 5.814046450363715\n",
      "---------------------\n",
      "Iteration Number: 6258\n",
      "Loss: 33.04824012007761\n",
      "l2 norm of gradients: 0.22160376319900962\n",
      "l2 norm of weights: 5.813968185690299\n",
      "---------------------\n",
      "Iteration Number: 6259\n",
      "Loss: 33.04577741058166\n",
      "l2 norm of gradients: 0.22158007261231716\n",
      "l2 norm of weights: 5.813889927810116\n",
      "---------------------\n",
      "Iteration Number: 6260\n",
      "Loss: 33.04331522807064\n",
      "l2 norm of gradients: 0.2215563888986958\n",
      "l2 norm of weights: 5.813811676721928\n",
      "---------------------\n",
      "Iteration Number: 6261\n",
      "Loss: 33.04085357232492\n",
      "l2 norm of gradients: 0.22153271205420194\n",
      "l2 norm of weights: 5.813733432424494\n",
      "---------------------\n",
      "Iteration Number: 6262\n",
      "Loss: 33.03839244314807\n",
      "l2 norm of gradients: 0.2215090420748954\n",
      "l2 norm of weights: 5.813655194916573\n",
      "---------------------\n",
      "Iteration Number: 6263\n",
      "Loss: 33.03593184032276\n",
      "l2 norm of gradients: 0.2214853789568395\n",
      "l2 norm of weights: 5.813576964196931\n",
      "---------------------\n",
      "Iteration Number: 6264\n",
      "Loss: 33.03347176364064\n",
      "l2 norm of gradients: 0.22146172269610087\n",
      "l2 norm of weights: 5.813498740264327\n",
      "---------------------\n",
      "Iteration Number: 6265\n",
      "Loss: 33.031012212899775\n",
      "l2 norm of gradients: 0.22143807328874984\n",
      "l2 norm of weights: 5.813420523117525\n",
      "---------------------\n",
      "Iteration Number: 6266\n",
      "Loss: 33.02855318788569\n",
      "l2 norm of gradients: 0.2214144307308602\n",
      "l2 norm of weights: 5.813342312755286\n",
      "---------------------\n",
      "Iteration Number: 6267\n",
      "Loss: 33.02609468839518\n",
      "l2 norm of gradients: 0.22139079501850897\n",
      "l2 norm of weights: 5.813264109176376\n",
      "---------------------\n",
      "Iteration Number: 6268\n",
      "Loss: 33.02363671421571\n",
      "l2 norm of gradients: 0.22136716614777682\n",
      "l2 norm of weights: 5.813185912379556\n",
      "---------------------\n",
      "Iteration Number: 6269\n",
      "Loss: 33.02117926514032\n",
      "l2 norm of gradients: 0.2213435441147479\n",
      "l2 norm of weights: 5.8131077223635925\n",
      "---------------------\n",
      "Iteration Number: 6270\n",
      "Loss: 33.01872234096165\n",
      "l2 norm of gradients: 0.22131992891550967\n",
      "l2 norm of weights: 5.813029539127251\n",
      "---------------------\n",
      "Iteration Number: 6271\n",
      "Loss: 33.01626594147461\n",
      "l2 norm of gradients: 0.2212963205461532\n",
      "l2 norm of weights: 5.812951362669294\n",
      "---------------------\n",
      "Iteration Number: 6272\n",
      "Loss: 33.013810066469425\n",
      "l2 norm of gradients: 0.2212727190027728\n",
      "l2 norm of weights: 5.812873192988489\n",
      "---------------------\n",
      "Iteration Number: 6273\n",
      "Loss: 33.01135471573852\n",
      "l2 norm of gradients: 0.22124912428146645\n",
      "l2 norm of weights: 5.8127950300836\n",
      "---------------------\n",
      "Iteration Number: 6274\n",
      "Loss: 33.008899889076496\n",
      "l2 norm of gradients: 0.2212255363783354\n",
      "l2 norm of weights: 5.812716873953398\n",
      "---------------------\n",
      "Iteration Number: 6275\n",
      "Loss: 33.00644558627198\n",
      "l2 norm of gradients: 0.22120195528948441\n",
      "l2 norm of weights: 5.812638724596645\n",
      "---------------------\n",
      "Iteration Number: 6276\n",
      "Loss: 33.003991807125416\n",
      "l2 norm of gradients: 0.22117838101102155\n",
      "l2 norm of weights: 5.812560582012111\n",
      "---------------------\n",
      "Iteration Number: 6277\n",
      "Loss: 33.00153855142279\n",
      "l2 norm of gradients: 0.2211548135390586\n",
      "l2 norm of weights: 5.812482446198565\n",
      "---------------------\n",
      "Iteration Number: 6278\n",
      "Loss: 32.99908581896517\n",
      "l2 norm of gradients: 0.2211312528697105\n",
      "l2 norm of weights: 5.8124043171547735\n",
      "---------------------\n",
      "Iteration Number: 6279\n",
      "Loss: 32.99663360953851\n",
      "l2 norm of gradients: 0.22110769899909563\n",
      "l2 norm of weights: 5.812326194879505\n",
      "---------------------\n",
      "Iteration Number: 6280\n",
      "Loss: 32.9941819229402\n",
      "l2 norm of gradients: 0.2210841519233359\n",
      "l2 norm of weights: 5.812248079371531\n",
      "---------------------\n",
      "Iteration Number: 6281\n",
      "Loss: 32.99173075895962\n",
      "l2 norm of gradients: 0.2210606116385565\n",
      "l2 norm of weights: 5.81216997062962\n",
      "---------------------\n",
      "Iteration Number: 6282\n",
      "Loss: 32.989280117401776\n",
      "l2 norm of gradients: 0.22103707814088625\n",
      "l2 norm of weights: 5.8120918686525425\n",
      "---------------------\n",
      "Iteration Number: 6283\n",
      "Loss: 32.98682999804502\n",
      "l2 norm of gradients: 0.22101355142645707\n",
      "l2 norm of weights: 5.812013773439068\n",
      "---------------------\n",
      "Iteration Number: 6284\n",
      "Loss: 32.98438040069559\n",
      "l2 norm of gradients: 0.2209900314914045\n",
      "l2 norm of weights: 5.81193568498797\n",
      "---------------------\n",
      "Iteration Number: 6285\n",
      "Loss: 32.98193132514554\n",
      "l2 norm of gradients: 0.22096651833186748\n",
      "l2 norm of weights: 5.811857603298017\n",
      "---------------------\n",
      "Iteration Number: 6286\n",
      "Loss: 32.97948277118464\n",
      "l2 norm of gradients: 0.22094301194398816\n",
      "l2 norm of weights: 5.8117795283679845\n",
      "---------------------\n",
      "Iteration Number: 6287\n",
      "Loss: 32.97703473861141\n",
      "l2 norm of gradients: 0.2209195123239123\n",
      "l2 norm of weights: 5.811701460196643\n",
      "---------------------\n",
      "Iteration Number: 6288\n",
      "Loss: 32.974587227222514\n",
      "l2 norm of gradients: 0.22089601946778883\n",
      "l2 norm of weights: 5.811623398782765\n",
      "---------------------\n",
      "Iteration Number: 6289\n",
      "Loss: 32.97214023680401\n",
      "l2 norm of gradients: 0.2208725333717703\n",
      "l2 norm of weights: 5.811545344125125\n",
      "---------------------\n",
      "Iteration Number: 6290\n",
      "Loss: 32.96969376716234\n",
      "l2 norm of gradients: 0.2208490540320124\n",
      "l2 norm of weights: 5.811467296222497\n",
      "---------------------\n",
      "Iteration Number: 6291\n",
      "Loss: 32.96724781808478\n",
      "l2 norm of gradients: 0.2208255814446744\n",
      "l2 norm of weights: 5.811389255073652\n",
      "---------------------\n",
      "Iteration Number: 6292\n",
      "Loss: 32.964802389368636\n",
      "l2 norm of gradients: 0.22080211560591884\n",
      "l2 norm of weights: 5.8113112206773705\n",
      "---------------------\n",
      "Iteration Number: 6293\n",
      "Loss: 32.9623574808113\n",
      "l2 norm of gradients: 0.22077865651191153\n",
      "l2 norm of weights: 5.811233193032424\n",
      "---------------------\n",
      "Iteration Number: 6294\n",
      "Loss: 32.95991309220265\n",
      "l2 norm of gradients: 0.22075520415882197\n",
      "l2 norm of weights: 5.811155172137589\n",
      "---------------------\n",
      "Iteration Number: 6295\n",
      "Loss: 32.95746922334294\n",
      "l2 norm of gradients: 0.22073175854282256\n",
      "l2 norm of weights: 5.81107715799164\n",
      "---------------------\n",
      "Iteration Number: 6296\n",
      "Loss: 32.95502587402716\n",
      "l2 norm of gradients: 0.2207083196600895\n",
      "l2 norm of weights: 5.810999150593356\n",
      "---------------------\n",
      "Iteration Number: 6297\n",
      "Loss: 32.95258304405136\n",
      "l2 norm of gradients: 0.220684887506802\n",
      "l2 norm of weights: 5.810921149941513\n",
      "---------------------\n",
      "Iteration Number: 6298\n",
      "Loss: 32.95014073320878\n",
      "l2 norm of gradients: 0.2206614620791429\n",
      "l2 norm of weights: 5.810843156034888\n",
      "---------------------\n",
      "Iteration Number: 6299\n",
      "Loss: 32.94769894129842\n",
      "l2 norm of gradients: 0.2206380433732981\n",
      "l2 norm of weights: 5.81076516887226\n",
      "---------------------\n",
      "Iteration Number: 6300\n",
      "Loss: 32.94525766811773\n",
      "l2 norm of gradients: 0.22061463138545712\n",
      "l2 norm of weights: 5.810687188452406\n",
      "---------------------\n",
      "Iteration Number: 6301\n",
      "Loss: 32.942816913461655\n",
      "l2 norm of gradients: 0.22059122611181264\n",
      "l2 norm of weights: 5.8106092147741055\n",
      "---------------------\n",
      "Iteration Number: 6302\n",
      "Loss: 32.9403766771241\n",
      "l2 norm of gradients: 0.2205678275485608\n",
      "l2 norm of weights: 5.8105312478361375\n",
      "---------------------\n",
      "Iteration Number: 6303\n",
      "Loss: 32.93793695890487\n",
      "l2 norm of gradients: 0.2205444356919009\n",
      "l2 norm of weights: 5.810453287637282\n",
      "---------------------\n",
      "Iteration Number: 6304\n",
      "Loss: 32.93549775860023\n",
      "l2 norm of gradients: 0.2205210505380357\n",
      "l2 norm of weights: 5.810375334176319\n",
      "---------------------\n",
      "Iteration Number: 6305\n",
      "Loss: 32.933059076007936\n",
      "l2 norm of gradients: 0.2204976720831713\n",
      "l2 norm of weights: 5.810297387452028\n",
      "---------------------\n",
      "Iteration Number: 6306\n",
      "Loss: 32.9306209109221\n",
      "l2 norm of gradients: 0.2204743003235171\n",
      "l2 norm of weights: 5.810219447463192\n",
      "---------------------\n",
      "Iteration Number: 6307\n",
      "Loss: 32.92818326314308\n",
      "l2 norm of gradients: 0.22045093525528572\n",
      "l2 norm of weights: 5.810141514208592\n",
      "---------------------\n",
      "Iteration Number: 6308\n",
      "Loss: 32.92574613246603\n",
      "l2 norm of gradients: 0.22042757687469317\n",
      "l2 norm of weights: 5.810063587687008\n",
      "---------------------\n",
      "Iteration Number: 6309\n",
      "Loss: 32.92330951868927\n",
      "l2 norm of gradients: 0.22040422517795882\n",
      "l2 norm of weights: 5.809985667897224\n",
      "---------------------\n",
      "Iteration Number: 6310\n",
      "Loss: 32.920873421606224\n",
      "l2 norm of gradients: 0.22038088016130528\n",
      "l2 norm of weights: 5.809907754838023\n",
      "---------------------\n",
      "Iteration Number: 6311\n",
      "Loss: 32.91843784102429\n",
      "l2 norm of gradients: 0.22035754182095849\n",
      "l2 norm of weights: 5.809829848508188\n",
      "---------------------\n",
      "Iteration Number: 6312\n",
      "Loss: 32.91600277673293\n",
      "l2 norm of gradients: 0.22033421015314766\n",
      "l2 norm of weights: 5.809751948906501\n",
      "---------------------\n",
      "Iteration Number: 6313\n",
      "Loss: 32.91356822853245\n",
      "l2 norm of gradients: 0.22031088515410538\n",
      "l2 norm of weights: 5.809674056031749\n",
      "---------------------\n",
      "Iteration Number: 6314\n",
      "Loss: 32.9111341962203\n",
      "l2 norm of gradients: 0.22028756682006737\n",
      "l2 norm of weights: 5.809596169882714\n",
      "---------------------\n",
      "Iteration Number: 6315\n",
      "Loss: 32.90870067959621\n",
      "l2 norm of gradients: 0.22026425514727282\n",
      "l2 norm of weights: 5.8095182904581835\n",
      "---------------------\n",
      "Iteration Number: 6316\n",
      "Loss: 32.906267678458185\n",
      "l2 norm of gradients: 0.22024095013196412\n",
      "l2 norm of weights: 5.8094404177569405\n",
      "---------------------\n",
      "Iteration Number: 6317\n",
      "Loss: 32.90383519260331\n",
      "l2 norm of gradients: 0.22021765177038696\n",
      "l2 norm of weights: 5.809362551777772\n",
      "---------------------\n",
      "Iteration Number: 6318\n",
      "Loss: 32.90140322183178\n",
      "l2 norm of gradients: 0.2201943600587902\n",
      "l2 norm of weights: 5.809284692519465\n",
      "---------------------\n",
      "Iteration Number: 6319\n",
      "Loss: 32.89897176594282\n",
      "l2 norm of gradients: 0.2201710749934262\n",
      "l2 norm of weights: 5.809206839980805\n",
      "---------------------\n",
      "Iteration Number: 6320\n",
      "Loss: 32.89654082473448\n",
      "l2 norm of gradients: 0.22014779657055045\n",
      "l2 norm of weights: 5.809128994160579\n",
      "---------------------\n",
      "Iteration Number: 6321\n",
      "Loss: 32.89411039799986\n",
      "l2 norm of gradients: 0.2201245247864216\n",
      "l2 norm of weights: 5.809051155057577\n",
      "---------------------\n",
      "Iteration Number: 6322\n",
      "Loss: 32.891680485549216\n",
      "l2 norm of gradients: 0.22010125963730187\n",
      "l2 norm of weights: 5.808973322670584\n",
      "---------------------\n",
      "Iteration Number: 6323\n",
      "Loss: 32.889251087172646\n",
      "l2 norm of gradients: 0.22007800111945644\n",
      "l2 norm of weights: 5.808895496998392\n",
      "---------------------\n",
      "Iteration Number: 6324\n",
      "Loss: 32.88682220267347\n",
      "l2 norm of gradients: 0.2200547492291539\n",
      "l2 norm of weights: 5.808817678039787\n",
      "---------------------\n",
      "Iteration Number: 6325\n",
      "Loss: 32.884393831854865\n",
      "l2 norm of gradients: 0.2200315039626661\n",
      "l2 norm of weights: 5.8087398657935605\n",
      "---------------------\n",
      "Iteration Number: 6326\n",
      "Loss: 32.88196597450903\n",
      "l2 norm of gradients: 0.22000826531626805\n",
      "l2 norm of weights: 5.8086620602584995\n",
      "---------------------\n",
      "Iteration Number: 6327\n",
      "Loss: 32.87953863043461\n",
      "l2 norm of gradients: 0.21998503328623809\n",
      "l2 norm of weights: 5.808584261433398\n",
      "---------------------\n",
      "Iteration Number: 6328\n",
      "Loss: 32.87711179943898\n",
      "l2 norm of gradients: 0.21996180786885774\n",
      "l2 norm of weights: 5.808506469317045\n",
      "---------------------\n",
      "Iteration Number: 6329\n",
      "Loss: 32.87468548132015\n",
      "l2 norm of gradients: 0.21993858906041186\n",
      "l2 norm of weights: 5.808428683908231\n",
      "---------------------\n",
      "Iteration Number: 6330\n",
      "Loss: 32.87225967588027\n",
      "l2 norm of gradients: 0.21991537685718845\n",
      "l2 norm of weights: 5.8083509052057485\n",
      "---------------------\n",
      "Iteration Number: 6331\n",
      "Loss: 32.869834382908024\n",
      "l2 norm of gradients: 0.21989217125547872\n",
      "l2 norm of weights: 5.808273133208388\n",
      "---------------------\n",
      "Iteration Number: 6332\n",
      "Loss: 32.867409602218956\n",
      "l2 norm of gradients: 0.2198689722515773\n",
      "l2 norm of weights: 5.808195367914945\n",
      "---------------------\n",
      "Iteration Number: 6333\n",
      "Loss: 32.86498533360452\n",
      "l2 norm of gradients: 0.21984577984178183\n",
      "l2 norm of weights: 5.80811760932421\n",
      "---------------------\n",
      "Iteration Number: 6334\n",
      "Loss: 32.86256157686472\n",
      "l2 norm of gradients: 0.21982259402239324\n",
      "l2 norm of weights: 5.808039857434977\n",
      "---------------------\n",
      "Iteration Number: 6335\n",
      "Loss: 32.86013833180663\n",
      "l2 norm of gradients: 0.21979941478971574\n",
      "l2 norm of weights: 5.807962112246041\n",
      "---------------------\n",
      "Iteration Number: 6336\n",
      "Loss: 32.8577155982251\n",
      "l2 norm of gradients: 0.21977624214005664\n",
      "l2 norm of weights: 5.8078843737561945\n",
      "---------------------\n",
      "Iteration Number: 6337\n",
      "Loss: 32.8552933759231\n",
      "l2 norm of gradients: 0.21975307606972666\n",
      "l2 norm of weights: 5.807806641964233\n",
      "---------------------\n",
      "Iteration Number: 6338\n",
      "Loss: 32.85287166470763\n",
      "l2 norm of gradients: 0.21972991657503949\n",
      "l2 norm of weights: 5.807728916868951\n",
      "---------------------\n",
      "Iteration Number: 6339\n",
      "Loss: 32.85045046436469\n",
      "l2 norm of gradients: 0.2197067636523122\n",
      "l2 norm of weights: 5.807651198469145\n",
      "---------------------\n",
      "Iteration Number: 6340\n",
      "Loss: 32.84802977471459\n",
      "l2 norm of gradients: 0.21968361729786498\n",
      "l2 norm of weights: 5.80757348676361\n",
      "---------------------\n",
      "Iteration Number: 6341\n",
      "Loss: 32.84560959554241\n",
      "l2 norm of gradients: 0.2196604775080212\n",
      "l2 norm of weights: 5.807495781751144\n",
      "---------------------\n",
      "Iteration Number: 6342\n",
      "Loss: 32.84318992666341\n",
      "l2 norm of gradients: 0.21963734427910753\n",
      "l2 norm of weights: 5.807418083430542\n",
      "---------------------\n",
      "Iteration Number: 6343\n",
      "Loss: 32.84077076787206\n",
      "l2 norm of gradients: 0.21961421760745375\n",
      "l2 norm of weights: 5.807340391800603\n",
      "---------------------\n",
      "Iteration Number: 6344\n",
      "Loss: 32.838352118968345\n",
      "l2 norm of gradients: 0.21959109748939284\n",
      "l2 norm of weights: 5.807262706860123\n",
      "---------------------\n",
      "Iteration Number: 6345\n",
      "Loss: 32.835933979759886\n",
      "l2 norm of gradients: 0.21956798392126098\n",
      "l2 norm of weights: 5.807185028607901\n",
      "---------------------\n",
      "Iteration Number: 6346\n",
      "Loss: 32.83351635004091\n",
      "l2 norm of gradients: 0.21954487689939753\n",
      "l2 norm of weights: 5.807107357042736\n",
      "---------------------\n",
      "Iteration Number: 6347\n",
      "Loss: 32.831099229622275\n",
      "l2 norm of gradients: 0.21952177642014498\n",
      "l2 norm of weights: 5.807029692163427\n",
      "---------------------\n",
      "Iteration Number: 6348\n",
      "Loss: 32.82868261830279\n",
      "l2 norm of gradients: 0.21949868247984908\n",
      "l2 norm of weights: 5.806952033968773\n",
      "---------------------\n",
      "Iteration Number: 6349\n",
      "Loss: 32.826266515886715\n",
      "l2 norm of gradients: 0.21947559507485867\n",
      "l2 norm of weights: 5.806874382457574\n",
      "---------------------\n",
      "Iteration Number: 6350\n",
      "Loss: 32.82385092217376\n",
      "l2 norm of gradients: 0.2194525142015258\n",
      "l2 norm of weights: 5.80679673762863\n",
      "---------------------\n",
      "Iteration Number: 6351\n",
      "Loss: 32.82143583696661\n",
      "l2 norm of gradients: 0.21942943985620578\n",
      "l2 norm of weights: 5.806719099480742\n",
      "---------------------\n",
      "Iteration Number: 6352\n",
      "Loss: 32.819021260068894\n",
      "l2 norm of gradients: 0.2194063720352568\n",
      "l2 norm of weights: 5.80664146801271\n",
      "---------------------\n",
      "Iteration Number: 6353\n",
      "Loss: 32.81660719128619\n",
      "l2 norm of gradients: 0.21938331073504055\n",
      "l2 norm of weights: 5.80656384322334\n",
      "---------------------\n",
      "Iteration Number: 6354\n",
      "Loss: 32.814193630417456\n",
      "l2 norm of gradients: 0.21936025595192157\n",
      "l2 norm of weights: 5.806486225111429\n",
      "---------------------\n",
      "Iteration Number: 6355\n",
      "Loss: 32.81178057727115\n",
      "l2 norm of gradients: 0.21933720768226786\n",
      "l2 norm of weights: 5.8064086136757815\n",
      "---------------------\n",
      "Iteration Number: 6356\n",
      "Loss: 32.80936803164377\n",
      "l2 norm of gradients: 0.2193141659224503\n",
      "l2 norm of weights: 5.806331008915201\n",
      "---------------------\n",
      "Iteration Number: 6357\n",
      "Loss: 32.80695599334556\n",
      "l2 norm of gradients: 0.21929113066884293\n",
      "l2 norm of weights: 5.80625341082849\n",
      "---------------------\n",
      "Iteration Number: 6358\n",
      "Loss: 32.80454446217249\n",
      "l2 norm of gradients: 0.21926810191782317\n",
      "l2 norm of weights: 5.806175819414453\n",
      "---------------------\n",
      "Iteration Number: 6359\n",
      "Loss: 32.80213343793943\n",
      "l2 norm of gradients: 0.21924507966577142\n",
      "l2 norm of weights: 5.806098234671893\n",
      "---------------------\n",
      "Iteration Number: 6360\n",
      "Loss: 32.799722920440004\n",
      "l2 norm of gradients: 0.2192220639090711\n",
      "l2 norm of weights: 5.806020656599617\n",
      "---------------------\n",
      "Iteration Number: 6361\n",
      "Loss: 32.79731290948145\n",
      "l2 norm of gradients: 0.2191990546441089\n",
      "l2 norm of weights: 5.805943085196427\n",
      "---------------------\n",
      "Iteration Number: 6362\n",
      "Loss: 32.794903404867796\n",
      "l2 norm of gradients: 0.21917605186727473\n",
      "l2 norm of weights: 5.80586552046113\n",
      "---------------------\n",
      "Iteration Number: 6363\n",
      "Loss: 32.79249440640993\n",
      "l2 norm of gradients: 0.21915305557496137\n",
      "l2 norm of weights: 5.8057879623925315\n",
      "---------------------\n",
      "Iteration Number: 6364\n",
      "Loss: 32.790085913900306\n",
      "l2 norm of gradients: 0.21913006576356495\n",
      "l2 norm of weights: 5.805710410989439\n",
      "---------------------\n",
      "Iteration Number: 6365\n",
      "Loss: 32.787677927148714\n",
      "l2 norm of gradients: 0.2191070824294845\n",
      "l2 norm of weights: 5.805632866250658\n",
      "---------------------\n",
      "Iteration Number: 6366\n",
      "Loss: 32.78527044596501\n",
      "l2 norm of gradients: 0.2190841055691225\n",
      "l2 norm of weights: 5.805555328174997\n",
      "---------------------\n",
      "Iteration Number: 6367\n",
      "Loss: 32.78286347014521\n",
      "l2 norm of gradients: 0.21906113517888415\n",
      "l2 norm of weights: 5.805477796761264\n",
      "---------------------\n",
      "Iteration Number: 6368\n",
      "Loss: 32.780456999500814\n",
      "l2 norm of gradients: 0.21903817125517802\n",
      "l2 norm of weights: 5.805400272008264\n",
      "---------------------\n",
      "Iteration Number: 6369\n",
      "Loss: 32.77805103382831\n",
      "l2 norm of gradients: 0.21901521379441563\n",
      "l2 norm of weights: 5.805322753914808\n",
      "---------------------\n",
      "Iteration Number: 6370\n",
      "Loss: 32.775645572946175\n",
      "l2 norm of gradients: 0.21899226279301168\n",
      "l2 norm of weights: 5.805245242479704\n",
      "---------------------\n",
      "Iteration Number: 6371\n",
      "Loss: 32.77324061664721\n",
      "l2 norm of gradients: 0.21896931824738408\n",
      "l2 norm of weights: 5.805167737701763\n",
      "---------------------\n",
      "Iteration Number: 6372\n",
      "Loss: 32.77083616474056\n",
      "l2 norm of gradients: 0.21894638015395357\n",
      "l2 norm of weights: 5.805090239579792\n",
      "---------------------\n",
      "Iteration Number: 6373\n",
      "Loss: 32.768432217036256\n",
      "l2 norm of gradients: 0.21892344850914416\n",
      "l2 norm of weights: 5.805012748112604\n",
      "---------------------\n",
      "Iteration Number: 6374\n",
      "Loss: 32.76602877333666\n",
      "l2 norm of gradients: 0.2189005233093829\n",
      "l2 norm of weights: 5.804935263299008\n",
      "---------------------\n",
      "Iteration Number: 6375\n",
      "Loss: 32.7636258334449\n",
      "l2 norm of gradients: 0.21887760455109995\n",
      "l2 norm of weights: 5.804857785137815\n",
      "---------------------\n",
      "Iteration Number: 6376\n",
      "Loss: 32.761223397169644\n",
      "l2 norm of gradients: 0.2188546922307285\n",
      "l2 norm of weights: 5.8047803136278375\n",
      "---------------------\n",
      "Iteration Number: 6377\n",
      "Loss: 32.75882146431631\n",
      "l2 norm of gradients: 0.21883178634470488\n",
      "l2 norm of weights: 5.804702848767886\n",
      "---------------------\n",
      "Iteration Number: 6378\n",
      "Loss: 32.7564200346922\n",
      "l2 norm of gradients: 0.21880888688946842\n",
      "l2 norm of weights: 5.804625390556774\n",
      "---------------------\n",
      "Iteration Number: 6379\n",
      "Loss: 32.75401910809996\n",
      "l2 norm of gradients: 0.2187859938614616\n",
      "l2 norm of weights: 5.804547938993315\n",
      "---------------------\n",
      "Iteration Number: 6380\n",
      "Loss: 32.75161868435154\n",
      "l2 norm of gradients: 0.2187631072571299\n",
      "l2 norm of weights: 5.80447049407632\n",
      "---------------------\n",
      "Iteration Number: 6381\n",
      "Loss: 32.74921876324473\n",
      "l2 norm of gradients: 0.21874022707292187\n",
      "l2 norm of weights: 5.804393055804603\n",
      "---------------------\n",
      "Iteration Number: 6382\n",
      "Loss: 32.74681934459194\n",
      "l2 norm of gradients: 0.21871735330528919\n",
      "l2 norm of weights: 5.804315624176979\n",
      "---------------------\n",
      "Iteration Number: 6383\n",
      "Loss: 32.74442042820645\n",
      "l2 norm of gradients: 0.21869448595068658\n",
      "l2 norm of weights: 5.8042381991922625\n",
      "---------------------\n",
      "Iteration Number: 6384\n",
      "Loss: 32.74202201387599\n",
      "l2 norm of gradients: 0.21867162500557163\n",
      "l2 norm of weights: 5.804160780849269\n",
      "---------------------\n",
      "Iteration Number: 6385\n",
      "Loss: 32.73962410142846\n",
      "l2 norm of gradients: 0.21864877046640527\n",
      "l2 norm of weights: 5.804083369146811\n",
      "---------------------\n",
      "Iteration Number: 6386\n",
      "Loss: 32.73722669065781\n",
      "l2 norm of gradients: 0.21862592232965125\n",
      "l2 norm of weights: 5.804005964083707\n",
      "---------------------\n",
      "Iteration Number: 6387\n",
      "Loss: 32.73482978137517\n",
      "l2 norm of gradients: 0.2186030805917765\n",
      "l2 norm of weights: 5.803928565658773\n",
      "---------------------\n",
      "Iteration Number: 6388\n",
      "Loss: 32.732433373390556\n",
      "l2 norm of gradients: 0.2185802452492509\n",
      "l2 norm of weights: 5.803851173870823\n",
      "---------------------\n",
      "Iteration Number: 6389\n",
      "Loss: 32.73003746650596\n",
      "l2 norm of gradients: 0.21855741629854747\n",
      "l2 norm of weights: 5.803773788718677\n",
      "---------------------\n",
      "Iteration Number: 6390\n",
      "Loss: 32.727642060533206\n",
      "l2 norm of gradients: 0.2185345937361421\n",
      "l2 norm of weights: 5.80369641020115\n",
      "---------------------\n",
      "Iteration Number: 6391\n",
      "Loss: 32.72524715527534\n",
      "l2 norm of gradients: 0.2185117775585139\n",
      "l2 norm of weights: 5.803619038317062\n",
      "---------------------\n",
      "Iteration Number: 6392\n",
      "Loss: 32.72285275054417\n",
      "l2 norm of gradients: 0.2184889677621449\n",
      "l2 norm of weights: 5.803541673065229\n",
      "---------------------\n",
      "Iteration Number: 6393\n",
      "Loss: 32.72045884614638\n",
      "l2 norm of gradients: 0.21846616434352015\n",
      "l2 norm of weights: 5.803464314444472\n",
      "---------------------\n",
      "Iteration Number: 6394\n",
      "Loss: 32.71806544189181\n",
      "l2 norm of gradients: 0.21844336729912772\n",
      "l2 norm of weights: 5.803386962453608\n",
      "---------------------\n",
      "Iteration Number: 6395\n",
      "Loss: 32.715672537584375\n",
      "l2 norm of gradients: 0.21842057662545886\n",
      "l2 norm of weights: 5.803309617091458\n",
      "---------------------\n",
      "Iteration Number: 6396\n",
      "Loss: 32.71328013303764\n",
      "l2 norm of gradients: 0.21839779231900747\n",
      "l2 norm of weights: 5.803232278356842\n",
      "---------------------\n",
      "Iteration Number: 6397\n",
      "Loss: 32.71088822805293\n",
      "l2 norm of gradients: 0.2183750143762708\n",
      "l2 norm of weights: 5.803154946248579\n",
      "---------------------\n",
      "Iteration Number: 6398\n",
      "Loss: 32.70849682244664\n",
      "l2 norm of gradients: 0.218352242793749\n",
      "l2 norm of weights: 5.803077620765491\n",
      "---------------------\n",
      "Iteration Number: 6399\n",
      "Loss: 32.706105916020284\n",
      "l2 norm of gradients: 0.21832947756794518\n",
      "l2 norm of weights: 5.803000301906397\n",
      "---------------------\n",
      "Iteration Number: 6400\n",
      "Loss: 32.70371550858573\n",
      "l2 norm of gradients: 0.21830671869536553\n",
      "l2 norm of weights: 5.8029229896701215\n",
      "---------------------\n",
      "Iteration Number: 6401\n",
      "Loss: 32.70132559995102\n",
      "l2 norm of gradients: 0.21828396617251916\n",
      "l2 norm of weights: 5.802845684055486\n",
      "---------------------\n",
      "Iteration Number: 6402\n",
      "Loss: 32.698936189929135\n",
      "l2 norm of gradients: 0.21826121999591822\n",
      "l2 norm of weights: 5.80276838506131\n",
      "---------------------\n",
      "Iteration Number: 6403\n",
      "Loss: 32.69654727832339\n",
      "l2 norm of gradients: 0.21823848016207775\n",
      "l2 norm of weights: 5.802691092686421\n",
      "---------------------\n",
      "Iteration Number: 6404\n",
      "Loss: 32.69415886494681\n",
      "l2 norm of gradients: 0.218215746667516\n",
      "l2 norm of weights: 5.802613806929639\n",
      "---------------------\n",
      "Iteration Number: 6405\n",
      "Loss: 32.69177094960949\n",
      "l2 norm of gradients: 0.21819301950875405\n",
      "l2 norm of weights: 5.802536527789788\n",
      "---------------------\n",
      "Iteration Number: 6406\n",
      "Loss: 32.689383532112195\n",
      "l2 norm of gradients: 0.2181702986823159\n",
      "l2 norm of weights: 5.802459255265693\n",
      "---------------------\n",
      "Iteration Number: 6407\n",
      "Loss: 32.68699661227558\n",
      "l2 norm of gradients: 0.21814758418472863\n",
      "l2 norm of weights: 5.802381989356178\n",
      "---------------------\n",
      "Iteration Number: 6408\n",
      "Loss: 32.684610189905364\n",
      "l2 norm of gradients: 0.21812487601252234\n",
      "l2 norm of weights: 5.802304730060068\n",
      "---------------------\n",
      "Iteration Number: 6409\n",
      "Loss: 32.682224264807886\n",
      "l2 norm of gradients: 0.21810217416222993\n",
      "l2 norm of weights: 5.802227477376189\n",
      "---------------------\n",
      "Iteration Number: 6410\n",
      "Loss: 32.679838836798446\n",
      "l2 norm of gradients: 0.2180794786303874\n",
      "l2 norm of weights: 5.802150231303366\n",
      "---------------------\n",
      "Iteration Number: 6411\n",
      "Loss: 32.67745390568146\n",
      "l2 norm of gradients: 0.21805678941353385\n",
      "l2 norm of weights: 5.802072991840425\n",
      "---------------------\n",
      "Iteration Number: 6412\n",
      "Loss: 32.675069471275755\n",
      "l2 norm of gradients: 0.21803410650821095\n",
      "l2 norm of weights: 5.801995758986193\n",
      "---------------------\n",
      "Iteration Number: 6413\n",
      "Loss: 32.67268553338036\n",
      "l2 norm of gradients: 0.21801142991096364\n",
      "l2 norm of weights: 5.8019185327394975\n",
      "---------------------\n",
      "Iteration Number: 6414\n",
      "Loss: 32.670302091810164\n",
      "l2 norm of gradients: 0.21798875961833983\n",
      "l2 norm of weights: 5.801841313099165\n",
      "---------------------\n",
      "Iteration Number: 6415\n",
      "Loss: 32.6679191463819\n",
      "l2 norm of gradients: 0.21796609562689015\n",
      "l2 norm of weights: 5.801764100064024\n",
      "---------------------\n",
      "Iteration Number: 6416\n",
      "Loss: 32.66553669689467\n",
      "l2 norm of gradients: 0.2179434379331684\n",
      "l2 norm of weights: 5.801686893632903\n",
      "---------------------\n",
      "Iteration Number: 6417\n",
      "Loss: 32.66315474316893\n",
      "l2 norm of gradients: 0.2179207865337312\n",
      "l2 norm of weights: 5.80160969380463\n",
      "---------------------\n",
      "Iteration Number: 6418\n",
      "Loss: 32.66077328501174\n",
      "l2 norm of gradients: 0.21789814142513814\n",
      "l2 norm of weights: 5.801532500578034\n",
      "---------------------\n",
      "Iteration Number: 6419\n",
      "Loss: 32.65839232223302\n",
      "l2 norm of gradients: 0.2178755026039518\n",
      "l2 norm of weights: 5.801455313951945\n",
      "---------------------\n",
      "Iteration Number: 6420\n",
      "Loss: 32.65601185464807\n",
      "l2 norm of gradients: 0.21785287006673767\n",
      "l2 norm of weights: 5.801378133925193\n",
      "---------------------\n",
      "Iteration Number: 6421\n",
      "Loss: 32.65363188206025\n",
      "l2 norm of gradients: 0.21783024381006408\n",
      "l2 norm of weights: 5.801300960496608\n",
      "---------------------\n",
      "Iteration Number: 6422\n",
      "Loss: 32.65125240428997\n",
      "l2 norm of gradients: 0.21780762383050245\n",
      "l2 norm of weights: 5.801223793665021\n",
      "---------------------\n",
      "Iteration Number: 6423\n",
      "Loss: 32.64887342113952\n",
      "l2 norm of gradients: 0.21778501012462703\n",
      "l2 norm of weights: 5.801146633429263\n",
      "---------------------\n",
      "Iteration Number: 6424\n",
      "Loss: 32.64649493242765\n",
      "l2 norm of gradients: 0.21776240268901498\n",
      "l2 norm of weights: 5.801069479788165\n",
      "---------------------\n",
      "Iteration Number: 6425\n",
      "Loss: 32.644116937965926\n",
      "l2 norm of gradients: 0.21773980152024644\n",
      "l2 norm of weights: 5.80099233274056\n",
      "---------------------\n",
      "Iteration Number: 6426\n",
      "Loss: 32.641739437561725\n",
      "l2 norm of gradients: 0.21771720661490443\n",
      "l2 norm of weights: 5.80091519228528\n",
      "---------------------\n",
      "Iteration Number: 6427\n",
      "Loss: 32.639362431029795\n",
      "l2 norm of gradients: 0.21769461796957487\n",
      "l2 norm of weights: 5.800838058421157\n",
      "---------------------\n",
      "Iteration Number: 6428\n",
      "Loss: 32.63698591818013\n",
      "l2 norm of gradients: 0.21767203558084666\n",
      "l2 norm of weights: 5.800760931147026\n",
      "---------------------\n",
      "Iteration Number: 6429\n",
      "Loss: 32.63460989883058\n",
      "l2 norm of gradients: 0.2176494594453115\n",
      "l2 norm of weights: 5.800683810461718\n",
      "---------------------\n",
      "Iteration Number: 6430\n",
      "Loss: 32.63223437278054\n",
      "l2 norm of gradients: 0.2176268895595641\n",
      "l2 norm of weights: 5.800606696364071\n",
      "---------------------\n",
      "Iteration Number: 6431\n",
      "Loss: 32.62985933985764\n",
      "l2 norm of gradients: 0.2176043259202019\n",
      "l2 norm of weights: 5.800529588852914\n",
      "---------------------\n",
      "Iteration Number: 6432\n",
      "Loss: 32.627484799864526\n",
      "l2 norm of gradients: 0.21758176852382557\n",
      "l2 norm of weights: 5.800452487927087\n",
      "---------------------\n",
      "Iteration Number: 6433\n",
      "Loss: 32.6251107526163\n",
      "l2 norm of gradients: 0.21755921736703832\n",
      "l2 norm of weights: 5.800375393585423\n",
      "---------------------\n",
      "Iteration Number: 6434\n",
      "Loss: 32.62273719792649\n",
      "l2 norm of gradients: 0.21753667244644648\n",
      "l2 norm of weights: 5.8002983058267565\n",
      "---------------------\n",
      "Iteration Number: 6435\n",
      "Loss: 32.620364135605186\n",
      "l2 norm of gradients: 0.21751413375865902\n",
      "l2 norm of weights: 5.800221224649926\n",
      "---------------------\n",
      "Iteration Number: 6436\n",
      "Loss: 32.61799156547269\n",
      "l2 norm of gradients: 0.21749160130028822\n",
      "l2 norm of weights: 5.800144150053766\n",
      "---------------------\n",
      "Iteration Number: 6437\n",
      "Loss: 32.61561948733462\n",
      "l2 norm of gradients: 0.21746907506794874\n",
      "l2 norm of weights: 5.800067082037115\n",
      "---------------------\n",
      "Iteration Number: 6438\n",
      "Loss: 32.613247901004414\n",
      "l2 norm of gradients: 0.21744655505825855\n",
      "l2 norm of weights: 5.79999002059881\n",
      "---------------------\n",
      "Iteration Number: 6439\n",
      "Loss: 32.610876806296936\n",
      "l2 norm of gradients: 0.21742404126783813\n",
      "l2 norm of weights: 5.799912965737687\n",
      "---------------------\n",
      "Iteration Number: 6440\n",
      "Loss: 32.60850620302844\n",
      "l2 norm of gradients: 0.2174015336933111\n",
      "l2 norm of weights: 5.799835917452587\n",
      "---------------------\n",
      "Iteration Number: 6441\n",
      "Loss: 32.60613609100762\n",
      "l2 norm of gradients: 0.21737903233130393\n",
      "l2 norm of weights: 5.799758875742345\n",
      "---------------------\n",
      "Iteration Number: 6442\n",
      "Loss: 32.60376647005267\n",
      "l2 norm of gradients: 0.21735653717844577\n",
      "l2 norm of weights: 5.799681840605803\n",
      "---------------------\n",
      "Iteration Number: 6443\n",
      "Loss: 32.601397339975925\n",
      "l2 norm of gradients: 0.21733404823136876\n",
      "l2 norm of weights: 5.7996048120418\n",
      "---------------------\n",
      "Iteration Number: 6444\n",
      "Loss: 32.59902870058152\n",
      "l2 norm of gradients: 0.21731156548670794\n",
      "l2 norm of weights: 5.799527790049174\n",
      "---------------------\n",
      "Iteration Number: 6445\n",
      "Loss: 32.596660551700246\n",
      "l2 norm of gradients: 0.21728908894110113\n",
      "l2 norm of weights: 5.799450774626766\n",
      "---------------------\n",
      "Iteration Number: 6446\n",
      "Loss: 32.59429289313718\n",
      "l2 norm of gradients: 0.217266618591189\n",
      "l2 norm of weights: 5.799373765773416\n",
      "---------------------\n",
      "Iteration Number: 6447\n",
      "Loss: 32.59192572470804\n",
      "l2 norm of gradients: 0.21724415443361517\n",
      "l2 norm of weights: 5.799296763487967\n",
      "---------------------\n",
      "Iteration Number: 6448\n",
      "Loss: 32.58955904622155\n",
      "l2 norm of gradients: 0.21722169646502598\n",
      "l2 norm of weights: 5.799219767769259\n",
      "---------------------\n",
      "Iteration Number: 6449\n",
      "Loss: 32.587192857500455\n",
      "l2 norm of gradients: 0.21719924468207072\n",
      "l2 norm of weights: 5.799142778616133\n",
      "---------------------\n",
      "Iteration Number: 6450\n",
      "Loss: 32.58482715835788\n",
      "l2 norm of gradients: 0.2171767990814014\n",
      "l2 norm of weights: 5.799065796027433\n",
      "---------------------\n",
      "Iteration Number: 6451\n",
      "Loss: 32.582461948603\n",
      "l2 norm of gradients: 0.21715435965967295\n",
      "l2 norm of weights: 5.798988820002\n",
      "---------------------\n",
      "Iteration Number: 6452\n",
      "Loss: 32.58009722805882\n",
      "l2 norm of gradients: 0.2171319264135432\n",
      "l2 norm of weights: 5.798911850538677\n",
      "---------------------\n",
      "Iteration Number: 6453\n",
      "Loss: 32.577732996533484\n",
      "l2 norm of gradients: 0.2171094993396727\n",
      "l2 norm of weights: 5.798834887636309\n",
      "---------------------\n",
      "Iteration Number: 6454\n",
      "Loss: 32.575369253836186\n",
      "l2 norm of gradients: 0.2170870784347248\n",
      "l2 norm of weights: 5.798757931293738\n",
      "---------------------\n",
      "Iteration Number: 6455\n",
      "Loss: 32.573005999800486\n",
      "l2 norm of gradients: 0.2170646636953658\n",
      "l2 norm of weights: 5.7986809815098095\n",
      "---------------------\n",
      "Iteration Number: 6456\n",
      "Loss: 32.57064323422492\n",
      "l2 norm of gradients: 0.21704225511826478\n",
      "l2 norm of weights: 5.798604038283368\n",
      "---------------------\n",
      "Iteration Number: 6457\n",
      "Loss: 32.56828095693064\n",
      "l2 norm of gradients: 0.2170198527000936\n",
      "l2 norm of weights: 5.7985271016132565\n",
      "---------------------\n",
      "Iteration Number: 6458\n",
      "Loss: 32.5659191677372\n",
      "l2 norm of gradients: 0.2169974564375269\n",
      "l2 norm of weights: 5.798450171498324\n",
      "---------------------\n",
      "Iteration Number: 6459\n",
      "Loss: 32.56355786645091\n",
      "l2 norm of gradients: 0.21697506632724234\n",
      "l2 norm of weights: 5.7983732479374135\n",
      "---------------------\n",
      "Iteration Number: 6460\n",
      "Loss: 32.56119705289475\n",
      "l2 norm of gradients: 0.2169526823659201\n",
      "l2 norm of weights: 5.798296330929372\n",
      "---------------------\n",
      "Iteration Number: 6461\n",
      "Loss: 32.558836726884834\n",
      "l2 norm of gradients: 0.2169303045502434\n",
      "l2 norm of weights: 5.798219420473046\n",
      "---------------------\n",
      "Iteration Number: 6462\n",
      "Loss: 32.55647688823167\n",
      "l2 norm of gradients: 0.21690793287689805\n",
      "l2 norm of weights: 5.7981425165672835\n",
      "---------------------\n",
      "Iteration Number: 6463\n",
      "Loss: 32.55411753675537\n",
      "l2 norm of gradients: 0.21688556734257297\n",
      "l2 norm of weights: 5.798065619210931\n",
      "---------------------\n",
      "Iteration Number: 6464\n",
      "Loss: 32.55175867226916\n",
      "l2 norm of gradients: 0.21686320794395958\n",
      "l2 norm of weights: 5.797988728402835\n",
      "---------------------\n",
      "Iteration Number: 6465\n",
      "Loss: 32.5494002945926\n",
      "l2 norm of gradients: 0.2168408546777522\n",
      "l2 norm of weights: 5.797911844141847\n",
      "---------------------\n",
      "Iteration Number: 6466\n",
      "Loss: 32.54704240353787\n",
      "l2 norm of gradients: 0.21681850754064794\n",
      "l2 norm of weights: 5.797834966426813\n",
      "---------------------\n",
      "Iteration Number: 6467\n",
      "Loss: 32.54468499892608\n",
      "l2 norm of gradients: 0.21679616652934683\n",
      "l2 norm of weights: 5.797758095256583\n",
      "---------------------\n",
      "Iteration Number: 6468\n",
      "Loss: 32.542328080572645\n",
      "l2 norm of gradients: 0.2167738316405514\n",
      "l2 norm of weights: 5.7976812306300065\n",
      "---------------------\n",
      "Iteration Number: 6469\n",
      "Loss: 32.53997164829016\n",
      "l2 norm of gradients: 0.2167515028709672\n",
      "l2 norm of weights: 5.797604372545933\n",
      "---------------------\n",
      "Iteration Number: 6470\n",
      "Loss: 32.53761570189839\n",
      "l2 norm of gradients: 0.21672918021730248\n",
      "l2 norm of weights: 5.797527521003214\n",
      "---------------------\n",
      "Iteration Number: 6471\n",
      "Loss: 32.5352602412161\n",
      "l2 norm of gradients: 0.2167068636762683\n",
      "l2 norm of weights: 5.797450676000698\n",
      "---------------------\n",
      "Iteration Number: 6472\n",
      "Loss: 32.53290526605651\n",
      "l2 norm of gradients: 0.21668455324457847\n",
      "l2 norm of weights: 5.797373837537237\n",
      "---------------------\n",
      "Iteration Number: 6473\n",
      "Loss: 32.53055077624127\n",
      "l2 norm of gradients: 0.21666224891894945\n",
      "l2 norm of weights: 5.797297005611682\n",
      "---------------------\n",
      "Iteration Number: 6474\n",
      "Loss: 32.52819677158641\n",
      "l2 norm of gradients: 0.2166399506961007\n",
      "l2 norm of weights: 5.797220180222886\n",
      "---------------------\n",
      "Iteration Number: 6475\n",
      "Loss: 32.52584325190133\n",
      "l2 norm of gradients: 0.21661765857275433\n",
      "l2 norm of weights: 5.7971433613697005\n",
      "---------------------\n",
      "Iteration Number: 6476\n",
      "Loss: 32.52349021701416\n",
      "l2 norm of gradients: 0.21659537254563516\n",
      "l2 norm of weights: 5.797066549050979\n",
      "---------------------\n",
      "Iteration Number: 6477\n",
      "Loss: 32.52113766673751\n",
      "l2 norm of gradients: 0.21657309261147084\n",
      "l2 norm of weights: 5.7969897432655735\n",
      "---------------------\n",
      "Iteration Number: 6478\n",
      "Loss: 32.51878560088956\n",
      "l2 norm of gradients: 0.21655081876699173\n",
      "l2 norm of weights: 5.796912944012336\n",
      "---------------------\n",
      "Iteration Number: 6479\n",
      "Loss: 32.516434019287615\n",
      "l2 norm of gradients: 0.21652855100893095\n",
      "l2 norm of weights: 5.796836151290124\n",
      "---------------------\n",
      "Iteration Number: 6480\n",
      "Loss: 32.51408292175561\n",
      "l2 norm of gradients: 0.21650628933402438\n",
      "l2 norm of weights: 5.796759365097788\n",
      "---------------------\n",
      "Iteration Number: 6481\n",
      "Loss: 32.511732308099205\n",
      "l2 norm of gradients: 0.21648403373901073\n",
      "l2 norm of weights: 5.796682585434186\n",
      "---------------------\n",
      "Iteration Number: 6482\n",
      "Loss: 32.50938217814273\n",
      "l2 norm of gradients: 0.21646178422063125\n",
      "l2 norm of weights: 5.7966058122981705\n",
      "---------------------\n",
      "Iteration Number: 6483\n",
      "Loss: 32.507032531710735\n",
      "l2 norm of gradients: 0.2164395407756302\n",
      "l2 norm of weights: 5.796529045688598\n",
      "---------------------\n",
      "Iteration Number: 6484\n",
      "Loss: 32.50468336861464\n",
      "l2 norm of gradients: 0.2164173034007543\n",
      "l2 norm of weights: 5.796452285604324\n",
      "---------------------\n",
      "Iteration Number: 6485\n",
      "Loss: 32.50233468867342\n",
      "l2 norm of gradients: 0.21639507209275313\n",
      "l2 norm of weights: 5.7963755320442045\n",
      "---------------------\n",
      "Iteration Number: 6486\n",
      "Loss: 32.49998649170504\n",
      "l2 norm of gradients: 0.2163728468483791\n",
      "l2 norm of weights: 5.796298785007097\n",
      "---------------------\n",
      "Iteration Number: 6487\n",
      "Loss: 32.497638777529964\n",
      "l2 norm of gradients: 0.21635062766438717\n",
      "l2 norm of weights: 5.796222044491857\n",
      "---------------------\n",
      "Iteration Number: 6488\n",
      "Loss: 32.49529154597179\n",
      "l2 norm of gradients: 0.21632841453753515\n",
      "l2 norm of weights: 5.796145310497343\n",
      "---------------------\n",
      "Iteration Number: 6489\n",
      "Loss: 32.492944796838074\n",
      "l2 norm of gradients: 0.2163062074645835\n",
      "l2 norm of weights: 5.796068583022413\n",
      "---------------------\n",
      "Iteration Number: 6490\n",
      "Loss: 32.490598529957325\n",
      "l2 norm of gradients: 0.21628400644229542\n",
      "l2 norm of weights: 5.795991862065924\n",
      "---------------------\n",
      "Iteration Number: 6491\n",
      "Loss: 32.488252745142105\n",
      "l2 norm of gradients: 0.2162618114674369\n",
      "l2 norm of weights: 5.7959151476267365\n",
      "---------------------\n",
      "Iteration Number: 6492\n",
      "Loss: 32.485907442213744\n",
      "l2 norm of gradients: 0.21623962253677648\n",
      "l2 norm of weights: 5.795838439703707\n",
      "---------------------\n",
      "Iteration Number: 6493\n",
      "Loss: 32.48356262099647\n",
      "l2 norm of gradients: 0.21621743964708556\n",
      "l2 norm of weights: 5.795761738295697\n",
      "---------------------\n",
      "Iteration Number: 6494\n",
      "Loss: 32.4812182813062\n",
      "l2 norm of gradients: 0.21619526279513815\n",
      "l2 norm of weights: 5.795685043401566\n",
      "---------------------\n",
      "Iteration Number: 6495\n",
      "Loss: 32.47887442295895\n",
      "l2 norm of gradients: 0.21617309197771106\n",
      "l2 norm of weights: 5.795608355020173\n",
      "---------------------\n",
      "Iteration Number: 6496\n",
      "Loss: 32.47653104577719\n",
      "l2 norm of gradients: 0.21615092719158374\n",
      "l2 norm of weights: 5.795531673150379\n",
      "---------------------\n",
      "Iteration Number: 6497\n",
      "Loss: 32.47418814958367\n",
      "l2 norm of gradients: 0.2161287684335383\n",
      "l2 norm of weights: 5.795454997791046\n",
      "---------------------\n",
      "Iteration Number: 6498\n",
      "Loss: 32.47184573419428\n",
      "l2 norm of gradients: 0.2161066157003596\n",
      "l2 norm of weights: 5.795378328941033\n",
      "---------------------\n",
      "Iteration Number: 6499\n",
      "Loss: 32.46950379943065\n",
      "l2 norm of gradients: 0.21608446898883524\n",
      "l2 norm of weights: 5.795301666599204\n",
      "---------------------\n",
      "Iteration Number: 6500\n",
      "Loss: 32.46716234511428\n",
      "l2 norm of gradients: 0.21606232829575536\n",
      "l2 norm of weights: 5.79522501076442\n",
      "---------------------\n",
      "Iteration Number: 6501\n",
      "Loss: 32.464821371059905\n",
      "l2 norm of gradients: 0.216040193617913\n",
      "l2 norm of weights: 5.795148361435543\n",
      "---------------------\n",
      "Iteration Number: 6502\n",
      "Loss: 32.46248087709086\n",
      "l2 norm of gradients: 0.21601806495210366\n",
      "l2 norm of weights: 5.795071718611438\n",
      "---------------------\n",
      "Iteration Number: 6503\n",
      "Loss: 32.460140863034155\n",
      "l2 norm of gradients: 0.2159959422951256\n",
      "l2 norm of weights: 5.794995082290967\n",
      "---------------------\n",
      "Iteration Number: 6504\n",
      "Loss: 32.45780132869728\n",
      "l2 norm of gradients: 0.21597382564377984\n",
      "l2 norm of weights: 5.794918452472992\n",
      "---------------------\n",
      "Iteration Number: 6505\n",
      "Loss: 32.45546227391202\n",
      "l2 norm of gradients: 0.21595171499486998\n",
      "l2 norm of weights: 5.794841829156381\n",
      "---------------------\n",
      "Iteration Number: 6506\n",
      "Loss: 32.45312369849567\n",
      "l2 norm of gradients: 0.21592961034520233\n",
      "l2 norm of weights: 5.794765212339994\n",
      "---------------------\n",
      "Iteration Number: 6507\n",
      "Loss: 32.45078560226752\n",
      "l2 norm of gradients: 0.21590751169158587\n",
      "l2 norm of weights: 5.7946886020227\n",
      "---------------------\n",
      "Iteration Number: 6508\n",
      "Loss: 32.448447985049235\n",
      "l2 norm of gradients: 0.21588541903083222\n",
      "l2 norm of weights: 5.794611998203362\n",
      "---------------------\n",
      "Iteration Number: 6509\n",
      "Loss: 32.4461108466622\n",
      "l2 norm of gradients: 0.21586333235975574\n",
      "l2 norm of weights: 5.794535400880846\n",
      "---------------------\n",
      "Iteration Number: 6510\n",
      "Loss: 32.443774186928536\n",
      "l2 norm of gradients: 0.21584125167517326\n",
      "l2 norm of weights: 5.794458810054017\n",
      "---------------------\n",
      "Iteration Number: 6511\n",
      "Loss: 32.441438005667166\n",
      "l2 norm of gradients: 0.21581917697390454\n",
      "l2 norm of weights: 5.794382225721743\n",
      "---------------------\n",
      "Iteration Number: 6512\n",
      "Loss: 32.43910230269996\n",
      "l2 norm of gradients: 0.21579710825277176\n",
      "l2 norm of weights: 5.79430564788289\n",
      "---------------------\n",
      "Iteration Number: 6513\n",
      "Loss: 32.43676707784646\n",
      "l2 norm of gradients: 0.21577504550859986\n",
      "l2 norm of weights: 5.794229076536327\n",
      "---------------------\n",
      "Iteration Number: 6514\n",
      "Loss: 32.43443233093727\n",
      "l2 norm of gradients: 0.21575298873821638\n",
      "l2 norm of weights: 5.7941525116809185\n",
      "---------------------\n",
      "Iteration Number: 6515\n",
      "Loss: 32.43209806178531\n",
      "l2 norm of gradients: 0.21573093793845155\n",
      "l2 norm of weights: 5.794075953315534\n",
      "---------------------\n",
      "Iteration Number: 6516\n",
      "Loss: 32.42976427021346\n",
      "l2 norm of gradients: 0.2157088931061382\n",
      "l2 norm of weights: 5.793999401439042\n",
      "---------------------\n",
      "Iteration Number: 6517\n",
      "Loss: 32.42743095604738\n",
      "l2 norm of gradients: 0.21568685423811193\n",
      "l2 norm of weights: 5.793922856050313\n",
      "---------------------\n",
      "Iteration Number: 6518\n",
      "Loss: 32.42509811910558\n",
      "l2 norm of gradients: 0.2156648213312107\n",
      "l2 norm of weights: 5.793846317148212\n",
      "---------------------\n",
      "Iteration Number: 6519\n",
      "Loss: 32.42276575921119\n",
      "l2 norm of gradients: 0.2156427943822754\n",
      "l2 norm of weights: 5.793769784731612\n",
      "---------------------\n",
      "Iteration Number: 6520\n",
      "Loss: 32.420433876188795\n",
      "l2 norm of gradients: 0.2156207733881494\n",
      "l2 norm of weights: 5.793693258799382\n",
      "---------------------\n",
      "Iteration Number: 6521\n",
      "Loss: 32.41810246985226\n",
      "l2 norm of gradients: 0.21559875834567868\n",
      "l2 norm of weights: 5.793616739350392\n",
      "---------------------\n",
      "Iteration Number: 6522\n",
      "Loss: 32.41577154003978\n",
      "l2 norm of gradients: 0.21557674925171189\n",
      "l2 norm of weights: 5.793540226383512\n",
      "---------------------\n",
      "Iteration Number: 6523\n",
      "Loss: 32.413441086558166\n",
      "l2 norm of gradients: 0.2155547461031003\n",
      "l2 norm of weights: 5.793463719897615\n",
      "---------------------\n",
      "Iteration Number: 6524\n",
      "Loss: 32.4111111092387\n",
      "l2 norm of gradients: 0.21553274889669782\n",
      "l2 norm of weights: 5.793387219891571\n",
      "---------------------\n",
      "Iteration Number: 6525\n",
      "Loss: 32.40878160790123\n",
      "l2 norm of gradients: 0.21551075762936087\n",
      "l2 norm of weights: 5.793310726364252\n",
      "---------------------\n",
      "Iteration Number: 6526\n",
      "Loss: 32.40645258236943\n",
      "l2 norm of gradients: 0.21548877229794866\n",
      "l2 norm of weights: 5.79323423931453\n",
      "---------------------\n",
      "Iteration Number: 6527\n",
      "Loss: 32.40412403246922\n",
      "l2 norm of gradients: 0.21546679289932283\n",
      "l2 norm of weights: 5.793157758741279\n",
      "---------------------\n",
      "Iteration Number: 6528\n",
      "Loss: 32.40179595801607\n",
      "l2 norm of gradients: 0.2154448194303478\n",
      "l2 norm of weights: 5.79308128464337\n",
      "---------------------\n",
      "Iteration Number: 6529\n",
      "Loss: 32.39946835883886\n",
      "l2 norm of gradients: 0.21542285188789045\n",
      "l2 norm of weights: 5.793004817019678\n",
      "---------------------\n",
      "Iteration Number: 6530\n",
      "Loss: 32.397141234759964\n",
      "l2 norm of gradients: 0.21540089026882023\n",
      "l2 norm of weights: 5.792928355869077\n",
      "---------------------\n",
      "Iteration Number: 6531\n",
      "Loss: 32.39481458560466\n",
      "l2 norm of gradients: 0.21537893457000937\n",
      "l2 norm of weights: 5.79285190119044\n",
      "---------------------\n",
      "Iteration Number: 6532\n",
      "Loss: 32.39248841119041\n",
      "l2 norm of gradients: 0.21535698478833262\n",
      "l2 norm of weights: 5.792775452982641\n",
      "---------------------\n",
      "Iteration Number: 6533\n",
      "Loss: 32.39016271134852\n",
      "l2 norm of gradients: 0.21533504092066721\n",
      "l2 norm of weights: 5.792699011244558\n",
      "---------------------\n",
      "Iteration Number: 6534\n",
      "Loss: 32.38783748589723\n",
      "l2 norm of gradients: 0.21531310296389314\n",
      "l2 norm of weights: 5.792622575975063\n",
      "---------------------\n",
      "Iteration Number: 6535\n",
      "Loss: 32.38551273466513\n",
      "l2 norm of gradients: 0.21529117091489286\n",
      "l2 norm of weights: 5.7925461471730335\n",
      "---------------------\n",
      "Iteration Number: 6536\n",
      "Loss: 32.38318845747109\n",
      "l2 norm of gradients: 0.21526924477055137\n",
      "l2 norm of weights: 5.7924697248373445\n",
      "---------------------\n",
      "Iteration Number: 6537\n",
      "Loss: 32.38086465414079\n",
      "l2 norm of gradients: 0.21524732452775652\n",
      "l2 norm of weights: 5.792393308966873\n",
      "---------------------\n",
      "Iteration Number: 6538\n",
      "Loss: 32.37854132450177\n",
      "l2 norm of gradients: 0.21522541018339836\n",
      "l2 norm of weights: 5.792316899560497\n",
      "---------------------\n",
      "Iteration Number: 6539\n",
      "Loss: 32.376218468369956\n",
      "l2 norm of gradients: 0.21520350173436983\n",
      "l2 norm of weights: 5.792240496617092\n",
      "---------------------\n",
      "Iteration Number: 6540\n",
      "Loss: 32.373896085582494\n",
      "l2 norm of gradients: 0.2151815991775662\n",
      "l2 norm of weights: 5.792164100135536\n",
      "---------------------\n",
      "Iteration Number: 6541\n",
      "Loss: 32.3715741759494\n",
      "l2 norm of gradients: 0.2151597025098856\n",
      "l2 norm of weights: 5.792087710114708\n",
      "---------------------\n",
      "Iteration Number: 6542\n",
      "Loss: 32.36925273930807\n",
      "l2 norm of gradients: 0.21513781172822838\n",
      "l2 norm of weights: 5.792011326553486\n",
      "---------------------\n",
      "Iteration Number: 6543\n",
      "Loss: 32.36693177547411\n",
      "l2 norm of gradients: 0.21511592682949773\n",
      "l2 norm of weights: 5.791934949450748\n",
      "---------------------\n",
      "Iteration Number: 6544\n",
      "Loss: 32.364611284276265\n",
      "l2 norm of gradients: 0.21509404781059924\n",
      "l2 norm of weights: 5.791858578805374\n",
      "---------------------\n",
      "Iteration Number: 6545\n",
      "Loss: 32.36229126554124\n",
      "l2 norm of gradients: 0.21507217466844114\n",
      "l2 norm of weights: 5.7917822146162425\n",
      "---------------------\n",
      "Iteration Number: 6546\n",
      "Loss: 32.35997171908803\n",
      "l2 norm of gradients: 0.21505030739993417\n",
      "l2 norm of weights: 5.791705856882236\n",
      "---------------------\n",
      "Iteration Number: 6547\n",
      "Loss: 32.357652644748796\n",
      "l2 norm of gradients: 0.2150284460019917\n",
      "l2 norm of weights: 5.791629505602231\n",
      "---------------------\n",
      "Iteration Number: 6548\n",
      "Loss: 32.35533404234001\n",
      "l2 norm of gradients: 0.21500659047152956\n",
      "l2 norm of weights: 5.79155316077511\n",
      "---------------------\n",
      "Iteration Number: 6549\n",
      "Loss: 32.35301591169702\n",
      "l2 norm of gradients: 0.21498474080546612\n",
      "l2 norm of weights: 5.791476822399756\n",
      "---------------------\n",
      "Iteration Number: 6550\n",
      "Loss: 32.3506982526358\n",
      "l2 norm of gradients: 0.21496289700072238\n",
      "l2 norm of weights: 5.791400490475047\n",
      "---------------------\n",
      "Iteration Number: 6551\n",
      "Loss: 32.34838106499587\n",
      "l2 norm of gradients: 0.21494105905422187\n",
      "l2 norm of weights: 5.791324164999867\n",
      "---------------------\n",
      "Iteration Number: 6552\n",
      "Loss: 32.346064348586864\n",
      "l2 norm of gradients: 0.21491922696289054\n",
      "l2 norm of weights: 5.791247845973097\n",
      "---------------------\n",
      "Iteration Number: 6553\n",
      "Loss: 32.34374810324297\n",
      "l2 norm of gradients: 0.21489740072365698\n",
      "l2 norm of weights: 5.791171533393621\n",
      "---------------------\n",
      "Iteration Number: 6554\n",
      "Loss: 32.34143232878476\n",
      "l2 norm of gradients: 0.2148755803334524\n",
      "l2 norm of weights: 5.791095227260321\n",
      "---------------------\n",
      "Iteration Number: 6555\n",
      "Loss: 32.33911702504452\n",
      "l2 norm of gradients: 0.21485376578921028\n",
      "l2 norm of weights: 5.791018927572081\n",
      "---------------------\n",
      "Iteration Number: 6556\n",
      "Loss: 32.3368021918428\n",
      "l2 norm of gradients: 0.21483195708786687\n",
      "l2 norm of weights: 5.790942634327784\n",
      "---------------------\n",
      "Iteration Number: 6557\n",
      "Loss: 32.3344878290152\n",
      "l2 norm of gradients: 0.21481015422636082\n",
      "l2 norm of weights: 5.790866347526314\n",
      "---------------------\n",
      "Iteration Number: 6558\n",
      "Loss: 32.33217393637395\n",
      "l2 norm of gradients: 0.21478835720163333\n",
      "l2 norm of weights: 5.790790067166557\n",
      "---------------------\n",
      "Iteration Number: 6559\n",
      "Loss: 32.32986051375436\n",
      "l2 norm of gradients: 0.21476656601062818\n",
      "l2 norm of weights: 5.790713793247396\n",
      "---------------------\n",
      "Iteration Number: 6560\n",
      "Loss: 32.32754756098284\n",
      "l2 norm of gradients: 0.2147447806502915\n",
      "l2 norm of weights: 5.790637525767717\n",
      "---------------------\n",
      "Iteration Number: 6561\n",
      "Loss: 32.32523507788176\n",
      "l2 norm of gradients: 0.21472300111757217\n",
      "l2 norm of weights: 5.790561264726405\n",
      "---------------------\n",
      "Iteration Number: 6562\n",
      "Loss: 32.32292306428192\n",
      "l2 norm of gradients: 0.21470122740942138\n",
      "l2 norm of weights: 5.790485010122348\n",
      "---------------------\n",
      "Iteration Number: 6563\n",
      "Loss: 32.32061152000599\n",
      "l2 norm of gradients: 0.2146794595227929\n",
      "l2 norm of weights: 5.7904087619544296\n",
      "---------------------\n",
      "Iteration Number: 6564\n",
      "Loss: 32.31830044488436\n",
      "l2 norm of gradients: 0.214657697454643\n",
      "l2 norm of weights: 5.79033252022154\n",
      "---------------------\n",
      "Iteration Number: 6565\n",
      "Loss: 32.31598983874368\n",
      "l2 norm of gradients: 0.21463594120193055\n",
      "l2 norm of weights: 5.790256284922562\n",
      "---------------------\n",
      "Iteration Number: 6566\n",
      "Loss: 32.313679701408766\n",
      "l2 norm of gradients: 0.2146141907616167\n",
      "l2 norm of weights: 5.7901800560563865\n",
      "---------------------\n",
      "Iteration Number: 6567\n",
      "Loss: 32.31137003270814\n",
      "l2 norm of gradients: 0.21459244613066525\n",
      "l2 norm of weights: 5.7901038336219\n",
      "---------------------\n",
      "Iteration Number: 6568\n",
      "Loss: 32.30906083247095\n",
      "l2 norm of gradients: 0.21457070730604255\n",
      "l2 norm of weights: 5.790027617617992\n",
      "---------------------\n",
      "Iteration Number: 6569\n",
      "Loss: 32.30675210052024\n",
      "l2 norm of gradients: 0.21454897428471725\n",
      "l2 norm of weights: 5.7899514080435495\n",
      "---------------------\n",
      "Iteration Number: 6570\n",
      "Loss: 32.3044438366875\n",
      "l2 norm of gradients: 0.21452724706366066\n",
      "l2 norm of weights: 5.789875204897461\n",
      "---------------------\n",
      "Iteration Number: 6571\n",
      "Loss: 32.302136040798814\n",
      "l2 norm of gradients: 0.21450552563984646\n",
      "l2 norm of weights: 5.789799008178618\n",
      "---------------------\n",
      "Iteration Number: 6572\n",
      "Loss: 32.29982871268206\n",
      "l2 norm of gradients: 0.2144838100102509\n",
      "l2 norm of weights: 5.7897228178859095\n",
      "---------------------\n",
      "Iteration Number: 6573\n",
      "Loss: 32.29752185216083\n",
      "l2 norm of gradients: 0.2144621001718527\n",
      "l2 norm of weights: 5.789646634018226\n",
      "---------------------\n",
      "Iteration Number: 6574\n",
      "Loss: 32.295215459071414\n",
      "l2 norm of gradients: 0.214440396121633\n",
      "l2 norm of weights: 5.789570456574457\n",
      "---------------------\n",
      "Iteration Number: 6575\n",
      "Loss: 32.29290953323482\n",
      "l2 norm of gradients: 0.2144186978565754\n",
      "l2 norm of weights: 5.789494285553492\n",
      "---------------------\n",
      "Iteration Number: 6576\n",
      "Loss: 32.29060407448265\n",
      "l2 norm of gradients: 0.21439700537366607\n",
      "l2 norm of weights: 5.7894181209542275\n",
      "---------------------\n",
      "Iteration Number: 6577\n",
      "Loss: 32.28829908264416\n",
      "l2 norm of gradients: 0.21437531866989357\n",
      "l2 norm of weights: 5.789341962775549\n",
      "---------------------\n",
      "Iteration Number: 6578\n",
      "Loss: 32.28599455754142\n",
      "l2 norm of gradients: 0.21435363774224894\n",
      "l2 norm of weights: 5.789265811016354\n",
      "---------------------\n",
      "Iteration Number: 6579\n",
      "Loss: 32.283690499009715\n",
      "l2 norm of gradients: 0.21433196258772574\n",
      "l2 norm of weights: 5.78918966567553\n",
      "---------------------\n",
      "Iteration Number: 6580\n",
      "Loss: 32.281386906873635\n",
      "l2 norm of gradients: 0.21431029320331985\n",
      "l2 norm of weights: 5.789113526751972\n",
      "---------------------\n",
      "Iteration Number: 6581\n",
      "Loss: 32.27908378096426\n",
      "l2 norm of gradients: 0.2142886295860298\n",
      "l2 norm of weights: 5.789037394244573\n",
      "---------------------\n",
      "Iteration Number: 6582\n",
      "Loss: 32.276781121104975\n",
      "l2 norm of gradients: 0.21426697173285641\n",
      "l2 norm of weights: 5.788961268152228\n",
      "---------------------\n",
      "Iteration Number: 6583\n",
      "Loss: 32.27447892713139\n",
      "l2 norm of gradients: 0.2142453196408031\n",
      "l2 norm of weights: 5.788885148473828\n",
      "---------------------\n",
      "Iteration Number: 6584\n",
      "Loss: 32.272177198870004\n",
      "l2 norm of gradients: 0.21422367330687553\n",
      "l2 norm of weights: 5.788809035208271\n",
      "---------------------\n",
      "Iteration Number: 6585\n",
      "Loss: 32.26987593615247\n",
      "l2 norm of gradients: 0.21420203272808203\n",
      "l2 norm of weights: 5.788732928354447\n",
      "---------------------\n",
      "Iteration Number: 6586\n",
      "Loss: 32.267575138800375\n",
      "l2 norm of gradients: 0.2141803979014332\n",
      "l2 norm of weights: 5.788656827911255\n",
      "---------------------\n",
      "Iteration Number: 6587\n",
      "Loss: 32.26527480664734\n",
      "l2 norm of gradients: 0.21415876882394227\n",
      "l2 norm of weights: 5.788580733877588\n",
      "---------------------\n",
      "Iteration Number: 6588\n",
      "Loss: 32.262974939523126\n",
      "l2 norm of gradients: 0.21413714549262472\n",
      "l2 norm of weights: 5.788504646252342\n",
      "---------------------\n",
      "Iteration Number: 6589\n",
      "Loss: 32.26067553725809\n",
      "l2 norm of gradients: 0.21411552790449856\n",
      "l2 norm of weights: 5.788428565034414\n",
      "---------------------\n",
      "Iteration Number: 6590\n",
      "Loss: 32.25837659967565\n",
      "l2 norm of gradients: 0.2140939160565842\n",
      "l2 norm of weights: 5.7883524902227\n",
      "---------------------\n",
      "Iteration Number: 6591\n",
      "Loss: 32.25607812661377\n",
      "l2 norm of gradients: 0.21407230994590448\n",
      "l2 norm of weights: 5.7882764218160965\n",
      "---------------------\n",
      "Iteration Number: 6592\n",
      "Loss: 32.253780117896824\n",
      "l2 norm of gradients: 0.21405070956948474\n",
      "l2 norm of weights: 5.7882003598135014\n",
      "---------------------\n",
      "Iteration Number: 6593\n",
      "Loss: 32.251482573362324\n",
      "l2 norm of gradients: 0.21402911492435264\n",
      "l2 norm of weights: 5.788124304213813\n",
      "---------------------\n",
      "Iteration Number: 6594\n",
      "Loss: 32.24918549282541\n",
      "l2 norm of gradients: 0.2140075260075384\n",
      "l2 norm of weights: 5.788048255015928\n",
      "---------------------\n",
      "Iteration Number: 6595\n",
      "Loss: 32.24688887612924\n",
      "l2 norm of gradients: 0.21398594281607441\n",
      "l2 norm of weights: 5.787972212218745\n",
      "---------------------\n",
      "Iteration Number: 6596\n",
      "Loss: 32.24459272310006\n",
      "l2 norm of gradients: 0.2139643653469958\n",
      "l2 norm of weights: 5.787896175821163\n",
      "---------------------\n",
      "Iteration Number: 6597\n",
      "Loss: 32.24229703356609\n",
      "l2 norm of gradients: 0.2139427935973398\n",
      "l2 norm of weights: 5.787820145822081\n",
      "---------------------\n",
      "Iteration Number: 6598\n",
      "Loss: 32.24000180735725\n",
      "l2 norm of gradients: 0.21392122756414636\n",
      "l2 norm of weights: 5.787744122220397\n",
      "---------------------\n",
      "Iteration Number: 6599\n",
      "Loss: 32.23770704431079\n",
      "l2 norm of gradients: 0.2138996672444576\n",
      "l2 norm of weights: 5.787668105015013\n",
      "---------------------\n",
      "Iteration Number: 6600\n",
      "Loss: 32.23541274424936\n",
      "l2 norm of gradients: 0.21387811263531814\n",
      "l2 norm of weights: 5.787592094204828\n",
      "---------------------\n",
      "Iteration Number: 6601\n",
      "Loss: 32.23311890700642\n",
      "l2 norm of gradients: 0.21385656373377498\n",
      "l2 norm of weights: 5.787516089788743\n",
      "---------------------\n",
      "Iteration Number: 6602\n",
      "Loss: 32.23082553240984\n",
      "l2 norm of gradients: 0.21383502053687747\n",
      "l2 norm of weights: 5.78744009176566\n",
      "---------------------\n",
      "Iteration Number: 6603\n",
      "Loss: 32.2285326202977\n",
      "l2 norm of gradients: 0.21381348304167752\n",
      "l2 norm of weights: 5.787364100134478\n",
      "---------------------\n",
      "Iteration Number: 6604\n",
      "Loss: 32.22624017049267\n",
      "l2 norm of gradients: 0.2137919512452293\n",
      "l2 norm of weights: 5.787288114894101\n",
      "---------------------\n",
      "Iteration Number: 6605\n",
      "Loss: 32.22394818283073\n",
      "l2 norm of gradients: 0.21377042514458947\n",
      "l2 norm of weights: 5.787212136043428\n",
      "---------------------\n",
      "Iteration Number: 6606\n",
      "Loss: 32.22165665714479\n",
      "l2 norm of gradients: 0.21374890473681687\n",
      "l2 norm of weights: 5.787136163581365\n",
      "---------------------\n",
      "Iteration Number: 6607\n",
      "Loss: 32.21936559325808\n",
      "l2 norm of gradients: 0.213727390018973\n",
      "l2 norm of weights: 5.787060197506812\n",
      "---------------------\n",
      "Iteration Number: 6608\n",
      "Loss: 32.21707499101093\n",
      "l2 norm of gradients: 0.21370588098812152\n",
      "l2 norm of weights: 5.786984237818674\n",
      "---------------------\n",
      "Iteration Number: 6609\n",
      "Loss: 32.21478485022797\n",
      "l2 norm of gradients: 0.2136843776413287\n",
      "l2 norm of weights: 5.786908284515853\n",
      "---------------------\n",
      "Iteration Number: 6610\n",
      "Loss: 32.21249517074526\n",
      "l2 norm of gradients: 0.21366287997566294\n",
      "l2 norm of weights: 5.786832337597255\n",
      "---------------------\n",
      "Iteration Number: 6611\n",
      "Loss: 32.21020595238994\n",
      "l2 norm of gradients: 0.21364138798819524\n",
      "l2 norm of weights: 5.786756397061781\n",
      "---------------------\n",
      "Iteration Number: 6612\n",
      "Loss: 32.20791719499968\n",
      "l2 norm of gradients: 0.2136199016759988\n",
      "l2 norm of weights: 5.786680462908339\n",
      "---------------------\n",
      "Iteration Number: 6613\n",
      "Loss: 32.20562889839986\n",
      "l2 norm of gradients: 0.21359842103614923\n",
      "l2 norm of weights: 5.786604535135831\n",
      "---------------------\n",
      "Iteration Number: 6614\n",
      "Loss: 32.20334106242897\n",
      "l2 norm of gradients: 0.2135769460657246\n",
      "l2 norm of weights: 5.786528613743164\n",
      "---------------------\n",
      "Iteration Number: 6615\n",
      "Loss: 32.201053686911344\n",
      "l2 norm of gradients: 0.21355547676180528\n",
      "l2 norm of weights: 5.786452698729244\n",
      "---------------------\n",
      "Iteration Number: 6616\n",
      "Loss: 32.19876677168307\n",
      "l2 norm of gradients: 0.21353401312147402\n",
      "l2 norm of weights: 5.786376790092977\n",
      "---------------------\n",
      "Iteration Number: 6617\n",
      "Loss: 32.196480316581614\n",
      "l2 norm of gradients: 0.21351255514181589\n",
      "l2 norm of weights: 5.786300887833267\n",
      "---------------------\n",
      "Iteration Number: 6618\n",
      "Loss: 32.19419432142978\n",
      "l2 norm of gradients: 0.21349110281991837\n",
      "l2 norm of weights: 5.786224991949024\n",
      "---------------------\n",
      "Iteration Number: 6619\n",
      "Loss: 32.191908786063635\n",
      "l2 norm of gradients: 0.21346965615287125\n",
      "l2 norm of weights: 5.786149102439154\n",
      "---------------------\n",
      "Iteration Number: 6620\n",
      "Loss: 32.18962371031809\n",
      "l2 norm of gradients: 0.21344821513776674\n",
      "l2 norm of weights: 5.786073219302565\n",
      "---------------------\n",
      "Iteration Number: 6621\n",
      "Loss: 32.187339094025845\n",
      "l2 norm of gradients: 0.2134267797716993\n",
      "l2 norm of weights: 5.785997342538164\n",
      "---------------------\n",
      "Iteration Number: 6622\n",
      "Loss: 32.1850549370135\n",
      "l2 norm of gradients: 0.21340535005176584\n",
      "l2 norm of weights: 5.785921472144859\n",
      "---------------------\n",
      "Iteration Number: 6623\n",
      "Loss: 32.18277123912154\n",
      "l2 norm of gradients: 0.21338392597506556\n",
      "l2 norm of weights: 5.7858456081215595\n",
      "---------------------\n",
      "Iteration Number: 6624\n",
      "Loss: 32.180488000176894\n",
      "l2 norm of gradients: 0.21336250753869998\n",
      "l2 norm of weights: 5.785769750467175\n",
      "---------------------\n",
      "Iteration Number: 6625\n",
      "Loss: 32.17820522001566\n",
      "l2 norm of gradients: 0.213341094739773\n",
      "l2 norm of weights: 5.785693899180614\n",
      "---------------------\n",
      "Iteration Number: 6626\n",
      "Loss: 32.17592289846926\n",
      "l2 norm of gradients: 0.2133196875753909\n",
      "l2 norm of weights: 5.785618054260786\n",
      "---------------------\n",
      "Iteration Number: 6627\n",
      "Loss: 32.173641035375\n",
      "l2 norm of gradients: 0.2132982860426622\n",
      "l2 norm of weights: 5.785542215706601\n",
      "---------------------\n",
      "Iteration Number: 6628\n",
      "Loss: 32.17135963056066\n",
      "l2 norm of gradients: 0.2132768901386978\n",
      "l2 norm of weights: 5.785466383516971\n",
      "---------------------\n",
      "Iteration Number: 6629\n",
      "Loss: 32.16907868386434\n",
      "l2 norm of gradients: 0.2132554998606109\n",
      "l2 norm of weights: 5.785390557690805\n",
      "---------------------\n",
      "Iteration Number: 6630\n",
      "Loss: 32.16679819511309\n",
      "l2 norm of gradients: 0.2132341152055171\n",
      "l2 norm of weights: 5.785314738227015\n",
      "---------------------\n",
      "Iteration Number: 6631\n",
      "Loss: 32.1645181641482\n",
      "l2 norm of gradients: 0.21321273617053418\n",
      "l2 norm of weights: 5.785238925124513\n",
      "---------------------\n",
      "Iteration Number: 6632\n",
      "Loss: 32.16223859079684\n",
      "l2 norm of gradients: 0.21319136275278244\n",
      "l2 norm of weights: 5.78516311838221\n",
      "---------------------\n",
      "Iteration Number: 6633\n",
      "Loss: 32.159959474895324\n",
      "l2 norm of gradients: 0.21316999494938432\n",
      "l2 norm of weights: 5.785087317999018\n",
      "---------------------\n",
      "Iteration Number: 6634\n",
      "Loss: 32.157680816279466\n",
      "l2 norm of gradients: 0.2131486327574647\n",
      "l2 norm of weights: 5.785011523973851\n",
      "---------------------\n",
      "Iteration Number: 6635\n",
      "Loss: 32.15540261477982\n",
      "l2 norm of gradients: 0.21312727617415073\n",
      "l2 norm of weights: 5.78493573630562\n",
      "---------------------\n",
      "Iteration Number: 6636\n",
      "Loss: 32.15312487023229\n",
      "l2 norm of gradients: 0.21310592519657182\n",
      "l2 norm of weights: 5.784859954993241\n",
      "---------------------\n",
      "Iteration Number: 6637\n",
      "Loss: 32.15084758247322\n",
      "l2 norm of gradients: 0.21308457982185977\n",
      "l2 norm of weights: 5.784784180035626\n",
      "---------------------\n",
      "Iteration Number: 6638\n",
      "Loss: 32.14857075133057\n",
      "l2 norm of gradients: 0.21306324004714855\n",
      "l2 norm of weights: 5.784708411431689\n",
      "---------------------\n",
      "Iteration Number: 6639\n",
      "Loss: 32.14629437664807\n",
      "l2 norm of gradients: 0.21304190586957467\n",
      "l2 norm of weights: 5.784632649180345\n",
      "---------------------\n",
      "Iteration Number: 6640\n",
      "Loss: 32.144018458247004\n",
      "l2 norm of gradients: 0.21302057728627674\n",
      "l2 norm of weights: 5.7845568932805085\n",
      "---------------------\n",
      "Iteration Number: 6641\n",
      "Loss: 32.14174299597745\n",
      "l2 norm of gradients: 0.2129992542943957\n",
      "l2 norm of weights: 5.784481143731095\n",
      "---------------------\n",
      "Iteration Number: 6642\n",
      "Loss: 32.139467989662535\n",
      "l2 norm of gradients: 0.21297793689107486\n",
      "l2 norm of weights: 5.784405400531019\n",
      "---------------------\n",
      "Iteration Number: 6643\n",
      "Loss: 32.137193439135885\n",
      "l2 norm of gradients: 0.2129566250734597\n",
      "l2 norm of weights: 5.784329663679197\n",
      "---------------------\n",
      "Iteration Number: 6644\n",
      "Loss: 32.13491934424222\n",
      "l2 norm of gradients: 0.21293531883869815\n",
      "l2 norm of weights: 5.784253933174546\n",
      "---------------------\n",
      "Iteration Number: 6645\n",
      "Loss: 32.13264570481226\n",
      "l2 norm of gradients: 0.21291401818394035\n",
      "l2 norm of weights: 5.784178209015982\n",
      "---------------------\n",
      "Iteration Number: 6646\n",
      "Loss: 32.130372520677824\n",
      "l2 norm of gradients: 0.21289272310633867\n",
      "l2 norm of weights: 5.784102491202422\n",
      "---------------------\n",
      "Iteration Number: 6647\n",
      "Loss: 32.12809979167607\n",
      "l2 norm of gradients: 0.21287143360304786\n",
      "l2 norm of weights: 5.784026779732784\n",
      "---------------------\n",
      "Iteration Number: 6648\n",
      "Loss: 32.125827517643685\n",
      "l2 norm of gradients: 0.2128501496712249\n",
      "l2 norm of weights: 5.783951074605983\n",
      "---------------------\n",
      "Iteration Number: 6649\n",
      "Loss: 32.123555698410954\n",
      "l2 norm of gradients: 0.212828871308029\n",
      "l2 norm of weights: 5.78387537582094\n",
      "---------------------\n",
      "Iteration Number: 6650\n",
      "Loss: 32.12128433382277\n",
      "l2 norm of gradients: 0.2128075985106217\n",
      "l2 norm of weights: 5.783799683376572\n",
      "---------------------\n",
      "Iteration Number: 6651\n",
      "Loss: 32.119013423702576\n",
      "l2 norm of gradients: 0.21278633127616686\n",
      "l2 norm of weights: 5.783723997271799\n",
      "---------------------\n",
      "Iteration Number: 6652\n",
      "Loss: 32.11674296789329\n",
      "l2 norm of gradients: 0.2127650696018305\n",
      "l2 norm of weights: 5.783648317505538\n",
      "---------------------\n",
      "Iteration Number: 6653\n",
      "Loss: 32.11447296623393\n",
      "l2 norm of gradients: 0.212743813484781\n",
      "l2 norm of weights: 5.783572644076711\n",
      "---------------------\n",
      "Iteration Number: 6654\n",
      "Loss: 32.11220341855099\n",
      "l2 norm of gradients: 0.212722562922189\n",
      "l2 norm of weights: 5.783496976984234\n",
      "---------------------\n",
      "Iteration Number: 6655\n",
      "Loss: 32.10993432468782\n",
      "l2 norm of gradients: 0.2127013179112273\n",
      "l2 norm of weights: 5.783421316227032\n",
      "---------------------\n",
      "Iteration Number: 6656\n",
      "Loss: 32.10766568447785\n",
      "l2 norm of gradients: 0.21268007844907105\n",
      "l2 norm of weights: 5.783345661804022\n",
      "---------------------\n",
      "Iteration Number: 6657\n",
      "Loss: 32.105397497755575\n",
      "l2 norm of gradients: 0.21265884453289768\n",
      "l2 norm of weights: 5.7832700137141275\n",
      "---------------------\n",
      "Iteration Number: 6658\n",
      "Loss: 32.10312976435879\n",
      "l2 norm of gradients: 0.21263761615988674\n",
      "l2 norm of weights: 5.783194371956267\n",
      "---------------------\n",
      "Iteration Number: 6659\n",
      "Loss: 32.100862484121464\n",
      "l2 norm of gradients: 0.21261639332722024\n",
      "l2 norm of weights: 5.783118736529364\n",
      "---------------------\n",
      "Iteration Number: 6660\n",
      "Loss: 32.0985956568882\n",
      "l2 norm of gradients: 0.21259517603208228\n",
      "l2 norm of weights: 5.78304310743234\n",
      "---------------------\n",
      "Iteration Number: 6661\n",
      "Loss: 32.09632928248106\n",
      "l2 norm of gradients: 0.21257396427165914\n",
      "l2 norm of weights: 5.782967484664116\n",
      "---------------------\n",
      "Iteration Number: 6662\n",
      "Loss: 32.09406336074903\n",
      "l2 norm of gradients: 0.21255275804313967\n",
      "l2 norm of weights: 5.782891868223618\n",
      "---------------------\n",
      "Iteration Number: 6663\n",
      "Loss: 32.09179789152585\n",
      "l2 norm of gradients: 0.2125315573437146\n",
      "l2 norm of weights: 5.782816258109765\n",
      "---------------------\n",
      "Iteration Number: 6664\n",
      "Loss: 32.08953287464493\n",
      "l2 norm of gradients: 0.21251036217057712\n",
      "l2 norm of weights: 5.782740654321484\n",
      "---------------------\n",
      "Iteration Number: 6665\n",
      "Loss: 32.08726830994347\n",
      "l2 norm of gradients: 0.21248917252092245\n",
      "l2 norm of weights: 5.782665056857697\n",
      "---------------------\n",
      "Iteration Number: 6666\n",
      "Loss: 32.085004197263\n",
      "l2 norm of gradients: 0.21246798839194828\n",
      "l2 norm of weights: 5.782589465717328\n",
      "---------------------\n",
      "Iteration Number: 6667\n",
      "Loss: 32.082740536436546\n",
      "l2 norm of gradients: 0.21244680978085445\n",
      "l2 norm of weights: 5.782513880899301\n",
      "---------------------\n",
      "Iteration Number: 6668\n",
      "Loss: 32.08047732730007\n",
      "l2 norm of gradients: 0.21242563668484296\n",
      "l2 norm of weights: 5.782438302402541\n",
      "---------------------\n",
      "Iteration Number: 6669\n",
      "Loss: 32.078214569692925\n",
      "l2 norm of gradients: 0.2124044691011181\n",
      "l2 norm of weights: 5.782362730225975\n",
      "---------------------\n",
      "Iteration Number: 6670\n",
      "Loss: 32.07595226345165\n",
      "l2 norm of gradients: 0.21238330702688638\n",
      "l2 norm of weights: 5.782287164368527\n",
      "---------------------\n",
      "Iteration Number: 6671\n",
      "Loss: 32.073690408418976\n",
      "l2 norm of gradients: 0.2123621504593565\n",
      "l2 norm of weights: 5.782211604829123\n",
      "---------------------\n",
      "Iteration Number: 6672\n",
      "Loss: 32.07142900442249\n",
      "l2 norm of gradients: 0.21234099939573942\n",
      "l2 norm of weights: 5.782136051606688\n",
      "---------------------\n",
      "Iteration Number: 6673\n",
      "Loss: 32.069168051308885\n",
      "l2 norm of gradients: 0.21231985383324825\n",
      "l2 norm of weights: 5.782060504700152\n",
      "---------------------\n",
      "Iteration Number: 6674\n",
      "Loss: 32.06690754891072\n",
      "l2 norm of gradients: 0.21229871376909842\n",
      "l2 norm of weights: 5.781984964108439\n",
      "---------------------\n",
      "Iteration Number: 6675\n",
      "Loss: 32.06464749706375\n",
      "l2 norm of gradients: 0.21227757920050747\n",
      "l2 norm of weights: 5.781909429830477\n",
      "---------------------\n",
      "Iteration Number: 6676\n",
      "Loss: 32.06238789561561\n",
      "l2 norm of gradients: 0.21225645012469518\n",
      "l2 norm of weights: 5.781833901865194\n",
      "---------------------\n",
      "Iteration Number: 6677\n",
      "Loss: 32.060128744392735\n",
      "l2 norm of gradients: 0.2122353265388837\n",
      "l2 norm of weights: 5.781758380211517\n",
      "---------------------\n",
      "Iteration Number: 6678\n",
      "Loss: 32.057870043237784\n",
      "l2 norm of gradients: 0.21221420844029698\n",
      "l2 norm of weights: 5.781682864868376\n",
      "---------------------\n",
      "Iteration Number: 6679\n",
      "Loss: 32.055611791989676\n",
      "l2 norm of gradients: 0.21219309582616164\n",
      "l2 norm of weights: 5.781607355834699\n",
      "---------------------\n",
      "Iteration Number: 6680\n",
      "Loss: 32.05335399048596\n",
      "l2 norm of gradients: 0.21217198869370615\n",
      "l2 norm of weights: 5.781531853109415\n",
      "---------------------\n",
      "Iteration Number: 6681\n",
      "Loss: 32.05109663856622\n",
      "l2 norm of gradients: 0.21215088704016133\n",
      "l2 norm of weights: 5.781456356691453\n",
      "---------------------\n",
      "Iteration Number: 6682\n",
      "Loss: 32.04883973606788\n",
      "l2 norm of gradients: 0.2121297908627603\n",
      "l2 norm of weights: 5.781380866579743\n",
      "---------------------\n",
      "Iteration Number: 6683\n",
      "Loss: 32.04658328282901\n",
      "l2 norm of gradients: 0.21210870015873812\n",
      "l2 norm of weights: 5.7813053827732155\n",
      "---------------------\n",
      "Iteration Number: 6684\n",
      "Loss: 32.04432727869097\n",
      "l2 norm of gradients: 0.2120876149253322\n",
      "l2 norm of weights: 5.7812299052708\n",
      "---------------------\n",
      "Iteration Number: 6685\n",
      "Loss: 32.04207172348587\n",
      "l2 norm of gradients: 0.21206653515978205\n",
      "l2 norm of weights: 5.7811544340714285\n",
      "---------------------\n",
      "Iteration Number: 6686\n",
      "Loss: 32.03981661705909\n",
      "l2 norm of gradients: 0.21204546085932952\n",
      "l2 norm of weights: 5.781078969174032\n",
      "---------------------\n",
      "Iteration Number: 6687\n",
      "Loss: 32.03756195924606\n",
      "l2 norm of gradients: 0.2120243920212185\n",
      "l2 norm of weights: 5.78100351057754\n",
      "---------------------\n",
      "Iteration Number: 6688\n",
      "Loss: 32.03530774988509\n",
      "l2 norm of gradients: 0.21200332864269508\n",
      "l2 norm of weights: 5.780928058280888\n",
      "---------------------\n",
      "Iteration Number: 6689\n",
      "Loss: 32.033053988821614\n",
      "l2 norm of gradients: 0.21198227072100762\n",
      "l2 norm of weights: 5.780852612283006\n",
      "---------------------\n",
      "Iteration Number: 6690\n",
      "Loss: 32.030800675887214\n",
      "l2 norm of gradients: 0.21196121825340644\n",
      "l2 norm of weights: 5.780777172582826\n",
      "---------------------\n",
      "Iteration Number: 6691\n",
      "Loss: 32.028547810923\n",
      "l2 norm of gradients: 0.21194017123714431\n",
      "l2 norm of weights: 5.780701739179282\n",
      "---------------------\n",
      "Iteration Number: 6692\n",
      "Loss: 32.02629539376909\n",
      "l2 norm of gradients: 0.21191912966947601\n",
      "l2 norm of weights: 5.780626312071307\n",
      "---------------------\n",
      "Iteration Number: 6693\n",
      "Loss: 32.02404342426775\n",
      "l2 norm of gradients: 0.21189809354765848\n",
      "l2 norm of weights: 5.780550891257835\n",
      "---------------------\n",
      "Iteration Number: 6694\n",
      "Loss: 32.021791902252964\n",
      "l2 norm of gradients: 0.21187706286895083\n",
      "l2 norm of weights: 5.7804754767378\n",
      "---------------------\n",
      "Iteration Number: 6695\n",
      "Loss: 32.01954082757212\n",
      "l2 norm of gradients: 0.2118560376306144\n",
      "l2 norm of weights: 5.7804000685101355\n",
      "---------------------\n",
      "Iteration Number: 6696\n",
      "Loss: 32.017290200059335\n",
      "l2 norm of gradients: 0.21183501782991265\n",
      "l2 norm of weights: 5.780324666573777\n",
      "---------------------\n",
      "Iteration Number: 6697\n",
      "Loss: 32.01504001955133\n",
      "l2 norm of gradients: 0.21181400346411117\n",
      "l2 norm of weights: 5.780249270927659\n",
      "---------------------\n",
      "Iteration Number: 6698\n",
      "Loss: 32.01279028589271\n",
      "l2 norm of gradients: 0.21179299453047779\n",
      "l2 norm of weights: 5.780173881570717\n",
      "---------------------\n",
      "Iteration Number: 6699\n",
      "Loss: 32.0105409989249\n",
      "l2 norm of gradients: 0.21177199102628233\n",
      "l2 norm of weights: 5.7800984985018875\n",
      "---------------------\n",
      "Iteration Number: 6700\n",
      "Loss: 32.008292158482945\n",
      "l2 norm of gradients: 0.21175099294879696\n",
      "l2 norm of weights: 5.780023121720105\n",
      "---------------------\n",
      "Iteration Number: 6701\n",
      "Loss: 32.00604376441095\n",
      "l2 norm of gradients: 0.21173000029529587\n",
      "l2 norm of weights: 5.779947751224307\n",
      "---------------------\n",
      "Iteration Number: 6702\n",
      "Loss: 32.00379581654638\n",
      "l2 norm of gradients: 0.21170901306305548\n",
      "l2 norm of weights: 5.77987238701343\n",
      "---------------------\n",
      "Iteration Number: 6703\n",
      "Loss: 32.00154831473293\n",
      "l2 norm of gradients: 0.21168803124935423\n",
      "l2 norm of weights: 5.779797029086412\n",
      "---------------------\n",
      "Iteration Number: 6704\n",
      "Loss: 31.999301258812427\n",
      "l2 norm of gradients: 0.21166705485147275\n",
      "l2 norm of weights: 5.779721677442189\n",
      "---------------------\n",
      "Iteration Number: 6705\n",
      "Loss: 31.997054648613243\n",
      "l2 norm of gradients: 0.21164608386669392\n",
      "l2 norm of weights: 5.779646332079699\n",
      "---------------------\n",
      "Iteration Number: 6706\n",
      "Loss: 31.994808483994685\n",
      "l2 norm of gradients: 0.21162511829230266\n",
      "l2 norm of weights: 5.77957099299788\n",
      "---------------------\n",
      "Iteration Number: 6707\n",
      "Loss: 31.99256276478306\n",
      "l2 norm of gradients: 0.21160415812558594\n",
      "l2 norm of weights: 5.779495660195673\n",
      "---------------------\n",
      "Iteration Number: 6708\n",
      "Loss: 31.99031749082728\n",
      "l2 norm of gradients: 0.21158320336383302\n",
      "l2 norm of weights: 5.7794203336720145\n",
      "---------------------\n",
      "Iteration Number: 6709\n",
      "Loss: 31.98807266195865\n",
      "l2 norm of gradients: 0.21156225400433518\n",
      "l2 norm of weights: 5.779345013425844\n",
      "---------------------\n",
      "Iteration Number: 6710\n",
      "Loss: 31.98582827803178\n",
      "l2 norm of gradients: 0.2115413100443859\n",
      "l2 norm of weights: 5.7792696994561\n",
      "---------------------\n",
      "Iteration Number: 6711\n",
      "Loss: 31.983584338873484\n",
      "l2 norm of gradients: 0.21152037148128072\n",
      "l2 norm of weights: 5.779194391761725\n",
      "---------------------\n",
      "Iteration Number: 6712\n",
      "Loss: 31.981340844337648\n",
      "l2 norm of gradients: 0.2114994383123174\n",
      "l2 norm of weights: 5.779119090341657\n",
      "---------------------\n",
      "Iteration Number: 6713\n",
      "Loss: 31.979097794257953\n",
      "l2 norm of gradients: 0.2114785105347957\n",
      "l2 norm of weights: 5.779043795194838\n",
      "---------------------\n",
      "Iteration Number: 6714\n",
      "Loss: 31.976855188475852\n",
      "l2 norm of gradients: 0.21145758814601753\n",
      "l2 norm of weights: 5.778968506320209\n",
      "---------------------\n",
      "Iteration Number: 6715\n",
      "Loss: 31.974613026835858\n",
      "l2 norm of gradients: 0.21143667114328696\n",
      "l2 norm of weights: 5.778893223716709\n",
      "---------------------\n",
      "Iteration Number: 6716\n",
      "Loss: 31.972371309175863\n",
      "l2 norm of gradients: 0.21141575952391017\n",
      "l2 norm of weights: 5.778817947383282\n",
      "---------------------\n",
      "Iteration Number: 6717\n",
      "Loss: 31.970130035342976\n",
      "l2 norm of gradients: 0.2113948532851953\n",
      "l2 norm of weights: 5.77874267731887\n",
      "---------------------\n",
      "Iteration Number: 6718\n",
      "Loss: 31.96788920517227\n",
      "l2 norm of gradients: 0.2113739524244529\n",
      "l2 norm of weights: 5.778667413522413\n",
      "---------------------\n",
      "Iteration Number: 6719\n",
      "Loss: 31.965648818513717\n",
      "l2 norm of gradients: 0.21135305693899525\n",
      "l2 norm of weights: 5.778592155992857\n",
      "---------------------\n",
      "Iteration Number: 6720\n",
      "Loss: 31.9634088751995\n",
      "l2 norm of gradients: 0.21133216682613712\n",
      "l2 norm of weights: 5.778516904729142\n",
      "---------------------\n",
      "Iteration Number: 6721\n",
      "Loss: 31.961169375078946\n",
      "l2 norm of gradients: 0.2113112820831951\n",
      "l2 norm of weights: 5.778441659730213\n",
      "---------------------\n",
      "Iteration Number: 6722\n",
      "Loss: 31.9589303179923\n",
      "l2 norm of gradients: 0.21129040270748792\n",
      "l2 norm of weights: 5.778366420995013\n",
      "---------------------\n",
      "Iteration Number: 6723\n",
      "Loss: 31.956691703781384\n",
      "l2 norm of gradients: 0.2112695286963365\n",
      "l2 norm of weights: 5.778291188522487\n",
      "---------------------\n",
      "Iteration Number: 6724\n",
      "Loss: 31.95445353228901\n",
      "l2 norm of gradients: 0.21124866004706375\n",
      "l2 norm of weights: 5.7782159623115765\n",
      "---------------------\n",
      "Iteration Number: 6725\n",
      "Loss: 31.95221580335492\n",
      "l2 norm of gradients: 0.2112277967569948\n",
      "l2 norm of weights: 5.778140742361231\n",
      "---------------------\n",
      "Iteration Number: 6726\n",
      "Loss: 31.94997851682482\n",
      "l2 norm of gradients: 0.2112069388234568\n",
      "l2 norm of weights: 5.778065528670392\n",
      "---------------------\n",
      "Iteration Number: 6727\n",
      "Loss: 31.947741672540626\n",
      "l2 norm of gradients: 0.2111860862437789\n",
      "l2 norm of weights: 5.777990321238006\n",
      "---------------------\n",
      "Iteration Number: 6728\n",
      "Loss: 31.94550527034341\n",
      "l2 norm of gradients: 0.21116523901529247\n",
      "l2 norm of weights: 5.777915120063018\n",
      "---------------------\n",
      "Iteration Number: 6729\n",
      "Loss: 31.94326931007342\n",
      "l2 norm of gradients: 0.2111443971353309\n",
      "l2 norm of weights: 5.7778399251443755\n",
      "---------------------\n",
      "Iteration Number: 6730\n",
      "Loss: 31.941033791578082\n",
      "l2 norm of gradients: 0.21112356060122958\n",
      "l2 norm of weights: 5.777764736481024\n",
      "---------------------\n",
      "Iteration Number: 6731\n",
      "Loss: 31.938798714701797\n",
      "l2 norm of gradients: 0.21110272941032618\n",
      "l2 norm of weights: 5.77768955407191\n",
      "---------------------\n",
      "Iteration Number: 6732\n",
      "Loss: 31.93656407928098\n",
      "l2 norm of gradients: 0.21108190355996023\n",
      "l2 norm of weights: 5.777614377915982\n",
      "---------------------\n",
      "Iteration Number: 6733\n",
      "Loss: 31.93432988516738\n",
      "l2 norm of gradients: 0.21106108304747345\n",
      "l2 norm of weights: 5.777539208012186\n",
      "---------------------\n",
      "Iteration Number: 6734\n",
      "Loss: 31.93209613219306\n",
      "l2 norm of gradients: 0.21104026787020966\n",
      "l2 norm of weights: 5.77746404435947\n",
      "---------------------\n",
      "Iteration Number: 6735\n",
      "Loss: 31.92986282020944\n",
      "l2 norm of gradients: 0.21101945802551464\n",
      "l2 norm of weights: 5.777388886956783\n",
      "---------------------\n",
      "Iteration Number: 6736\n",
      "Loss: 31.927629949061085\n",
      "l2 norm of gradients: 0.21099865351073618\n",
      "l2 norm of weights: 5.777313735803072\n",
      "---------------------\n",
      "Iteration Number: 6737\n",
      "Loss: 31.92539751858467\n",
      "l2 norm of gradients: 0.21097785432322447\n",
      "l2 norm of weights: 5.777238590897288\n",
      "---------------------\n",
      "Iteration Number: 6738\n",
      "Loss: 31.92316552862779\n",
      "l2 norm of gradients: 0.21095706046033139\n",
      "l2 norm of weights: 5.777163452238379\n",
      "---------------------\n",
      "Iteration Number: 6739\n",
      "Loss: 31.92093397903432\n",
      "l2 norm of gradients: 0.21093627191941094\n",
      "l2 norm of weights: 5.777088319825294\n",
      "---------------------\n",
      "Iteration Number: 6740\n",
      "Loss: 31.91870286964461\n",
      "l2 norm of gradients: 0.2109154886978194\n",
      "l2 norm of weights: 5.777013193656984\n",
      "---------------------\n",
      "Iteration Number: 6741\n",
      "Loss: 31.916472200307503\n",
      "l2 norm of gradients: 0.21089471079291486\n",
      "l2 norm of weights: 5.776938073732397\n",
      "---------------------\n",
      "Iteration Number: 6742\n",
      "Loss: 31.914241970860967\n",
      "l2 norm of gradients: 0.2108739382020576\n",
      "l2 norm of weights: 5.776862960050487\n",
      "---------------------\n",
      "Iteration Number: 6743\n",
      "Loss: 31.912012181156026\n",
      "l2 norm of gradients: 0.2108531709226099\n",
      "l2 norm of weights: 5.776787852610203\n",
      "---------------------\n",
      "Iteration Number: 6744\n",
      "Loss: 31.90978283103018\n",
      "l2 norm of gradients: 0.21083240895193608\n",
      "l2 norm of weights: 5.7767127514104954\n",
      "---------------------\n",
      "Iteration Number: 6745\n",
      "Loss: 31.9075539203329\n",
      "l2 norm of gradients: 0.21081165228740253\n",
      "l2 norm of weights: 5.7766376564503155\n",
      "---------------------\n",
      "Iteration Number: 6746\n",
      "Loss: 31.905325448901426\n",
      "l2 norm of gradients: 0.2107909009263776\n",
      "l2 norm of weights: 5.776562567728618\n",
      "---------------------\n",
      "Iteration Number: 6747\n",
      "Loss: 31.90309741658852\n",
      "l2 norm of gradients: 0.21077015486623185\n",
      "l2 norm of weights: 5.7764874852443535\n",
      "---------------------\n",
      "Iteration Number: 6748\n",
      "Loss: 31.9008698232309\n",
      "l2 norm of gradients: 0.21074941410433767\n",
      "l2 norm of weights: 5.776412408996474\n",
      "---------------------\n",
      "Iteration Number: 6749\n",
      "Loss: 31.8986426686811\n",
      "l2 norm of gradients: 0.21072867863806968\n",
      "l2 norm of weights: 5.776337338983933\n",
      "---------------------\n",
      "Iteration Number: 6750\n",
      "Loss: 31.896415952776355\n",
      "l2 norm of gradients: 0.21070794846480434\n",
      "l2 norm of weights: 5.776262275205683\n",
      "---------------------\n",
      "Iteration Number: 6751\n",
      "Loss: 31.894189675365016\n",
      "l2 norm of gradients: 0.21068722358192032\n",
      "l2 norm of weights: 5.776187217660679\n",
      "---------------------\n",
      "Iteration Number: 6752\n",
      "Loss: 31.89196383628982\n",
      "l2 norm of gradients: 0.21066650398679823\n",
      "l2 norm of weights: 5.7761121663478745\n",
      "---------------------\n",
      "Iteration Number: 6753\n",
      "Loss: 31.88973843539481\n",
      "l2 norm of gradients: 0.21064578967682063\n",
      "l2 norm of weights: 5.7760371212662225\n",
      "---------------------\n",
      "Iteration Number: 6754\n",
      "Loss: 31.887513472531666\n",
      "l2 norm of gradients: 0.21062508064937233\n",
      "l2 norm of weights: 5.7759620824146785\n",
      "---------------------\n",
      "Iteration Number: 6755\n",
      "Loss: 31.885288947541454\n",
      "l2 norm of gradients: 0.21060437690183992\n",
      "l2 norm of weights: 5.775887049792199\n",
      "---------------------\n",
      "Iteration Number: 6756\n",
      "Loss: 31.88306486026421\n",
      "l2 norm of gradients: 0.21058367843161202\n",
      "l2 norm of weights: 5.775812023397736\n",
      "---------------------\n",
      "Iteration Number: 6757\n",
      "Loss: 31.88084121054977\n",
      "l2 norm of gradients: 0.21056298523607953\n",
      "l2 norm of weights: 5.775737003230249\n",
      "---------------------\n",
      "Iteration Number: 6758\n",
      "Loss: 31.878617998244064\n",
      "l2 norm of gradients: 0.21054229731263507\n",
      "l2 norm of weights: 5.775661989288691\n",
      "---------------------\n",
      "Iteration Number: 6759\n",
      "Loss: 31.876395223190052\n",
      "l2 norm of gradients: 0.21052161465867353\n",
      "l2 norm of weights: 5.775586981572019\n",
      "---------------------\n",
      "Iteration Number: 6760\n",
      "Loss: 31.874172885238742\n",
      "l2 norm of gradients: 0.2105009372715915\n",
      "l2 norm of weights: 5.77551198007919\n",
      "---------------------\n",
      "Iteration Number: 6761\n",
      "Loss: 31.87195098422676\n",
      "l2 norm of gradients: 0.21048026514878776\n",
      "l2 norm of weights: 5.775436984809161\n",
      "---------------------\n",
      "Iteration Number: 6762\n",
      "Loss: 31.86972952000462\n",
      "l2 norm of gradients: 0.21045959828766317\n",
      "l2 norm of weights: 5.775361995760889\n",
      "---------------------\n",
      "Iteration Number: 6763\n",
      "Loss: 31.86750849241975\n",
      "l2 norm of gradients: 0.21043893668562047\n",
      "l2 norm of weights: 5.775287012933331\n",
      "---------------------\n",
      "Iteration Number: 6764\n",
      "Loss: 31.86528790131422\n",
      "l2 norm of gradients: 0.2104182803400644\n",
      "l2 norm of weights: 5.775212036325447\n",
      "---------------------\n",
      "Iteration Number: 6765\n",
      "Loss: 31.863067746541073\n",
      "l2 norm of gradients: 0.21039762924840175\n",
      "l2 norm of weights: 5.775137065936195\n",
      "---------------------\n",
      "Iteration Number: 6766\n",
      "Loss: 31.86084802793112\n",
      "l2 norm of gradients: 0.2103769834080413\n",
      "l2 norm of weights: 5.775062101764531\n",
      "---------------------\n",
      "Iteration Number: 6767\n",
      "Loss: 31.858628745348003\n",
      "l2 norm of gradients: 0.21035634281639384\n",
      "l2 norm of weights: 5.774987143809416\n",
      "---------------------\n",
      "Iteration Number: 6768\n",
      "Loss: 31.856409898626467\n",
      "l2 norm of gradients: 0.21033570747087207\n",
      "l2 norm of weights: 5.774912192069809\n",
      "---------------------\n",
      "Iteration Number: 6769\n",
      "Loss: 31.85419148761852\n",
      "l2 norm of gradients: 0.21031507736889077\n",
      "l2 norm of weights: 5.774837246544671\n",
      "---------------------\n",
      "Iteration Number: 6770\n",
      "Loss: 31.85197351216924\n",
      "l2 norm of gradients: 0.2102944525078666\n",
      "l2 norm of weights: 5.77476230723296\n",
      "---------------------\n",
      "Iteration Number: 6771\n",
      "Loss: 31.84975597212233\n",
      "l2 norm of gradients: 0.2102738328852184\n",
      "l2 norm of weights: 5.774687374133636\n",
      "---------------------\n",
      "Iteration Number: 6772\n",
      "Loss: 31.847538867325753\n",
      "l2 norm of gradients: 0.21025321849836667\n",
      "l2 norm of weights: 5.774612447245662\n",
      "---------------------\n",
      "Iteration Number: 6773\n",
      "Loss: 31.845322197626142\n",
      "l2 norm of gradients: 0.21023260934473434\n",
      "l2 norm of weights: 5.774537526567998\n",
      "---------------------\n",
      "Iteration Number: 6774\n",
      "Loss: 31.84310596287065\n",
      "l2 norm of gradients: 0.2102120054217459\n",
      "l2 norm of weights: 5.774462612099604\n",
      "---------------------\n",
      "Iteration Number: 6775\n",
      "Loss: 31.840890162905808\n",
      "l2 norm of gradients: 0.21019140672682796\n",
      "l2 norm of weights: 5.7743877038394436\n",
      "---------------------\n",
      "Iteration Number: 6776\n",
      "Loss: 31.838674797577916\n",
      "l2 norm of gradients: 0.21017081325740922\n",
      "l2 norm of weights: 5.774312801786477\n",
      "---------------------\n",
      "Iteration Number: 6777\n",
      "Loss: 31.836459866737783\n",
      "l2 norm of gradients: 0.21015022501092023\n",
      "l2 norm of weights: 5.7742379059396685\n",
      "---------------------\n",
      "Iteration Number: 6778\n",
      "Loss: 31.834245370227904\n",
      "l2 norm of gradients: 0.21012964198479347\n",
      "l2 norm of weights: 5.77416301629798\n",
      "---------------------\n",
      "Iteration Number: 6779\n",
      "Loss: 31.832031307896006\n",
      "l2 norm of gradients: 0.21010906417646358\n",
      "l2 norm of weights: 5.774088132860373\n",
      "---------------------\n",
      "Iteration Number: 6780\n",
      "Loss: 31.82981767958725\n",
      "l2 norm of gradients: 0.21008849158336695\n",
      "l2 norm of weights: 5.774013255625813\n",
      "---------------------\n",
      "Iteration Number: 6781\n",
      "Loss: 31.827604485155003\n",
      "l2 norm of gradients: 0.21006792420294199\n",
      "l2 norm of weights: 5.773938384593263\n",
      "---------------------\n",
      "Iteration Number: 6782\n",
      "Loss: 31.825391724440557\n",
      "l2 norm of gradients: 0.21004736203262916\n",
      "l2 norm of weights: 5.7738635197616865\n",
      "---------------------\n",
      "Iteration Number: 6783\n",
      "Loss: 31.823179397295405\n",
      "l2 norm of gradients: 0.21002680506987084\n",
      "l2 norm of weights: 5.773788661130047\n",
      "---------------------\n",
      "Iteration Number: 6784\n",
      "Loss: 31.82096750356306\n",
      "l2 norm of gradients: 0.21000625331211126\n",
      "l2 norm of weights: 5.7737138086973125\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 6785\n",
      "Loss: 31.818756043097906\n",
      "l2 norm of gradients: 0.20998570675679676\n",
      "l2 norm of weights: 5.773638962462445\n",
      "---------------------\n",
      "Iteration Number: 6786\n",
      "Loss: 31.816545015740353\n",
      "l2 norm of gradients: 0.2099651654013755\n",
      "l2 norm of weights: 5.7735641224244105\n",
      "---------------------\n",
      "Iteration Number: 6787\n",
      "Loss: 31.81433442134114\n",
      "l2 norm of gradients: 0.20994462924329774\n",
      "l2 norm of weights: 5.773489288582176\n",
      "---------------------\n",
      "Iteration Number: 6788\n",
      "Loss: 31.81212425975061\n",
      "l2 norm of gradients: 0.2099240982800155\n",
      "l2 norm of weights: 5.7734144609347045\n",
      "---------------------\n",
      "Iteration Number: 6789\n",
      "Loss: 31.80991453081423\n",
      "l2 norm of gradients: 0.20990357250898292\n",
      "l2 norm of weights: 5.773339639480965\n",
      "---------------------\n",
      "Iteration Number: 6790\n",
      "Loss: 31.807705234378133\n",
      "l2 norm of gradients: 0.2098830519276559\n",
      "l2 norm of weights: 5.773264824219923\n",
      "---------------------\n",
      "Iteration Number: 6791\n",
      "Loss: 31.80549637029439\n",
      "l2 norm of gradients: 0.20986253653349252\n",
      "l2 norm of weights: 5.773190015150546\n",
      "---------------------\n",
      "Iteration Number: 6792\n",
      "Loss: 31.803287938406054\n",
      "l2 norm of gradients: 0.20984202632395252\n",
      "l2 norm of weights: 5.773115212271802\n",
      "---------------------\n",
      "Iteration Number: 6793\n",
      "Loss: 31.801079938566954\n",
      "l2 norm of gradients: 0.20982152129649784\n",
      "l2 norm of weights: 5.773040415582657\n",
      "---------------------\n",
      "Iteration Number: 6794\n",
      "Loss: 31.79887237061976\n",
      "l2 norm of gradients: 0.20980102144859214\n",
      "l2 norm of weights: 5.77296562508208\n",
      "---------------------\n",
      "Iteration Number: 6795\n",
      "Loss: 31.796665234418374\n",
      "l2 norm of gradients: 0.20978052677770118\n",
      "l2 norm of weights: 5.77289084076904\n",
      "---------------------\n",
      "Iteration Number: 6796\n",
      "Loss: 31.794458529812808\n",
      "l2 norm of gradients: 0.20976003728129256\n",
      "l2 norm of weights: 5.772816062642504\n",
      "---------------------\n",
      "Iteration Number: 6797\n",
      "Loss: 31.792252256646577\n",
      "l2 norm of gradients: 0.20973955295683575\n",
      "l2 norm of weights: 5.772741290701442\n",
      "---------------------\n",
      "Iteration Number: 6798\n",
      "Loss: 31.790046414765296\n",
      "l2 norm of gradients: 0.20971907380180224\n",
      "l2 norm of weights: 5.772666524944823\n",
      "---------------------\n",
      "Iteration Number: 6799\n",
      "Loss: 31.787841004028067\n",
      "l2 norm of gradients: 0.2096985998136655\n",
      "l2 norm of weights: 5.772591765371616\n",
      "---------------------\n",
      "Iteration Number: 6800\n",
      "Loss: 31.78563602427545\n",
      "l2 norm of gradients: 0.20967813098990076\n",
      "l2 norm of weights: 5.772517011980794\n",
      "---------------------\n",
      "Iteration Number: 6801\n",
      "Loss: 31.783431475357418\n",
      "l2 norm of gradients: 0.20965766732798535\n",
      "l2 norm of weights: 5.772442264771323\n",
      "---------------------\n",
      "Iteration Number: 6802\n",
      "Loss: 31.78122735712941\n",
      "l2 norm of gradients: 0.20963720882539824\n",
      "l2 norm of weights: 5.772367523742176\n",
      "---------------------\n",
      "Iteration Number: 6803\n",
      "Loss: 31.779023669433435\n",
      "l2 norm of gradients: 0.20961675547962066\n",
      "l2 norm of weights: 5.772292788892325\n",
      "---------------------\n",
      "Iteration Number: 6804\n",
      "Loss: 31.776820412118724\n",
      "l2 norm of gradients: 0.20959630728813555\n",
      "l2 norm of weights: 5.772218060220737\n",
      "---------------------\n",
      "Iteration Number: 6805\n",
      "Loss: 31.77461758503817\n",
      "l2 norm of gradients: 0.20957586424842767\n",
      "l2 norm of weights: 5.77214333772639\n",
      "---------------------\n",
      "Iteration Number: 6806\n",
      "Loss: 31.772415188045862\n",
      "l2 norm of gradients: 0.20955542635798396\n",
      "l2 norm of weights: 5.77206862140825\n",
      "---------------------\n",
      "Iteration Number: 6807\n",
      "Loss: 31.7702132209807\n",
      "l2 norm of gradients: 0.209534993614293\n",
      "l2 norm of weights: 5.771993911265294\n",
      "---------------------\n",
      "Iteration Number: 6808\n",
      "Loss: 31.768011683693526\n",
      "l2 norm of gradients: 0.20951456601484553\n",
      "l2 norm of weights: 5.771919207296491\n",
      "---------------------\n",
      "Iteration Number: 6809\n",
      "Loss: 31.76581057604562\n",
      "l2 norm of gradients: 0.2094941435571339\n",
      "l2 norm of weights: 5.771844509500815\n",
      "---------------------\n",
      "Iteration Number: 6810\n",
      "Loss: 31.763609897871607\n",
      "l2 norm of gradients: 0.20947372623865262\n",
      "l2 norm of weights: 5.77176981787724\n",
      "---------------------\n",
      "Iteration Number: 6811\n",
      "Loss: 31.761409649030902\n",
      "l2 norm of gradients: 0.20945331405689796\n",
      "l2 norm of weights: 5.771695132424739\n",
      "---------------------\n",
      "Iteration Number: 6812\n",
      "Loss: 31.759209829372253\n",
      "l2 norm of gradients: 0.2094329070093681\n",
      "l2 norm of weights: 5.771620453142287\n",
      "---------------------\n",
      "Iteration Number: 6813\n",
      "Loss: 31.75701043874279\n",
      "l2 norm of gradients: 0.2094125050935631\n",
      "l2 norm of weights: 5.771545780028856\n",
      "---------------------\n",
      "Iteration Number: 6814\n",
      "Loss: 31.75481147699414\n",
      "l2 norm of gradients: 0.20939210830698501\n",
      "l2 norm of weights: 5.7714711130834235\n",
      "---------------------\n",
      "Iteration Number: 6815\n",
      "Loss: 31.752612943975276\n",
      "l2 norm of gradients: 0.2093717166471376\n",
      "l2 norm of weights: 5.771396452304962\n",
      "---------------------\n",
      "Iteration Number: 6816\n",
      "Loss: 31.750414839539403\n",
      "l2 norm of gradients: 0.20935133011152668\n",
      "l2 norm of weights: 5.771321797692447\n",
      "---------------------\n",
      "Iteration Number: 6817\n",
      "Loss: 31.748217163532416\n",
      "l2 norm of gradients: 0.20933094869765992\n",
      "l2 norm of weights: 5.7712471492448545\n",
      "---------------------\n",
      "Iteration Number: 6818\n",
      "Loss: 31.746019915807224\n",
      "l2 norm of gradients: 0.2093105724030468\n",
      "l2 norm of weights: 5.771172506961162\n",
      "---------------------\n",
      "Iteration Number: 6819\n",
      "Loss: 31.7438230962143\n",
      "l2 norm of gradients: 0.2092902012251987\n",
      "l2 norm of weights: 5.771097870840343\n",
      "---------------------\n",
      "Iteration Number: 6820\n",
      "Loss: 31.741626704607082\n",
      "l2 norm of gradients: 0.20926983516162898\n",
      "l2 norm of weights: 5.771023240881375\n",
      "---------------------\n",
      "Iteration Number: 6821\n",
      "Loss: 31.739430740830322\n",
      "l2 norm of gradients: 0.20924947420985265\n",
      "l2 norm of weights: 5.770948617083236\n",
      "---------------------\n",
      "Iteration Number: 6822\n",
      "Loss: 31.73723520473896\n",
      "l2 norm of gradients: 0.20922911836738686\n",
      "l2 norm of weights: 5.770873999444901\n",
      "---------------------\n",
      "Iteration Number: 6823\n",
      "Loss: 31.73504009618113\n",
      "l2 norm of gradients: 0.2092087676317505\n",
      "l2 norm of weights: 5.770799387965349\n",
      "---------------------\n",
      "Iteration Number: 6824\n",
      "Loss: 31.732845415010715\n",
      "l2 norm of gradients: 0.20918842200046425\n",
      "l2 norm of weights: 5.770724782643558\n",
      "---------------------\n",
      "Iteration Number: 6825\n",
      "Loss: 31.73065116107606\n",
      "l2 norm of gradients: 0.20916808147105084\n",
      "l2 norm of weights: 5.770650183478507\n",
      "---------------------\n",
      "Iteration Number: 6826\n",
      "Loss: 31.728457334229446\n",
      "l2 norm of gradients: 0.20914774604103473\n",
      "l2 norm of weights: 5.770575590469172\n",
      "---------------------\n",
      "Iteration Number: 6827\n",
      "Loss: 31.726263934322972\n",
      "l2 norm of gradients: 0.20912741570794233\n",
      "l2 norm of weights: 5.770501003614534\n",
      "---------------------\n",
      "Iteration Number: 6828\n",
      "Loss: 31.724070961205836\n",
      "l2 norm of gradients: 0.20910709046930182\n",
      "l2 norm of weights: 5.77042642291357\n",
      "---------------------\n",
      "Iteration Number: 6829\n",
      "Loss: 31.721878414731034\n",
      "l2 norm of gradients: 0.20908677032264328\n",
      "l2 norm of weights: 5.770351848365261\n",
      "---------------------\n",
      "Iteration Number: 6830\n",
      "Loss: 31.719686294748588\n",
      "l2 norm of gradients: 0.2090664552654987\n",
      "l2 norm of weights: 5.770277279968587\n",
      "---------------------\n",
      "Iteration Number: 6831\n",
      "Loss: 31.717494601111923\n",
      "l2 norm of gradients: 0.20904614529540183\n",
      "l2 norm of weights: 5.770202717722527\n",
      "---------------------\n",
      "Iteration Number: 6832\n",
      "Loss: 31.715303333667244\n",
      "l2 norm of gradients: 0.20902584040988834\n",
      "l2 norm of weights: 5.770128161626063\n",
      "---------------------\n",
      "Iteration Number: 6833\n",
      "Loss: 31.71311249227697\n",
      "l2 norm of gradients: 0.20900554060649573\n",
      "l2 norm of weights: 5.770053611678175\n",
      "---------------------\n",
      "Iteration Number: 6834\n",
      "Loss: 31.710922076780683\n",
      "l2 norm of gradients: 0.20898524588276338\n",
      "l2 norm of weights: 5.769979067877842\n",
      "---------------------\n",
      "Iteration Number: 6835\n",
      "Loss: 31.708732087037003\n",
      "l2 norm of gradients: 0.2089649562362324\n",
      "l2 norm of weights: 5.769904530224049\n",
      "---------------------\n",
      "Iteration Number: 6836\n",
      "Loss: 31.70654252289718\n",
      "l2 norm of gradients: 0.20894467166444594\n",
      "l2 norm of weights: 5.7698299987157755\n",
      "---------------------\n",
      "Iteration Number: 6837\n",
      "Loss: 31.704353384211345\n",
      "l2 norm of gradients: 0.20892439216494882\n",
      "l2 norm of weights: 5.769755473352005\n",
      "---------------------\n",
      "Iteration Number: 6838\n",
      "Loss: 31.70216467083475\n",
      "l2 norm of gradients: 0.20890411773528775\n",
      "l2 norm of weights: 5.769680954131717\n",
      "---------------------\n",
      "Iteration Number: 6839\n",
      "Loss: 31.699976382617976\n",
      "l2 norm of gradients: 0.20888384837301135\n",
      "l2 norm of weights: 5.769606441053897\n",
      "---------------------\n",
      "Iteration Number: 6840\n",
      "Loss: 31.69778851940828\n",
      "l2 norm of gradients: 0.20886358407567\n",
      "l2 norm of weights: 5.769531934117528\n",
      "---------------------\n",
      "Iteration Number: 6841\n",
      "Loss: 31.695601081067398\n",
      "l2 norm of gradients: 0.20884332484081586\n",
      "l2 norm of weights: 5.769457433321591\n",
      "---------------------\n",
      "Iteration Number: 6842\n",
      "Loss: 31.693414067435768\n",
      "l2 norm of gradients: 0.20882307066600314\n",
      "l2 norm of weights: 5.769382938665072\n",
      "---------------------\n",
      "Iteration Number: 6843\n",
      "Loss: 31.69122747837975\n",
      "l2 norm of gradients: 0.20880282154878757\n",
      "l2 norm of weights: 5.769308450146955\n",
      "---------------------\n",
      "Iteration Number: 6844\n",
      "Loss: 31.689041313739033\n",
      "l2 norm of gradients: 0.208782577486727\n",
      "l2 norm of weights: 5.7692339677662225\n",
      "---------------------\n",
      "Iteration Number: 6845\n",
      "Loss: 31.68685557337848\n",
      "l2 norm of gradients: 0.2087623384773809\n",
      "l2 norm of weights: 5.76915949152186\n",
      "---------------------\n",
      "Iteration Number: 6846\n",
      "Loss: 31.684670257140517\n",
      "l2 norm of gradients: 0.2087421045183107\n",
      "l2 norm of weights: 5.769085021412853\n",
      "---------------------\n",
      "Iteration Number: 6847\n",
      "Loss: 31.682485364883227\n",
      "l2 norm of gradients: 0.20872187560707967\n",
      "l2 norm of weights: 5.769010557438187\n",
      "---------------------\n",
      "Iteration Number: 6848\n",
      "Loss: 31.6803008964566\n",
      "l2 norm of gradients: 0.2087016517412526\n",
      "l2 norm of weights: 5.768936099596846\n",
      "---------------------\n",
      "Iteration Number: 6849\n",
      "Loss: 31.678116851713177\n",
      "l2 norm of gradients: 0.20868143291839644\n",
      "l2 norm of weights: 5.768861647887817\n",
      "---------------------\n",
      "Iteration Number: 6850\n",
      "Loss: 31.67593323051417\n",
      "l2 norm of gradients: 0.20866121913607996\n",
      "l2 norm of weights: 5.768787202310087\n",
      "---------------------\n",
      "Iteration Number: 6851\n",
      "Loss: 31.67375003270061\n",
      "l2 norm of gradients: 0.20864101039187344\n",
      "l2 norm of weights: 5.7687127628626405\n",
      "---------------------\n",
      "Iteration Number: 6852\n",
      "Loss: 31.671567258131432\n",
      "l2 norm of gradients: 0.2086208066833492\n",
      "l2 norm of weights: 5.7686383295444665\n",
      "---------------------\n",
      "Iteration Number: 6853\n",
      "Loss: 31.669384906665577\n",
      "l2 norm of gradients: 0.20860060800808142\n",
      "l2 norm of weights: 5.7685639023545505\n",
      "---------------------\n",
      "Iteration Number: 6854\n",
      "Loss: 31.66720297814824\n",
      "l2 norm of gradients: 0.20858041436364588\n",
      "l2 norm of weights: 5.768489481291882\n",
      "---------------------\n",
      "Iteration Number: 6855\n",
      "Loss: 31.665021472432763\n",
      "l2 norm of gradients: 0.2085602257476203\n",
      "l2 norm of weights: 5.7684150663554465\n",
      "---------------------\n",
      "Iteration Number: 6856\n",
      "Loss: 31.6628403893802\n",
      "l2 norm of gradients: 0.20854004215758418\n",
      "l2 norm of weights: 5.7683406575442335\n",
      "---------------------\n",
      "Iteration Number: 6857\n",
      "Loss: 31.660659728836865\n",
      "l2 norm of gradients: 0.20851986359111885\n",
      "l2 norm of weights: 5.768266254857233\n",
      "---------------------\n",
      "Iteration Number: 6858\n",
      "Loss: 31.658479490658422\n",
      "l2 norm of gradients: 0.20849969004580743\n",
      "l2 norm of weights: 5.76819185829343\n",
      "---------------------\n",
      "Iteration Number: 6859\n",
      "Loss: 31.656299674697348\n",
      "l2 norm of gradients: 0.20847952151923474\n",
      "l2 norm of weights: 5.768117467851816\n",
      "---------------------\n",
      "Iteration Number: 6860\n",
      "Loss: 31.654120280811505\n",
      "l2 norm of gradients: 0.20845935800898754\n",
      "l2 norm of weights: 5.768043083531381\n",
      "---------------------\n",
      "Iteration Number: 6861\n",
      "Loss: 31.651941308853818\n",
      "l2 norm of gradients: 0.2084391995126543\n",
      "l2 norm of weights: 5.767968705331113\n",
      "---------------------\n",
      "Iteration Number: 6862\n",
      "Loss: 31.649762758671727\n",
      "l2 norm of gradients: 0.20841904602782527\n",
      "l2 norm of weights: 5.767894333250005\n",
      "---------------------\n",
      "Iteration Number: 6863\n",
      "Loss: 31.647584630132386\n",
      "l2 norm of gradients: 0.20839889755209257\n",
      "l2 norm of weights: 5.7678199672870445\n",
      "---------------------\n",
      "Iteration Number: 6864\n",
      "Loss: 31.645406923076038\n",
      "l2 norm of gradients: 0.20837875408305\n",
      "l2 norm of weights: 5.767745607441223\n",
      "---------------------\n",
      "Iteration Number: 6865\n",
      "Loss: 31.643229637364442\n",
      "l2 norm of gradients: 0.20835861561829325\n",
      "l2 norm of weights: 5.767671253711531\n",
      "---------------------\n",
      "Iteration Number: 6866\n",
      "Loss: 31.641052772851264\n",
      "l2 norm of gradients: 0.2083384821554197\n",
      "l2 norm of weights: 5.767596906096962\n",
      "---------------------\n",
      "Iteration Number: 6867\n",
      "Loss: 31.63887632939136\n",
      "l2 norm of gradients: 0.20831835369202858\n",
      "l2 norm of weights: 5.767522564596505\n",
      "---------------------\n",
      "Iteration Number: 6868\n",
      "Loss: 31.636700306835902\n",
      "l2 norm of gradients: 0.20829823022572083\n",
      "l2 norm of weights: 5.767448229209154\n",
      "---------------------\n",
      "Iteration Number: 6869\n",
      "Loss: 31.634524705041148\n",
      "l2 norm of gradients: 0.20827811175409933\n",
      "l2 norm of weights: 5.7673738999339\n",
      "---------------------\n",
      "Iteration Number: 6870\n",
      "Loss: 31.632349523866004\n",
      "l2 norm of gradients: 0.2082579982747685\n",
      "l2 norm of weights: 5.767299576769737\n",
      "---------------------\n",
      "Iteration Number: 6871\n",
      "Loss: 31.63017476315772\n",
      "l2 norm of gradients: 0.20823788978533472\n",
      "l2 norm of weights: 5.767225259715657\n",
      "---------------------\n",
      "Iteration Number: 6872\n",
      "Loss: 31.628000422775763\n",
      "l2 norm of gradients: 0.20821778628340598\n",
      "l2 norm of weights: 5.767150948770652\n",
      "---------------------\n",
      "Iteration Number: 6873\n",
      "Loss: 31.625826502571975\n",
      "l2 norm of gradients: 0.20819768776659225\n",
      "l2 norm of weights: 5.767076643933719\n",
      "---------------------\n",
      "Iteration Number: 6874\n",
      "Loss: 31.62365300240887\n",
      "l2 norm of gradients: 0.208177594232505\n",
      "l2 norm of weights: 5.767002345203849\n",
      "---------------------\n",
      "Iteration Number: 6875\n",
      "Loss: 31.6214799221316\n",
      "l2 norm of gradients: 0.20815750567875782\n",
      "l2 norm of weights: 5.766928052580037\n",
      "---------------------\n",
      "Iteration Number: 6876\n",
      "Loss: 31.619307261599857\n",
      "l2 norm of gradients: 0.20813742210296568\n",
      "l2 norm of weights: 5.766853766061278\n",
      "---------------------\n",
      "Iteration Number: 6877\n",
      "Loss: 31.61713502067152\n",
      "l2 norm of gradients: 0.20811734350274555\n",
      "l2 norm of weights: 5.766779485646566\n",
      "---------------------\n",
      "Iteration Number: 6878\n",
      "Loss: 31.614963199197238\n",
      "l2 norm of gradients: 0.2080972698757161\n",
      "l2 norm of weights: 5.766705211334897\n",
      "---------------------\n",
      "Iteration Number: 6879\n",
      "Loss: 31.612791797032635\n",
      "l2 norm of gradients: 0.2080772012194978\n",
      "l2 norm of weights: 5.766630943125266\n",
      "---------------------\n",
      "Iteration Number: 6880\n",
      "Loss: 31.61062081403594\n",
      "l2 norm of gradients: 0.20805713753171276\n",
      "l2 norm of weights: 5.766556681016669\n",
      "---------------------\n",
      "Iteration Number: 6881\n",
      "Loss: 31.60845025006178\n",
      "l2 norm of gradients: 0.20803707880998493\n",
      "l2 norm of weights: 5.766482425008102\n",
      "---------------------\n",
      "Iteration Number: 6882\n",
      "Loss: 31.6062801049641\n",
      "l2 norm of gradients: 0.20801702505193997\n",
      "l2 norm of weights: 5.7664081750985625\n",
      "---------------------\n",
      "Iteration Number: 6883\n",
      "Loss: 31.604110378604975\n",
      "l2 norm of gradients: 0.2079969762552054\n",
      "l2 norm of weights: 5.766333931287045\n",
      "---------------------\n",
      "Iteration Number: 6884\n",
      "Loss: 31.60194107082844\n",
      "l2 norm of gradients: 0.2079769324174104\n",
      "l2 norm of weights: 5.7662596935725485\n",
      "---------------------\n",
      "Iteration Number: 6885\n",
      "Loss: 31.599772181500544\n",
      "l2 norm of gradients: 0.20795689353618582\n",
      "l2 norm of weights: 5.766185461954069\n",
      "---------------------\n",
      "Iteration Number: 6886\n",
      "Loss: 31.59760371047466\n",
      "l2 norm of gradients: 0.20793685960916436\n",
      "l2 norm of weights: 5.766111236430605\n",
      "---------------------\n",
      "Iteration Number: 6887\n",
      "Loss: 31.595435657602955\n",
      "l2 norm of gradients: 0.20791683063398045\n",
      "l2 norm of weights: 5.766037017001156\n",
      "---------------------\n",
      "Iteration Number: 6888\n",
      "Loss: 31.593268022748326\n",
      "l2 norm of gradients: 0.20789680660827028\n",
      "l2 norm of weights: 5.765962803664717\n",
      "---------------------\n",
      "Iteration Number: 6889\n",
      "Loss: 31.59110080575757\n",
      "l2 norm of gradients: 0.20787678752967167\n",
      "l2 norm of weights: 5.765888596420288\n",
      "---------------------\n",
      "Iteration Number: 6890\n",
      "Loss: 31.588934006495073\n",
      "l2 norm of gradients: 0.20785677339582428\n",
      "l2 norm of weights: 5.76581439526687\n",
      "---------------------\n",
      "Iteration Number: 6891\n",
      "Loss: 31.58676762481373\n",
      "l2 norm of gradients: 0.20783676420436953\n",
      "l2 norm of weights: 5.765740200203459\n",
      "---------------------\n",
      "Iteration Number: 6892\n",
      "Loss: 31.584601660569405\n",
      "l2 norm of gradients: 0.2078167599529504\n",
      "l2 norm of weights: 5.765666011229056\n",
      "---------------------\n",
      "Iteration Number: 6893\n",
      "Loss: 31.582436113622098\n",
      "l2 norm of gradients: 0.20779676063921187\n",
      "l2 norm of weights: 5.765591828342663\n",
      "---------------------\n",
      "Iteration Number: 6894\n",
      "Loss: 31.58027098382533\n",
      "l2 norm of gradients: 0.2077767662608003\n",
      "l2 norm of weights: 5.765517651543276\n",
      "---------------------\n",
      "Iteration Number: 6895\n",
      "Loss: 31.578106271036127\n",
      "l2 norm of gradients: 0.2077567768153642\n",
      "l2 norm of weights: 5.7654434808299\n",
      "---------------------\n",
      "Iteration Number: 6896\n",
      "Loss: 31.575941975111533\n",
      "l2 norm of gradients: 0.20773679230055334\n",
      "l2 norm of weights: 5.765369316201532\n",
      "---------------------\n",
      "Iteration Number: 6897\n",
      "Loss: 31.573778095905446\n",
      "l2 norm of gradients: 0.20771681271401957\n",
      "l2 norm of weights: 5.7652951576571745\n",
      "---------------------\n",
      "Iteration Number: 6898\n",
      "Loss: 31.571614633283076\n",
      "l2 norm of gradients: 0.20769683805341635\n",
      "l2 norm of weights: 5.76522100519583\n",
      "---------------------\n",
      "Iteration Number: 6899\n",
      "Loss: 31.569451587090704\n",
      "l2 norm of gradients: 0.20767686831639884\n",
      "l2 norm of weights: 5.765146858816499\n",
      "---------------------\n",
      "Iteration Number: 6900\n",
      "Loss: 31.567288957192723\n",
      "l2 norm of gradients: 0.20765690350062382\n",
      "l2 norm of weights: 5.765072718518185\n",
      "---------------------\n",
      "Iteration Number: 6901\n",
      "Loss: 31.565126743445656\n",
      "l2 norm of gradients: 0.20763694360375004\n",
      "l2 norm of weights: 5.764998584299889\n",
      "---------------------\n",
      "Iteration Number: 6902\n",
      "Loss: 31.56296494570219\n",
      "l2 norm of gradients: 0.20761698862343772\n",
      "l2 norm of weights: 5.764924456160615\n",
      "---------------------\n",
      "Iteration Number: 6903\n",
      "Loss: 31.560803563825885\n",
      "l2 norm of gradients: 0.20759703855734896\n",
      "l2 norm of weights: 5.764850334099363\n",
      "---------------------\n",
      "Iteration Number: 6904\n",
      "Loss: 31.558642597663802\n",
      "l2 norm of gradients: 0.20757709340314734\n",
      "l2 norm of weights: 5.76477621811514\n",
      "---------------------\n",
      "Iteration Number: 6905\n",
      "Loss: 31.556482047084312\n",
      "l2 norm of gradients: 0.20755715315849838\n",
      "l2 norm of weights: 5.764702108206947\n",
      "---------------------\n",
      "Iteration Number: 6906\n",
      "Loss: 31.554321911943198\n",
      "l2 norm of gradients: 0.20753721782106935\n",
      "l2 norm of weights: 5.76462800437379\n",
      "---------------------\n",
      "Iteration Number: 6907\n",
      "Loss: 31.552162192090886\n",
      "l2 norm of gradients: 0.20751728738852884\n",
      "l2 norm of weights: 5.764553906614673\n",
      "---------------------\n",
      "Iteration Number: 6908\n",
      "Loss: 31.550002887393994\n",
      "l2 norm of gradients: 0.20749736185854756\n",
      "l2 norm of weights: 5.7644798149286\n",
      "---------------------\n",
      "Iteration Number: 6909\n",
      "Loss: 31.547843997702802\n",
      "l2 norm of gradients: 0.2074774412287978\n",
      "l2 norm of weights: 5.764405729314575\n",
      "---------------------\n",
      "Iteration Number: 6910\n",
      "Loss: 31.545685522879403\n",
      "l2 norm of gradients: 0.2074575254969534\n",
      "l2 norm of weights: 5.764331649771604\n",
      "---------------------\n",
      "Iteration Number: 6911\n",
      "Loss: 31.54352746278093\n",
      "l2 norm of gradients: 0.20743761466069002\n",
      "l2 norm of weights: 5.764257576298694\n",
      "---------------------\n",
      "Iteration Number: 6912\n",
      "Loss: 31.541369817262566\n",
      "l2 norm of gradients: 0.20741770871768497\n",
      "l2 norm of weights: 5.764183508894847\n",
      "---------------------\n",
      "Iteration Number: 6913\n",
      "Loss: 31.539212586188444\n",
      "l2 norm of gradients: 0.20739780766561736\n",
      "l2 norm of weights: 5.764109447559074\n",
      "---------------------\n",
      "Iteration Number: 6914\n",
      "Loss: 31.537055769409445\n",
      "l2 norm of gradients: 0.2073779115021678\n",
      "l2 norm of weights: 5.764035392290379\n",
      "---------------------\n",
      "Iteration Number: 6915\n",
      "Loss: 31.53489936678882\n",
      "l2 norm of gradients: 0.2073580202250188\n",
      "l2 norm of weights: 5.763961343087767\n",
      "---------------------\n",
      "Iteration Number: 6916\n",
      "Loss: 31.53274337817972\n",
      "l2 norm of gradients: 0.20733813383185437\n",
      "l2 norm of weights: 5.763887299950247\n",
      "---------------------\n",
      "Iteration Number: 6917\n",
      "Loss: 31.53058780345019\n",
      "l2 norm of gradients: 0.20731825232036039\n",
      "l2 norm of weights: 5.763813262876828\n",
      "---------------------\n",
      "Iteration Number: 6918\n",
      "Loss: 31.528432642449403\n",
      "l2 norm of gradients: 0.20729837568822412\n",
      "l2 norm of weights: 5.763739231866515\n",
      "---------------------\n",
      "Iteration Number: 6919\n",
      "Loss: 31.52627789503846\n",
      "l2 norm of gradients: 0.2072785039331349\n",
      "l2 norm of weights: 5.763665206918317\n",
      "---------------------\n",
      "Iteration Number: 6920\n",
      "Loss: 31.524123561075502\n",
      "l2 norm of gradients: 0.20725863705278347\n",
      "l2 norm of weights: 5.763591188031242\n",
      "---------------------\n",
      "Iteration Number: 6921\n",
      "Loss: 31.52196964041958\n",
      "l2 norm of gradients: 0.2072387750448622\n",
      "l2 norm of weights: 5.763517175204299\n",
      "---------------------\n",
      "Iteration Number: 6922\n",
      "Loss: 31.51981613293449\n",
      "l2 norm of gradients: 0.2072189179070655\n",
      "l2 norm of weights: 5.763443168436496\n",
      "---------------------\n",
      "Iteration Number: 6923\n",
      "Loss: 31.517663038469223\n",
      "l2 norm of gradients: 0.207199065637089\n",
      "l2 norm of weights: 5.763369167726844\n",
      "---------------------\n",
      "Iteration Number: 6924\n",
      "Loss: 31.515510356892854\n",
      "l2 norm of gradients: 0.20717921823263033\n",
      "l2 norm of weights: 5.763295173074351\n",
      "---------------------\n",
      "Iteration Number: 6925\n",
      "Loss: 31.513358088056066\n",
      "l2 norm of gradients: 0.20715937569138862\n",
      "l2 norm of weights: 5.763221184478027\n",
      "---------------------\n",
      "Iteration Number: 6926\n",
      "Loss: 31.511206231820733\n",
      "l2 norm of gradients: 0.20713953801106472\n",
      "l2 norm of weights: 5.763147201936884\n",
      "---------------------\n",
      "Iteration Number: 6927\n",
      "Loss: 31.509054788046623\n",
      "l2 norm of gradients: 0.2071197051893612\n",
      "l2 norm of weights: 5.76307322544993\n",
      "---------------------\n",
      "Iteration Number: 6928\n",
      "Loss: 31.506903756592255\n",
      "l2 norm of gradients: 0.20709987722398215\n",
      "l2 norm of weights: 5.762999255016178\n",
      "---------------------\n",
      "Iteration Number: 6929\n",
      "Loss: 31.504753137319042\n",
      "l2 norm of gradients: 0.20708005411263347\n",
      "l2 norm of weights: 5.762925290634637\n",
      "---------------------\n",
      "Iteration Number: 6930\n",
      "Loss: 31.502602930083658\n",
      "l2 norm of gradients: 0.2070602358530227\n",
      "l2 norm of weights: 5.76285133230432\n",
      "---------------------\n",
      "Iteration Number: 6931\n",
      "Loss: 31.500453134744575\n",
      "l2 norm of gradients: 0.20704042244285886\n",
      "l2 norm of weights: 5.762777380024236\n",
      "---------------------\n",
      "Iteration Number: 6932\n",
      "Loss: 31.498303751166507\n",
      "l2 norm of gradients: 0.20702061387985288\n",
      "l2 norm of weights: 5.7627034337934\n",
      "---------------------\n",
      "Iteration Number: 6933\n",
      "Loss: 31.496154779205316\n",
      "l2 norm of gradients: 0.20700081016171717\n",
      "l2 norm of weights: 5.762629493610825\n",
      "---------------------\n",
      "Iteration Number: 6934\n",
      "Loss: 31.49400621871918\n",
      "l2 norm of gradients: 0.20698101128616594\n",
      "l2 norm of weights: 5.762555559475519\n",
      "---------------------\n",
      "Iteration Number: 6935\n",
      "Loss: 31.491858069570338\n",
      "l2 norm of gradients: 0.20696121725091488\n",
      "l2 norm of weights: 5.7624816313865\n",
      "---------------------\n",
      "Iteration Number: 6936\n",
      "Loss: 31.489710331617292\n",
      "l2 norm of gradients: 0.20694142805368146\n",
      "l2 norm of weights: 5.762407709342779\n",
      "---------------------\n",
      "Iteration Number: 6937\n",
      "Loss: 31.487563004719558\n",
      "l2 norm of gradients: 0.2069216436921847\n",
      "l2 norm of weights: 5.762333793343369\n",
      "---------------------\n",
      "Iteration Number: 6938\n",
      "Loss: 31.485416088740234\n",
      "l2 norm of gradients: 0.20690186416414533\n",
      "l2 norm of weights: 5.762259883387285\n",
      "---------------------\n",
      "Iteration Number: 6939\n",
      "Loss: 31.483269583539325\n",
      "l2 norm of gradients: 0.20688208946728573\n",
      "l2 norm of weights: 5.762185979473541\n",
      "---------------------\n",
      "Iteration Number: 6940\n",
      "Loss: 31.481123488969956\n",
      "l2 norm of gradients: 0.2068623195993299\n",
      "l2 norm of weights: 5.76211208160115\n",
      "---------------------\n",
      "Iteration Number: 6941\n",
      "Loss: 31.478977804898566\n",
      "l2 norm of gradients: 0.20684255455800346\n",
      "l2 norm of weights: 5.762038189769129\n",
      "---------------------\n",
      "Iteration Number: 6942\n",
      "Loss: 31.476832531183494\n",
      "l2 norm of gradients: 0.2068227943410337\n",
      "l2 norm of weights: 5.761964303976493\n",
      "---------------------\n",
      "Iteration Number: 6943\n",
      "Loss: 31.474687667686982\n",
      "l2 norm of gradients: 0.20680303894614954\n",
      "l2 norm of weights: 5.761890424222254\n",
      "---------------------\n",
      "Iteration Number: 6944\n",
      "Loss: 31.47254321426575\n",
      "l2 norm of gradients: 0.20678328837108154\n",
      "l2 norm of weights: 5.761816550505431\n",
      "---------------------\n",
      "Iteration Number: 6945\n",
      "Loss: 31.470399170784464\n",
      "l2 norm of gradients: 0.2067635426135618\n",
      "l2 norm of weights: 5.761742682825039\n",
      "---------------------\n",
      "Iteration Number: 6946\n",
      "Loss: 31.46825553709952\n",
      "l2 norm of gradients: 0.20674380167132422\n",
      "l2 norm of weights: 5.761668821180095\n",
      "---------------------\n",
      "Iteration Number: 6947\n",
      "Loss: 31.46611231307585\n",
      "l2 norm of gradients: 0.20672406554210424\n",
      "l2 norm of weights: 5.761594965569614\n",
      "---------------------\n",
      "Iteration Number: 6948\n",
      "Loss: 31.46396949857446\n",
      "l2 norm of gradients: 0.20670433422363887\n",
      "l2 norm of weights: 5.761521115992616\n",
      "---------------------\n",
      "Iteration Number: 6949\n",
      "Loss: 31.46182709344998\n",
      "l2 norm of gradients: 0.2066846077136668\n",
      "l2 norm of weights: 5.761447272448114\n",
      "---------------------\n",
      "Iteration Number: 6950\n",
      "Loss: 31.4596850975656\n",
      "l2 norm of gradients: 0.2066648860099285\n",
      "l2 norm of weights: 5.761373434935128\n",
      "---------------------\n",
      "Iteration Number: 6951\n",
      "Loss: 31.457543510784873\n",
      "l2 norm of gradients: 0.20664516911016573\n",
      "l2 norm of weights: 5.7612996034526756\n",
      "---------------------\n",
      "Iteration Number: 6952\n",
      "Loss: 31.45540233296865\n",
      "l2 norm of gradients: 0.20662545701212212\n",
      "l2 norm of weights: 5.761225777999775\n",
      "---------------------\n",
      "Iteration Number: 6953\n",
      "Loss: 31.45326156397435\n",
      "l2 norm of gradients: 0.20660574971354284\n",
      "l2 norm of weights: 5.761151958575444\n",
      "---------------------\n",
      "Iteration Number: 6954\n",
      "Loss: 31.451121203668276\n",
      "l2 norm of gradients: 0.2065860472121747\n",
      "l2 norm of weights: 5.761078145178701\n",
      "---------------------\n",
      "Iteration Number: 6955\n",
      "Loss: 31.44898125190871\n",
      "l2 norm of gradients: 0.20656634950576608\n",
      "l2 norm of weights: 5.761004337808568\n",
      "---------------------\n",
      "Iteration Number: 6956\n",
      "Loss: 31.446841708556132\n",
      "l2 norm of gradients: 0.206546656592067\n",
      "l2 norm of weights: 5.76093053646406\n",
      "---------------------\n",
      "Iteration Number: 6957\n",
      "Loss: 31.44470257347551\n",
      "l2 norm of gradients: 0.20652696846882915\n",
      "l2 norm of weights: 5.760856741144201\n",
      "---------------------\n",
      "Iteration Number: 6958\n",
      "Loss: 31.442563846519946\n",
      "l2 norm of gradients: 0.20650728513380567\n",
      "l2 norm of weights: 5.7607829518480065\n",
      "---------------------\n",
      "Iteration Number: 6959\n",
      "Loss: 31.440425527562475\n",
      "l2 norm of gradients: 0.20648760658475146\n",
      "l2 norm of weights: 5.7607091685745\n",
      "---------------------\n",
      "Iteration Number: 6960\n",
      "Loss: 31.43828761645593\n",
      "l2 norm of gradients: 0.20646793281942308\n",
      "l2 norm of weights: 5.760635391322701\n",
      "---------------------\n",
      "Iteration Number: 6961\n",
      "Loss: 31.436150113069022\n",
      "l2 norm of gradients: 0.20644826383557832\n",
      "l2 norm of weights: 5.7605616200916305\n",
      "---------------------\n",
      "Iteration Number: 6962\n",
      "Loss: 31.434013017257623\n",
      "l2 norm of gradients: 0.20642859963097715\n",
      "l2 norm of weights: 5.760487854880311\n",
      "---------------------\n",
      "Iteration Number: 6963\n",
      "Loss: 31.431876328884567\n",
      "l2 norm of gradients: 0.2064089402033807\n",
      "l2 norm of weights: 5.76041409568776\n",
      "---------------------\n",
      "Iteration Number: 6964\n",
      "Loss: 31.429740047813045\n",
      "l2 norm of gradients: 0.20638928555055178\n",
      "l2 norm of weights: 5.760340342513003\n",
      "---------------------\n",
      "Iteration Number: 6965\n",
      "Loss: 31.42760417390472\n",
      "l2 norm of gradients: 0.2063696356702549\n",
      "l2 norm of weights: 5.760266595355061\n",
      "---------------------\n",
      "Iteration Number: 6966\n",
      "Loss: 31.425468707020073\n",
      "l2 norm of gradients: 0.20634999056025607\n",
      "l2 norm of weights: 5.760192854212957\n",
      "---------------------\n",
      "Iteration Number: 6967\n",
      "Loss: 31.423333647025128\n",
      "l2 norm of gradients: 0.206330350218323\n",
      "l2 norm of weights: 5.760119119085712\n",
      "---------------------\n",
      "Iteration Number: 6968\n",
      "Loss: 31.42119899378164\n",
      "l2 norm of gradients: 0.2063107146422249\n",
      "l2 norm of weights: 5.76004538997235\n",
      "---------------------\n",
      "Iteration Number: 6969\n",
      "Loss: 31.419064747144958\n",
      "l2 norm of gradients: 0.20629108382973257\n",
      "l2 norm of weights: 5.759971666871895\n",
      "---------------------\n",
      "Iteration Number: 6970\n",
      "Loss: 31.41693090698388\n",
      "l2 norm of gradients: 0.2062714577786184\n",
      "l2 norm of weights: 5.759897949783369\n",
      "---------------------\n",
      "Iteration Number: 6971\n",
      "Loss: 31.414797473160778\n",
      "l2 norm of gradients: 0.20625183648665651\n",
      "l2 norm of weights: 5.7598242387057965\n",
      "---------------------\n",
      "Iteration Number: 6972\n",
      "Loss: 31.412664445537487\n",
      "l2 norm of gradients: 0.20623221995162236\n",
      "l2 norm of weights: 5.759750533638202\n",
      "---------------------\n",
      "Iteration Number: 6973\n",
      "Loss: 31.41053182397504\n",
      "l2 norm of gradients: 0.20621260817129317\n",
      "l2 norm of weights: 5.7596768345796105\n",
      "---------------------\n",
      "Iteration Number: 6974\n",
      "Loss: 31.408399608337625\n",
      "l2 norm of gradients: 0.2061930011434477\n",
      "l2 norm of weights: 5.759603141529046\n",
      "---------------------\n",
      "Iteration Number: 6975\n",
      "Loss: 31.40626779848608\n",
      "l2 norm of gradients: 0.20617339886586622\n",
      "l2 norm of weights: 5.759529454485533\n",
      "---------------------\n",
      "Iteration Number: 6976\n",
      "Loss: 31.404136394280435\n",
      "l2 norm of gradients: 0.2061538013363307\n",
      "l2 norm of weights: 5.7594557734480984\n",
      "---------------------\n",
      "Iteration Number: 6977\n",
      "Loss: 31.40200539559539\n",
      "l2 norm of gradients: 0.2061342085526246\n",
      "l2 norm of weights: 5.759382098415767\n",
      "---------------------\n",
      "Iteration Number: 6978\n",
      "Loss: 31.399874802278564\n",
      "l2 norm of gradients: 0.20611462051253293\n",
      "l2 norm of weights: 5.759308429387565\n",
      "---------------------\n",
      "Iteration Number: 6979\n",
      "Loss: 31.39774461420348\n",
      "l2 norm of gradients: 0.20609503721384245\n",
      "l2 norm of weights: 5.7592347663625185\n",
      "---------------------\n",
      "Iteration Number: 6980\n",
      "Loss: 31.39561483123207\n",
      "l2 norm of gradients: 0.2060754586543412\n",
      "l2 norm of weights: 5.7591611093396535\n",
      "---------------------\n",
      "Iteration Number: 6981\n",
      "Loss: 31.393485453224386\n",
      "l2 norm of gradients: 0.20605588483181905\n",
      "l2 norm of weights: 5.759087458317999\n",
      "---------------------\n",
      "Iteration Number: 6982\n",
      "Loss: 31.39135648004445\n",
      "l2 norm of gradients: 0.20603631574406733\n",
      "l2 norm of weights: 5.75901381329658\n",
      "---------------------\n",
      "Iteration Number: 6983\n",
      "Loss: 31.38922791155302\n",
      "l2 norm of gradients: 0.2060167513888789\n",
      "l2 norm of weights: 5.758940174274424\n",
      "---------------------\n",
      "Iteration Number: 6984\n",
      "Loss: 31.387099747620677\n",
      "l2 norm of gradients: 0.20599719176404818\n",
      "l2 norm of weights: 5.758866541250561\n",
      "---------------------\n",
      "Iteration Number: 6985\n",
      "Loss: 31.384971988103487\n",
      "l2 norm of gradients: 0.20597763686737125\n",
      "l2 norm of weights: 5.758792914224017\n",
      "---------------------\n",
      "Iteration Number: 6986\n",
      "Loss: 31.38284463286887\n",
      "l2 norm of gradients: 0.20595808669664575\n",
      "l2 norm of weights: 5.758719293193821\n",
      "---------------------\n",
      "Iteration Number: 6987\n",
      "Loss: 31.380717681778826\n",
      "l2 norm of gradients: 0.20593854124967076\n",
      "l2 norm of weights: 5.758645678159002\n",
      "---------------------\n",
      "Iteration Number: 6988\n",
      "Loss: 31.378591134699256\n",
      "l2 norm of gradients: 0.20591900052424697\n",
      "l2 norm of weights: 5.758572069118588\n",
      "---------------------\n",
      "Iteration Number: 6989\n",
      "Loss: 31.376464991489623\n",
      "l2 norm of gradients: 0.20589946451817662\n",
      "l2 norm of weights: 5.758498466071609\n",
      "---------------------\n",
      "Iteration Number: 6990\n",
      "Loss: 31.374339252019325\n",
      "l2 norm of gradients: 0.2058799332292636\n",
      "l2 norm of weights: 5.758424869017096\n",
      "---------------------\n",
      "Iteration Number: 6991\n",
      "Loss: 31.372213916151328\n",
      "l2 norm of gradients: 0.20586040665531322\n",
      "l2 norm of weights: 5.758351277954077\n",
      "---------------------\n",
      "Iteration Number: 6992\n",
      "Loss: 31.370088983741944\n",
      "l2 norm of gradients: 0.20584088479413237\n",
      "l2 norm of weights: 5.758277692881583\n",
      "---------------------\n",
      "Iteration Number: 6993\n",
      "Loss: 31.367964454664556\n",
      "l2 norm of gradients: 0.2058213676435296\n",
      "l2 norm of weights: 5.7582041137986435\n",
      "---------------------\n",
      "Iteration Number: 6994\n",
      "Loss: 31.365840328780884\n",
      "l2 norm of gradients: 0.20580185520131478\n",
      "l2 norm of weights: 5.75813054070429\n",
      "---------------------\n",
      "Iteration Number: 6995\n",
      "Loss: 31.363716605950923\n",
      "l2 norm of gradients: 0.20578234746529955\n",
      "l2 norm of weights: 5.7580569735975535\n",
      "---------------------\n",
      "Iteration Number: 6996\n",
      "Loss: 31.361593286042403\n",
      "l2 norm of gradients: 0.205762844433297\n",
      "l2 norm of weights: 5.757983412477466\n",
      "---------------------\n",
      "Iteration Number: 6997\n",
      "Loss: 31.359470368922082\n",
      "l2 norm of gradients: 0.2057433461031217\n",
      "l2 norm of weights: 5.757909857343058\n",
      "---------------------\n",
      "Iteration Number: 6998\n",
      "Loss: 31.357347854447976\n",
      "l2 norm of gradients: 0.20572385247258992\n",
      "l2 norm of weights: 5.757836308193362\n",
      "---------------------\n",
      "Iteration Number: 6999\n",
      "Loss: 31.355225742493225\n",
      "l2 norm of gradients: 0.20570436353951935\n",
      "l2 norm of weights: 5.75776276502741\n",
      "---------------------\n",
      "Iteration Number: 7000\n",
      "Loss: 31.353104032910466\n",
      "l2 norm of gradients: 0.20568487930172913\n",
      "l2 norm of weights: 5.757689227844235\n",
      "---------------------\n",
      "Iteration Number: 7001\n",
      "Loss: 31.35098272557355\n",
      "l2 norm of gradients: 0.20566539975704018\n",
      "l2 norm of weights: 5.75761569664287\n",
      "---------------------\n",
      "Iteration Number: 7002\n",
      "Loss: 31.348861820344432\n",
      "l2 norm of gradients: 0.20564592490327477\n",
      "l2 norm of weights: 5.757542171422348\n",
      "---------------------\n",
      "Iteration Number: 7003\n",
      "Loss: 31.346741317087783\n",
      "l2 norm of gradients: 0.20562645473825672\n",
      "l2 norm of weights: 5.757468652181702\n",
      "---------------------\n",
      "Iteration Number: 7004\n",
      "Loss: 31.34462121567125\n",
      "l2 norm of gradients: 0.20560698925981147\n",
      "l2 norm of weights: 5.757395138919967\n",
      "---------------------\n",
      "Iteration Number: 7005\n",
      "Loss: 31.342501515952936\n",
      "l2 norm of gradients: 0.20558752846576586\n",
      "l2 norm of weights: 5.757321631636175\n",
      "---------------------\n",
      "Iteration Number: 7006\n",
      "Loss: 31.34038221780351\n",
      "l2 norm of gradients: 0.20556807235394836\n",
      "l2 norm of weights: 5.757248130329361\n",
      "---------------------\n",
      "Iteration Number: 7007\n",
      "Loss: 31.338263321088995\n",
      "l2 norm of gradients: 0.2055486209221889\n",
      "l2 norm of weights: 5.757174634998561\n",
      "---------------------\n",
      "Iteration Number: 7008\n",
      "Loss: 31.33614482566974\n",
      "l2 norm of gradients: 0.20552917416831895\n",
      "l2 norm of weights: 5.757101145642809\n",
      "---------------------\n",
      "Iteration Number: 7009\n",
      "Loss: 31.33402673141233\n",
      "l2 norm of gradients: 0.20550973209017162\n",
      "l2 norm of weights: 5.75702766226114\n",
      "---------------------\n",
      "Iteration Number: 7010\n",
      "Loss: 31.331909038186197\n",
      "l2 norm of gradients: 0.2054902946855813\n",
      "l2 norm of weights: 5.7569541848525905\n",
      "---------------------\n",
      "Iteration Number: 7011\n",
      "Loss: 31.32979174585185\n",
      "l2 norm of gradients: 0.20547086195238404\n",
      "l2 norm of weights: 5.756880713416195\n",
      "---------------------\n",
      "Iteration Number: 7012\n",
      "Loss: 31.327674854276086\n",
      "l2 norm of gradients: 0.20545143388841747\n",
      "l2 norm of weights: 5.756807247950991\n",
      "---------------------\n",
      "Iteration Number: 7013\n",
      "Loss: 31.325558363324106\n",
      "l2 norm of gradients: 0.20543201049152063\n",
      "l2 norm of weights: 5.756733788456014\n",
      "---------------------\n",
      "Iteration Number: 7014\n",
      "Loss: 31.323442272860014\n",
      "l2 norm of gradients: 0.20541259175953405\n",
      "l2 norm of weights: 5.7566603349303005\n",
      "---------------------\n",
      "Iteration Number: 7015\n",
      "Loss: 31.32132658275708\n",
      "l2 norm of gradients: 0.20539317769029997\n",
      "l2 norm of weights: 5.756586887372888\n",
      "---------------------\n",
      "Iteration Number: 7016\n",
      "Loss: 31.319211292870193\n",
      "l2 norm of gradients: 0.20537376828166176\n",
      "l2 norm of weights: 5.756513445782813\n",
      "---------------------\n",
      "Iteration Number: 7017\n",
      "Loss: 31.317096403071353\n",
      "l2 norm of gradients: 0.20535436353146475\n",
      "l2 norm of weights: 5.756440010159114\n",
      "---------------------\n",
      "Iteration Number: 7018\n",
      "Loss: 31.314981913228074\n",
      "l2 norm of gradients: 0.2053349634375554\n",
      "l2 norm of weights: 5.7563665805008295\n",
      "---------------------\n",
      "Iteration Number: 7019\n",
      "Loss: 31.312867823199028\n",
      "l2 norm of gradients: 0.20531556799778192\n",
      "l2 norm of weights: 5.756293156806996\n",
      "---------------------\n",
      "Iteration Number: 7020\n",
      "Loss: 31.31075413285671\n",
      "l2 norm of gradients: 0.20529617720999394\n",
      "l2 norm of weights: 5.756219739076653\n",
      "---------------------\n",
      "Iteration Number: 7021\n",
      "Loss: 31.308640842066392\n",
      "l2 norm of gradients: 0.20527679107204255\n",
      "l2 norm of weights: 5.75614632730884\n",
      "---------------------\n",
      "Iteration Number: 7022\n",
      "Loss: 31.30652795069226\n",
      "l2 norm of gradients: 0.2052574095817804\n",
      "l2 norm of weights: 5.756072921502594\n",
      "---------------------\n",
      "Iteration Number: 7023\n",
      "Loss: 31.304415458595745\n",
      "l2 norm of gradients: 0.20523803273706162\n",
      "l2 norm of weights: 5.755999521656955\n",
      "---------------------\n",
      "Iteration Number: 7024\n",
      "Loss: 31.302303365654605\n",
      "l2 norm of gradients: 0.20521866053574184\n",
      "l2 norm of weights: 5.755926127770964\n",
      "---------------------\n",
      "Iteration Number: 7025\n",
      "Loss: 31.3001916717269\n",
      "l2 norm of gradients: 0.2051992929756781\n",
      "l2 norm of weights: 5.75585273984366\n",
      "---------------------\n",
      "Iteration Number: 7026\n",
      "Loss: 31.29808037667983\n",
      "l2 norm of gradients: 0.20517993005472906\n",
      "l2 norm of weights: 5.755779357874084\n",
      "---------------------\n",
      "Iteration Number: 7027\n",
      "Loss: 31.29596948038557\n",
      "l2 norm of gradients: 0.20516057177075486\n",
      "l2 norm of weights: 5.755705981861276\n",
      "---------------------\n",
      "Iteration Number: 7028\n",
      "Loss: 31.293858982702023\n",
      "l2 norm of gradients: 0.20514121812161706\n",
      "l2 norm of weights: 5.755632611804275\n",
      "---------------------\n",
      "Iteration Number: 7029\n",
      "Loss: 31.291748883503885\n",
      "l2 norm of gradients: 0.2051218691051787\n",
      "l2 norm of weights: 5.7555592477021245\n",
      "---------------------\n",
      "Iteration Number: 7030\n",
      "Loss: 31.289639182650827\n",
      "l2 norm of gradients: 0.20510252471930446\n",
      "l2 norm of weights: 5.755485889553865\n",
      "---------------------\n",
      "Iteration Number: 7031\n",
      "Loss: 31.287529880014656\n",
      "l2 norm of gradients: 0.20508318496186023\n",
      "l2 norm of weights: 5.755412537358539\n",
      "---------------------\n",
      "Iteration Number: 7032\n",
      "Loss: 31.285420975458074\n",
      "l2 norm of gradients: 0.20506384983071368\n",
      "l2 norm of weights: 5.755339191115188\n",
      "---------------------\n",
      "Iteration Number: 7033\n",
      "Loss: 31.283312468849715\n",
      "l2 norm of gradients: 0.20504451932373374\n",
      "l2 norm of weights: 5.755265850822854\n",
      "---------------------\n",
      "Iteration Number: 7034\n",
      "Loss: 31.281204360060542\n",
      "l2 norm of gradients: 0.20502519343879097\n",
      "l2 norm of weights: 5.755192516480578\n",
      "---------------------\n",
      "Iteration Number: 7035\n",
      "Loss: 31.279096648951217\n",
      "l2 norm of gradients: 0.20500587217375738\n",
      "l2 norm of weights: 5.755119188087407\n",
      "---------------------\n",
      "Iteration Number: 7036\n",
      "Loss: 31.276989335395232\n",
      "l2 norm of gradients: 0.20498655552650624\n",
      "l2 norm of weights: 5.755045865642381\n",
      "---------------------\n",
      "Iteration Number: 7037\n",
      "Loss: 31.274882419252478\n",
      "l2 norm of gradients: 0.2049672434949127\n",
      "l2 norm of weights: 5.7549725491445445\n",
      "---------------------\n",
      "Iteration Number: 7038\n",
      "Loss: 31.272775900395033\n",
      "l2 norm of gradients: 0.204947936076853\n",
      "l2 norm of weights: 5.75489923859294\n",
      "---------------------\n",
      "Iteration Number: 7039\n",
      "Loss: 31.270669778690277\n",
      "l2 norm of gradients: 0.20492863327020513\n",
      "l2 norm of weights: 5.754825933986614\n",
      "---------------------\n",
      "Iteration Number: 7040\n",
      "Loss: 31.268564054003978\n",
      "l2 norm of gradients: 0.2049093350728484\n",
      "l2 norm of weights: 5.754752635324608\n",
      "---------------------\n",
      "Iteration Number: 7041\n",
      "Loss: 31.266458726205446\n",
      "l2 norm of gradients: 0.2048900414826636\n",
      "l2 norm of weights: 5.754679342605969\n",
      "---------------------\n",
      "Iteration Number: 7042\n",
      "Loss: 31.264353795158698\n",
      "l2 norm of gradients: 0.20487075249753303\n",
      "l2 norm of weights: 5.754606055829739\n",
      "---------------------\n",
      "Iteration Number: 7043\n",
      "Loss: 31.26224926073379\n",
      "l2 norm of gradients: 0.20485146811534044\n",
      "l2 norm of weights: 5.754532774994967\n",
      "---------------------\n",
      "Iteration Number: 7044\n",
      "Loss: 31.260145122801507\n",
      "l2 norm of gradients: 0.20483218833397102\n",
      "l2 norm of weights: 5.754459500100697\n",
      "---------------------\n",
      "Iteration Number: 7045\n",
      "Loss: 31.258041381225496\n",
      "l2 norm of gradients: 0.20481291315131148\n",
      "l2 norm of weights: 5.754386231145973\n",
      "---------------------\n",
      "Iteration Number: 7046\n",
      "Loss: 31.25593803586951\n",
      "l2 norm of gradients: 0.20479364256524993\n",
      "l2 norm of weights: 5.754312968129843\n",
      "---------------------\n",
      "Iteration Number: 7047\n",
      "Loss: 31.253835086611137\n",
      "l2 norm of gradients: 0.204774376573676\n",
      "l2 norm of weights: 5.754239711051352\n",
      "---------------------\n",
      "Iteration Number: 7048\n",
      "Loss: 31.251732533312346\n",
      "l2 norm of gradients: 0.20475511517448067\n",
      "l2 norm of weights: 5.7541664599095474\n",
      "---------------------\n",
      "Iteration Number: 7049\n",
      "Loss: 31.249630375841853\n",
      "l2 norm of gradients: 0.20473585836555655\n",
      "l2 norm of weights: 5.7540932147034765\n",
      "---------------------\n",
      "Iteration Number: 7050\n",
      "Loss: 31.247528614065907\n",
      "l2 norm of gradients: 0.20471660614479753\n",
      "l2 norm of weights: 5.754019975432187\n",
      "---------------------\n",
      "Iteration Number: 7051\n",
      "Loss: 31.245427247858515\n",
      "l2 norm of gradients: 0.2046973585100991\n",
      "l2 norm of weights: 5.753946742094724\n",
      "---------------------\n",
      "Iteration Number: 7052\n",
      "Loss: 31.24332627708217\n",
      "l2 norm of gradients: 0.20467811545935802\n",
      "l2 norm of weights: 5.753873514690139\n",
      "---------------------\n",
      "Iteration Number: 7053\n",
      "Loss: 31.241225701606766\n",
      "l2 norm of gradients: 0.20465887699047272\n",
      "l2 norm of weights: 5.753800293217476\n",
      "---------------------\n",
      "Iteration Number: 7054\n",
      "Loss: 31.239125521303578\n",
      "l2 norm of gradients: 0.20463964310134292\n",
      "l2 norm of weights: 5.753727077675787\n",
      "---------------------\n",
      "Iteration Number: 7055\n",
      "Loss: 31.2370257360379\n",
      "l2 norm of gradients: 0.20462041378986984\n",
      "l2 norm of weights: 5.753653868064118\n",
      "---------------------\n",
      "Iteration Number: 7056\n",
      "Loss: 31.234926345678403\n",
      "l2 norm of gradients: 0.20460118905395616\n",
      "l2 norm of weights: 5.75358066438152\n",
      "---------------------\n",
      "Iteration Number: 7057\n",
      "Loss: 31.232827350098017\n",
      "l2 norm of gradients: 0.20458196889150593\n",
      "l2 norm of weights: 5.753507466627041\n",
      "---------------------\n",
      "Iteration Number: 7058\n",
      "Loss: 31.230728749156764\n",
      "l2 norm of gradients: 0.20456275330042473\n",
      "l2 norm of weights: 5.753434274799731\n",
      "---------------------\n",
      "Iteration Number: 7059\n",
      "Loss: 31.228630542729846\n",
      "l2 norm of gradients: 0.2045435422786196\n",
      "l2 norm of weights: 5.753361088898639\n",
      "---------------------\n",
      "Iteration Number: 7060\n",
      "Loss: 31.226532730683598\n",
      "l2 norm of gradients: 0.2045243358239989\n",
      "l2 norm of weights: 5.753287908922816\n",
      "---------------------\n",
      "Iteration Number: 7061\n",
      "Loss: 31.224435312890474\n",
      "l2 norm of gradients: 0.20450513393447253\n",
      "l2 norm of weights: 5.753214734871312\n",
      "---------------------\n",
      "Iteration Number: 7062\n",
      "Loss: 31.222338289213514\n",
      "l2 norm of gradients: 0.2044859366079518\n",
      "l2 norm of weights: 5.753141566743179\n",
      "---------------------\n",
      "Iteration Number: 7063\n",
      "Loss: 31.220241659527403\n",
      "l2 norm of gradients: 0.20446674384234936\n",
      "l2 norm of weights: 5.753068404537466\n",
      "---------------------\n",
      "Iteration Number: 7064\n",
      "Loss: 31.218145423696367\n",
      "l2 norm of gradients: 0.20444755563557945\n",
      "l2 norm of weights: 5.752995248253225\n",
      "---------------------\n",
      "Iteration Number: 7065\n",
      "Loss: 31.216049581592184\n",
      "l2 norm of gradients: 0.20442837198555774\n",
      "l2 norm of weights: 5.752922097889509\n",
      "---------------------\n",
      "Iteration Number: 7066\n",
      "Loss: 31.213954133086215\n",
      "l2 norm of gradients: 0.2044091928902011\n",
      "l2 norm of weights: 5.752848953445366\n",
      "---------------------\n",
      "Iteration Number: 7067\n",
      "Loss: 31.211859078043673\n",
      "l2 norm of gradients: 0.20439001834742812\n",
      "l2 norm of weights: 5.752775814919853\n",
      "---------------------\n",
      "Iteration Number: 7068\n",
      "Loss: 31.209764416333506\n",
      "l2 norm of gradients: 0.2043708483551586\n",
      "l2 norm of weights: 5.75270268231202\n",
      "---------------------\n",
      "Iteration Number: 7069\n",
      "Loss: 31.207670147831763\n",
      "l2 norm of gradients: 0.20435168291131395\n",
      "l2 norm of weights: 5.75262955562092\n",
      "---------------------\n",
      "Iteration Number: 7070\n",
      "Loss: 31.205576272398275\n",
      "l2 norm of gradients: 0.20433252201381683\n",
      "l2 norm of weights: 5.752556434845604\n",
      "---------------------\n",
      "Iteration Number: 7071\n",
      "Loss: 31.203482789910616\n",
      "l2 norm of gradients: 0.2043133656605914\n",
      "l2 norm of weights: 5.752483319985129\n",
      "---------------------\n",
      "Iteration Number: 7072\n",
      "Loss: 31.201389700236746\n",
      "l2 norm of gradients: 0.20429421384956326\n",
      "l2 norm of weights: 5.752410211038545\n",
      "---------------------\n",
      "Iteration Number: 7073\n",
      "Loss: 31.199297003239696\n",
      "l2 norm of gradients: 0.20427506657865932\n",
      "l2 norm of weights: 5.75233710800491\n",
      "---------------------\n",
      "Iteration Number: 7074\n",
      "Loss: 31.197204698800817\n",
      "l2 norm of gradients: 0.20425592384580818\n",
      "l2 norm of weights: 5.752264010883273\n",
      "---------------------\n",
      "Iteration Number: 7075\n",
      "Loss: 31.19511278677999\n",
      "l2 norm of gradients: 0.20423678564893946\n",
      "l2 norm of weights: 5.752190919672692\n",
      "---------------------\n",
      "Iteration Number: 7076\n",
      "Loss: 31.193021267050607\n",
      "l2 norm of gradients: 0.20421765198598457\n",
      "l2 norm of weights: 5.752117834372223\n",
      "---------------------\n",
      "Iteration Number: 7077\n",
      "Loss: 31.19093013948302\n",
      "l2 norm of gradients: 0.20419852285487602\n",
      "l2 norm of weights: 5.752044754980916\n",
      "---------------------\n",
      "Iteration Number: 7078\n",
      "Loss: 31.18883940394685\n",
      "l2 norm of gradients: 0.20417939825354806\n",
      "l2 norm of weights: 5.751971681497831\n",
      "---------------------\n",
      "Iteration Number: 7079\n",
      "Loss: 31.18674906031431\n",
      "l2 norm of gradients: 0.20416027817993607\n",
      "l2 norm of weights: 5.7518986139220205\n",
      "---------------------\n",
      "Iteration Number: 7080\n",
      "Loss: 31.18465910845198\n",
      "l2 norm of gradients: 0.20414116263197685\n",
      "l2 norm of weights: 5.7518255522525426\n",
      "---------------------\n",
      "Iteration Number: 7081\n",
      "Loss: 31.182569548228578\n",
      "l2 norm of gradients: 0.20412205160760888\n",
      "l2 norm of weights: 5.75175249648845\n",
      "---------------------\n",
      "Iteration Number: 7082\n",
      "Loss: 31.18048037952165\n",
      "l2 norm of gradients: 0.2041029451047717\n",
      "l2 norm of weights: 5.7516794466288035\n",
      "---------------------\n",
      "Iteration Number: 7083\n",
      "Loss: 31.17839160219778\n",
      "l2 norm of gradients: 0.2040838431214065\n",
      "l2 norm of weights: 5.751606402672658\n",
      "---------------------\n",
      "Iteration Number: 7084\n",
      "Loss: 31.176303216126982\n",
      "l2 norm of gradients: 0.20406474565545576\n",
      "l2 norm of weights: 5.751533364619069\n",
      "---------------------\n",
      "Iteration Number: 7085\n",
      "Loss: 31.17421522117534\n",
      "l2 norm of gradients: 0.2040456527048634\n",
      "l2 norm of weights: 5.751460332467096\n",
      "---------------------\n",
      "Iteration Number: 7086\n",
      "Loss: 31.17212761722093\n",
      "l2 norm of gradients: 0.20402656426757473\n",
      "l2 norm of weights: 5.751387306215796\n",
      "---------------------\n",
      "Iteration Number: 7087\n",
      "Loss: 31.17004040413092\n",
      "l2 norm of gradients: 0.2040074803415365\n",
      "l2 norm of weights: 5.751314285864226\n",
      "---------------------\n",
      "Iteration Number: 7088\n",
      "Loss: 31.167953581775116\n",
      "l2 norm of gradients: 0.20398840092469667\n",
      "l2 norm of weights: 5.751241271411446\n",
      "---------------------\n",
      "Iteration Number: 7089\n",
      "Loss: 31.165867150028053\n",
      "l2 norm of gradients: 0.20396932601500492\n",
      "l2 norm of weights: 5.751168262856511\n",
      "---------------------\n",
      "Iteration Number: 7090\n",
      "Loss: 31.16378110875714\n",
      "l2 norm of gradients: 0.203950255610412\n",
      "l2 norm of weights: 5.751095260198483\n",
      "---------------------\n",
      "Iteration Number: 7091\n",
      "Loss: 31.161695457835215\n",
      "l2 norm of gradients: 0.20393118970887025\n",
      "l2 norm of weights: 5.751022263436419\n",
      "---------------------\n",
      "Iteration Number: 7092\n",
      "Loss: 31.15961019712913\n",
      "l2 norm of gradients: 0.20391212830833338\n",
      "l2 norm of weights: 5.750949272569379\n",
      "---------------------\n",
      "Iteration Number: 7093\n",
      "Loss: 31.157525326519206\n",
      "l2 norm of gradients: 0.20389307140675636\n",
      "l2 norm of weights: 5.750876287596424\n",
      "---------------------\n",
      "Iteration Number: 7094\n",
      "Loss: 31.155440845867048\n",
      "l2 norm of gradients: 0.20387401900209573\n",
      "l2 norm of weights: 5.750803308516611\n",
      "---------------------\n",
      "Iteration Number: 7095\n",
      "Loss: 31.15335675504592\n",
      "l2 norm of gradients: 0.20385497109230927\n",
      "l2 norm of weights: 5.750730335329002\n",
      "---------------------\n",
      "Iteration Number: 7096\n",
      "Loss: 31.151273053929156\n",
      "l2 norm of gradients: 0.20383592767535627\n",
      "l2 norm of weights: 5.750657368032656\n",
      "---------------------\n",
      "Iteration Number: 7097\n",
      "Loss: 31.149189742387126\n",
      "l2 norm of gradients: 0.20381688874919723\n",
      "l2 norm of weights: 5.750584406626636\n",
      "---------------------\n",
      "Iteration Number: 7098\n",
      "Loss: 31.147106820292617\n",
      "l2 norm of gradients: 0.2037978543117942\n",
      "l2 norm of weights: 5.750511451110001\n",
      "---------------------\n",
      "Iteration Number: 7099\n",
      "Loss: 31.14502428751694\n",
      "l2 norm of gradients: 0.20377882436111056\n",
      "l2 norm of weights: 5.750438501481813\n",
      "---------------------\n",
      "Iteration Number: 7100\n",
      "Loss: 31.142942143925733\n",
      "l2 norm of gradients: 0.20375979889511106\n",
      "l2 norm of weights: 5.750365557741135\n",
      "---------------------\n",
      "Iteration Number: 7101\n",
      "Loss: 31.140860389398526\n",
      "l2 norm of gradients: 0.2037407779117617\n",
      "l2 norm of weights: 5.750292619887024\n",
      "---------------------\n",
      "Iteration Number: 7102\n",
      "Loss: 31.138779023801067\n",
      "l2 norm of gradients: 0.2037217614090302\n",
      "l2 norm of weights: 5.750219687918547\n",
      "---------------------\n",
      "Iteration Number: 7103\n",
      "Loss: 31.136698047012583\n",
      "l2 norm of gradients: 0.2037027493848852\n",
      "l2 norm of weights: 5.750146761834766\n",
      "---------------------\n",
      "Iteration Number: 7104\n",
      "Loss: 31.134617458899285\n",
      "l2 norm of gradients: 0.2036837418372971\n",
      "l2 norm of weights: 5.750073841634741\n",
      "---------------------\n",
      "Iteration Number: 7105\n",
      "Loss: 31.13253725933028\n",
      "l2 norm of gradients: 0.2036647387642375\n",
      "l2 norm of weights: 5.750000927317536\n",
      "---------------------\n",
      "Iteration Number: 7106\n",
      "Loss: 31.130457448181797\n",
      "l2 norm of gradients: 0.20364574016367934\n",
      "l2 norm of weights: 5.7499280188822155\n",
      "---------------------\n",
      "Iteration Number: 7107\n",
      "Loss: 31.128378025328626\n",
      "l2 norm of gradients: 0.20362674603359696\n",
      "l2 norm of weights: 5.749855116327842\n",
      "---------------------\n",
      "Iteration Number: 7108\n",
      "Loss: 31.126298990636208\n",
      "l2 norm of gradients: 0.20360775637196615\n",
      "l2 norm of weights: 5.749782219653477\n",
      "---------------------\n",
      "Iteration Number: 7109\n",
      "Loss: 31.124220343981598\n",
      "l2 norm of gradients: 0.20358877117676394\n",
      "l2 norm of weights: 5.74970932885819\n",
      "---------------------\n",
      "Iteration Number: 7110\n",
      "Loss: 31.12214208523032\n",
      "l2 norm of gradients: 0.20356979044596882\n",
      "l2 norm of weights: 5.749636443941041\n",
      "---------------------\n",
      "Iteration Number: 7111\n",
      "Loss: 31.120064214266435\n",
      "l2 norm of gradients: 0.20355081417756063\n",
      "l2 norm of weights: 5.749563564901096\n",
      "---------------------\n",
      "Iteration Number: 7112\n",
      "Loss: 31.117986730951248\n",
      "l2 norm of gradients: 0.2035318423695205\n",
      "l2 norm of weights: 5.749490691737421\n",
      "---------------------\n",
      "Iteration Number: 7113\n",
      "Loss: 31.115909635160925\n",
      "l2 norm of gradients: 0.20351287501983092\n",
      "l2 norm of weights: 5.749417824449078\n",
      "---------------------\n",
      "Iteration Number: 7114\n",
      "Loss: 31.11383292676708\n",
      "l2 norm of gradients: 0.20349391212647594\n",
      "l2 norm of weights: 5.749344963035136\n",
      "---------------------\n",
      "Iteration Number: 7115\n",
      "Loss: 31.11175660564428\n",
      "l2 norm of gradients: 0.20347495368744065\n",
      "l2 norm of weights: 5.749272107494657\n",
      "---------------------\n",
      "Iteration Number: 7116\n",
      "Loss: 31.109680671666784\n",
      "l2 norm of gradients: 0.20345599970071174\n",
      "l2 norm of weights: 5.749199257826712\n",
      "---------------------\n",
      "Iteration Number: 7117\n",
      "Loss: 31.10760512470314\n",
      "l2 norm of gradients: 0.20343705016427713\n",
      "l2 norm of weights: 5.749126414030363\n",
      "---------------------\n",
      "Iteration Number: 7118\n",
      "Loss: 31.10552996462915\n",
      "l2 norm of gradients: 0.20341810507612623\n",
      "l2 norm of weights: 5.74905357610468\n",
      "---------------------\n",
      "Iteration Number: 7119\n",
      "Loss: 31.103455191310772\n",
      "l2 norm of gradients: 0.20339916443424955\n",
      "l2 norm of weights: 5.748980744048726\n",
      "---------------------\n",
      "Iteration Number: 7120\n",
      "Loss: 31.101380804626857\n",
      "l2 norm of gradients: 0.20338022823663923\n",
      "l2 norm of weights: 5.7489079178615725\n",
      "---------------------\n",
      "Iteration Number: 7121\n",
      "Loss: 31.099306804454177\n",
      "l2 norm of gradients: 0.20336129648128862\n",
      "l2 norm of weights: 5.748835097542283\n",
      "---------------------\n",
      "Iteration Number: 7122\n",
      "Loss: 31.097233190657207\n",
      "l2 norm of gradients: 0.2033423691661924\n",
      "l2 norm of weights: 5.748762283089928\n",
      "---------------------\n",
      "Iteration Number: 7123\n",
      "Loss: 31.09515996311548\n",
      "l2 norm of gradients: 0.20332344628934662\n",
      "l2 norm of weights: 5.748689474503573\n",
      "---------------------\n",
      "Iteration Number: 7124\n",
      "Loss: 31.093087121696655\n",
      "l2 norm of gradients: 0.2033045278487487\n",
      "l2 norm of weights: 5.748616671782289\n",
      "---------------------\n",
      "Iteration Number: 7125\n",
      "Loss: 31.091014666278127\n",
      "l2 norm of gradients: 0.20328561384239738\n",
      "l2 norm of weights: 5.748543874925144\n",
      "---------------------\n",
      "Iteration Number: 7126\n",
      "Loss: 31.08894259672944\n",
      "l2 norm of gradients: 0.20326670426829277\n",
      "l2 norm of weights: 5.748471083931204\n",
      "---------------------\n",
      "Iteration Number: 7127\n",
      "Loss: 31.08687091292907\n",
      "l2 norm of gradients: 0.20324779912443622\n",
      "l2 norm of weights: 5.7483982987995415\n",
      "---------------------\n",
      "Iteration Number: 7128\n",
      "Loss: 31.084799614748842\n",
      "l2 norm of gradients: 0.20322889840883054\n",
      "l2 norm of weights: 5.748325519529224\n",
      "---------------------\n",
      "Iteration Number: 7129\n",
      "Loss: 31.082728702056073\n",
      "l2 norm of gradients: 0.2032100021194799\n",
      "l2 norm of weights: 5.748252746119323\n",
      "---------------------\n",
      "Iteration Number: 7130\n",
      "Loss: 31.080658174731646\n",
      "l2 norm of gradients: 0.20319111025438957\n",
      "l2 norm of weights: 5.748179978568905\n",
      "---------------------\n",
      "Iteration Number: 7131\n",
      "Loss: 31.078588032647264\n",
      "l2 norm of gradients: 0.2031722228115665\n",
      "l2 norm of weights: 5.748107216877045\n",
      "---------------------\n",
      "Iteration Number: 7132\n",
      "Loss: 31.0765182756733\n",
      "l2 norm of gradients: 0.2031533397890187\n",
      "l2 norm of weights: 5.748034461042809\n",
      "---------------------\n",
      "Iteration Number: 7133\n",
      "Loss: 31.07444890368911\n",
      "l2 norm of gradients: 0.20313446118475556\n",
      "l2 norm of weights: 5.747961711065271\n",
      "---------------------\n",
      "Iteration Number: 7134\n",
      "Loss: 31.072379916560923\n",
      "l2 norm of gradients: 0.20311558699678797\n",
      "l2 norm of weights: 5.7478889669435\n",
      "---------------------\n",
      "Iteration Number: 7135\n",
      "Loss: 31.070311314171292\n",
      "l2 norm of gradients: 0.20309671722312794\n",
      "l2 norm of weights: 5.7478162286765695\n",
      "---------------------\n",
      "Iteration Number: 7136\n",
      "Loss: 31.068243096390173\n",
      "l2 norm of gradients: 0.2030778518617889\n",
      "l2 norm of weights: 5.747743496263548\n",
      "---------------------\n",
      "Iteration Number: 7137\n",
      "Loss: 31.066175263088958\n",
      "l2 norm of gradients: 0.2030589909107856\n",
      "l2 norm of weights: 5.747670769703511\n",
      "---------------------\n",
      "Iteration Number: 7138\n",
      "Loss: 31.064107814144062\n",
      "l2 norm of gradients: 0.2030401343681341\n",
      "l2 norm of weights: 5.7475980489955285\n",
      "---------------------\n",
      "Iteration Number: 7139\n",
      "Loss: 31.06204074943152\n",
      "l2 norm of gradients: 0.20302128223185184\n",
      "l2 norm of weights: 5.747525334138672\n",
      "---------------------\n",
      "Iteration Number: 7140\n",
      "Loss: 31.05997406882064\n",
      "l2 norm of gradients: 0.20300243449995753\n",
      "l2 norm of weights: 5.7474526251320155\n",
      "---------------------\n",
      "Iteration Number: 7141\n",
      "Loss: 31.057907772188837\n",
      "l2 norm of gradients: 0.20298359117047107\n",
      "l2 norm of weights: 5.747379921974633\n",
      "---------------------\n",
      "Iteration Number: 7142\n",
      "Loss: 31.05584185941357\n",
      "l2 norm of gradients: 0.20296475224141408\n",
      "l2 norm of weights: 5.747307224665597\n",
      "---------------------\n",
      "Iteration Number: 7143\n",
      "Loss: 31.053776330359895\n",
      "l2 norm of gradients: 0.202945917710809\n",
      "l2 norm of weights: 5.747234533203981\n",
      "---------------------\n",
      "Iteration Number: 7144\n",
      "Loss: 31.051711184913145\n",
      "l2 norm of gradients: 0.2029270875766799\n",
      "l2 norm of weights: 5.747161847588858\n",
      "---------------------\n",
      "Iteration Number: 7145\n",
      "Loss: 31.049646422939684\n",
      "l2 norm of gradients: 0.20290826183705207\n",
      "l2 norm of weights: 5.747089167819303\n",
      "---------------------\n",
      "Iteration Number: 7146\n",
      "Loss: 31.047582044317462\n",
      "l2 norm of gradients: 0.20288944048995214\n",
      "l2 norm of weights: 5.747016493894391\n",
      "---------------------\n",
      "Iteration Number: 7147\n",
      "Loss: 31.045518048920854\n",
      "l2 norm of gradients: 0.20287062353340804\n",
      "l2 norm of weights: 5.7469438258131955\n",
      "---------------------\n",
      "Iteration Number: 7148\n",
      "Loss: 31.043454436624963\n",
      "l2 norm of gradients: 0.2028518109654491\n",
      "l2 norm of weights: 5.7468711635747916\n",
      "---------------------\n",
      "Iteration Number: 7149\n",
      "Loss: 31.041391207303835\n",
      "l2 norm of gradients: 0.20283300278410568\n",
      "l2 norm of weights: 5.746798507178255\n",
      "---------------------\n",
      "Iteration Number: 7150\n",
      "Loss: 31.039328360831192\n",
      "l2 norm of gradients: 0.20281419898740982\n",
      "l2 norm of weights: 5.74672585662266\n",
      "---------------------\n",
      "Iteration Number: 7151\n",
      "Loss: 31.037265897084964\n",
      "l2 norm of gradients: 0.20279539957339454\n",
      "l2 norm of weights: 5.746653211907085\n",
      "---------------------\n",
      "Iteration Number: 7152\n",
      "Loss: 31.03520381593445\n",
      "l2 norm of gradients: 0.20277660454009444\n",
      "l2 norm of weights: 5.746580573030603\n",
      "---------------------\n",
      "Iteration Number: 7153\n",
      "Loss: 31.033142117262347\n",
      "l2 norm of gradients: 0.20275781388554523\n",
      "l2 norm of weights: 5.746507939992291\n",
      "---------------------\n",
      "Iteration Number: 7154\n",
      "Loss: 31.031080800938383\n",
      "l2 norm of gradients: 0.2027390276077839\n",
      "l2 norm of weights: 5.746435312791229\n",
      "---------------------\n",
      "Iteration Number: 7155\n",
      "Loss: 31.02901986684199\n",
      "l2 norm of gradients: 0.20272024570484898\n",
      "l2 norm of weights: 5.74636269142649\n",
      "---------------------\n",
      "Iteration Number: 7156\n",
      "Loss: 31.02695931484071\n",
      "l2 norm of gradients: 0.20270146817478005\n",
      "l2 norm of weights: 5.746290075897151\n",
      "---------------------\n",
      "Iteration Number: 7157\n",
      "Loss: 31.02489914481684\n",
      "l2 norm of gradients: 0.20268269501561811\n",
      "l2 norm of weights: 5.746217466202292\n",
      "---------------------\n",
      "Iteration Number: 7158\n",
      "Loss: 31.022839356644326\n",
      "l2 norm of gradients: 0.20266392622540547\n",
      "l2 norm of weights: 5.746144862340989\n",
      "---------------------\n",
      "Iteration Number: 7159\n",
      "Loss: 31.020779950198204\n",
      "l2 norm of gradients: 0.20264516180218564\n",
      "l2 norm of weights: 5.746072264312321\n",
      "---------------------\n",
      "Iteration Number: 7160\n",
      "Loss: 31.01872092534989\n",
      "l2 norm of gradients: 0.2026264017440035\n",
      "l2 norm of weights: 5.7459996721153646\n",
      "---------------------\n",
      "Iteration Number: 7161\n",
      "Loss: 31.01666228198172\n",
      "l2 norm of gradients: 0.2026076460489053\n",
      "l2 norm of weights: 5.7459270857492\n",
      "---------------------\n",
      "Iteration Number: 7162\n",
      "Loss: 31.01460401996454\n",
      "l2 norm of gradients: 0.20258889471493832\n",
      "l2 norm of weights: 5.745854505212905\n",
      "---------------------\n",
      "Iteration Number: 7163\n",
      "Loss: 31.012546139176553\n",
      "l2 norm of gradients: 0.20257014774015145\n",
      "l2 norm of weights: 5.74578193050556\n",
      "---------------------\n",
      "Iteration Number: 7164\n",
      "Loss: 31.01048863949134\n",
      "l2 norm of gradients: 0.20255140512259462\n",
      "l2 norm of weights: 5.745709361626242\n",
      "---------------------\n",
      "Iteration Number: 7165\n",
      "Loss: 31.008431520785575\n",
      "l2 norm of gradients: 0.2025326668603192\n",
      "l2 norm of weights: 5.745636798574033\n",
      "---------------------\n",
      "Iteration Number: 7166\n",
      "Loss: 31.006374782937154\n",
      "l2 norm of gradients: 0.20251393295137776\n",
      "l2 norm of weights: 5.745564241348011\n",
      "---------------------\n",
      "Iteration Number: 7167\n",
      "Loss: 31.004318425820152\n",
      "l2 norm of gradients: 0.20249520339382415\n",
      "l2 norm of weights: 5.745491689947258\n",
      "---------------------\n",
      "Iteration Number: 7168\n",
      "Loss: 31.0022624493078\n",
      "l2 norm of gradients: 0.20247647818571363\n",
      "l2 norm of weights: 5.745419144370852\n",
      "---------------------\n",
      "Iteration Number: 7169\n",
      "Loss: 31.000206853283395\n",
      "l2 norm of gradients: 0.2024577573251026\n",
      "l2 norm of weights: 5.745346604617876\n",
      "---------------------\n",
      "Iteration Number: 7170\n",
      "Loss: 30.99815163761356\n",
      "l2 norm of gradients: 0.2024390408100488\n",
      "l2 norm of weights: 5.74527407068741\n",
      "---------------------\n",
      "Iteration Number: 7171\n",
      "Loss: 30.996096802181007\n",
      "l2 norm of gradients: 0.2024203286386113\n",
      "l2 norm of weights: 5.745201542578536\n",
      "---------------------\n",
      "Iteration Number: 7172\n",
      "Loss: 30.9940423468628\n",
      "l2 norm of gradients: 0.20240162080885035\n",
      "l2 norm of weights: 5.745129020290334\n",
      "---------------------\n",
      "Iteration Number: 7173\n",
      "Loss: 30.991988271528513\n",
      "l2 norm of gradients: 0.2023829173188274\n",
      "l2 norm of weights: 5.745056503821886\n",
      "---------------------\n",
      "Iteration Number: 7174\n",
      "Loss: 30.98993457606341\n",
      "l2 norm of gradients: 0.20236421816660552\n",
      "l2 norm of weights: 5.744983993172277\n",
      "---------------------\n",
      "Iteration Number: 7175\n",
      "Loss: 30.987881260335314\n",
      "l2 norm of gradients: 0.2023455233502487\n",
      "l2 norm of weights: 5.744911488340586\n",
      "---------------------\n",
      "Iteration Number: 7176\n",
      "Loss: 30.985828324228397\n",
      "l2 norm of gradients: 0.20232683286782235\n",
      "l2 norm of weights: 5.744838989325896\n",
      "---------------------\n",
      "Iteration Number: 7177\n",
      "Loss: 30.983775767612826\n",
      "l2 norm of gradients: 0.2023081467173931\n",
      "l2 norm of weights: 5.744766496127292\n",
      "---------------------\n",
      "Iteration Number: 7178\n",
      "Loss: 30.98172359036894\n",
      "l2 norm of gradients: 0.20228946489702898\n",
      "l2 norm of weights: 5.744694008743855\n",
      "---------------------\n",
      "Iteration Number: 7179\n",
      "Loss: 30.979671792370116\n",
      "l2 norm of gradients: 0.2022707874047991\n",
      "l2 norm of weights: 5.744621527174671\n",
      "---------------------\n",
      "Iteration Number: 7180\n",
      "Loss: 30.977620373500496\n",
      "l2 norm of gradients: 0.20225211423877396\n",
      "l2 norm of weights: 5.7445490514188196\n",
      "---------------------\n",
      "Iteration Number: 7181\n",
      "Loss: 30.975569333628798\n",
      "l2 norm of gradients: 0.20223344539702534\n",
      "l2 norm of weights: 5.7444765814753875\n",
      "---------------------\n",
      "Iteration Number: 7182\n",
      "Loss: 30.97351867263289\n",
      "l2 norm of gradients: 0.20221478087762623\n",
      "l2 norm of weights: 5.744404117343459\n",
      "---------------------\n",
      "Iteration Number: 7183\n",
      "Loss: 30.971468390394776\n",
      "l2 norm of gradients: 0.20219612067865084\n",
      "l2 norm of weights: 5.7443316590221185\n",
      "---------------------\n",
      "Iteration Number: 7184\n",
      "Loss: 30.969418486786775\n",
      "l2 norm of gradients: 0.2021774647981748\n",
      "l2 norm of weights: 5.744259206510449\n",
      "---------------------\n",
      "Iteration Number: 7185\n",
      "Loss: 30.967368961685672\n",
      "l2 norm of gradients: 0.20215881323427484\n",
      "l2 norm of weights: 5.74418675980754\n",
      "---------------------\n",
      "Iteration Number: 7186\n",
      "Loss: 30.96531981497195\n",
      "l2 norm of gradients: 0.20214016598502907\n",
      "l2 norm of weights: 5.744114318912471\n",
      "---------------------\n",
      "Iteration Number: 7187\n",
      "Loss: 30.96327104652191\n",
      "l2 norm of gradients: 0.2021215230485167\n",
      "l2 norm of weights: 5.744041883824332\n",
      "---------------------\n",
      "Iteration Number: 7188\n",
      "Loss: 30.961222656212918\n",
      "l2 norm of gradients: 0.2021028844228184\n",
      "l2 norm of weights: 5.743969454542206\n",
      "---------------------\n",
      "Iteration Number: 7189\n",
      "Loss: 30.95917464391965\n",
      "l2 norm of gradients: 0.2020842501060159\n",
      "l2 norm of weights: 5.743897031065182\n",
      "---------------------\n",
      "Iteration Number: 7190\n",
      "Loss: 30.95712700952279\n",
      "l2 norm of gradients: 0.2020656200961924\n",
      "l2 norm of weights: 5.743824613392344\n",
      "---------------------\n",
      "Iteration Number: 7191\n",
      "Loss: 30.955079752895166\n",
      "l2 norm of gradients: 0.20204699439143214\n",
      "l2 norm of weights: 5.74375220152278\n",
      "---------------------\n",
      "Iteration Number: 7192\n",
      "Loss: 30.953032873923373\n",
      "l2 norm of gradients: 0.2020283729898208\n",
      "l2 norm of weights: 5.743679795455577\n",
      "---------------------\n",
      "Iteration Number: 7193\n",
      "Loss: 30.950986372473043\n",
      "l2 norm of gradients: 0.2020097558894451\n",
      "l2 norm of weights: 5.743607395189821\n",
      "---------------------\n",
      "Iteration Number: 7194\n",
      "Loss: 30.94894024843039\n",
      "l2 norm of gradients: 0.20199114308839322\n",
      "l2 norm of weights: 5.743535000724601\n",
      "---------------------\n",
      "Iteration Number: 7195\n",
      "Loss: 30.94689450167008\n",
      "l2 norm of gradients: 0.20197253458475445\n",
      "l2 norm of weights: 5.7434626120590035\n",
      "---------------------\n",
      "Iteration Number: 7196\n",
      "Loss: 30.944849132068782\n",
      "l2 norm of gradients: 0.20195393037661943\n",
      "l2 norm of weights: 5.7433902291921175\n",
      "---------------------\n",
      "Iteration Number: 7197\n",
      "Loss: 30.942804139508137\n",
      "l2 norm of gradients: 0.20193533046207987\n",
      "l2 norm of weights: 5.74331785212303\n",
      "---------------------\n",
      "Iteration Number: 7198\n",
      "Loss: 30.940759523863253\n",
      "l2 norm of gradients: 0.20191673483922903\n",
      "l2 norm of weights: 5.743245480850831\n",
      "---------------------\n",
      "Iteration Number: 7199\n",
      "Loss: 30.938715285013664\n",
      "l2 norm of gradients: 0.2018981435061611\n",
      "l2 norm of weights: 5.743173115374609\n",
      "---------------------\n",
      "Iteration Number: 7200\n",
      "Loss: 30.93667142283371\n",
      "l2 norm of gradients: 0.20187955646097167\n",
      "l2 norm of weights: 5.743100755693453\n",
      "---------------------\n",
      "Iteration Number: 7201\n",
      "Loss: 30.934627937204148\n",
      "l2 norm of gradients: 0.20186097370175757\n",
      "l2 norm of weights: 5.743028401806452\n",
      "---------------------\n",
      "Iteration Number: 7202\n",
      "Loss: 30.932584828003215\n",
      "l2 norm of gradients: 0.2018423952266168\n",
      "l2 norm of weights: 5.742956053712697\n",
      "---------------------\n",
      "Iteration Number: 7203\n",
      "Loss: 30.93054209510792\n",
      "l2 norm of gradients: 0.20182382103364863\n",
      "l2 norm of weights: 5.742883711411276\n",
      "---------------------\n",
      "Iteration Number: 7204\n",
      "Loss: 30.928499738399317\n",
      "l2 norm of gradients: 0.20180525112095365\n",
      "l2 norm of weights: 5.7428113749012795\n",
      "---------------------\n",
      "Iteration Number: 7205\n",
      "Loss: 30.9264577577527\n",
      "l2 norm of gradients: 0.20178668548663356\n",
      "l2 norm of weights: 5.7427390441818\n",
      "---------------------\n",
      "Iteration Number: 7206\n",
      "Loss: 30.92441615304439\n",
      "l2 norm of gradients: 0.20176812412879136\n",
      "l2 norm of weights: 5.742666719251925\n",
      "---------------------\n",
      "Iteration Number: 7207\n",
      "Loss: 30.92237492415976\n",
      "l2 norm of gradients: 0.2017495670455313\n",
      "l2 norm of weights: 5.742594400110748\n",
      "---------------------\n",
      "Iteration Number: 7208\n",
      "Loss: 30.920334070972284\n",
      "l2 norm of gradients: 0.2017310142349588\n",
      "l2 norm of weights: 5.74252208675736\n",
      "---------------------\n",
      "Iteration Number: 7209\n",
      "Loss: 30.918293593366403\n",
      "l2 norm of gradients: 0.20171246569518053\n",
      "l2 norm of weights: 5.742449779190851\n",
      "---------------------\n",
      "Iteration Number: 7210\n",
      "Loss: 30.916253491210373\n",
      "l2 norm of gradients: 0.20169392142430445\n",
      "l2 norm of weights: 5.742377477410314\n",
      "---------------------\n",
      "Iteration Number: 7211\n",
      "Loss: 30.91421376438866\n",
      "l2 norm of gradients: 0.2016753814204397\n",
      "l2 norm of weights: 5.742305181414842\n",
      "---------------------\n",
      "Iteration Number: 7212\n",
      "Loss: 30.912174412782687\n",
      "l2 norm of gradients: 0.20165684568169667\n",
      "l2 norm of weights: 5.742232891203526\n",
      "---------------------\n",
      "Iteration Number: 7213\n",
      "Loss: 30.910135436266835\n",
      "l2 norm of gradients: 0.20163831420618683\n",
      "l2 norm of weights: 5.742160606775458\n",
      "---------------------\n",
      "Iteration Number: 7214\n",
      "Loss: 30.908096834721974\n",
      "l2 norm of gradients: 0.20161978699202318\n",
      "l2 norm of weights: 5.742088328129732\n",
      "---------------------\n",
      "Iteration Number: 7215\n",
      "Loss: 30.906058608023436\n",
      "l2 norm of gradients: 0.20160126403731968\n",
      "l2 norm of weights: 5.742016055265442\n",
      "---------------------\n",
      "Iteration Number: 7216\n",
      "Loss: 30.904020756057268\n",
      "l2 norm of gradients: 0.20158274534019152\n",
      "l2 norm of weights: 5.741943788181678\n",
      "---------------------\n",
      "Iteration Number: 7217\n",
      "Loss: 30.901983278698147\n",
      "l2 norm of gradients: 0.2015642308987554\n",
      "l2 norm of weights: 5.741871526877538\n",
      "---------------------\n",
      "Iteration Number: 7218\n",
      "Loss: 30.899946175826617\n",
      "l2 norm of gradients: 0.2015457207111288\n",
      "l2 norm of weights: 5.741799271352113\n",
      "---------------------\n",
      "Iteration Number: 7219\n",
      "Loss: 30.897909447316376\n",
      "l2 norm of gradients: 0.20152721477543084\n",
      "l2 norm of weights: 5.741727021604498\n",
      "---------------------\n",
      "Iteration Number: 7220\n",
      "Loss: 30.895873093055798\n",
      "l2 norm of gradients: 0.2015087130897815\n",
      "l2 norm of weights: 5.741654777633787\n",
      "---------------------\n",
      "Iteration Number: 7221\n",
      "Loss: 30.893837112919474\n",
      "l2 norm of gradients: 0.20149021565230224\n",
      "l2 norm of weights: 5.741582539439077\n",
      "---------------------\n",
      "Iteration Number: 7222\n",
      "Loss: 30.891801506786493\n",
      "l2 norm of gradients: 0.20147172246111564\n",
      "l2 norm of weights: 5.7415103070194595\n",
      "---------------------\n",
      "Iteration Number: 7223\n",
      "Loss: 30.889766274533777\n",
      "l2 norm of gradients: 0.20145323351434544\n",
      "l2 norm of weights: 5.741438080374032\n",
      "---------------------\n",
      "Iteration Number: 7224\n",
      "Loss: 30.887731416046584\n",
      "l2 norm of gradients: 0.2014347488101166\n",
      "l2 norm of weights: 5.74136585950189\n",
      "---------------------\n",
      "Iteration Number: 7225\n",
      "Loss: 30.88569693120108\n",
      "l2 norm of gradients: 0.2014162683465555\n",
      "l2 norm of weights: 5.741293644402129\n",
      "---------------------\n",
      "Iteration Number: 7226\n",
      "Loss: 30.88366281987745\n",
      "l2 norm of gradients: 0.20139779212178938\n",
      "l2 norm of weights: 5.741221435073845\n",
      "---------------------\n",
      "Iteration Number: 7227\n",
      "Loss: 30.881629081954685\n",
      "l2 norm of gradients: 0.20137932013394702\n",
      "l2 norm of weights: 5.741149231516133\n",
      "---------------------\n",
      "Iteration Number: 7228\n",
      "Loss: 30.879595717314494\n",
      "l2 norm of gradients: 0.20136085238115817\n",
      "l2 norm of weights: 5.741077033728092\n",
      "---------------------\n",
      "Iteration Number: 7229\n",
      "Loss: 30.877562725835208\n",
      "l2 norm of gradients: 0.20134238886155387\n",
      "l2 norm of weights: 5.7410048417088175\n",
      "---------------------\n",
      "Iteration Number: 7230\n",
      "Loss: 30.8755301073983\n",
      "l2 norm of gradients: 0.20132392957326645\n",
      "l2 norm of weights: 5.740932655457407\n",
      "---------------------\n",
      "Iteration Number: 7231\n",
      "Loss: 30.873497861880693\n",
      "l2 norm of gradients: 0.20130547451442926\n",
      "l2 norm of weights: 5.7408604749729575\n",
      "---------------------\n",
      "Iteration Number: 7232\n",
      "Loss: 30.871465989161166\n",
      "l2 norm of gradients: 0.20128702368317705\n",
      "l2 norm of weights: 5.740788300254567\n",
      "---------------------\n",
      "Iteration Number: 7233\n",
      "Loss: 30.869434489124085\n",
      "l2 norm of gradients: 0.20126857707764564\n",
      "l2 norm of weights: 5.740716131301333\n",
      "---------------------\n",
      "Iteration Number: 7234\n",
      "Loss: 30.86740336164803\n",
      "l2 norm of gradients: 0.20125013469597208\n",
      "l2 norm of weights: 5.740643968112355\n",
      "---------------------\n",
      "Iteration Number: 7235\n",
      "Loss: 30.86537260661517\n",
      "l2 norm of gradients: 0.2012316965362947\n",
      "l2 norm of weights: 5.74057181068673\n",
      "---------------------\n",
      "Iteration Number: 7236\n",
      "Loss: 30.863342223899764\n",
      "l2 norm of gradients: 0.2012132625967529\n",
      "l2 norm of weights: 5.740499659023557\n",
      "---------------------\n",
      "Iteration Number: 7237\n",
      "Loss: 30.86131221338856\n",
      "l2 norm of gradients: 0.20119483287548728\n",
      "l2 norm of weights: 5.7404275131219356\n",
      "---------------------\n",
      "Iteration Number: 7238\n",
      "Loss: 30.859282574958538\n",
      "l2 norm of gradients: 0.20117640737063974\n",
      "l2 norm of weights: 5.740355372980964\n",
      "---------------------\n",
      "Iteration Number: 7239\n",
      "Loss: 30.857253308493036\n",
      "l2 norm of gradients: 0.20115798608035337\n",
      "l2 norm of weights: 5.740283238599743\n",
      "---------------------\n",
      "Iteration Number: 7240\n",
      "Loss: 30.855224413868534\n",
      "l2 norm of gradients: 0.20113956900277236\n",
      "l2 norm of weights: 5.740211109977372\n",
      "---------------------\n",
      "Iteration Number: 7241\n",
      "Loss: 30.853195890968603\n",
      "l2 norm of gradients: 0.2011211561360422\n",
      "l2 norm of weights: 5.7401389871129505\n",
      "---------------------\n",
      "Iteration Number: 7242\n",
      "Loss: 30.851167739671318\n",
      "l2 norm of gradients: 0.20110274747830945\n",
      "l2 norm of weights: 5.740066870005578\n",
      "---------------------\n",
      "Iteration Number: 7243\n",
      "Loss: 30.849139959859116\n",
      "l2 norm of gradients: 0.20108434302772188\n",
      "l2 norm of weights: 5.739994758654357\n",
      "---------------------\n",
      "Iteration Number: 7244\n",
      "Loss: 30.847112551412305\n",
      "l2 norm of gradients: 0.20106594278242862\n",
      "l2 norm of weights: 5.739922653058389\n",
      "---------------------\n",
      "Iteration Number: 7245\n",
      "Loss: 30.845085514210556\n",
      "l2 norm of gradients: 0.2010475467405797\n",
      "l2 norm of weights: 5.739850553216771\n",
      "---------------------\n",
      "Iteration Number: 7246\n",
      "Loss: 30.843058848134174\n",
      "l2 norm of gradients: 0.2010291549003266\n",
      "l2 norm of weights: 5.739778459128608\n",
      "---------------------\n",
      "Iteration Number: 7247\n",
      "Loss: 30.84103255306889\n",
      "l2 norm of gradients: 0.20101076725982184\n",
      "l2 norm of weights: 5.739706370793\n",
      "---------------------\n",
      "Iteration Number: 7248\n",
      "Loss: 30.839006628890715\n",
      "l2 norm of gradients: 0.20099238381721918\n",
      "l2 norm of weights: 5.73963428820905\n",
      "---------------------\n",
      "Iteration Number: 7249\n",
      "Loss: 30.836981075480338\n",
      "l2 norm of gradients: 0.20097400457067355\n",
      "l2 norm of weights: 5.739562211375858\n",
      "---------------------\n",
      "Iteration Number: 7250\n",
      "Loss: 30.834955892726295\n",
      "l2 norm of gradients: 0.200955629518341\n",
      "l2 norm of weights: 5.739490140292531\n",
      "---------------------\n",
      "Iteration Number: 7251\n",
      "Loss: 30.832931080499094\n",
      "l2 norm of gradients: 0.2009372586583789\n",
      "l2 norm of weights: 5.7394180749581665\n",
      "---------------------\n",
      "Iteration Number: 7252\n",
      "Loss: 30.830906638687708\n",
      "l2 norm of gradients: 0.2009188919889456\n",
      "l2 norm of weights: 5.739346015371869\n",
      "---------------------\n",
      "Iteration Number: 7253\n",
      "Loss: 30.828882567170627\n",
      "l2 norm of gradients: 0.20090052950820086\n",
      "l2 norm of weights: 5.739273961532744\n",
      "---------------------\n",
      "Iteration Number: 7254\n",
      "Loss: 30.826858865828356\n",
      "l2 norm of gradients: 0.2008821712143054\n",
      "l2 norm of weights: 5.739201913439894\n",
      "---------------------\n",
      "Iteration Number: 7255\n",
      "Loss: 30.82483553454265\n",
      "l2 norm of gradients: 0.20086381710542134\n",
      "l2 norm of weights: 5.73912987109242\n",
      "---------------------\n",
      "Iteration Number: 7256\n",
      "Loss: 30.82281257319385\n",
      "l2 norm of gradients: 0.20084546717971172\n",
      "l2 norm of weights: 5.7390578344894285\n",
      "---------------------\n",
      "Iteration Number: 7257\n",
      "Loss: 30.820789981669783\n",
      "l2 norm of gradients: 0.20082712143534093\n",
      "l2 norm of weights: 5.7389858036300225\n",
      "---------------------\n",
      "Iteration Number: 7258\n",
      "Loss: 30.818767759844132\n",
      "l2 norm of gradients: 0.2008087798704745\n",
      "l2 norm of weights: 5.738913778513308\n",
      "---------------------\n",
      "Iteration Number: 7259\n",
      "Loss: 30.816745907601703\n",
      "l2 norm of gradients: 0.20079044248327912\n",
      "l2 norm of weights: 5.7388417591383885\n",
      "---------------------\n",
      "Iteration Number: 7260\n",
      "Loss: 30.814724424824178\n",
      "l2 norm of gradients: 0.20077210927192266\n",
      "l2 norm of weights: 5.73876974550437\n",
      "---------------------\n",
      "Iteration Number: 7261\n",
      "Loss: 30.812703311395232\n",
      "l2 norm of gradients: 0.20075378023457405\n",
      "l2 norm of weights: 5.738697737610357\n",
      "---------------------\n",
      "Iteration Number: 7262\n",
      "Loss: 30.810682567191595\n",
      "l2 norm of gradients: 0.2007354553694036\n",
      "l2 norm of weights: 5.7386257354554555\n",
      "---------------------\n",
      "Iteration Number: 7263\n",
      "Loss: 30.808662192100634\n",
      "l2 norm of gradients: 0.20071713467458255\n",
      "l2 norm of weights: 5.738553739038772\n",
      "---------------------\n",
      "Iteration Number: 7264\n",
      "Loss: 30.806642185999138\n",
      "l2 norm of gradients: 0.20069881814828353\n",
      "l2 norm of weights: 5.73848174835941\n",
      "---------------------\n",
      "Iteration Number: 7265\n",
      "Loss: 30.80462254877275\n",
      "l2 norm of gradients: 0.2006805057886802\n",
      "l2 norm of weights: 5.738409763416478\n",
      "---------------------\n",
      "Iteration Number: 7266\n",
      "Loss: 30.802603280304034\n",
      "l2 norm of gradients: 0.20066219759394735\n",
      "l2 norm of weights: 5.738337784209083\n",
      "---------------------\n",
      "Iteration Number: 7267\n",
      "Loss: 30.800584380473374\n",
      "l2 norm of gradients: 0.20064389356226106\n",
      "l2 norm of weights: 5.738265810736331\n",
      "---------------------\n",
      "Iteration Number: 7268\n",
      "Loss: 30.798565849161477\n",
      "l2 norm of gradients: 0.20062559369179847\n",
      "l2 norm of weights: 5.738193842997328\n",
      "---------------------\n",
      "Iteration Number: 7269\n",
      "Loss: 30.796547686253426\n",
      "l2 norm of gradients: 0.2006072979807379\n",
      "l2 norm of weights: 5.738121880991184\n",
      "---------------------\n",
      "Iteration Number: 7270\n",
      "Loss: 30.794529891628045\n",
      "l2 norm of gradients: 0.20058900642725894\n",
      "l2 norm of weights: 5.738049924717004\n",
      "---------------------\n",
      "Iteration Number: 7271\n",
      "Loss: 30.79251246517129\n",
      "l2 norm of gradients: 0.20057071902954207\n",
      "l2 norm of weights: 5.737977974173897\n",
      "---------------------\n",
      "Iteration Number: 7272\n",
      "Loss: 30.79049540676275\n",
      "l2 norm of gradients: 0.20055243578576923\n",
      "l2 norm of weights: 5.737906029360971\n",
      "---------------------\n",
      "Iteration Number: 7273\n",
      "Loss: 30.788478716288964\n",
      "l2 norm of gradients: 0.2005341566941233\n",
      "l2 norm of weights: 5.737834090277335\n",
      "---------------------\n",
      "Iteration Number: 7274\n",
      "Loss: 30.786462393627726\n",
      "l2 norm of gradients: 0.20051588175278845\n",
      "l2 norm of weights: 5.737762156922098\n",
      "---------------------\n",
      "Iteration Number: 7275\n",
      "Loss: 30.78444643866363\n",
      "l2 norm of gradients: 0.20049761095994992\n",
      "l2 norm of weights: 5.737690229294367\n",
      "---------------------\n",
      "Iteration Number: 7276\n",
      "Loss: 30.782430851279784\n",
      "l2 norm of gradients: 0.2004793443137941\n",
      "l2 norm of weights: 5.737618307393253\n",
      "---------------------\n",
      "Iteration Number: 7277\n",
      "Loss: 30.780415631359674\n",
      "l2 norm of gradients: 0.20046108181250857\n",
      "l2 norm of weights: 5.737546391217865\n",
      "---------------------\n",
      "Iteration Number: 7278\n",
      "Loss: 30.778400778783038\n",
      "l2 norm of gradients: 0.20044282345428205\n",
      "l2 norm of weights: 5.737474480767313\n",
      "---------------------\n",
      "Iteration Number: 7279\n",
      "Loss: 30.77638629343188\n",
      "l2 norm of gradients: 0.20042456923730448\n",
      "l2 norm of weights: 5.737402576040705\n",
      "---------------------\n",
      "Iteration Number: 7280\n",
      "Loss: 30.774372175195488\n",
      "l2 norm of gradients: 0.20040631915976678\n",
      "l2 norm of weights: 5.737330677037154\n",
      "---------------------\n",
      "Iteration Number: 7281\n",
      "Loss: 30.77235842395283\n",
      "l2 norm of gradients: 0.2003880732198611\n",
      "l2 norm of weights: 5.737258783755769\n",
      "---------------------\n",
      "Iteration Number: 7282\n",
      "Loss: 30.770345039582594\n",
      "l2 norm of gradients: 0.2003698314157808\n",
      "l2 norm of weights: 5.737186896195662\n",
      "---------------------\n",
      "Iteration Number: 7283\n",
      "Loss: 30.768332021976285\n",
      "l2 norm of gradients: 0.20035159374572037\n",
      "l2 norm of weights: 5.737115014355942\n",
      "---------------------\n",
      "Iteration Number: 7284\n",
      "Loss: 30.76631937100881\n",
      "l2 norm of gradients: 0.20033336020787526\n",
      "l2 norm of weights: 5.737043138235721\n",
      "---------------------\n",
      "Iteration Number: 7285\n",
      "Loss: 30.764307086573307\n",
      "l2 norm of gradients: 0.2003151308004423\n",
      "l2 norm of weights: 5.736971267834113\n",
      "---------------------\n",
      "Iteration Number: 7286\n",
      "Loss: 30.762295168543005\n",
      "l2 norm of gradients: 0.20029690552161936\n",
      "l2 norm of weights: 5.736899403150225\n",
      "---------------------\n",
      "Iteration Number: 7287\n",
      "Loss: 30.760283616805477\n",
      "l2 norm of gradients: 0.20027868436960536\n",
      "l2 norm of weights: 5.736827544183174\n",
      "---------------------\n",
      "Iteration Number: 7288\n",
      "Loss: 30.75827243124281\n",
      "l2 norm of gradients: 0.2002604673426006\n",
      "l2 norm of weights: 5.736755690932068\n",
      "---------------------\n",
      "Iteration Number: 7289\n",
      "Loss: 30.75626161174433\n",
      "l2 norm of gradients: 0.20024225443880628\n",
      "l2 norm of weights: 5.7366838433960226\n",
      "---------------------\n",
      "Iteration Number: 7290\n",
      "Loss: 30.754251158182417\n",
      "l2 norm of gradients: 0.20022404565642477\n",
      "l2 norm of weights: 5.73661200157415\n",
      "---------------------\n",
      "Iteration Number: 7291\n",
      "Loss: 30.75224107045126\n",
      "l2 norm of gradients: 0.20020584099365973\n",
      "l2 norm of weights: 5.736540165465562\n",
      "---------------------\n",
      "Iteration Number: 7292\n",
      "Loss: 30.750231348426915\n",
      "l2 norm of gradients: 0.2001876404487158\n",
      "l2 norm of weights: 5.736468335069373\n",
      "---------------------\n",
      "Iteration Number: 7293\n",
      "Loss: 30.748221991994683\n",
      "l2 norm of gradients: 0.20016944401979883\n",
      "l2 norm of weights: 5.7363965103846954\n",
      "---------------------\n",
      "Iteration Number: 7294\n",
      "Loss: 30.746213001043454\n",
      "l2 norm of gradients: 0.20015125170511575\n",
      "l2 norm of weights: 5.736324691410645\n",
      "---------------------\n",
      "Iteration Number: 7295\n",
      "Loss: 30.744204375449886\n",
      "l2 norm of gradients: 0.20013306350287463\n",
      "l2 norm of weights: 5.7362528781463356\n",
      "---------------------\n",
      "Iteration Number: 7296\n",
      "Loss: 30.742196115102345\n",
      "l2 norm of gradients: 0.20011487941128467\n",
      "l2 norm of weights: 5.736181070590879\n",
      "---------------------\n",
      "Iteration Number: 7297\n",
      "Loss: 30.740188219882736\n",
      "l2 norm of gradients: 0.2000966994285563\n",
      "l2 norm of weights: 5.736109268743394\n",
      "---------------------\n",
      "Iteration Number: 7298\n",
      "Loss: 30.738180689673086\n",
      "l2 norm of gradients: 0.20007852355290096\n",
      "l2 norm of weights: 5.736037472602992\n",
      "---------------------\n",
      "Iteration Number: 7299\n",
      "Loss: 30.73617352436403\n",
      "l2 norm of gradients: 0.2000603517825312\n",
      "l2 norm of weights: 5.735965682168788\n",
      "---------------------\n",
      "Iteration Number: 7300\n",
      "Loss: 30.734166723834964\n",
      "l2 norm of gradients: 0.2000421841156608\n",
      "l2 norm of weights: 5.735893897439901\n",
      "---------------------\n",
      "Iteration Number: 7301\n",
      "Loss: 30.732160287968497\n",
      "l2 norm of gradients: 0.20002402055050456\n",
      "l2 norm of weights: 5.7358221184154425\n",
      "---------------------\n",
      "Iteration Number: 7302\n",
      "Loss: 30.730154216649574\n",
      "l2 norm of gradients: 0.20000586108527846\n",
      "l2 norm of weights: 5.735750345094531\n",
      "---------------------\n",
      "Iteration Number: 7303\n",
      "Loss: 30.7281485097677\n",
      "l2 norm of gradients: 0.19998770571819965\n",
      "l2 norm of weights: 5.735678577476282\n",
      "---------------------\n",
      "Iteration Number: 7304\n",
      "Loss: 30.72614316720023\n",
      "l2 norm of gradients: 0.19996955444748624\n",
      "l2 norm of weights: 5.7356068155598114\n",
      "---------------------\n",
      "Iteration Number: 7305\n",
      "Loss: 30.724138188833177\n",
      "l2 norm of gradients: 0.19995140727135768\n",
      "l2 norm of weights: 5.735535059344237\n",
      "---------------------\n",
      "Iteration Number: 7306\n",
      "Loss: 30.722133574555695\n",
      "l2 norm of gradients: 0.1999332641880343\n",
      "l2 norm of weights: 5.735463308828675\n",
      "---------------------\n",
      "Iteration Number: 7307\n",
      "Loss: 30.720129324243917\n",
      "l2 norm of gradients: 0.1999151251957378\n",
      "l2 norm of weights: 5.735391564012242\n",
      "---------------------\n",
      "Iteration Number: 7308\n",
      "Loss: 30.718125437787503\n",
      "l2 norm of gradients: 0.1998969902926907\n",
      "l2 norm of weights: 5.735319824894056\n",
      "---------------------\n",
      "Iteration Number: 7309\n",
      "Loss: 30.716121915072094\n",
      "l2 norm of gradients: 0.19987885947711698\n",
      "l2 norm of weights: 5.7352480914732356\n",
      "---------------------\n",
      "Iteration Number: 7310\n",
      "Loss: 30.71411875598037\n",
      "l2 norm of gradients: 0.19986073274724145\n",
      "l2 norm of weights: 5.735176363748897\n",
      "---------------------\n",
      "Iteration Number: 7311\n",
      "Loss: 30.712115960397945\n",
      "l2 norm of gradients: 0.19984261010129017\n",
      "l2 norm of weights: 5.735104641720159\n",
      "---------------------\n",
      "Iteration Number: 7312\n",
      "Loss: 30.71011352820887\n",
      "l2 norm of gradients: 0.19982449153749024\n",
      "l2 norm of weights: 5.7350329253861405\n",
      "---------------------\n",
      "Iteration Number: 7313\n",
      "Loss: 30.7081114592965\n",
      "l2 norm of gradients: 0.19980637705406998\n",
      "l2 norm of weights: 5.73496121474596\n",
      "---------------------\n",
      "Iteration Number: 7314\n",
      "Loss: 30.70610975354912\n",
      "l2 norm of gradients: 0.19978826664925872\n",
      "l2 norm of weights: 5.734889509798738\n",
      "---------------------\n",
      "Iteration Number: 7315\n",
      "Loss: 30.70410841084856\n",
      "l2 norm of gradients: 0.19977016032128686\n",
      "l2 norm of weights: 5.73481781054359\n",
      "---------------------\n",
      "Iteration Number: 7316\n",
      "Loss: 30.702107431080563\n",
      "l2 norm of gradients: 0.1997520580683861\n",
      "l2 norm of weights: 5.73474611697964\n",
      "---------------------\n",
      "Iteration Number: 7317\n",
      "Loss: 30.700106814131257\n",
      "l2 norm of gradients: 0.19973395988878911\n",
      "l2 norm of weights: 5.734674429106003\n",
      "---------------------\n",
      "Iteration Number: 7318\n",
      "Loss: 30.69810655988465\n",
      "l2 norm of gradients: 0.19971586578072958\n",
      "l2 norm of weights: 5.734602746921802\n",
      "---------------------\n",
      "Iteration Number: 7319\n",
      "Loss: 30.696106668227227\n",
      "l2 norm of gradients: 0.19969777574244252\n",
      "l2 norm of weights: 5.734531070426158\n",
      "---------------------\n",
      "Iteration Number: 7320\n",
      "Loss: 30.694107139041915\n",
      "l2 norm of gradients: 0.19967968977216385\n",
      "l2 norm of weights: 5.734459399618189\n",
      "---------------------\n",
      "Iteration Number: 7321\n",
      "Loss: 30.692107972214412\n",
      "l2 norm of gradients: 0.19966160786813075\n",
      "l2 norm of weights: 5.734387734497017\n",
      "---------------------\n",
      "Iteration Number: 7322\n",
      "Loss: 30.690109167629476\n",
      "l2 norm of gradients: 0.19964353002858135\n",
      "l2 norm of weights: 5.734316075061764\n",
      "---------------------\n",
      "Iteration Number: 7323\n",
      "Loss: 30.688110725178547\n",
      "l2 norm of gradients: 0.199625456251755\n",
      "l2 norm of weights: 5.7342444213115495\n",
      "---------------------\n",
      "Iteration Number: 7324\n",
      "Loss: 30.686112644736518\n",
      "l2 norm of gradients: 0.1996073865358921\n",
      "l2 norm of weights: 5.734172773245494\n",
      "---------------------\n",
      "Iteration Number: 7325\n",
      "Loss: 30.68411492620018\n",
      "l2 norm of gradients: 0.19958932087923423\n",
      "l2 norm of weights: 5.734101130862723\n",
      "---------------------\n",
      "Iteration Number: 7326\n",
      "Loss: 30.682117569446845\n",
      "l2 norm of gradients: 0.1995712592800238\n",
      "l2 norm of weights: 5.734029494162357\n",
      "---------------------\n",
      "Iteration Number: 7327\n",
      "Loss: 30.68012057436486\n",
      "l2 norm of gradients: 0.1995532017365047\n",
      "l2 norm of weights: 5.733957863143516\n",
      "---------------------\n",
      "Iteration Number: 7328\n",
      "Loss: 30.678123940838603\n",
      "l2 norm of gradients: 0.19953514824692167\n",
      "l2 norm of weights: 5.733886237805325\n",
      "---------------------\n",
      "Iteration Number: 7329\n",
      "Loss: 30.676127668759072\n",
      "l2 norm of gradients: 0.19951709880952057\n",
      "l2 norm of weights: 5.733814618146906\n",
      "---------------------\n",
      "Iteration Number: 7330\n",
      "Loss: 30.674131758003295\n",
      "l2 norm of gradients: 0.19949905342254834\n",
      "l2 norm of weights: 5.733743004167381\n",
      "---------------------\n",
      "Iteration Number: 7331\n",
      "Loss: 30.672136208465062\n",
      "l2 norm of gradients: 0.19948101208425317\n",
      "l2 norm of weights: 5.733671395865875\n",
      "---------------------\n",
      "Iteration Number: 7332\n",
      "Loss: 30.670141020023674\n",
      "l2 norm of gradients: 0.1994629747928841\n",
      "l2 norm of weights: 5.7335997932415115\n",
      "---------------------\n",
      "Iteration Number: 7333\n",
      "Loss: 30.66814619257094\n",
      "l2 norm of gradients: 0.1994449415466915\n",
      "l2 norm of weights: 5.733528196293412\n",
      "---------------------\n",
      "Iteration Number: 7334\n",
      "Loss: 30.666151725989284\n",
      "l2 norm of gradients: 0.19942691234392673\n",
      "l2 norm of weights: 5.733456605020704\n",
      "---------------------\n",
      "Iteration Number: 7335\n",
      "Loss: 30.66415762016637\n",
      "l2 norm of gradients: 0.1994088871828421\n",
      "l2 norm of weights: 5.733385019422509\n",
      "---------------------\n",
      "Iteration Number: 7336\n",
      "Loss: 30.662163874987503\n",
      "l2 norm of gradients: 0.19939086606169124\n",
      "l2 norm of weights: 5.7333134394979535\n",
      "---------------------\n",
      "Iteration Number: 7337\n",
      "Loss: 30.660170490338373\n",
      "l2 norm of gradients: 0.19937284897872867\n",
      "l2 norm of weights: 5.7332418652461605\n",
      "---------------------\n",
      "Iteration Number: 7338\n",
      "Loss: 30.65817746610618\n",
      "l2 norm of gradients: 0.19935483593221018\n",
      "l2 norm of weights: 5.733170296666255\n",
      "---------------------\n",
      "Iteration Number: 7339\n",
      "Loss: 30.656184802178934\n",
      "l2 norm of gradients: 0.19933682692039245\n",
      "l2 norm of weights: 5.733098733757364\n",
      "---------------------\n",
      "Iteration Number: 7340\n",
      "Loss: 30.654192498437077\n",
      "l2 norm of gradients: 0.19931882194153344\n",
      "l2 norm of weights: 5.733027176518614\n",
      "---------------------\n",
      "Iteration Number: 7341\n",
      "Loss: 30.652200554774456\n",
      "l2 norm of gradients: 0.19930082099389204\n",
      "l2 norm of weights: 5.732955624949127\n",
      "---------------------\n",
      "Iteration Number: 7342\n",
      "Loss: 30.650208971070356\n",
      "l2 norm of gradients: 0.1992828240757283\n",
      "l2 norm of weights: 5.7328840790480315\n",
      "---------------------\n",
      "Iteration Number: 7343\n",
      "Loss: 30.648217747216968\n",
      "l2 norm of gradients: 0.1992648311853033\n",
      "l2 norm of weights: 5.7328125388144535\n",
      "---------------------\n",
      "Iteration Number: 7344\n",
      "Loss: 30.646226883098606\n",
      "l2 norm of gradients: 0.19924684232087925\n",
      "l2 norm of weights: 5.732741004247519\n",
      "---------------------\n",
      "Iteration Number: 7345\n",
      "Loss: 30.644236378601367\n",
      "l2 norm of gradients: 0.19922885748071936\n",
      "l2 norm of weights: 5.732669475346356\n",
      "---------------------\n",
      "Iteration Number: 7346\n",
      "Loss: 30.642246233611253\n",
      "l2 norm of gradients: 0.199210876663088\n",
      "l2 norm of weights: 5.73259795211009\n",
      "---------------------\n",
      "Iteration Number: 7347\n",
      "Loss: 30.64025644802036\n",
      "l2 norm of gradients: 0.1991928998662506\n",
      "l2 norm of weights: 5.732526434537849\n",
      "---------------------\n",
      "Iteration Number: 7348\n",
      "Loss: 30.638267021708202\n",
      "l2 norm of gradients: 0.19917492708847365\n",
      "l2 norm of weights: 5.7324549226287616\n",
      "---------------------\n",
      "Iteration Number: 7349\n",
      "Loss: 30.63627795456611\n",
      "l2 norm of gradients: 0.19915695832802474\n",
      "l2 norm of weights: 5.732383416381954\n",
      "---------------------\n",
      "Iteration Number: 7350\n",
      "Loss: 30.63428924647976\n",
      "l2 norm of gradients: 0.19913899358317247\n",
      "l2 norm of weights: 5.732311915796555\n",
      "---------------------\n",
      "Iteration Number: 7351\n",
      "Loss: 30.632300897334208\n",
      "l2 norm of gradients: 0.19912103285218652\n",
      "l2 norm of weights: 5.732240420871693\n",
      "---------------------\n",
      "Iteration Number: 7352\n",
      "Loss: 30.63031290702035\n",
      "l2 norm of gradients: 0.19910307613333775\n",
      "l2 norm of weights: 5.7321689316064965\n",
      "---------------------\n",
      "Iteration Number: 7353\n",
      "Loss: 30.62832527542458\n",
      "l2 norm of gradients: 0.19908512342489798\n",
      "l2 norm of weights: 5.732097448000094\n",
      "---------------------\n",
      "Iteration Number: 7354\n",
      "Loss: 30.626338002428373\n",
      "l2 norm of gradients: 0.1990671747251401\n",
      "l2 norm of weights: 5.732025970051615\n",
      "---------------------\n",
      "Iteration Number: 7355\n",
      "Loss: 30.6243510879274\n",
      "l2 norm of gradients: 0.19904923003233815\n",
      "l2 norm of weights: 5.731954497760189\n",
      "---------------------\n",
      "Iteration Number: 7356\n",
      "Loss: 30.622364531803576\n",
      "l2 norm of gradients: 0.1990312893447672\n",
      "l2 norm of weights: 5.731883031124945\n",
      "---------------------\n",
      "Iteration Number: 7357\n",
      "Loss: 30.62037833394365\n",
      "l2 norm of gradients: 0.1990133526607033\n",
      "l2 norm of weights: 5.7318115701450125\n",
      "---------------------\n",
      "Iteration Number: 7358\n",
      "Loss: 30.618392494239522\n",
      "l2 norm of gradients: 0.19899541997842368\n",
      "l2 norm of weights: 5.731740114819522\n",
      "---------------------\n",
      "Iteration Number: 7359\n",
      "Loss: 30.61640701257173\n",
      "l2 norm of gradients: 0.19897749129620662\n",
      "l2 norm of weights: 5.7316686651476045\n",
      "---------------------\n",
      "Iteration Number: 7360\n",
      "Loss: 30.61442188883491\n",
      "l2 norm of gradients: 0.19895956661233138\n",
      "l2 norm of weights: 5.73159722112839\n",
      "---------------------\n",
      "Iteration Number: 7361\n",
      "Loss: 30.612437122913644\n",
      "l2 norm of gradients: 0.1989416459250784\n",
      "l2 norm of weights: 5.731525782761008\n",
      "---------------------\n",
      "Iteration Number: 7362\n",
      "Loss: 30.610452714694112\n",
      "l2 norm of gradients: 0.19892372923272905\n",
      "l2 norm of weights: 5.731454350044593\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 7363\n",
      "Loss: 30.608468664066333\n",
      "l2 norm of gradients: 0.19890581653356595\n",
      "l2 norm of weights: 5.731382922978272\n",
      "---------------------\n",
      "Iteration Number: 7364\n",
      "Loss: 30.606484970912526\n",
      "l2 norm of gradients: 0.1988879078258725\n",
      "l2 norm of weights: 5.731311501561179\n",
      "---------------------\n",
      "Iteration Number: 7365\n",
      "Loss: 30.60450163512955\n",
      "l2 norm of gradients: 0.19887000310793346\n",
      "l2 norm of weights: 5.731240085792447\n",
      "---------------------\n",
      "Iteration Number: 7366\n",
      "Loss: 30.602518656598555\n",
      "l2 norm of gradients: 0.1988521023780344\n",
      "l2 norm of weights: 5.731168675671205\n",
      "---------------------\n",
      "Iteration Number: 7367\n",
      "Loss: 30.600536035209352\n",
      "l2 norm of gradients: 0.19883420563446214\n",
      "l2 norm of weights: 5.731097271196587\n",
      "---------------------\n",
      "Iteration Number: 7368\n",
      "Loss: 30.59855377084897\n",
      "l2 norm of gradients: 0.19881631287550444\n",
      "l2 norm of weights: 5.731025872367725\n",
      "---------------------\n",
      "Iteration Number: 7369\n",
      "Loss: 30.596571863408283\n",
      "l2 norm of gradients: 0.19879842409945014\n",
      "l2 norm of weights: 5.730954479183753\n",
      "---------------------\n",
      "Iteration Number: 7370\n",
      "Loss: 30.594590312768904\n",
      "l2 norm of gradients: 0.19878053930458903\n",
      "l2 norm of weights: 5.730883091643803\n",
      "---------------------\n",
      "Iteration Number: 7371\n",
      "Loss: 30.592609118827546\n",
      "l2 norm of gradients: 0.1987626584892123\n",
      "l2 norm of weights: 5.730811709747009\n",
      "---------------------\n",
      "Iteration Number: 7372\n",
      "Loss: 30.590628281467584\n",
      "l2 norm of gradients: 0.19874478165161175\n",
      "l2 norm of weights: 5.730740333492504\n",
      "---------------------\n",
      "Iteration Number: 7373\n",
      "Loss: 30.588647800573455\n",
      "l2 norm of gradients: 0.19872690879008048\n",
      "l2 norm of weights: 5.730668962879421\n",
      "---------------------\n",
      "Iteration Number: 7374\n",
      "Loss: 30.58666767604385\n",
      "l2 norm of gradients: 0.1987090399029126\n",
      "l2 norm of weights: 5.730597597906896\n",
      "---------------------\n",
      "Iteration Number: 7375\n",
      "Loss: 30.58468790775631\n",
      "l2 norm of gradients: 0.19869117498840327\n",
      "l2 norm of weights: 5.730526238574061\n",
      "---------------------\n",
      "Iteration Number: 7376\n",
      "Loss: 30.58270849560335\n",
      "l2 norm of gradients: 0.19867331404484867\n",
      "l2 norm of weights: 5.730454884880053\n",
      "---------------------\n",
      "Iteration Number: 7377\n",
      "Loss: 30.58072943947596\n",
      "l2 norm of gradients: 0.1986554570705461\n",
      "l2 norm of weights: 5.730383536824004\n",
      "---------------------\n",
      "Iteration Number: 7378\n",
      "Loss: 30.57875073925749\n",
      "l2 norm of gradients: 0.19863760406379383\n",
      "l2 norm of weights: 5.730312194405051\n",
      "---------------------\n",
      "Iteration Number: 7379\n",
      "Loss: 30.576772394841438\n",
      "l2 norm of gradients: 0.19861975502289114\n",
      "l2 norm of weights: 5.730240857622329\n",
      "---------------------\n",
      "Iteration Number: 7380\n",
      "Loss: 30.574794406114247\n",
      "l2 norm of gradients: 0.19860190994613844\n",
      "l2 norm of weights: 5.730169526474973\n",
      "---------------------\n",
      "Iteration Number: 7381\n",
      "Loss: 30.572816772962913\n",
      "l2 norm of gradients: 0.19858406883183716\n",
      "l2 norm of weights: 5.73009820096212\n",
      "---------------------\n",
      "Iteration Number: 7382\n",
      "Loss: 30.57083949527753\n",
      "l2 norm of gradients: 0.19856623167828974\n",
      "l2 norm of weights: 5.730026881082905\n",
      "---------------------\n",
      "Iteration Number: 7383\n",
      "Loss: 30.568862572948866\n",
      "l2 norm of gradients: 0.19854839848379977\n",
      "l2 norm of weights: 5.729955566836464\n",
      "---------------------\n",
      "Iteration Number: 7384\n",
      "Loss: 30.566886005862678\n",
      "l2 norm of gradients: 0.1985305692466717\n",
      "l2 norm of weights: 5.729884258221934\n",
      "---------------------\n",
      "Iteration Number: 7385\n",
      "Loss: 30.56490979390863\n",
      "l2 norm of gradients: 0.19851274396521118\n",
      "l2 norm of weights: 5.729812955238451\n",
      "---------------------\n",
      "Iteration Number: 7386\n",
      "Loss: 30.56293393697775\n",
      "l2 norm of gradients: 0.19849492263772478\n",
      "l2 norm of weights: 5.729741657885153\n",
      "---------------------\n",
      "Iteration Number: 7387\n",
      "Loss: 30.5609584349548\n",
      "l2 norm of gradients: 0.19847710526252021\n",
      "l2 norm of weights: 5.729670366161178\n",
      "---------------------\n",
      "Iteration Number: 7388\n",
      "Loss: 30.558983287734577\n",
      "l2 norm of gradients: 0.19845929183790614\n",
      "l2 norm of weights: 5.72959908006566\n",
      "---------------------\n",
      "Iteration Number: 7389\n",
      "Loss: 30.557008495198993\n",
      "l2 norm of gradients: 0.19844148236219233\n",
      "l2 norm of weights: 5.729527799597741\n",
      "---------------------\n",
      "Iteration Number: 7390\n",
      "Loss: 30.55503405724369\n",
      "l2 norm of gradients: 0.19842367683368956\n",
      "l2 norm of weights: 5.729456524756557\n",
      "---------------------\n",
      "Iteration Number: 7391\n",
      "Loss: 30.553059973752173\n",
      "l2 norm of gradients: 0.19840587525070955\n",
      "l2 norm of weights: 5.729385255541247\n",
      "---------------------\n",
      "Iteration Number: 7392\n",
      "Loss: 30.55108624461862\n",
      "l2 norm of gradients: 0.1983880776115652\n",
      "l2 norm of weights: 5.7293139919509475\n",
      "---------------------\n",
      "Iteration Number: 7393\n",
      "Loss: 30.549112869732593\n",
      "l2 norm of gradients: 0.19837028391457043\n",
      "l2 norm of weights: 5.729242733984799\n",
      "---------------------\n",
      "Iteration Number: 7394\n",
      "Loss: 30.547139848976926\n",
      "l2 norm of gradients: 0.19835249415804002\n",
      "l2 norm of weights: 5.729171481641941\n",
      "---------------------\n",
      "Iteration Number: 7395\n",
      "Loss: 30.545167182249756\n",
      "l2 norm of gradients: 0.19833470834029\n",
      "l2 norm of weights: 5.72910023492151\n",
      "---------------------\n",
      "Iteration Number: 7396\n",
      "Loss: 30.54319486943005\n",
      "l2 norm of gradients: 0.1983169264596372\n",
      "l2 norm of weights: 5.729028993822648\n",
      "---------------------\n",
      "Iteration Number: 7397\n",
      "Loss: 30.541222910418085\n",
      "l2 norm of gradients: 0.19829914851439978\n",
      "l2 norm of weights: 5.728957758344494\n",
      "---------------------\n",
      "Iteration Number: 7398\n",
      "Loss: 30.53925130509752\n",
      "l2 norm of gradients: 0.1982813745028966\n",
      "l2 norm of weights: 5.728886528486187\n",
      "---------------------\n",
      "Iteration Number: 7399\n",
      "Loss: 30.53728005336146\n",
      "l2 norm of gradients: 0.1982636044234478\n",
      "l2 norm of weights: 5.7288153042468695\n",
      "---------------------\n",
      "Iteration Number: 7400\n",
      "Loss: 30.535309155092328\n",
      "l2 norm of gradients: 0.19824583827437436\n",
      "l2 norm of weights: 5.728744085625681\n",
      "---------------------\n",
      "Iteration Number: 7401\n",
      "Loss: 30.533338610187215\n",
      "l2 norm of gradients: 0.19822807605399842\n",
      "l2 norm of weights: 5.728672872621759\n",
      "---------------------\n",
      "Iteration Number: 7402\n",
      "Loss: 30.531368418534196\n",
      "l2 norm of gradients: 0.19821031776064307\n",
      "l2 norm of weights: 5.728601665234249\n",
      "---------------------\n",
      "Iteration Number: 7403\n",
      "Loss: 30.529398580021784\n",
      "l2 norm of gradients: 0.1981925633926325\n",
      "l2 norm of weights: 5.72853046346229\n",
      "---------------------\n",
      "Iteration Number: 7404\n",
      "Loss: 30.527429094539897\n",
      "l2 norm of gradients: 0.19817481294829184\n",
      "l2 norm of weights: 5.728459267305023\n",
      "---------------------\n",
      "Iteration Number: 7405\n",
      "Loss: 30.525459961981102\n",
      "l2 norm of gradients: 0.19815706642594727\n",
      "l2 norm of weights: 5.728388076761591\n",
      "---------------------\n",
      "Iteration Number: 7406\n",
      "Loss: 30.523491182228582\n",
      "l2 norm of gradients: 0.19813932382392588\n",
      "l2 norm of weights: 5.728316891831135\n",
      "---------------------\n",
      "Iteration Number: 7407\n",
      "Loss: 30.521522755180925\n",
      "l2 norm of gradients: 0.19812158514055606\n",
      "l2 norm of weights: 5.7282457125127975\n",
      "---------------------\n",
      "Iteration Number: 7408\n",
      "Loss: 30.519554680724106\n",
      "l2 norm of gradients: 0.19810385037416695\n",
      "l2 norm of weights: 5.728174538805721\n",
      "---------------------\n",
      "Iteration Number: 7409\n",
      "Loss: 30.517586958749625\n",
      "l2 norm of gradients: 0.19808611952308877\n",
      "l2 norm of weights: 5.728103370709048\n",
      "---------------------\n",
      "Iteration Number: 7410\n",
      "Loss: 30.515619589144666\n",
      "l2 norm of gradients: 0.19806839258565287\n",
      "l2 norm of weights: 5.728032208221921\n",
      "---------------------\n",
      "Iteration Number: 7411\n",
      "Loss: 30.513652571802886\n",
      "l2 norm of gradients: 0.1980506695601915\n",
      "l2 norm of weights: 5.727961051343484\n",
      "---------------------\n",
      "Iteration Number: 7412\n",
      "Loss: 30.511685906609966\n",
      "l2 norm of gradients: 0.19803295044503796\n",
      "l2 norm of weights: 5.727889900072879\n",
      "---------------------\n",
      "Iteration Number: 7413\n",
      "Loss: 30.509719593461877\n",
      "l2 norm of gradients: 0.19801523523852654\n",
      "l2 norm of weights: 5.727818754409251\n",
      "---------------------\n",
      "Iteration Number: 7414\n",
      "Loss: 30.507753632249024\n",
      "l2 norm of gradients: 0.19799752393899256\n",
      "l2 norm of weights: 5.727747614351743\n",
      "---------------------\n",
      "Iteration Number: 7415\n",
      "Loss: 30.505788022855924\n",
      "l2 norm of gradients: 0.1979798165447724\n",
      "l2 norm of weights: 5.727676479899499\n",
      "---------------------\n",
      "Iteration Number: 7416\n",
      "Loss: 30.503822765178903\n",
      "l2 norm of gradients: 0.1979621130542034\n",
      "l2 norm of weights: 5.727605351051664\n",
      "---------------------\n",
      "Iteration Number: 7417\n",
      "Loss: 30.501857859104863\n",
      "l2 norm of gradients: 0.1979444134656239\n",
      "l2 norm of weights: 5.727534227807382\n",
      "---------------------\n",
      "Iteration Number: 7418\n",
      "Loss: 30.499893304526747\n",
      "l2 norm of gradients: 0.19792671777737322\n",
      "l2 norm of weights: 5.7274631101657985\n",
      "---------------------\n",
      "Iteration Number: 7419\n",
      "Loss: 30.49792910133726\n",
      "l2 norm of gradients: 0.19790902598779178\n",
      "l2 norm of weights: 5.727391998126059\n",
      "---------------------\n",
      "Iteration Number: 7420\n",
      "Loss: 30.495965249418788\n",
      "l2 norm of gradients: 0.19789133809522105\n",
      "l2 norm of weights: 5.727320891687306\n",
      "---------------------\n",
      "Iteration Number: 7421\n",
      "Loss: 30.494001748669703\n",
      "l2 norm of gradients: 0.19787365409800325\n",
      "l2 norm of weights: 5.727249790848689\n",
      "---------------------\n",
      "Iteration Number: 7422\n",
      "Loss: 30.49203859898127\n",
      "l2 norm of gradients: 0.19785597399448185\n",
      "l2 norm of weights: 5.727178695609351\n",
      "---------------------\n",
      "Iteration Number: 7423\n",
      "Loss: 30.490075800240827\n",
      "l2 norm of gradients: 0.19783829778300124\n",
      "l2 norm of weights: 5.727107605968439\n",
      "---------------------\n",
      "Iteration Number: 7424\n",
      "Loss: 30.48811335234049\n",
      "l2 norm of gradients: 0.1978206254619069\n",
      "l2 norm of weights: 5.727036521925099\n",
      "---------------------\n",
      "Iteration Number: 7425\n",
      "Loss: 30.486151255172608\n",
      "l2 norm of gradients: 0.19780295702954515\n",
      "l2 norm of weights: 5.726965443478478\n",
      "---------------------\n",
      "Iteration Number: 7426\n",
      "Loss: 30.484189508625413\n",
      "l2 norm of gradients: 0.19778529248426338\n",
      "l2 norm of weights: 5.726894370627722\n",
      "---------------------\n",
      "Iteration Number: 7427\n",
      "Loss: 30.48222811259259\n",
      "l2 norm of gradients: 0.19776763182441004\n",
      "l2 norm of weights: 5.726823303371978\n",
      "---------------------\n",
      "Iteration Number: 7428\n",
      "Loss: 30.480267066963865\n",
      "l2 norm of gradients: 0.19774997504833458\n",
      "l2 norm of weights: 5.726752241710394\n",
      "---------------------\n",
      "Iteration Number: 7429\n",
      "Loss: 30.478306371631593\n",
      "l2 norm of gradients: 0.1977323221543873\n",
      "l2 norm of weights: 5.726681185642117\n",
      "---------------------\n",
      "Iteration Number: 7430\n",
      "Loss: 30.47634602648495\n",
      "l2 norm of gradients: 0.1977146731409197\n",
      "l2 norm of weights: 5.726610135166295\n",
      "---------------------\n",
      "Iteration Number: 7431\n",
      "Loss: 30.47438603141879\n",
      "l2 norm of gradients: 0.19769702800628414\n",
      "l2 norm of weights: 5.726539090282076\n",
      "---------------------\n",
      "Iteration Number: 7432\n",
      "Loss: 30.47242638632052\n",
      "l2 norm of gradients: 0.19767938674883404\n",
      "l2 norm of weights: 5.726468050988609\n",
      "---------------------\n",
      "Iteration Number: 7433\n",
      "Loss: 30.47046709108455\n",
      "l2 norm of gradients: 0.1976617493669238\n",
      "l2 norm of weights: 5.72639701728504\n",
      "---------------------\n",
      "Iteration Number: 7434\n",
      "Loss: 30.468508145599976\n",
      "l2 norm of gradients: 0.19764411585890876\n",
      "l2 norm of weights: 5.72632598917052\n",
      "---------------------\n",
      "Iteration Number: 7435\n",
      "Loss: 30.466549549763545\n",
      "l2 norm of gradients: 0.19762648622314538\n",
      "l2 norm of weights: 5.726254966644196\n",
      "---------------------\n",
      "Iteration Number: 7436\n",
      "Loss: 30.464591303459706\n",
      "l2 norm of gradients: 0.197608860457991\n",
      "l2 norm of weights: 5.72618394970522\n",
      "---------------------\n",
      "Iteration Number: 7437\n",
      "Loss: 30.462633406583635\n",
      "l2 norm of gradients: 0.19759123856180397\n",
      "l2 norm of weights: 5.726112938352738\n",
      "---------------------\n",
      "Iteration Number: 7438\n",
      "Loss: 30.460675859029706\n",
      "l2 norm of gradients: 0.19757362053294372\n",
      "l2 norm of weights: 5.726041932585903\n",
      "---------------------\n",
      "Iteration Number: 7439\n",
      "Loss: 30.45871866068491\n",
      "l2 norm of gradients: 0.19755600636977055\n",
      "l2 norm of weights: 5.725970932403862\n",
      "---------------------\n",
      "Iteration Number: 7440\n",
      "Loss: 30.456761811441346\n",
      "l2 norm of gradients: 0.1975383960706458\n",
      "l2 norm of weights: 5.725899937805767\n",
      "---------------------\n",
      "Iteration Number: 7441\n",
      "Loss: 30.45480531119688\n",
      "l2 norm of gradients: 0.19752078963393185\n",
      "l2 norm of weights: 5.725828948790767\n",
      "---------------------\n",
      "Iteration Number: 7442\n",
      "Loss: 30.452849159835818\n",
      "l2 norm of gradients: 0.197503187057992\n",
      "l2 norm of weights: 5.725757965358014\n",
      "---------------------\n",
      "Iteration Number: 7443\n",
      "Loss: 30.450893357254078\n",
      "l2 norm of gradients: 0.1974855883411905\n",
      "l2 norm of weights: 5.725686987506657\n",
      "---------------------\n",
      "Iteration Number: 7444\n",
      "Loss: 30.448937903342998\n",
      "l2 norm of gradients: 0.19746799348189276\n",
      "l2 norm of weights: 5.72561601523585\n",
      "---------------------\n",
      "Iteration Number: 7445\n",
      "Loss: 30.44698279799298\n",
      "l2 norm of gradients: 0.19745040247846496\n",
      "l2 norm of weights: 5.725545048544743\n",
      "---------------------\n",
      "Iteration Number: 7446\n",
      "Loss: 30.44502804110105\n",
      "l2 norm of gradients: 0.1974328153292744\n",
      "l2 norm of weights: 5.725474087432485\n",
      "---------------------\n",
      "Iteration Number: 7447\n",
      "Loss: 30.44307363255732\n",
      "l2 norm of gradients: 0.19741523203268938\n",
      "l2 norm of weights: 5.725403131898232\n",
      "---------------------\n",
      "Iteration Number: 7448\n",
      "Loss: 30.441119572250923\n",
      "l2 norm of gradients: 0.19739765258707903\n",
      "l2 norm of weights: 5.725332181941133\n",
      "---------------------\n",
      "Iteration Number: 7449\n",
      "Loss: 30.43916586007425\n",
      "l2 norm of gradients: 0.1973800769908136\n",
      "l2 norm of weights: 5.725261237560342\n",
      "---------------------\n",
      "Iteration Number: 7450\n",
      "Loss: 30.437212495925074\n",
      "l2 norm of gradients: 0.19736250524226437\n",
      "l2 norm of weights: 5.7251902987550105\n",
      "---------------------\n",
      "Iteration Number: 7451\n",
      "Loss: 30.43525947969023\n",
      "l2 norm of gradients: 0.19734493733980343\n",
      "l2 norm of weights: 5.725119365524291\n",
      "---------------------\n",
      "Iteration Number: 7452\n",
      "Loss: 30.433306811264856\n",
      "l2 norm of gradients: 0.19732737328180397\n",
      "l2 norm of weights: 5.725048437867338\n",
      "---------------------\n",
      "Iteration Number: 7453\n",
      "Loss: 30.431354490541647\n",
      "l2 norm of gradients: 0.1973098130666401\n",
      "l2 norm of weights: 5.724977515783304\n",
      "---------------------\n",
      "Iteration Number: 7454\n",
      "Loss: 30.42940251741063\n",
      "l2 norm of gradients: 0.19729225669268696\n",
      "l2 norm of weights: 5.724906599271342\n",
      "---------------------\n",
      "Iteration Number: 7455\n",
      "Loss: 30.4274508917681\n",
      "l2 norm of gradients: 0.1972747041583206\n",
      "l2 norm of weights: 5.724835688330606\n",
      "---------------------\n",
      "Iteration Number: 7456\n",
      "Loss: 30.425499613503266\n",
      "l2 norm of gradients: 0.1972571554619181\n",
      "l2 norm of weights: 5.72476478296025\n",
      "---------------------\n",
      "Iteration Number: 7457\n",
      "Loss: 30.423548682510706\n",
      "l2 norm of gradients: 0.19723961060185755\n",
      "l2 norm of weights: 5.7246938831594285\n",
      "---------------------\n",
      "Iteration Number: 7458\n",
      "Loss: 30.421598098685994\n",
      "l2 norm of gradients: 0.1972220695765179\n",
      "l2 norm of weights: 5.724622988927296\n",
      "---------------------\n",
      "Iteration Number: 7459\n",
      "Loss: 30.419647861915617\n",
      "l2 norm of gradients: 0.1972045323842792\n",
      "l2 norm of weights: 5.724552100263006\n",
      "---------------------\n",
      "Iteration Number: 7460\n",
      "Loss: 30.417697972096807\n",
      "l2 norm of gradients: 0.19718699902352235\n",
      "l2 norm of weights: 5.724481217165716\n",
      "---------------------\n",
      "Iteration Number: 7461\n",
      "Loss: 30.4157484291212\n",
      "l2 norm of gradients: 0.19716946949262934\n",
      "l2 norm of weights: 5.724410339634578\n",
      "---------------------\n",
      "Iteration Number: 7462\n",
      "Loss: 30.413799232880613\n",
      "l2 norm of gradients: 0.19715194378998308\n",
      "l2 norm of weights: 5.724339467668748\n",
      "---------------------\n",
      "Iteration Number: 7463\n",
      "Loss: 30.411850383272075\n",
      "l2 norm of gradients: 0.1971344219139674\n",
      "l2 norm of weights: 5.724268601267383\n",
      "---------------------\n",
      "Iteration Number: 7464\n",
      "Loss: 30.409901880184876\n",
      "l2 norm of gradients: 0.19711690386296718\n",
      "l2 norm of weights: 5.72419774042964\n",
      "---------------------\n",
      "Iteration Number: 7465\n",
      "Loss: 30.407953723512858\n",
      "l2 norm of gradients: 0.19709938963536824\n",
      "l2 norm of weights: 5.7241268851546705\n",
      "---------------------\n",
      "Iteration Number: 7466\n",
      "Loss: 30.40600591314957\n",
      "l2 norm of gradients: 0.1970818792295573\n",
      "l2 norm of weights: 5.724056035441636\n",
      "---------------------\n",
      "Iteration Number: 7467\n",
      "Loss: 30.40405844898575\n",
      "l2 norm of gradients: 0.19706437264392218\n",
      "l2 norm of weights: 5.72398519128969\n",
      "---------------------\n",
      "Iteration Number: 7468\n",
      "Loss: 30.40211133092019\n",
      "l2 norm of gradients: 0.1970468698768516\n",
      "l2 norm of weights: 5.72391435269799\n",
      "---------------------\n",
      "Iteration Number: 7469\n",
      "Loss: 30.40016455884452\n",
      "l2 norm of gradients: 0.19702937092673523\n",
      "l2 norm of weights: 5.723843519665694\n",
      "---------------------\n",
      "Iteration Number: 7470\n",
      "Loss: 30.398218132650154\n",
      "l2 norm of gradients: 0.19701187579196372\n",
      "l2 norm of weights: 5.723772692191957\n",
      "---------------------\n",
      "Iteration Number: 7471\n",
      "Loss: 30.396272052229666\n",
      "l2 norm of gradients: 0.19699438447092865\n",
      "l2 norm of weights: 5.7237018702759395\n",
      "---------------------\n",
      "Iteration Number: 7472\n",
      "Loss: 30.394326317477766\n",
      "l2 norm of gradients: 0.1969768969620226\n",
      "l2 norm of weights: 5.7236310539167965\n",
      "---------------------\n",
      "Iteration Number: 7473\n",
      "Loss: 30.392380928291317\n",
      "l2 norm of gradients: 0.19695941326363922\n",
      "l2 norm of weights: 5.723560243113687\n",
      "---------------------\n",
      "Iteration Number: 7474\n",
      "Loss: 30.390435884560198\n",
      "l2 norm of gradients: 0.1969419333741728\n",
      "l2 norm of weights: 5.723489437865771\n",
      "---------------------\n",
      "Iteration Number: 7475\n",
      "Loss: 30.38849118617522\n",
      "l2 norm of gradients: 0.19692445729201896\n",
      "l2 norm of weights: 5.723418638172204\n",
      "---------------------\n",
      "Iteration Number: 7476\n",
      "Loss: 30.38654683303822\n",
      "l2 norm of gradients: 0.1969069850155741\n",
      "l2 norm of weights: 5.723347844032147\n",
      "---------------------\n",
      "Iteration Number: 7477\n",
      "Loss: 30.384602825035877\n",
      "l2 norm of gradients: 0.19688951654323558\n",
      "l2 norm of weights: 5.723277055444758\n",
      "---------------------\n",
      "Iteration Number: 7478\n",
      "Loss: 30.382659162066265\n",
      "l2 norm of gradients: 0.19687205187340168\n",
      "l2 norm of weights: 5.723206272409195\n",
      "---------------------\n",
      "Iteration Number: 7479\n",
      "Loss: 30.380715844021456\n",
      "l2 norm of gradients: 0.1968545910044718\n",
      "l2 norm of weights: 5.72313549492462\n",
      "---------------------\n",
      "Iteration Number: 7480\n",
      "Loss: 30.37877287079309\n",
      "l2 norm of gradients: 0.19683713393484614\n",
      "l2 norm of weights: 5.72306472299019\n",
      "---------------------\n",
      "Iteration Number: 7481\n",
      "Loss: 30.376830242282615\n",
      "l2 norm of gradients: 0.19681968066292585\n",
      "l2 norm of weights: 5.722993956605067\n",
      "---------------------\n",
      "Iteration Number: 7482\n",
      "Loss: 30.374887958375602\n",
      "l2 norm of gradients: 0.19680223118711315\n",
      "l2 norm of weights: 5.72292319576841\n",
      "---------------------\n",
      "Iteration Number: 7483\n",
      "Loss: 30.372946018969067\n",
      "l2 norm of gradients: 0.19678478550581116\n",
      "l2 norm of weights: 5.72285244047938\n",
      "---------------------\n",
      "Iteration Number: 7484\n",
      "Loss: 30.37100442395522\n",
      "l2 norm of gradients: 0.19676734361742396\n",
      "l2 norm of weights: 5.722781690737137\n",
      "---------------------\n",
      "Iteration Number: 7485\n",
      "Loss: 30.369063173235357\n",
      "l2 norm of gradients: 0.19674990552035657\n",
      "l2 norm of weights: 5.7227109465408414\n",
      "---------------------\n",
      "Iteration Number: 7486\n",
      "Loss: 30.367122266697773\n",
      "l2 norm of gradients: 0.1967324712130149\n",
      "l2 norm of weights: 5.722640207889656\n",
      "---------------------\n",
      "Iteration Number: 7487\n",
      "Loss: 30.365181704236978\n",
      "l2 norm of gradients: 0.1967150406938059\n",
      "l2 norm of weights: 5.72256947478274\n",
      "---------------------\n",
      "Iteration Number: 7488\n",
      "Loss: 30.363241485748972\n",
      "l2 norm of gradients: 0.19669761396113752\n",
      "l2 norm of weights: 5.722498747219256\n",
      "---------------------\n",
      "Iteration Number: 7489\n",
      "Loss: 30.36130161112499\n",
      "l2 norm of gradients: 0.19668019101341858\n",
      "l2 norm of weights: 5.722428025198366\n",
      "---------------------\n",
      "Iteration Number: 7490\n",
      "Loss: 30.359362080267136\n",
      "l2 norm of gradients: 0.19666277184905867\n",
      "l2 norm of weights: 5.722357308719231\n",
      "---------------------\n",
      "Iteration Number: 7491\n",
      "Loss: 30.357422893059375\n",
      "l2 norm of gradients: 0.19664535646646877\n",
      "l2 norm of weights: 5.722286597781015\n",
      "---------------------\n",
      "Iteration Number: 7492\n",
      "Loss: 30.355484049403696\n",
      "l2 norm of gradients: 0.19662794486406043\n",
      "l2 norm of weights: 5.722215892382879\n",
      "---------------------\n",
      "Iteration Number: 7493\n",
      "Loss: 30.35354554919267\n",
      "l2 norm of gradients: 0.19661053704024622\n",
      "l2 norm of weights: 5.722145192523986\n",
      "---------------------\n",
      "Iteration Number: 7494\n",
      "Loss: 30.351607392318794\n",
      "l2 norm of gradients: 0.1965931329934398\n",
      "l2 norm of weights: 5.722074498203499\n",
      "---------------------\n",
      "Iteration Number: 7495\n",
      "Loss: 30.349669578679716\n",
      "l2 norm of gradients: 0.19657573272205553\n",
      "l2 norm of weights: 5.72200380942058\n",
      "---------------------\n",
      "Iteration Number: 7496\n",
      "Loss: 30.347732108167996\n",
      "l2 norm of gradients: 0.19655833622450905\n",
      "l2 norm of weights: 5.721933126174394\n",
      "---------------------\n",
      "Iteration Number: 7497\n",
      "Loss: 30.345794980681507\n",
      "l2 norm of gradients: 0.19654094349921655\n",
      "l2 norm of weights: 5.721862448464106\n",
      "---------------------\n",
      "Iteration Number: 7498\n",
      "Loss: 30.343858196113317\n",
      "l2 norm of gradients: 0.19652355454459552\n",
      "l2 norm of weights: 5.7217917762888755\n",
      "---------------------\n",
      "Iteration Number: 7499\n",
      "Loss: 30.34192175435689\n",
      "l2 norm of gradients: 0.19650616935906418\n",
      "l2 norm of weights: 5.721721109647871\n",
      "---------------------\n",
      "Iteration Number: 7500\n",
      "Loss: 30.339985655308322\n",
      "l2 norm of gradients: 0.1964887879410418\n",
      "l2 norm of weights: 5.721650448540254\n",
      "---------------------\n",
      "Iteration Number: 7501\n",
      "Loss: 30.338049898861804\n",
      "l2 norm of gradients: 0.1964714102889484\n",
      "l2 norm of weights: 5.72157979296519\n",
      "---------------------\n",
      "Iteration Number: 7502\n",
      "Loss: 30.336114484916994\n",
      "l2 norm of gradients: 0.19645403640120526\n",
      "l2 norm of weights: 5.721509142921844\n",
      "---------------------\n",
      "Iteration Number: 7503\n",
      "Loss: 30.33417941336186\n",
      "l2 norm of gradients: 0.19643666627623427\n",
      "l2 norm of weights: 5.721438498409381\n",
      "---------------------\n",
      "Iteration Number: 7504\n",
      "Loss: 30.33224468409572\n",
      "l2 norm of gradients: 0.1964192999124585\n",
      "l2 norm of weights: 5.721367859426966\n",
      "---------------------\n",
      "Iteration Number: 7505\n",
      "Loss: 30.33031029701315\n",
      "l2 norm of gradients: 0.1964019373083018\n",
      "l2 norm of weights: 5.721297225973764\n",
      "---------------------\n",
      "Iteration Number: 7506\n",
      "Loss: 30.32837625200932\n",
      "l2 norm of gradients: 0.19638457846218904\n",
      "l2 norm of weights: 5.7212265980489425\n",
      "---------------------\n",
      "Iteration Number: 7507\n",
      "Loss: 30.32644254897681\n",
      "l2 norm of gradients: 0.196367223372546\n",
      "l2 norm of weights: 5.721155975651667\n",
      "---------------------\n",
      "Iteration Number: 7508\n",
      "Loss: 30.32450918781897\n",
      "l2 norm of gradients: 0.19634987203779944\n",
      "l2 norm of weights: 5.721085358781101\n",
      "---------------------\n",
      "Iteration Number: 7509\n",
      "Loss: 30.322576168421374\n",
      "l2 norm of gradients: 0.1963325244563769\n",
      "l2 norm of weights: 5.721014747436414\n",
      "---------------------\n",
      "Iteration Number: 7510\n",
      "Loss: 30.320643490683057\n",
      "l2 norm of gradients: 0.19631518062670708\n",
      "l2 norm of weights: 5.720944141616772\n",
      "---------------------\n",
      "Iteration Number: 7511\n",
      "Loss: 30.318711154505085\n",
      "l2 norm of gradients: 0.19629784054721944\n",
      "l2 norm of weights: 5.720873541321341\n",
      "---------------------\n",
      "Iteration Number: 7512\n",
      "Loss: 30.31677915977501\n",
      "l2 norm of gradients: 0.1962805042163444\n",
      "l2 norm of weights: 5.720802946549289\n",
      "---------------------\n",
      "Iteration Number: 7513\n",
      "Loss: 30.314847506390645\n",
      "l2 norm of gradients: 0.19626317163251342\n",
      "l2 norm of weights: 5.720732357299783\n",
      "---------------------\n",
      "Iteration Number: 7514\n",
      "Loss: 30.312916194251564\n",
      "l2 norm of gradients: 0.19624584279415874\n",
      "l2 norm of weights: 5.720661773571991\n",
      "---------------------\n",
      "Iteration Number: 7515\n",
      "Loss: 30.310985223247265\n",
      "l2 norm of gradients: 0.1962285176997137\n",
      "l2 norm of weights: 5.7205911953650785\n",
      "---------------------\n",
      "Iteration Number: 7516\n",
      "Loss: 30.30905459327917\n",
      "l2 norm of gradients: 0.19621119634761236\n",
      "l2 norm of weights: 5.720520622678217\n",
      "---------------------\n",
      "Iteration Number: 7517\n",
      "Loss: 30.307124304242155\n",
      "l2 norm of gradients: 0.1961938787362898\n",
      "l2 norm of weights: 5.7204500555105735\n",
      "---------------------\n",
      "Iteration Number: 7518\n",
      "Loss: 30.305194356028355\n",
      "l2 norm of gradients: 0.19617656486418214\n",
      "l2 norm of weights: 5.720379493861316\n",
      "---------------------\n",
      "Iteration Number: 7519\n",
      "Loss: 30.303264748536236\n",
      "l2 norm of gradients: 0.1961592547297263\n",
      "l2 norm of weights: 5.720308937729614\n",
      "---------------------\n",
      "Iteration Number: 7520\n",
      "Loss: 30.301335481662747\n",
      "l2 norm of gradients: 0.19614194833136012\n",
      "l2 norm of weights: 5.720238387114635\n",
      "---------------------\n",
      "Iteration Number: 7521\n",
      "Loss: 30.299406555300507\n",
      "l2 norm of gradients: 0.1961246456675224\n",
      "l2 norm of weights: 5.72016784201555\n",
      "---------------------\n",
      "Iteration Number: 7522\n",
      "Loss: 30.29747796935035\n",
      "l2 norm of gradients: 0.19610734673665292\n",
      "l2 norm of weights: 5.720097302431527\n",
      "---------------------\n",
      "Iteration Number: 7523\n",
      "Loss: 30.295549723701807\n",
      "l2 norm of gradients: 0.19609005153719222\n",
      "l2 norm of weights: 5.720026768361736\n",
      "---------------------\n",
      "Iteration Number: 7524\n",
      "Loss: 30.293621818259947\n",
      "l2 norm of gradients: 0.19607276006758198\n",
      "l2 norm of weights: 5.719956239805349\n",
      "---------------------\n",
      "Iteration Number: 7525\n",
      "Loss: 30.291694252909853\n",
      "l2 norm of gradients: 0.1960554723262647\n",
      "l2 norm of weights: 5.7198857167615325\n",
      "---------------------\n",
      "Iteration Number: 7526\n",
      "Loss: 30.28976702755704\n",
      "l2 norm of gradients: 0.19603818831168368\n",
      "l2 norm of weights: 5.719815199229459\n",
      "---------------------\n",
      "Iteration Number: 7527\n",
      "Loss: 30.287840142094925\n",
      "l2 norm of gradients: 0.1960209080222833\n",
      "l2 norm of weights: 5.7197446872082995\n",
      "---------------------\n",
      "Iteration Number: 7528\n",
      "Loss: 30.28591359641909\n",
      "l2 norm of gradients: 0.1960036314565089\n",
      "l2 norm of weights: 5.719674180697225\n",
      "---------------------\n",
      "Iteration Number: 7529\n",
      "Loss: 30.283987390429388\n",
      "l2 norm of gradients: 0.19598635861280655\n",
      "l2 norm of weights: 5.719603679695404\n",
      "---------------------\n",
      "Iteration Number: 7530\n",
      "Loss: 30.282061524014217\n",
      "l2 norm of gradients: 0.19596908948962335\n",
      "l2 norm of weights: 5.71953318420201\n",
      "---------------------\n",
      "Iteration Number: 7531\n",
      "Loss: 30.280135997079473\n",
      "l2 norm of gradients: 0.19595182408540734\n",
      "l2 norm of weights: 5.719462694216214\n",
      "---------------------\n",
      "Iteration Number: 7532\n",
      "Loss: 30.278210809513297\n",
      "l2 norm of gradients: 0.1959345623986075\n",
      "l2 norm of weights: 5.7193922097371885\n",
      "---------------------\n",
      "Iteration Number: 7533\n",
      "Loss: 30.276285961217653\n",
      "l2 norm of gradients: 0.1959173044276735\n",
      "l2 norm of weights: 5.719321730764105\n",
      "---------------------\n",
      "Iteration Number: 7534\n",
      "Loss: 30.274361452090144\n",
      "l2 norm of gradients: 0.19590005017105624\n",
      "l2 norm of weights: 5.7192512572961345\n",
      "---------------------\n",
      "Iteration Number: 7535\n",
      "Loss: 30.272437282023414\n",
      "l2 norm of gradients: 0.19588279962720737\n",
      "l2 norm of weights: 5.719180789332451\n",
      "---------------------\n",
      "Iteration Number: 7536\n",
      "Loss: 30.27051345091605\n",
      "l2 norm of gradients: 0.1958655527945794\n",
      "l2 norm of weights: 5.719110326872226\n",
      "---------------------\n",
      "Iteration Number: 7537\n",
      "Loss: 30.268589958665682\n",
      "l2 norm of gradients: 0.19584830967162592\n",
      "l2 norm of weights: 5.719039869914633\n",
      "---------------------\n",
      "Iteration Number: 7538\n",
      "Loss: 30.266666805168768\n",
      "l2 norm of gradients: 0.19583107025680135\n",
      "l2 norm of weights: 5.718969418458844\n",
      "---------------------\n",
      "Iteration Number: 7539\n",
      "Loss: 30.26474399032218\n",
      "l2 norm of gradients: 0.1958138345485609\n",
      "l2 norm of weights: 5.718898972504036\n",
      "---------------------\n",
      "Iteration Number: 7540\n",
      "Loss: 30.26282151401981\n",
      "l2 norm of gradients: 0.1957966025453609\n",
      "l2 norm of weights: 5.718828532049378\n",
      "---------------------\n",
      "Iteration Number: 7541\n",
      "Loss: 30.260899376163373\n",
      "l2 norm of gradients: 0.19577937424565842\n",
      "l2 norm of weights: 5.718758097094047\n",
      "---------------------\n",
      "Iteration Number: 7542\n",
      "Loss: 30.258977576645886\n",
      "l2 norm of gradients: 0.19576214964791158\n",
      "l2 norm of weights: 5.718687667637215\n",
      "---------------------\n",
      "Iteration Number: 7543\n",
      "Loss: 30.257056115366723\n",
      "l2 norm of gradients: 0.1957449287505793\n",
      "l2 norm of weights: 5.718617243678058\n",
      "---------------------\n",
      "Iteration Number: 7544\n",
      "Loss: 30.255134992226044\n",
      "l2 norm of gradients: 0.19572771155212138\n",
      "l2 norm of weights: 5.718546825215749\n",
      "---------------------\n",
      "Iteration Number: 7545\n",
      "Loss: 30.253214207113494\n",
      "l2 norm of gradients: 0.19571049805099877\n",
      "l2 norm of weights: 5.718476412249463\n",
      "---------------------\n",
      "Iteration Number: 7546\n",
      "Loss: 30.25129375993407\n",
      "l2 norm of gradients: 0.195693288245673\n",
      "l2 norm of weights: 5.718406004778376\n",
      "---------------------\n",
      "Iteration Number: 7547\n",
      "Loss: 30.249373650579635\n",
      "l2 norm of gradients: 0.19567608213460672\n",
      "l2 norm of weights: 5.718335602801662\n",
      "---------------------\n",
      "Iteration Number: 7548\n",
      "Loss: 30.247453878952083\n",
      "l2 norm of gradients: 0.19565887971626342\n",
      "l2 norm of weights: 5.718265206318497\n",
      "---------------------\n",
      "Iteration Number: 7549\n",
      "Loss: 30.245534444941693\n",
      "l2 norm of gradients: 0.1956416809891075\n",
      "l2 norm of weights: 5.718194815328057\n",
      "---------------------\n",
      "Iteration Number: 7550\n",
      "Loss: 30.24361534845324\n",
      "l2 norm of gradients: 0.1956244859516042\n",
      "l2 norm of weights: 5.718124429829517\n",
      "---------------------\n",
      "Iteration Number: 7551\n",
      "Loss: 30.24169658937938\n",
      "l2 norm of gradients: 0.19560729460221987\n",
      "l2 norm of weights: 5.7180540498220545\n",
      "---------------------\n",
      "Iteration Number: 7552\n",
      "Loss: 30.23977816762223\n",
      "l2 norm of gradients: 0.19559010693942144\n",
      "l2 norm of weights: 5.717983675304843\n",
      "---------------------\n",
      "Iteration Number: 7553\n",
      "Loss: 30.237860083074843\n",
      "l2 norm of gradients: 0.19557292296167705\n",
      "l2 norm of weights: 5.717913306277063\n",
      "---------------------\n",
      "Iteration Number: 7554\n",
      "Loss: 30.235942335638526\n",
      "l2 norm of gradients: 0.19555574266745554\n",
      "l2 norm of weights: 5.717842942737888\n",
      "---------------------\n",
      "Iteration Number: 7555\n",
      "Loss: 30.234024925208075\n",
      "l2 norm of gradients: 0.19553856605522674\n",
      "l2 norm of weights: 5.717772584686497\n",
      "---------------------\n",
      "Iteration Number: 7556\n",
      "Loss: 30.232107851682212\n",
      "l2 norm of gradients: 0.19552139312346134\n",
      "l2 norm of weights: 5.717702232122065\n",
      "---------------------\n",
      "Iteration Number: 7557\n",
      "Loss: 30.23019111496074\n",
      "l2 norm of gradients: 0.19550422387063093\n",
      "l2 norm of weights: 5.7176318850437715\n",
      "---------------------\n",
      "Iteration Number: 7558\n",
      "Loss: 30.228274714938255\n",
      "l2 norm of gradients: 0.1954870582952081\n",
      "l2 norm of weights: 5.717561543450794\n",
      "---------------------\n",
      "Iteration Number: 7559\n",
      "Loss: 30.226358651514296\n",
      "l2 norm of gradients: 0.19546989639566623\n",
      "l2 norm of weights: 5.717491207342309\n",
      "---------------------\n",
      "Iteration Number: 7560\n",
      "Loss: 30.22444292458374\n",
      "l2 norm of gradients: 0.19545273817047956\n",
      "l2 norm of weights: 5.7174208767174965\n",
      "---------------------\n",
      "Iteration Number: 7561\n",
      "Loss: 30.222527534050737\n",
      "l2 norm of gradients: 0.19543558361812327\n",
      "l2 norm of weights: 5.7173505515755325\n",
      "---------------------\n",
      "Iteration Number: 7562\n",
      "Loss: 30.22061247981131\n",
      "l2 norm of gradients: 0.19541843273707354\n",
      "l2 norm of weights: 5.717280231915599\n",
      "---------------------\n",
      "Iteration Number: 7563\n",
      "Loss: 30.218697761759778\n",
      "l2 norm of gradients: 0.19540128552580732\n",
      "l2 norm of weights: 5.717209917736873\n",
      "---------------------\n",
      "Iteration Number: 7564\n",
      "Loss: 30.216783379798258\n",
      "l2 norm of gradients: 0.19538414198280246\n",
      "l2 norm of weights: 5.717139609038533\n",
      "---------------------\n",
      "Iteration Number: 7565\n",
      "Loss: 30.214869333822957\n",
      "l2 norm of gradients: 0.19536700210653776\n",
      "l2 norm of weights: 5.7170693058197575\n",
      "---------------------\n",
      "Iteration Number: 7566\n",
      "Loss: 30.21295562372987\n",
      "l2 norm of gradients: 0.19534986589549286\n",
      "l2 norm of weights: 5.716999008079728\n",
      "---------------------\n",
      "Iteration Number: 7567\n",
      "Loss: 30.211042249423826\n",
      "l2 norm of gradients: 0.19533273334814832\n",
      "l2 norm of weights: 5.716928715817625\n",
      "---------------------\n",
      "Iteration Number: 7568\n",
      "Loss: 30.209129210794426\n",
      "l2 norm of gradients: 0.19531560446298565\n",
      "l2 norm of weights: 5.716858429032625\n",
      "---------------------\n",
      "Iteration Number: 7569\n",
      "Loss: 30.207216507748463\n",
      "l2 norm of gradients: 0.19529847923848706\n",
      "l2 norm of weights: 5.71678814772391\n",
      "---------------------\n",
      "Iteration Number: 7570\n",
      "Loss: 30.20530414018132\n",
      "l2 norm of gradients: 0.1952813576731359\n",
      "l2 norm of weights: 5.716717871890661\n",
      "---------------------\n",
      "Iteration Number: 7571\n",
      "Loss: 30.203392107988318\n",
      "l2 norm of gradients: 0.1952642397654162\n",
      "l2 norm of weights: 5.716647601532058\n",
      "---------------------\n",
      "Iteration Number: 7572\n",
      "Loss: 30.20148041107433\n",
      "l2 norm of gradients: 0.19524712551381304\n",
      "l2 norm of weights: 5.7165773366472825\n",
      "---------------------\n",
      "Iteration Number: 7573\n",
      "Loss: 30.19956904933101\n",
      "l2 norm of gradients: 0.19523001491681224\n",
      "l2 norm of weights: 5.716507077235516\n",
      "---------------------\n",
      "Iteration Number: 7574\n",
      "Loss: 30.197658022662203\n",
      "l2 norm of gradients: 0.19521290797290064\n",
      "l2 norm of weights: 5.716436823295936\n",
      "---------------------\n",
      "Iteration Number: 7575\n",
      "Loss: 30.195747330962067\n",
      "l2 norm of gradients: 0.19519580468056585\n",
      "l2 norm of weights: 5.716366574827729\n",
      "---------------------\n",
      "Iteration Number: 7576\n",
      "Loss: 30.193836974133447\n",
      "l2 norm of gradients: 0.19517870503829648\n",
      "l2 norm of weights: 5.716296331830074\n",
      "---------------------\n",
      "Iteration Number: 7577\n",
      "Loss: 30.191926952071384\n",
      "l2 norm of gradients: 0.19516160904458194\n",
      "l2 norm of weights: 5.716226094302154\n",
      "---------------------\n",
      "Iteration Number: 7578\n",
      "Loss: 30.190017264681437\n",
      "l2 norm of gradients: 0.1951445166979125\n",
      "l2 norm of weights: 5.716155862243152\n",
      "---------------------\n",
      "Iteration Number: 7579\n",
      "Loss: 30.188107911856385\n",
      "l2 norm of gradients: 0.19512742799677946\n",
      "l2 norm of weights: 5.716085635652248\n",
      "---------------------\n",
      "Iteration Number: 7580\n",
      "Loss: 30.186198893492648\n",
      "l2 norm of gradients: 0.1951103429396748\n",
      "l2 norm of weights: 5.716015414528626\n",
      "---------------------\n",
      "Iteration Number: 7581\n",
      "Loss: 30.184290209495558\n",
      "l2 norm of gradients: 0.19509326152509157\n",
      "l2 norm of weights: 5.715945198871469\n",
      "---------------------\n",
      "Iteration Number: 7582\n",
      "Loss: 30.182381859762806\n",
      "l2 norm of gradients: 0.1950761837515236\n",
      "l2 norm of weights: 5.7158749886799605\n",
      "---------------------\n",
      "Iteration Number: 7583\n",
      "Loss: 30.18047384418944\n",
      "l2 norm of gradients: 0.19505910961746556\n",
      "l2 norm of weights: 5.715804783953283\n",
      "---------------------\n",
      "Iteration Number: 7584\n",
      "Loss: 30.178566162678173\n",
      "l2 norm of gradients: 0.19504203912141316\n",
      "l2 norm of weights: 5.71573458469062\n",
      "---------------------\n",
      "Iteration Number: 7585\n",
      "Loss: 30.176658815129937\n",
      "l2 norm of gradients: 0.19502497226186283\n",
      "l2 norm of weights: 5.715664390891155\n",
      "---------------------\n",
      "Iteration Number: 7586\n",
      "Loss: 30.174751801438116\n",
      "l2 norm of gradients: 0.1950079090373119\n",
      "l2 norm of weights: 5.715594202554073\n",
      "---------------------\n",
      "Iteration Number: 7587\n",
      "Loss: 30.172845121506207\n",
      "l2 norm of gradients: 0.1949908494462588\n",
      "l2 norm of weights: 5.7155240196785595\n",
      "---------------------\n",
      "Iteration Number: 7588\n",
      "Loss: 30.170938775233736\n",
      "l2 norm of gradients: 0.1949737934872024\n",
      "l2 norm of weights: 5.715453842263796\n",
      "---------------------\n",
      "Iteration Number: 7589\n",
      "Loss: 30.16903276251979\n",
      "l2 norm of gradients: 0.19495674115864287\n",
      "l2 norm of weights: 5.715383670308968\n",
      "---------------------\n",
      "Iteration Number: 7590\n",
      "Loss: 30.16712708325805\n",
      "l2 norm of gradients: 0.19493969245908102\n",
      "l2 norm of weights: 5.715313503813262\n",
      "---------------------\n",
      "Iteration Number: 7591\n",
      "Loss: 30.165221737357772\n",
      "l2 norm of gradients: 0.19492264738701862\n",
      "l2 norm of weights: 5.715243342775862\n",
      "---------------------\n",
      "Iteration Number: 7592\n",
      "Loss: 30.16331672471128\n",
      "l2 norm of gradients: 0.19490560594095832\n",
      "l2 norm of weights: 5.715173187195954\n",
      "---------------------\n",
      "Iteration Number: 7593\n",
      "Loss: 30.161412045220512\n",
      "l2 norm of gradients: 0.1948885681194036\n",
      "l2 norm of weights: 5.715103037072721\n",
      "---------------------\n",
      "Iteration Number: 7594\n",
      "Loss: 30.15950769878456\n",
      "l2 norm of gradients: 0.19487153392085882\n",
      "l2 norm of weights: 5.715032892405352\n",
      "---------------------\n",
      "Iteration Number: 7595\n",
      "Loss: 30.157603685303258\n",
      "l2 norm of gradients: 0.19485450334382928\n",
      "l2 norm of weights: 5.714962753193031\n",
      "---------------------\n",
      "Iteration Number: 7596\n",
      "Loss: 30.15570000467694\n",
      "l2 norm of gradients: 0.194837476386821\n",
      "l2 norm of weights: 5.714892619434946\n",
      "---------------------\n",
      "Iteration Number: 7597\n",
      "Loss: 30.153796656801312\n",
      "l2 norm of gradients: 0.19482045304834109\n",
      "l2 norm of weights: 5.714822491130282\n",
      "---------------------\n",
      "Iteration Number: 7598\n",
      "Loss: 30.151893641584344\n",
      "l2 norm of gradients: 0.19480343332689734\n",
      "l2 norm of weights: 5.714752368278226\n",
      "---------------------\n",
      "Iteration Number: 7599\n",
      "Loss: 30.14999095891541\n",
      "l2 norm of gradients: 0.19478641722099854\n",
      "l2 norm of weights: 5.714682250877965\n",
      "---------------------\n",
      "Iteration Number: 7600\n",
      "Loss: 30.148088608704324\n",
      "l2 norm of gradients: 0.19476940472915424\n",
      "l2 norm of weights: 5.714612138928688\n",
      "---------------------\n",
      "Iteration Number: 7601\n",
      "Loss: 30.14618659084569\n",
      "l2 norm of gradients: 0.19475239584987492\n",
      "l2 norm of weights: 5.714542032429579\n",
      "---------------------\n",
      "Iteration Number: 7602\n",
      "Loss: 30.144284905238685\n",
      "l2 norm of gradients: 0.1947353905816719\n",
      "l2 norm of weights: 5.714471931379828\n",
      "---------------------\n",
      "Iteration Number: 7603\n",
      "Loss: 30.14238355178896\n",
      "l2 norm of gradients: 0.1947183889230574\n",
      "l2 norm of weights: 5.714401835778621\n",
      "---------------------\n",
      "Iteration Number: 7604\n",
      "Loss: 30.14048253038887\n",
      "l2 norm of gradients: 0.19470139087254457\n",
      "l2 norm of weights: 5.714331745625147\n",
      "---------------------\n",
      "Iteration Number: 7605\n",
      "Loss: 30.13858184094425\n",
      "l2 norm of gradients: 0.19468439642864727\n",
      "l2 norm of weights: 5.714261660918595\n",
      "---------------------\n",
      "Iteration Number: 7606\n",
      "Loss: 30.13668148335124\n",
      "l2 norm of gradients: 0.19466740558988024\n",
      "l2 norm of weights: 5.714191581658152\n",
      "---------------------\n",
      "Iteration Number: 7607\n",
      "Loss: 30.134781457513817\n",
      "l2 norm of gradients: 0.19465041835475932\n",
      "l2 norm of weights: 5.714121507843008\n",
      "---------------------\n",
      "Iteration Number: 7608\n",
      "Loss: 30.132881763332044\n",
      "l2 norm of gradients: 0.1946334347218009\n",
      "l2 norm of weights: 5.7140514394723505\n",
      "---------------------\n",
      "Iteration Number: 7609\n",
      "Loss: 30.130982400700418\n",
      "l2 norm of gradients: 0.19461645468952243\n",
      "l2 norm of weights: 5.71398137654537\n",
      "---------------------\n",
      "Iteration Number: 7610\n",
      "Loss: 30.129083369528495\n",
      "l2 norm of gradients: 0.19459947825644222\n",
      "l2 norm of weights: 5.713911319061256\n",
      "---------------------\n",
      "Iteration Number: 7611\n",
      "Loss: 30.12718466970964\n",
      "l2 norm of gradients: 0.19458250542107933\n",
      "l2 norm of weights: 5.713841267019197\n",
      "---------------------\n",
      "Iteration Number: 7612\n",
      "Loss: 30.125286301145444\n",
      "l2 norm of gradients: 0.1945655361819538\n",
      "l2 norm of weights: 5.7137712204183835\n",
      "---------------------\n",
      "Iteration Number: 7613\n",
      "Loss: 30.12338826373948\n",
      "l2 norm of gradients: 0.1945485705375864\n",
      "l2 norm of weights: 5.713701179258004\n",
      "---------------------\n",
      "Iteration Number: 7614\n",
      "Loss: 30.1214905573878\n",
      "l2 norm of gradients: 0.19453160848649884\n",
      "l2 norm of weights: 5.713631143537251\n",
      "---------------------\n",
      "Iteration Number: 7615\n",
      "Loss: 30.11959318199436\n",
      "l2 norm of gradients: 0.19451465002721383\n",
      "l2 norm of weights: 5.713561113255313\n",
      "---------------------\n",
      "Iteration Number: 7616\n",
      "Loss: 30.11769613745843\n",
      "l2 norm of gradients: 0.1944976951582546\n",
      "l2 norm of weights: 5.7134910884113825\n",
      "---------------------\n",
      "Iteration Number: 7617\n",
      "Loss: 30.11579942368212\n",
      "l2 norm of gradients: 0.1944807438781455\n",
      "l2 norm of weights: 5.713421069004649\n",
      "---------------------\n",
      "Iteration Number: 7618\n",
      "Loss: 30.113903040563223\n",
      "l2 norm of gradients: 0.19446379618541174\n",
      "l2 norm of weights: 5.713351055034305\n",
      "---------------------\n",
      "Iteration Number: 7619\n",
      "Loss: 30.112006988006044\n",
      "l2 norm of gradients: 0.19444685207857929\n",
      "l2 norm of weights: 5.7132810464995405\n",
      "---------------------\n",
      "Iteration Number: 7620\n",
      "Loss: 30.110111265909392\n",
      "l2 norm of gradients: 0.19442991155617498\n",
      "l2 norm of weights: 5.713211043399547\n",
      "---------------------\n",
      "Iteration Number: 7621\n",
      "Loss: 30.108215874174167\n",
      "l2 norm of gradients: 0.19441297461672652\n",
      "l2 norm of weights: 5.713141045733518\n",
      "---------------------\n",
      "Iteration Number: 7622\n",
      "Loss: 30.106320812701142\n",
      "l2 norm of gradients: 0.1943960412587625\n",
      "l2 norm of weights: 5.713071053500643\n",
      "---------------------\n",
      "Iteration Number: 7623\n",
      "Loss: 30.104426081393065\n",
      "l2 norm of gradients: 0.19437911148081233\n",
      "l2 norm of weights: 5.713001066700117\n",
      "---------------------\n",
      "Iteration Number: 7624\n",
      "Loss: 30.102531680148033\n",
      "l2 norm of gradients: 0.19436218528140634\n",
      "l2 norm of weights: 5.712931085331131\n",
      "---------------------\n",
      "Iteration Number: 7625\n",
      "Loss: 30.100637608869143\n",
      "l2 norm of gradients: 0.1943452626590755\n",
      "l2 norm of weights: 5.712861109392876\n",
      "---------------------\n",
      "Iteration Number: 7626\n",
      "Loss: 30.098743867459138\n",
      "l2 norm of gradients: 0.194328343612352\n",
      "l2 norm of weights: 5.7127911388845485\n",
      "---------------------\n",
      "Iteration Number: 7627\n",
      "Loss: 30.09685045581266\n",
      "l2 norm of gradients: 0.19431142813976854\n",
      "l2 norm of weights: 5.712721173805339\n",
      "---------------------\n",
      "Iteration Number: 7628\n",
      "Loss: 30.094957373838476\n",
      "l2 norm of gradients: 0.19429451623985883\n",
      "l2 norm of weights: 5.712651214154441\n",
      "---------------------\n",
      "Iteration Number: 7629\n",
      "Loss: 30.09306462143288\n",
      "l2 norm of gradients: 0.19427760791115742\n",
      "l2 norm of weights: 5.712581259931049\n",
      "---------------------\n",
      "Iteration Number: 7630\n",
      "Loss: 30.0911721984996\n",
      "l2 norm of gradients: 0.19426070315219973\n",
      "l2 norm of weights: 5.7125113111343575\n",
      "---------------------\n",
      "Iteration Number: 7631\n",
      "Loss: 30.089280104940578\n",
      "l2 norm of gradients: 0.19424380196152194\n",
      "l2 norm of weights: 5.7124413677635575\n",
      "---------------------\n",
      "Iteration Number: 7632\n",
      "Loss: 30.08738834065492\n",
      "l2 norm of gradients: 0.19422690433766115\n",
      "l2 norm of weights: 5.712371429817846\n",
      "---------------------\n",
      "Iteration Number: 7633\n",
      "Loss: 30.085496905545707\n",
      "l2 norm of gradients: 0.19421001027915533\n",
      "l2 norm of weights: 5.712301497296417\n",
      "---------------------\n",
      "Iteration Number: 7634\n",
      "Loss: 30.083605799511304\n",
      "l2 norm of gradients: 0.1941931197845432\n",
      "l2 norm of weights: 5.712231570198464\n",
      "---------------------\n",
      "Iteration Number: 7635\n",
      "Loss: 30.081715022456706\n",
      "l2 norm of gradients: 0.1941762328523645\n",
      "l2 norm of weights: 5.712161648523182\n",
      "---------------------\n",
      "Iteration Number: 7636\n",
      "Loss: 30.07982457428508\n",
      "l2 norm of gradients: 0.19415934948115954\n",
      "l2 norm of weights: 5.7120917322697675\n",
      "---------------------\n",
      "Iteration Number: 7637\n",
      "Loss: 30.07793445489262\n",
      "l2 norm of gradients: 0.1941424696694698\n",
      "l2 norm of weights: 5.712021821437415\n",
      "---------------------\n",
      "Iteration Number: 7638\n",
      "Loss: 30.07604466418213\n",
      "l2 norm of gradients: 0.19412559341583738\n",
      "l2 norm of weights: 5.71195191602532\n",
      "---------------------\n",
      "Iteration Number: 7639\n",
      "Loss: 30.07415520206015\n",
      "l2 norm of gradients: 0.19410872071880522\n",
      "l2 norm of weights: 5.7118820160326775\n",
      "---------------------\n",
      "Iteration Number: 7640\n",
      "Loss: 30.072266068423296\n",
      "l2 norm of gradients: 0.19409185157691733\n",
      "l2 norm of weights: 5.711812121458684\n",
      "---------------------\n",
      "Iteration Number: 7641\n",
      "Loss: 30.070377263178464\n",
      "l2 norm of gradients: 0.19407498598871833\n",
      "l2 norm of weights: 5.711742232302537\n",
      "---------------------\n",
      "Iteration Number: 7642\n",
      "Loss: 30.06848878622036\n",
      "l2 norm of gradients: 0.19405812395275376\n",
      "l2 norm of weights: 5.7116723485634315\n",
      "---------------------\n",
      "Iteration Number: 7643\n",
      "Loss: 30.06660063745839\n",
      "l2 norm of gradients: 0.19404126546756997\n",
      "l2 norm of weights: 5.711602470240563\n",
      "---------------------\n",
      "Iteration Number: 7644\n",
      "Loss: 30.064712816788113\n",
      "l2 norm of gradients: 0.19402441053171432\n",
      "l2 norm of weights: 5.711532597333131\n",
      "---------------------\n",
      "Iteration Number: 7645\n",
      "Loss: 30.062825324114712\n",
      "l2 norm of gradients: 0.19400755914373471\n",
      "l2 norm of weights: 5.711462729840331\n",
      "---------------------\n",
      "Iteration Number: 7646\n",
      "Loss: 30.060938159341895\n",
      "l2 norm of gradients: 0.19399071130218012\n",
      "l2 norm of weights: 5.711392867761361\n",
      "---------------------\n",
      "Iteration Number: 7647\n",
      "Loss: 30.059051322369296\n",
      "l2 norm of gradients: 0.19397386700560038\n",
      "l2 norm of weights: 5.711323011095416\n",
      "---------------------\n",
      "Iteration Number: 7648\n",
      "Loss: 30.057164813098105\n",
      "l2 norm of gradients: 0.1939570262525459\n",
      "l2 norm of weights: 5.711253159841697\n",
      "---------------------\n",
      "Iteration Number: 7649\n",
      "Loss: 30.055278631429484\n",
      "l2 norm of gradients: 0.1939401890415683\n",
      "l2 norm of weights: 5.711183313999401\n",
      "---------------------\n",
      "Iteration Number: 7650\n",
      "Loss: 30.05339277727102\n",
      "l2 norm of gradients: 0.1939233553712197\n",
      "l2 norm of weights: 5.711113473567726\n",
      "---------------------\n",
      "Iteration Number: 7651\n",
      "Loss: 30.051507250522683\n",
      "l2 norm of gradients: 0.19390652524005328\n",
      "l2 norm of weights: 5.71104363854587\n",
      "---------------------\n",
      "Iteration Number: 7652\n",
      "Loss: 30.049622051083773\n",
      "l2 norm of gradients: 0.19388969864662292\n",
      "l2 norm of weights: 5.71097380893303\n",
      "---------------------\n",
      "Iteration Number: 7653\n",
      "Loss: 30.047737178859276\n",
      "l2 norm of gradients: 0.1938728755894835\n",
      "l2 norm of weights: 5.710903984728409\n",
      "---------------------\n",
      "Iteration Number: 7654\n",
      "Loss: 30.04585263375335\n",
      "l2 norm of gradients: 0.19385605606719047\n",
      "l2 norm of weights: 5.710834165931202\n",
      "---------------------\n",
      "Iteration Number: 7655\n",
      "Loss: 30.04396841566572\n",
      "l2 norm of gradients: 0.19383924007830042\n",
      "l2 norm of weights: 5.71076435254061\n",
      "---------------------\n",
      "Iteration Number: 7656\n",
      "Loss: 30.04208452449962\n",
      "l2 norm of gradients: 0.19382242762137056\n",
      "l2 norm of weights: 5.710694544555833\n",
      "---------------------\n",
      "Iteration Number: 7657\n",
      "Loss: 30.04020096015798\n",
      "l2 norm of gradients: 0.19380561869495905\n",
      "l2 norm of weights: 5.710624741976068\n",
      "---------------------\n",
      "Iteration Number: 7658\n",
      "Loss: 30.038317722540935\n",
      "l2 norm of gradients: 0.1937888132976248\n",
      "l2 norm of weights: 5.710554944800519\n",
      "---------------------\n",
      "Iteration Number: 7659\n",
      "Loss: 30.03643481155256\n",
      "l2 norm of gradients: 0.19377201142792752\n",
      "l2 norm of weights: 5.710485153028382\n",
      "---------------------\n",
      "Iteration Number: 7660\n",
      "Loss: 30.0345522270991\n",
      "l2 norm of gradients: 0.19375521308442792\n",
      "l2 norm of weights: 5.71041536665886\n",
      "---------------------\n",
      "Iteration Number: 7661\n",
      "Loss: 30.032669969076835\n",
      "l2 norm of gradients: 0.1937384182656875\n",
      "l2 norm of weights: 5.710345585691154\n",
      "---------------------\n",
      "Iteration Number: 7662\n",
      "Loss: 30.03078803739121\n",
      "l2 norm of gradients: 0.19372162697026843\n",
      "l2 norm of weights: 5.710275810124461\n",
      "---------------------\n",
      "Iteration Number: 7663\n",
      "Loss: 30.028906431948577\n",
      "l2 norm of gradients: 0.19370483919673379\n",
      "l2 norm of weights: 5.710206039957985\n",
      "---------------------\n",
      "Iteration Number: 7664\n",
      "Loss: 30.02702515264742\n",
      "l2 norm of gradients: 0.1936880549436476\n",
      "l2 norm of weights: 5.710136275190927\n",
      "---------------------\n",
      "Iteration Number: 7665\n",
      "Loss: 30.02514419939362\n",
      "l2 norm of gradients: 0.19367127420957458\n",
      "l2 norm of weights: 5.710066515822489\n",
      "---------------------\n",
      "Iteration Number: 7666\n",
      "Loss: 30.023263572090382\n",
      "l2 norm of gradients: 0.19365449699308035\n",
      "l2 norm of weights: 5.70999676185187\n",
      "---------------------\n",
      "Iteration Number: 7667\n",
      "Loss: 30.02138327063658\n",
      "l2 norm of gradients: 0.1936377232927313\n",
      "l2 norm of weights: 5.709927013278274\n",
      "---------------------\n",
      "Iteration Number: 7668\n",
      "Loss: 30.019503294937326\n",
      "l2 norm of gradients: 0.19362095310709468\n",
      "l2 norm of weights: 5.709857270100903\n",
      "---------------------\n",
      "Iteration Number: 7669\n",
      "Loss: 30.017623644895387\n",
      "l2 norm of gradients: 0.19360418643473856\n",
      "l2 norm of weights: 5.709787532318958\n",
      "---------------------\n",
      "Iteration Number: 7670\n",
      "Loss: 30.015744320416772\n",
      "l2 norm of gradients: 0.1935874232742319\n",
      "l2 norm of weights: 5.709717799931642\n",
      "---------------------\n",
      "Iteration Number: 7671\n",
      "Loss: 30.013865321401287\n",
      "l2 norm of gradients: 0.19357066362414438\n",
      "l2 norm of weights: 5.709648072938159\n",
      "---------------------\n",
      "Iteration Number: 7672\n",
      "Loss: 30.011986647753496\n",
      "l2 norm of gradients: 0.19355390748304646\n",
      "l2 norm of weights: 5.7095783513377105\n",
      "---------------------\n",
      "Iteration Number: 7673\n",
      "Loss: 30.01010829937743\n",
      "l2 norm of gradients: 0.19353715484950965\n",
      "l2 norm of weights: 5.709508635129499\n",
      "---------------------\n",
      "Iteration Number: 7674\n",
      "Loss: 30.00823027617309\n",
      "l2 norm of gradients: 0.19352040572210613\n",
      "l2 norm of weights: 5.70943892431273\n",
      "---------------------\n",
      "Iteration Number: 7675\n",
      "Loss: 30.006352578048705\n",
      "l2 norm of gradients: 0.19350366009940886\n",
      "l2 norm of weights: 5.7093692188866045\n",
      "---------------------\n",
      "Iteration Number: 7676\n",
      "Loss: 30.004475204903354\n",
      "l2 norm of gradients: 0.19348691797999176\n",
      "l2 norm of weights: 5.709299518850329\n",
      "---------------------\n",
      "Iteration Number: 7677\n",
      "Loss: 30.00259815664411\n",
      "l2 norm of gradients: 0.1934701793624294\n",
      "l2 norm of weights: 5.709229824203105\n",
      "---------------------\n",
      "Iteration Number: 7678\n",
      "Loss: 30.00072143317189\n",
      "l2 norm of gradients: 0.19345344424529737\n",
      "l2 norm of weights: 5.709160134944138\n",
      "---------------------\n",
      "Iteration Number: 7679\n",
      "Loss: 29.998845034391053\n",
      "l2 norm of gradients: 0.1934367126271719\n",
      "l2 norm of weights: 5.709090451072632\n",
      "---------------------\n",
      "Iteration Number: 7680\n",
      "Loss: 29.996968960205315\n",
      "l2 norm of gradients: 0.19341998450663023\n",
      "l2 norm of weights: 5.709020772587792\n",
      "---------------------\n",
      "Iteration Number: 7681\n",
      "Loss: 29.99509321051649\n",
      "l2 norm of gradients: 0.19340325988225018\n",
      "l2 norm of weights: 5.708951099488822\n",
      "---------------------\n",
      "Iteration Number: 7682\n",
      "Loss: 29.993217785233476\n",
      "l2 norm of gradients: 0.19338653875261055\n",
      "l2 norm of weights: 5.708881431774929\n",
      "---------------------\n",
      "Iteration Number: 7683\n",
      "Loss: 29.991342684255766\n",
      "l2 norm of gradients: 0.19336982111629103\n",
      "l2 norm of weights: 5.708811769445316\n",
      "---------------------\n",
      "Iteration Number: 7684\n",
      "Loss: 29.98946790748713\n",
      "l2 norm of gradients: 0.1933531069718719\n",
      "l2 norm of weights: 5.7087421124991895\n",
      "---------------------\n",
      "Iteration Number: 7685\n",
      "Loss: 29.98759345482928\n",
      "l2 norm of gradients: 0.19333639631793448\n",
      "l2 norm of weights: 5.708672460935755\n",
      "---------------------\n",
      "Iteration Number: 7686\n",
      "Loss: 29.985719326192598\n",
      "l2 norm of gradients: 0.1933196891530607\n",
      "l2 norm of weights: 5.70860281475422\n",
      "---------------------\n",
      "Iteration Number: 7687\n",
      "Loss: 29.983845521475814\n",
      "l2 norm of gradients: 0.19330298547583352\n",
      "l2 norm of weights: 5.708533173953788\n",
      "---------------------\n",
      "Iteration Number: 7688\n",
      "Loss: 29.981972040585934\n",
      "l2 norm of gradients: 0.19328628528483652\n",
      "l2 norm of weights: 5.708463538533666\n",
      "---------------------\n",
      "Iteration Number: 7689\n",
      "Loss: 29.98009888342296\n",
      "l2 norm of gradients: 0.1932695885786543\n",
      "l2 norm of weights: 5.708393908493062\n",
      "---------------------\n",
      "Iteration Number: 7690\n",
      "Loss: 29.978226049896286\n",
      "l2 norm of gradients: 0.1932528953558721\n",
      "l2 norm of weights: 5.708324283831182\n",
      "---------------------\n",
      "Iteration Number: 7691\n",
      "Loss: 29.97635353990299\n",
      "l2 norm of gradients: 0.193236205615076\n",
      "l2 norm of weights: 5.7082546645472325\n",
      "---------------------\n",
      "Iteration Number: 7692\n",
      "Loss: 29.97448135335511\n",
      "l2 norm of gradients: 0.193219519354853\n",
      "l2 norm of weights: 5.70818505064042\n",
      "---------------------\n",
      "Iteration Number: 7693\n",
      "Loss: 29.97260949015215\n",
      "l2 norm of gradients: 0.19320283657379078\n",
      "l2 norm of weights: 5.708115442109954\n",
      "---------------------\n",
      "Iteration Number: 7694\n",
      "Loss: 29.970737950197012\n",
      "l2 norm of gradients: 0.19318615727047794\n",
      "l2 norm of weights: 5.708045838955041\n",
      "---------------------\n",
      "Iteration Number: 7695\n",
      "Loss: 29.968866733398244\n",
      "l2 norm of gradients: 0.19316948144350385\n",
      "l2 norm of weights: 5.707976241174889\n",
      "---------------------\n",
      "Iteration Number: 7696\n",
      "Loss: 29.966995839657656\n",
      "l2 norm of gradients: 0.1931528090914587\n",
      "l2 norm of weights: 5.707906648768706\n",
      "---------------------\n",
      "Iteration Number: 7697\n",
      "Loss: 29.96512526888046\n",
      "l2 norm of gradients: 0.1931361402129334\n",
      "l2 norm of weights: 5.7078370617357\n",
      "---------------------\n",
      "Iteration Number: 7698\n",
      "Loss: 29.963255020970337\n",
      "l2 norm of gradients: 0.19311947480651986\n",
      "l2 norm of weights: 5.707767480075079\n",
      "---------------------\n",
      "Iteration Number: 7699\n",
      "Loss: 29.96138509583054\n",
      "l2 norm of gradients: 0.19310281287081058\n",
      "l2 norm of weights: 5.707697903786053\n",
      "---------------------\n",
      "Iteration Number: 7700\n",
      "Loss: 29.959515493368155\n",
      "l2 norm of gradients: 0.19308615440439908\n",
      "l2 norm of weights: 5.70762833286783\n",
      "---------------------\n",
      "Iteration Number: 7701\n",
      "Loss: 29.957646213485884\n",
      "l2 norm of gradients: 0.1930694994058795\n",
      "l2 norm of weights: 5.707558767319619\n",
      "---------------------\n",
      "Iteration Number: 7702\n",
      "Loss: 29.95577725608802\n",
      "l2 norm of gradients: 0.1930528478738469\n",
      "l2 norm of weights: 5.707489207140631\n",
      "---------------------\n",
      "Iteration Number: 7703\n",
      "Loss: 29.953908621084405\n",
      "l2 norm of gradients: 0.19303619980689718\n",
      "l2 norm of weights: 5.707419652330073\n",
      "---------------------\n",
      "Iteration Number: 7704\n",
      "Loss: 29.9520403083723\n",
      "l2 norm of gradients: 0.19301955520362699\n",
      "l2 norm of weights: 5.707350102887156\n",
      "---------------------\n",
      "Iteration Number: 7705\n",
      "Loss: 29.9501723178595\n",
      "l2 norm of gradients: 0.19300291406263365\n",
      "l2 norm of weights: 5.707280558811091\n",
      "---------------------\n",
      "Iteration Number: 7706\n",
      "Loss: 29.94830464945252\n",
      "l2 norm of gradients: 0.19298627638251553\n",
      "l2 norm of weights: 5.707211020101085\n",
      "---------------------\n",
      "Iteration Number: 7707\n",
      "Loss: 29.946437303053195\n",
      "l2 norm of gradients: 0.19296964216187168\n",
      "l2 norm of weights: 5.707141486756353\n",
      "---------------------\n",
      "Iteration Number: 7708\n",
      "Loss: 29.94457027856722\n",
      "l2 norm of gradients: 0.19295301139930202\n",
      "l2 norm of weights: 5.707071958776102\n",
      "---------------------\n",
      "Iteration Number: 7709\n",
      "Loss: 29.94270357590182\n",
      "l2 norm of gradients: 0.19293638409340708\n",
      "l2 norm of weights: 5.707002436159543\n",
      "---------------------\n",
      "Iteration Number: 7710\n",
      "Loss: 29.9408371949578\n",
      "l2 norm of gradients: 0.19291976024278853\n",
      "l2 norm of weights: 5.706932918905889\n",
      "---------------------\n",
      "Iteration Number: 7711\n",
      "Loss: 29.93897113564102\n",
      "l2 norm of gradients: 0.19290313984604854\n",
      "l2 norm of weights: 5.70686340701435\n",
      "---------------------\n",
      "Iteration Number: 7712\n",
      "Loss: 29.937105397863565\n",
      "l2 norm of gradients: 0.19288652290179015\n",
      "l2 norm of weights: 5.706793900484137\n",
      "---------------------\n",
      "Iteration Number: 7713\n",
      "Loss: 29.935239981519036\n",
      "l2 norm of gradients: 0.19286990940861737\n",
      "l2 norm of weights: 5.706724399314463\n",
      "---------------------\n",
      "Iteration Number: 7714\n",
      "Loss: 29.933374886522877\n",
      "l2 norm of gradients: 0.19285329936513476\n",
      "l2 norm of weights: 5.706654903504539\n",
      "---------------------\n",
      "Iteration Number: 7715\n",
      "Loss: 29.931510112772127\n",
      "l2 norm of gradients: 0.1928366927699479\n",
      "l2 norm of weights: 5.706585413053577\n",
      "---------------------\n",
      "Iteration Number: 7716\n",
      "Loss: 29.929645660176252\n",
      "l2 norm of gradients: 0.19282008962166308\n",
      "l2 norm of weights: 5.7065159279607895\n",
      "---------------------\n",
      "Iteration Number: 7717\n",
      "Loss: 29.927781528642374\n",
      "l2 norm of gradients: 0.19280348991888738\n",
      "l2 norm of weights: 5.706446448225388\n",
      "---------------------\n",
      "Iteration Number: 7718\n",
      "Loss: 29.925917718071766\n",
      "l2 norm of gradients: 0.1927868936602286\n",
      "l2 norm of weights: 5.706376973846588\n",
      "---------------------\n",
      "Iteration Number: 7719\n",
      "Loss: 29.92405422837124\n",
      "l2 norm of gradients: 0.19277030084429553\n",
      "l2 norm of weights: 5.706307504823599\n",
      "---------------------\n",
      "Iteration Number: 7720\n",
      "Loss: 29.92219105944401\n",
      "l2 norm of gradients: 0.19275371146969758\n",
      "l2 norm of weights: 5.706238041155636\n",
      "---------------------\n",
      "Iteration Number: 7721\n",
      "Loss: 29.92032821120003\n",
      "l2 norm of gradients: 0.19273712553504516\n",
      "l2 norm of weights: 5.706168582841913\n",
      "---------------------\n",
      "Iteration Number: 7722\n",
      "Loss: 29.918465683541193\n",
      "l2 norm of gradients: 0.19272054303894917\n",
      "l2 norm of weights: 5.706099129881642\n",
      "---------------------\n",
      "Iteration Number: 7723\n",
      "Loss: 29.91660347637684\n",
      "l2 norm of gradients: 0.19270396398002157\n",
      "l2 norm of weights: 5.706029682274037\n",
      "---------------------\n",
      "Iteration Number: 7724\n",
      "Loss: 29.914741589607022\n",
      "l2 norm of gradients: 0.19268738835687513\n",
      "l2 norm of weights: 5.705960240018315\n",
      "---------------------\n",
      "Iteration Number: 7725\n",
      "Loss: 29.91288002314192\n",
      "l2 norm of gradients: 0.1926708161681231\n",
      "l2 norm of weights: 5.705890803113684\n",
      "---------------------\n",
      "Iteration Number: 7726\n",
      "Loss: 29.911018776883914\n",
      "l2 norm of gradients: 0.19265424741238\n",
      "l2 norm of weights: 5.705821371559364\n",
      "---------------------\n",
      "Iteration Number: 7727\n",
      "Loss: 29.909157850740158\n",
      "l2 norm of gradients: 0.19263768208826063\n",
      "l2 norm of weights: 5.705751945354566\n",
      "---------------------\n",
      "Iteration Number: 7728\n",
      "Loss: 29.907297244618317\n",
      "l2 norm of gradients: 0.19262112019438105\n",
      "l2 norm of weights: 5.705682524498508\n",
      "---------------------\n",
      "Iteration Number: 7729\n",
      "Loss: 29.905436958421035\n",
      "l2 norm of gradients: 0.19260456172935775\n",
      "l2 norm of weights: 5.705613108990402\n",
      "---------------------\n",
      "Iteration Number: 7730\n",
      "Loss: 29.903576992057012\n",
      "l2 norm of gradients: 0.19258800669180826\n",
      "l2 norm of weights: 5.705543698829466\n",
      "---------------------\n",
      "Iteration Number: 7731\n",
      "Loss: 29.901717345425315\n",
      "l2 norm of gradients: 0.19257145508035078\n",
      "l2 norm of weights: 5.705474294014913\n",
      "---------------------\n",
      "Iteration Number: 7732\n",
      "Loss: 29.899858018443464\n",
      "l2 norm of gradients: 0.19255490689360433\n",
      "l2 norm of weights: 5.70540489454596\n",
      "---------------------\n",
      "Iteration Number: 7733\n",
      "Loss: 29.897999011007855\n",
      "l2 norm of gradients: 0.19253836213018877\n",
      "l2 norm of weights: 5.705335500421823\n",
      "---------------------\n",
      "Iteration Number: 7734\n",
      "Loss: 29.896140323029687\n",
      "l2 norm of gradients: 0.19252182078872454\n",
      "l2 norm of weights: 5.705266111641717\n",
      "---------------------\n",
      "Iteration Number: 7735\n",
      "Loss: 29.89428195440907\n",
      "l2 norm of gradients: 0.19250528286783322\n",
      "l2 norm of weights: 5.705196728204859\n",
      "---------------------\n",
      "Iteration Number: 7736\n",
      "Loss: 29.892423905058713\n",
      "l2 norm of gradients: 0.1924887483661369\n",
      "l2 norm of weights: 5.705127350110465\n",
      "---------------------\n",
      "Iteration Number: 7737\n",
      "Loss: 29.890566174881794\n",
      "l2 norm of gradients: 0.19247221728225858\n",
      "l2 norm of weights: 5.705057977357753\n",
      "---------------------\n",
      "Iteration Number: 7738\n",
      "Loss: 29.88870876378195\n",
      "l2 norm of gradients: 0.19245568961482198\n",
      "l2 norm of weights: 5.704988609945938\n",
      "---------------------\n",
      "Iteration Number: 7739\n",
      "Loss: 29.886851671672595\n",
      "l2 norm of gradients: 0.1924391653624517\n",
      "l2 norm of weights: 5.70491924787424\n",
      "---------------------\n",
      "Iteration Number: 7740\n",
      "Loss: 29.88499489845313\n",
      "l2 norm of gradients: 0.19242264452377306\n",
      "l2 norm of weights: 5.704849891141873\n",
      "---------------------\n",
      "Iteration Number: 7741\n",
      "Loss: 29.883138444031502\n",
      "l2 norm of gradients: 0.19240612709741212\n",
      "l2 norm of weights: 5.7047805397480555\n",
      "---------------------\n",
      "Iteration Number: 7742\n",
      "Loss: 29.881282308316067\n",
      "l2 norm of gradients: 0.19238961308199584\n",
      "l2 norm of weights: 5.704711193692007\n",
      "---------------------\n",
      "Iteration Number: 7743\n",
      "Loss: 29.87942649121337\n",
      "l2 norm of gradients: 0.19237310247615197\n",
      "l2 norm of weights: 5.7046418529729435\n",
      "---------------------\n",
      "Iteration Number: 7744\n",
      "Loss: 29.877570992625767\n",
      "l2 norm of gradients: 0.19235659527850893\n",
      "l2 norm of weights: 5.704572517590084\n",
      "---------------------\n",
      "Iteration Number: 7745\n",
      "Loss: 29.87571581246152\n",
      "l2 norm of gradients: 0.1923400914876959\n",
      "l2 norm of weights: 5.704503187542646\n",
      "---------------------\n",
      "Iteration Number: 7746\n",
      "Loss: 29.873860950631435\n",
      "l2 norm of gradients: 0.19232359110234312\n",
      "l2 norm of weights: 5.704433862829849\n",
      "---------------------\n",
      "Iteration Number: 7747\n",
      "Loss: 29.87200640703674\n",
      "l2 norm of gradients: 0.1923070941210813\n",
      "l2 norm of weights: 5.704364543450913\n",
      "---------------------\n",
      "Iteration Number: 7748\n",
      "Loss: 29.870152181585343\n",
      "l2 norm of gradients: 0.19229060054254202\n",
      "l2 norm of weights: 5.704295229405055\n",
      "---------------------\n",
      "Iteration Number: 7749\n",
      "Loss: 29.868298274184674\n",
      "l2 norm of gradients: 0.19227411036535777\n",
      "l2 norm of weights: 5.704225920691494\n",
      "---------------------\n",
      "Iteration Number: 7750\n",
      "Loss: 29.866444684743126\n",
      "l2 norm of gradients: 0.1922576235881617\n",
      "l2 norm of weights: 5.70415661730945\n",
      "---------------------\n",
      "Iteration Number: 7751\n",
      "Loss: 29.864591413164923\n",
      "l2 norm of gradients: 0.19224114020958777\n",
      "l2 norm of weights: 5.704087319258144\n",
      "---------------------\n",
      "Iteration Number: 7752\n",
      "Loss: 29.86273845935605\n",
      "l2 norm of gradients: 0.19222466022827073\n",
      "l2 norm of weights: 5.704018026536793\n",
      "---------------------\n",
      "Iteration Number: 7753\n",
      "Loss: 29.860885823226848\n",
      "l2 norm of gradients: 0.19220818364284606\n",
      "l2 norm of weights: 5.70394873914462\n",
      "---------------------\n",
      "Iteration Number: 7754\n",
      "Loss: 29.85903350467901\n",
      "l2 norm of gradients: 0.1921917104519501\n",
      "l2 norm of weights: 5.703879457080844\n",
      "---------------------\n",
      "Iteration Number: 7755\n",
      "Loss: 29.857181503624865\n",
      "l2 norm of gradients: 0.19217524065422004\n",
      "l2 norm of weights: 5.703810180344685\n",
      "---------------------\n",
      "Iteration Number: 7756\n",
      "Loss: 29.855329819969576\n",
      "l2 norm of gradients: 0.19215877424829353\n",
      "l2 norm of weights: 5.703740908935362\n",
      "---------------------\n",
      "Iteration Number: 7757\n",
      "Loss: 29.85347845361882\n",
      "l2 norm of gradients: 0.19214231123280934\n",
      "l2 norm of weights: 5.703671642852101\n",
      "---------------------\n",
      "Iteration Number: 7758\n",
      "Loss: 29.851627404477323\n",
      "l2 norm of gradients: 0.19212585160640697\n",
      "l2 norm of weights: 5.703602382094118\n",
      "---------------------\n",
      "Iteration Number: 7759\n",
      "Loss: 29.849776672459715\n",
      "l2 norm of gradients: 0.19210939536772648\n",
      "l2 norm of weights: 5.703533126660637\n",
      "---------------------\n",
      "Iteration Number: 7760\n",
      "Loss: 29.847926257467858\n",
      "l2 norm of gradients: 0.19209294251540882\n",
      "l2 norm of weights: 5.703463876550878\n",
      "---------------------\n",
      "Iteration Number: 7761\n",
      "Loss: 29.84607615940597\n",
      "l2 norm of gradients: 0.19207649304809593\n",
      "l2 norm of weights: 5.703394631764064\n",
      "---------------------\n",
      "Iteration Number: 7762\n",
      "Loss: 29.844226378190314\n",
      "l2 norm of gradients: 0.19206004696443021\n",
      "l2 norm of weights: 5.703325392299416\n",
      "---------------------\n",
      "Iteration Number: 7763\n",
      "Loss: 29.842376913720184\n",
      "l2 norm of gradients: 0.19204360426305492\n",
      "l2 norm of weights: 5.703256158156156\n",
      "---------------------\n",
      "Iteration Number: 7764\n",
      "Loss: 29.840527765903968\n",
      "l2 norm of gradients: 0.1920271649426142\n",
      "l2 norm of weights: 5.703186929333508\n",
      "---------------------\n",
      "Iteration Number: 7765\n",
      "Loss: 29.838678934652208\n",
      "l2 norm of gradients: 0.19201072900175298\n",
      "l2 norm of weights: 5.703117705830691\n",
      "---------------------\n",
      "Iteration Number: 7766\n",
      "Loss: 29.836830419873227\n",
      "l2 norm of gradients: 0.19199429643911678\n",
      "l2 norm of weights: 5.703048487646931\n",
      "---------------------\n",
      "Iteration Number: 7767\n",
      "Loss: 29.83498222146888\n",
      "l2 norm of gradients: 0.19197786725335206\n",
      "l2 norm of weights: 5.702979274781449\n",
      "---------------------\n",
      "Iteration Number: 7768\n",
      "Loss: 29.833134339349357\n",
      "l2 norm of gradients: 0.19196144144310595\n",
      "l2 norm of weights: 5.7029100672334705\n",
      "---------------------\n",
      "Iteration Number: 7769\n",
      "Loss: 29.831286773422324\n",
      "l2 norm of gradients: 0.19194501900702646\n",
      "l2 norm of weights: 5.702840865002217\n",
      "---------------------\n",
      "Iteration Number: 7770\n",
      "Loss: 29.82943952359632\n",
      "l2 norm of gradients: 0.1919285999437622\n",
      "l2 norm of weights: 5.702771668086911\n",
      "---------------------\n",
      "Iteration Number: 7771\n",
      "Loss: 29.827592589777215\n",
      "l2 norm of gradients: 0.19191218425196277\n",
      "l2 norm of weights: 5.702702476486778\n",
      "---------------------\n",
      "Iteration Number: 7772\n",
      "Loss: 29.82574597187786\n",
      "l2 norm of gradients: 0.19189577193027843\n",
      "l2 norm of weights: 5.702633290201042\n",
      "---------------------\n",
      "Iteration Number: 7773\n",
      "Loss: 29.823899669794294\n",
      "l2 norm of gradients: 0.19187936297736016\n",
      "l2 norm of weights: 5.702564109228927\n",
      "---------------------\n",
      "Iteration Number: 7774\n",
      "Loss: 29.82205368344571\n",
      "l2 norm of gradients: 0.19186295739185982\n",
      "l2 norm of weights: 5.702494933569657\n",
      "---------------------\n",
      "Iteration Number: 7775\n",
      "Loss: 29.820208012735446\n",
      "l2 norm of gradients: 0.1918465551724299\n",
      "l2 norm of weights: 5.702425763222457\n",
      "---------------------\n",
      "Iteration Number: 7776\n",
      "Loss: 29.81836265757285\n",
      "l2 norm of gradients: 0.19183015631772388\n",
      "l2 norm of weights: 5.70235659818655\n",
      "---------------------\n",
      "Iteration Number: 7777\n",
      "Loss: 29.816517617861905\n",
      "l2 norm of gradients: 0.19181376082639573\n",
      "l2 norm of weights: 5.7022874384611635\n",
      "---------------------\n",
      "Iteration Number: 7778\n",
      "Loss: 29.814672893513553\n",
      "l2 norm of gradients: 0.19179736869710035\n",
      "l2 norm of weights: 5.70221828404552\n",
      "---------------------\n",
      "Iteration Number: 7779\n",
      "Loss: 29.81282848443645\n",
      "l2 norm of gradients: 0.19178097992849352\n",
      "l2 norm of weights: 5.702149134938848\n",
      "---------------------\n",
      "Iteration Number: 7780\n",
      "Loss: 29.810984390536728\n",
      "l2 norm of gradients: 0.19176459451923153\n",
      "l2 norm of weights: 5.702079991140371\n",
      "---------------------\n",
      "Iteration Number: 7781\n",
      "Loss: 29.8091406117226\n",
      "l2 norm of gradients: 0.19174821246797158\n",
      "l2 norm of weights: 5.702010852649315\n",
      "---------------------\n",
      "Iteration Number: 7782\n",
      "Loss: 29.807297147903075\n",
      "l2 norm of gradients: 0.19173183377337166\n",
      "l2 norm of weights: 5.701941719464907\n",
      "---------------------\n",
      "Iteration Number: 7783\n",
      "Loss: 29.80545399898651\n",
      "l2 norm of gradients: 0.19171545843409046\n",
      "l2 norm of weights: 5.701872591586373\n",
      "---------------------\n",
      "Iteration Number: 7784\n",
      "Loss: 29.803611164878443\n",
      "l2 norm of gradients: 0.19169908644878747\n",
      "l2 norm of weights: 5.701803469012939\n",
      "---------------------\n",
      "Iteration Number: 7785\n",
      "Loss: 29.801768645490057\n",
      "l2 norm of gradients: 0.1916827178161229\n",
      "l2 norm of weights: 5.701734351743831\n",
      "---------------------\n",
      "Iteration Number: 7786\n",
      "Loss: 29.79992644073037\n",
      "l2 norm of gradients: 0.19166635253475778\n",
      "l2 norm of weights: 5.7016652397782766\n",
      "---------------------\n",
      "Iteration Number: 7787\n",
      "Loss: 29.798084550502505\n",
      "l2 norm of gradients: 0.19164999060335386\n",
      "l2 norm of weights: 5.701596133115502\n",
      "---------------------\n",
      "Iteration Number: 7788\n",
      "Loss: 29.79624297471875\n",
      "l2 norm of gradients: 0.19163363202057376\n",
      "l2 norm of weights: 5.701527031754735\n",
      "---------------------\n",
      "Iteration Number: 7789\n",
      "Loss: 29.79440171328765\n",
      "l2 norm of gradients: 0.19161727678508067\n",
      "l2 norm of weights: 5.701457935695205\n",
      "---------------------\n",
      "Iteration Number: 7790\n",
      "Loss: 29.79256076611572\n",
      "l2 norm of gradients: 0.1916009248955387\n",
      "l2 norm of weights: 5.701388844936137\n",
      "---------------------\n",
      "Iteration Number: 7791\n",
      "Loss: 29.79072013311464\n",
      "l2 norm of gradients: 0.19158457635061268\n",
      "l2 norm of weights: 5.701319759476759\n",
      "---------------------\n",
      "Iteration Number: 7792\n",
      "Loss: 29.788879814190125\n",
      "l2 norm of gradients: 0.1915682311489682\n",
      "l2 norm of weights: 5.7012506793163\n",
      "---------------------\n",
      "Iteration Number: 7793\n",
      "Loss: 29.787039809251745\n",
      "l2 norm of gradients: 0.19155188928927158\n",
      "l2 norm of weights: 5.701181604453987\n",
      "---------------------\n",
      "Iteration Number: 7794\n",
      "Loss: 29.785200118202727\n",
      "l2 norm of gradients: 0.19153555077018994\n",
      "l2 norm of weights: 5.701112534889051\n",
      "---------------------\n",
      "Iteration Number: 7795\n",
      "Loss: 29.78336074096157\n",
      "l2 norm of gradients: 0.19151921559039115\n",
      "l2 norm of weights: 5.701043470620718\n",
      "---------------------\n",
      "Iteration Number: 7796\n",
      "Loss: 29.781521677429232\n",
      "l2 norm of gradients: 0.1915028837485438\n",
      "l2 norm of weights: 5.700974411648218\n",
      "---------------------\n",
      "Iteration Number: 7797\n",
      "Loss: 29.779682927519346\n",
      "l2 norm of gradients: 0.19148655524331734\n",
      "l2 norm of weights: 5.700905357970781\n",
      "---------------------\n",
      "Iteration Number: 7798\n",
      "Loss: 29.777844491138826\n",
      "l2 norm of gradients: 0.19147023007338185\n",
      "l2 norm of weights: 5.7008363095876335\n",
      "---------------------\n",
      "Iteration Number: 7799\n",
      "Loss: 29.77600636819342\n",
      "l2 norm of gradients: 0.19145390823740824\n",
      "l2 norm of weights: 5.700767266498008\n",
      "---------------------\n",
      "Iteration Number: 7800\n",
      "Loss: 29.774168558595335\n",
      "l2 norm of gradients: 0.1914375897340682\n",
      "l2 norm of weights: 5.700698228701133\n",
      "---------------------\n",
      "Iteration Number: 7801\n",
      "Loss: 29.77233106225244\n",
      "l2 norm of gradients: 0.19142127456203414\n",
      "l2 norm of weights: 5.700629196196237\n",
      "---------------------\n",
      "Iteration Number: 7802\n",
      "Loss: 29.770493879074987\n",
      "l2 norm of gradients: 0.19140496271997917\n",
      "l2 norm of weights: 5.700560168982553\n",
      "---------------------\n",
      "Iteration Number: 7803\n",
      "Loss: 29.7686570089705\n",
      "l2 norm of gradients: 0.1913886542065773\n",
      "l2 norm of weights: 5.700491147059309\n",
      "---------------------\n",
      "Iteration Number: 7804\n",
      "Loss: 29.766820451846478\n",
      "l2 norm of gradients: 0.1913723490205032\n",
      "l2 norm of weights: 5.700422130425736\n",
      "---------------------\n",
      "Iteration Number: 7805\n",
      "Loss: 29.76498420761642\n",
      "l2 norm of gradients: 0.1913560471604322\n",
      "l2 norm of weights: 5.700353119081065\n",
      "---------------------\n",
      "Iteration Number: 7806\n",
      "Loss: 29.76314827618325\n",
      "l2 norm of gradients: 0.19133974862504066\n",
      "l2 norm of weights: 5.7002841130245265\n",
      "---------------------\n",
      "Iteration Number: 7807\n",
      "Loss: 29.761312657463804\n",
      "l2 norm of gradients: 0.19132345341300544\n",
      "l2 norm of weights: 5.700215112255352\n",
      "---------------------\n",
      "Iteration Number: 7808\n",
      "Loss: 29.759477351361106\n",
      "l2 norm of gradients: 0.1913071615230042\n",
      "l2 norm of weights: 5.700146116772772\n",
      "---------------------\n",
      "Iteration Number: 7809\n",
      "Loss: 29.757642357785517\n",
      "l2 norm of gradients: 0.1912908729537155\n",
      "l2 norm of weights: 5.700077126576019\n",
      "---------------------\n",
      "Iteration Number: 7810\n",
      "Loss: 29.75580767664836\n",
      "l2 norm of gradients: 0.19127458770381847\n",
      "l2 norm of weights: 5.700008141664325\n",
      "---------------------\n",
      "Iteration Number: 7811\n",
      "Loss: 29.753973307858313\n",
      "l2 norm of gradients: 0.19125830577199307\n",
      "l2 norm of weights: 5.699939162036921\n",
      "---------------------\n",
      "Iteration Number: 7812\n",
      "Loss: 29.752139251321772\n",
      "l2 norm of gradients: 0.19124202715692007\n",
      "l2 norm of weights: 5.699870187693039\n",
      "---------------------\n",
      "Iteration Number: 7813\n",
      "Loss: 29.750305506951058\n",
      "l2 norm of gradients: 0.19122575185728088\n",
      "l2 norm of weights: 5.699801218631912\n",
      "---------------------\n",
      "Iteration Number: 7814\n",
      "Loss: 29.74847207465434\n",
      "l2 norm of gradients: 0.1912094798717577\n",
      "l2 norm of weights: 5.699732254852772\n",
      "---------------------\n",
      "Iteration Number: 7815\n",
      "Loss: 29.74663895434351\n",
      "l2 norm of gradients: 0.1911932111990335\n",
      "l2 norm of weights: 5.699663296354851\n",
      "---------------------\n",
      "Iteration Number: 7816\n",
      "Loss: 29.744806145925487\n",
      "l2 norm of gradients: 0.1911769458377921\n",
      "l2 norm of weights: 5.699594343137384\n",
      "---------------------\n",
      "Iteration Number: 7817\n",
      "Loss: 29.742973649309498\n",
      "l2 norm of gradients: 0.19116068378671786\n",
      "l2 norm of weights: 5.699525395199602\n",
      "---------------------\n",
      "Iteration Number: 7818\n",
      "Loss: 29.741141464405615\n",
      "l2 norm of gradients: 0.19114442504449594\n",
      "l2 norm of weights: 5.6994564525407405\n",
      "---------------------\n",
      "Iteration Number: 7819\n",
      "Loss: 29.73930959112573\n",
      "l2 norm of gradients: 0.19112816960981246\n",
      "l2 norm of weights: 5.699387515160031\n",
      "---------------------\n",
      "Iteration Number: 7820\n",
      "Loss: 29.737478029376824\n",
      "l2 norm of gradients: 0.19111191748135398\n",
      "l2 norm of weights: 5.699318583056708\n",
      "---------------------\n",
      "Iteration Number: 7821\n",
      "Loss: 29.735646779067462\n",
      "l2 norm of gradients: 0.19109566865780805\n",
      "l2 norm of weights: 5.699249656230005\n",
      "---------------------\n",
      "Iteration Number: 7822\n",
      "Loss: 29.733815840114744\n",
      "l2 norm of gradients: 0.1910794231378628\n",
      "l2 norm of weights: 5.699180734679156\n",
      "---------------------\n",
      "Iteration Number: 7823\n",
      "Loss: 29.731985212416117\n",
      "l2 norm of gradients: 0.19106318092020724\n",
      "l2 norm of weights: 5.699111818403397\n",
      "---------------------\n",
      "Iteration Number: 7824\n",
      "Loss: 29.73015489589309\n",
      "l2 norm of gradients: 0.19104694200353112\n",
      "l2 norm of weights: 5.699042907401961\n",
      "---------------------\n",
      "Iteration Number: 7825\n",
      "Loss: 29.72832489044824\n",
      "l2 norm of gradients: 0.19103070638652472\n",
      "l2 norm of weights: 5.698974001674083\n",
      "---------------------\n",
      "Iteration Number: 7826\n",
      "Loss: 29.726495195997792\n",
      "l2 norm of gradients: 0.19101447406787941\n",
      "l2 norm of weights: 5.6989051012189975\n",
      "---------------------\n",
      "Iteration Number: 7827\n",
      "Loss: 29.724665812443348\n",
      "l2 norm of gradients: 0.19099824504628699\n",
      "l2 norm of weights: 5.69883620603594\n",
      "---------------------\n",
      "Iteration Number: 7828\n",
      "Loss: 29.722836739702075\n",
      "l2 norm of gradients: 0.19098201932044018\n",
      "l2 norm of weights: 5.6987673161241466\n",
      "---------------------\n",
      "Iteration Number: 7829\n",
      "Loss: 29.721007977681566\n",
      "l2 norm of gradients: 0.19096579688903242\n",
      "l2 norm of weights: 5.6986984314828515\n",
      "---------------------\n",
      "Iteration Number: 7830\n",
      "Loss: 29.71917952629098\n",
      "l2 norm of gradients: 0.19094957775075785\n",
      "l2 norm of weights: 5.698629552111292\n",
      "---------------------\n",
      "Iteration Number: 7831\n",
      "Loss: 29.71735138544256\n",
      "l2 norm of gradients: 0.1909333619043114\n",
      "l2 norm of weights: 5.698560678008702\n",
      "---------------------\n",
      "Iteration Number: 7832\n",
      "Loss: 29.71552355504321\n",
      "l2 norm of gradients: 0.19091714934838874\n",
      "l2 norm of weights: 5.69849180917432\n",
      "---------------------\n",
      "Iteration Number: 7833\n",
      "Loss: 29.713696035006826\n",
      "l2 norm of gradients: 0.19090094008168615\n",
      "l2 norm of weights: 5.698422945607382\n",
      "---------------------\n",
      "Iteration Number: 7834\n",
      "Loss: 29.711868825238607\n",
      "l2 norm of gradients: 0.1908847341029009\n",
      "l2 norm of weights: 5.698354087307121\n",
      "---------------------\n",
      "Iteration Number: 7835\n",
      "Loss: 29.710041925657272\n",
      "l2 norm of gradients: 0.1908685314107309\n",
      "l2 norm of weights: 5.6982852342727774\n",
      "---------------------\n",
      "Iteration Number: 7836\n",
      "Loss: 29.708215336164887\n",
      "l2 norm of gradients: 0.19085233200387458\n",
      "l2 norm of weights: 5.698216386503587\n",
      "---------------------\n",
      "Iteration Number: 7837\n",
      "Loss: 29.706389056674855\n",
      "l2 norm of gradients: 0.19083613588103143\n",
      "l2 norm of weights: 5.698147543998787\n",
      "---------------------\n",
      "Iteration Number: 7838\n",
      "Loss: 29.704563087096872\n",
      "l2 norm of gradients: 0.19081994304090152\n",
      "l2 norm of weights: 5.698078706757614\n",
      "---------------------\n",
      "Iteration Number: 7839\n",
      "Loss: 29.70273742734403\n",
      "l2 norm of gradients: 0.19080375348218567\n",
      "l2 norm of weights: 5.6980098747793075\n",
      "---------------------\n",
      "Iteration Number: 7840\n",
      "Loss: 29.70091207732183\n",
      "l2 norm of gradients: 0.1907875672035855\n",
      "l2 norm of weights: 5.697941048063104\n",
      "---------------------\n",
      "Iteration Number: 7841\n",
      "Loss: 29.699087036949038\n",
      "l2 norm of gradients: 0.1907713842038032\n",
      "l2 norm of weights: 5.697872226608241\n",
      "---------------------\n",
      "Iteration Number: 7842\n",
      "Loss: 29.697262306126852\n",
      "l2 norm of gradients: 0.19075520448154204\n",
      "l2 norm of weights: 5.697803410413956\n",
      "---------------------\n",
      "Iteration Number: 7843\n",
      "Loss: 29.69543788477253\n",
      "l2 norm of gradients: 0.1907390280355056\n",
      "l2 norm of weights: 5.69773459947949\n",
      "---------------------\n",
      "Iteration Number: 7844\n",
      "Loss: 29.69361377279107\n",
      "l2 norm of gradients: 0.19072285486439855\n",
      "l2 norm of weights: 5.697665793804078\n",
      "---------------------\n",
      "Iteration Number: 7845\n",
      "Loss: 29.691789970099396\n",
      "l2 norm of gradients: 0.19070668496692605\n",
      "l2 norm of weights: 5.697596993386962\n",
      "---------------------\n",
      "Iteration Number: 7846\n",
      "Loss: 29.68996647660359\n",
      "l2 norm of gradients: 0.19069051834179418\n",
      "l2 norm of weights: 5.697528198227379\n",
      "---------------------\n",
      "Iteration Number: 7847\n",
      "Loss: 29.688143292216072\n",
      "l2 norm of gradients: 0.19067435498770963\n",
      "l2 norm of weights: 5.697459408324568\n",
      "---------------------\n",
      "Iteration Number: 7848\n",
      "Loss: 29.68632041684782\n",
      "l2 norm of gradients: 0.1906581949033799\n",
      "l2 norm of weights: 5.697390623677769\n",
      "---------------------\n",
      "Iteration Number: 7849\n",
      "Loss: 29.684497850410697\n",
      "l2 norm of gradients: 0.19064203808751318\n",
      "l2 norm of weights: 5.697321844286222\n",
      "---------------------\n",
      "Iteration Number: 7850\n",
      "Loss: 29.682675592813727\n",
      "l2 norm of gradients: 0.19062588453881837\n",
      "l2 norm of weights: 5.697253070149166\n",
      "---------------------\n",
      "Iteration Number: 7851\n",
      "Loss: 29.680853643970217\n",
      "l2 norm of gradients: 0.19060973425600516\n",
      "l2 norm of weights: 5.697184301265841\n",
      "---------------------\n",
      "Iteration Number: 7852\n",
      "Loss: 29.67903200378611\n",
      "l2 norm of gradients: 0.19059358723778405\n",
      "l2 norm of weights: 5.697115537635487\n",
      "---------------------\n",
      "Iteration Number: 7853\n",
      "Loss: 29.677210672175338\n",
      "l2 norm of gradients: 0.19057744348286607\n",
      "l2 norm of weights: 5.697046779257344\n",
      "---------------------\n",
      "Iteration Number: 7854\n",
      "Loss: 29.675389649050107\n",
      "l2 norm of gradients: 0.1905613029899632\n",
      "l2 norm of weights: 5.696978026130654\n",
      "---------------------\n",
      "Iteration Number: 7855\n",
      "Loss: 29.673568934325033\n",
      "l2 norm of gradients: 0.19054516575778796\n",
      "l2 norm of weights: 5.696909278254656\n",
      "---------------------\n",
      "Iteration Number: 7856\n",
      "Loss: 29.6717485279039\n",
      "l2 norm of gradients: 0.19052903178505365\n",
      "l2 norm of weights: 5.696840535628592\n",
      "---------------------\n",
      "Iteration Number: 7857\n",
      "Loss: 29.669928429699166\n",
      "l2 norm of gradients: 0.19051290107047447\n",
      "l2 norm of weights: 5.696771798251703\n",
      "---------------------\n",
      "Iteration Number: 7858\n",
      "Loss: 29.668108639624705\n",
      "l2 norm of gradients: 0.19049677361276512\n",
      "l2 norm of weights: 5.696703066123228\n",
      "---------------------\n",
      "Iteration Number: 7859\n",
      "Loss: 29.66628915759188\n",
      "l2 norm of gradients: 0.19048064941064122\n",
      "l2 norm of weights: 5.696634339242412\n",
      "---------------------\n",
      "Iteration Number: 7860\n",
      "Loss: 29.664469983512802\n",
      "l2 norm of gradients: 0.1904645284628189\n",
      "l2 norm of weights: 5.696565617608496\n",
      "---------------------\n",
      "Iteration Number: 7861\n",
      "Loss: 29.66265111729536\n",
      "l2 norm of gradients: 0.19044841076801527\n",
      "l2 norm of weights: 5.69649690122072\n",
      "---------------------\n",
      "Iteration Number: 7862\n",
      "Loss: 29.66083255885178\n",
      "l2 norm of gradients: 0.19043229632494804\n",
      "l2 norm of weights: 5.6964281900783265\n",
      "---------------------\n",
      "Iteration Number: 7863\n",
      "Loss: 29.659014308093997\n",
      "l2 norm of gradients: 0.19041618513233557\n",
      "l2 norm of weights: 5.696359484180559\n",
      "---------------------\n",
      "Iteration Number: 7864\n",
      "Loss: 29.65719636493589\n",
      "l2 norm of gradients: 0.19040007718889712\n",
      "l2 norm of weights: 5.696290783526659\n",
      "---------------------\n",
      "Iteration Number: 7865\n",
      "Loss: 29.655378729285317\n",
      "l2 norm of gradients: 0.19038397249335257\n",
      "l2 norm of weights: 5.696222088115871\n",
      "---------------------\n",
      "Iteration Number: 7866\n",
      "Loss: 29.65356140105614\n",
      "l2 norm of gradients: 0.19036787104442257\n",
      "l2 norm of weights: 5.696153397947436\n",
      "---------------------\n",
      "Iteration Number: 7867\n",
      "Loss: 29.65174438015713\n",
      "l2 norm of gradients: 0.19035177284082844\n",
      "l2 norm of weights: 5.696084713020597\n",
      "---------------------\n",
      "Iteration Number: 7868\n",
      "Loss: 29.649927666505377\n",
      "l2 norm of gradients: 0.19033567788129227\n",
      "l2 norm of weights: 5.6960160333345975\n",
      "---------------------\n",
      "Iteration Number: 7869\n",
      "Loss: 29.648111260004992\n",
      "l2 norm of gradients: 0.1903195861645369\n",
      "l2 norm of weights: 5.695947358888682\n",
      "---------------------\n",
      "Iteration Number: 7870\n",
      "Loss: 29.646295160574844\n",
      "l2 norm of gradients: 0.19030349768928584\n",
      "l2 norm of weights: 5.695878689682092\n",
      "---------------------\n",
      "Iteration Number: 7871\n",
      "Loss: 29.644479368121697\n",
      "l2 norm of gradients: 0.19028741245426342\n",
      "l2 norm of weights: 5.695810025714074\n",
      "---------------------\n",
      "Iteration Number: 7872\n",
      "Loss: 29.642663882560527\n",
      "l2 norm of gradients: 0.19027133045819455\n",
      "l2 norm of weights: 5.695741366983872\n",
      "---------------------\n",
      "Iteration Number: 7873\n",
      "Loss: 29.64084870380295\n",
      "l2 norm of gradients: 0.19025525169980498\n",
      "l2 norm of weights: 5.695672713490727\n",
      "---------------------\n",
      "Iteration Number: 7874\n",
      "Loss: 29.63903383175736\n",
      "l2 norm of gradients: 0.1902391761778211\n",
      "l2 norm of weights: 5.695604065233886\n",
      "---------------------\n",
      "Iteration Number: 7875\n",
      "Loss: 29.63721926633878\n",
      "l2 norm of gradients: 0.19022310389097016\n",
      "l2 norm of weights: 5.695535422212593\n",
      "---------------------\n",
      "Iteration Number: 7876\n",
      "Loss: 29.6354050074618\n",
      "l2 norm of gradients: 0.19020703483798\n",
      "l2 norm of weights: 5.695466784426094\n",
      "---------------------\n",
      "Iteration Number: 7877\n",
      "Loss: 29.63359105502942\n",
      "l2 norm of gradients: 0.19019096901757918\n",
      "l2 norm of weights: 5.695398151873633\n",
      "---------------------\n",
      "Iteration Number: 7878\n",
      "Loss: 29.631777408962876\n",
      "l2 norm of gradients: 0.19017490642849708\n",
      "l2 norm of weights: 5.695329524554455\n",
      "---------------------\n",
      "Iteration Number: 7879\n",
      "Loss: 29.62996406917021\n",
      "l2 norm of gradients: 0.19015884706946376\n",
      "l2 norm of weights: 5.695260902467806\n",
      "---------------------\n",
      "Iteration Number: 7880\n",
      "Loss: 29.62815103556335\n",
      "l2 norm of gradients: 0.19014279093920994\n",
      "l2 norm of weights: 5.695192285612931\n",
      "---------------------\n",
      "Iteration Number: 7881\n",
      "Loss: 29.626338308051945\n",
      "l2 norm of gradients: 0.19012673803646715\n",
      "l2 norm of weights: 5.695123673989076\n",
      "---------------------\n",
      "Iteration Number: 7882\n",
      "Loss: 29.62452588655644\n",
      "l2 norm of gradients: 0.19011068835996758\n",
      "l2 norm of weights: 5.6950550675954865\n",
      "---------------------\n",
      "Iteration Number: 7883\n",
      "Loss: 29.622713770981402\n",
      "l2 norm of gradients: 0.19009464190844422\n",
      "l2 norm of weights: 5.694986466431411\n",
      "---------------------\n",
      "Iteration Number: 7884\n",
      "Loss: 29.62090196124201\n",
      "l2 norm of gradients: 0.19007859868063062\n",
      "l2 norm of weights: 5.694917870496094\n",
      "---------------------\n",
      "Iteration Number: 7885\n",
      "Loss: 29.6190904572502\n",
      "l2 norm of gradients: 0.19006255867526126\n",
      "l2 norm of weights: 5.694849279788781\n",
      "---------------------\n",
      "Iteration Number: 7886\n",
      "Loss: 29.617279258919\n",
      "l2 norm of gradients: 0.19004652189107116\n",
      "l2 norm of weights: 5.694780694308721\n",
      "---------------------\n",
      "Iteration Number: 7887\n",
      "Loss: 29.61546836615802\n",
      "l2 norm of gradients: 0.19003048832679617\n",
      "l2 norm of weights: 5.694712114055161\n",
      "---------------------\n",
      "Iteration Number: 7888\n",
      "Loss: 29.61365777888348\n",
      "l2 norm of gradients: 0.19001445798117278\n",
      "l2 norm of weights: 5.694643539027347\n",
      "---------------------\n",
      "Iteration Number: 7889\n",
      "Loss: 29.611847497005446\n",
      "l2 norm of gradients: 0.1899984308529383\n",
      "l2 norm of weights: 5.694574969224526\n",
      "---------------------\n",
      "Iteration Number: 7890\n",
      "Loss: 29.610037520437864\n",
      "l2 norm of gradients: 0.18998240694083063\n",
      "l2 norm of weights: 5.6945064046459475\n",
      "---------------------\n",
      "Iteration Number: 7891\n",
      "Loss: 29.60822784909177\n",
      "l2 norm of gradients: 0.18996638624358847\n",
      "l2 norm of weights: 5.6944378452908575\n",
      "---------------------\n",
      "Iteration Number: 7892\n",
      "Loss: 29.606418482880844\n",
      "l2 norm of gradients: 0.1899503687599512\n",
      "l2 norm of weights: 5.694369291158504\n",
      "---------------------\n",
      "Iteration Number: 7893\n",
      "Loss: 29.60460942171548\n",
      "l2 norm of gradients: 0.18993435448865892\n",
      "l2 norm of weights: 5.694300742248137\n",
      "---------------------\n",
      "Iteration Number: 7894\n",
      "Loss: 29.602800665509687\n",
      "l2 norm of gradients: 0.18991834342845254\n",
      "l2 norm of weights: 5.6942321985590025\n",
      "---------------------\n",
      "Iteration Number: 7895\n",
      "Loss: 29.600992214179605\n",
      "l2 norm of gradients: 0.18990233557807348\n",
      "l2 norm of weights: 5.69416366009035\n",
      "---------------------\n",
      "Iteration Number: 7896\n",
      "Loss: 29.59918406763331\n",
      "l2 norm of gradients: 0.18988633093626414\n",
      "l2 norm of weights: 5.69409512684143\n",
      "---------------------\n",
      "Iteration Number: 7897\n",
      "Loss: 29.597376225787055\n",
      "l2 norm of gradients: 0.1898703295017674\n",
      "l2 norm of weights: 5.694026598811488\n",
      "---------------------\n",
      "Iteration Number: 7898\n",
      "Loss: 29.595568688548827\n",
      "l2 norm of gradients: 0.1898543312733269\n",
      "l2 norm of weights: 5.6939580759997765\n",
      "---------------------\n",
      "Iteration Number: 7899\n",
      "Loss: 29.593761455835693\n",
      "l2 norm of gradients: 0.1898383362496871\n",
      "l2 norm of weights: 5.693889558405543\n",
      "---------------------\n",
      "Iteration Number: 7900\n",
      "Loss: 29.591954527560073\n",
      "l2 norm of gradients: 0.18982234442959312\n",
      "l2 norm of weights: 5.693821046028036\n",
      "---------------------\n",
      "Iteration Number: 7901\n",
      "Loss: 29.590147903633053\n",
      "l2 norm of gradients: 0.18980635581179078\n",
      "l2 norm of weights: 5.693752538866508\n",
      "---------------------\n",
      "Iteration Number: 7902\n",
      "Loss: 29.588341583969758\n",
      "l2 norm of gradients: 0.18979037039502664\n",
      "l2 norm of weights: 5.693684036920207\n",
      "---------------------\n",
      "Iteration Number: 7903\n",
      "Loss: 29.58653556848056\n",
      "l2 norm of gradients: 0.18977438817804787\n",
      "l2 norm of weights: 5.693615540188383\n",
      "---------------------\n",
      "Iteration Number: 7904\n",
      "Loss: 29.584729857084888\n",
      "l2 norm of gradients: 0.18975840915960243\n",
      "l2 norm of weights: 5.693547048670288\n",
      "---------------------\n",
      "Iteration Number: 7905\n",
      "Loss: 29.582924449684654\n",
      "l2 norm of gradients: 0.18974243333843907\n",
      "l2 norm of weights: 5.693478562365171\n",
      "---------------------\n",
      "Iteration Number: 7906\n",
      "Loss: 29.581119346204623\n",
      "l2 norm of gradients: 0.18972646071330718\n",
      "l2 norm of weights: 5.693410081272283\n",
      "---------------------\n",
      "Iteration Number: 7907\n",
      "Loss: 29.57931454654897\n",
      "l2 norm of gradients: 0.18971049128295675\n",
      "l2 norm of weights: 5.693341605390874\n",
      "---------------------\n",
      "Iteration Number: 7908\n",
      "Loss: 29.577510050636377\n",
      "l2 norm of gradients: 0.18969452504613862\n",
      "l2 norm of weights: 5.693273134720197\n",
      "---------------------\n",
      "Iteration Number: 7909\n",
      "Loss: 29.57570585837994\n",
      "l2 norm of gradients: 0.1896785620016043\n",
      "l2 norm of weights: 5.693204669259501\n",
      "---------------------\n",
      "Iteration Number: 7910\n",
      "Loss: 29.573901969686553\n",
      "l2 norm of gradients: 0.189662602148106\n",
      "l2 norm of weights: 5.69313620900804\n",
      "---------------------\n",
      "Iteration Number: 7911\n",
      "Loss: 29.572098384477485\n",
      "l2 norm of gradients: 0.18964664548439672\n",
      "l2 norm of weights: 5.693067753965065\n",
      "---------------------\n",
      "Iteration Number: 7912\n",
      "Loss: 29.57029510266516\n",
      "l2 norm of gradients: 0.18963069200923002\n",
      "l2 norm of weights: 5.692999304129825\n",
      "---------------------\n",
      "Iteration Number: 7913\n",
      "Loss: 29.568492124157274\n",
      "l2 norm of gradients: 0.18961474172136028\n",
      "l2 norm of weights: 5.692930859501576\n",
      "---------------------\n",
      "Iteration Number: 7914\n",
      "Loss: 29.566689448875167\n",
      "l2 norm of gradients: 0.18959879461954252\n",
      "l2 norm of weights: 5.692862420079567\n",
      "---------------------\n",
      "Iteration Number: 7915\n",
      "Loss: 29.56488707672333\n",
      "l2 norm of gradients: 0.18958285070253247\n",
      "l2 norm of weights: 5.692793985863053\n",
      "---------------------\n",
      "Iteration Number: 7916\n",
      "Loss: 29.563085007623066\n",
      "l2 norm of gradients: 0.18956690996908668\n",
      "l2 norm of weights: 5.692725556851285\n",
      "---------------------\n",
      "Iteration Number: 7917\n",
      "Loss: 29.56128324148362\n",
      "l2 norm of gradients: 0.1895509724179622\n",
      "l2 norm of weights: 5.692657133043516\n",
      "---------------------\n",
      "Iteration Number: 7918\n",
      "Loss: 29.559481778219237\n",
      "l2 norm of gradients: 0.189535038047917\n",
      "l2 norm of weights: 5.692588714438999\n",
      "---------------------\n",
      "Iteration Number: 7919\n",
      "Loss: 29.55768061774686\n",
      "l2 norm of gradients: 0.18951910685770962\n",
      "l2 norm of weights: 5.692520301036989\n",
      "---------------------\n",
      "Iteration Number: 7920\n",
      "Loss: 29.55587975997415\n",
      "l2 norm of gradients: 0.18950317884609935\n",
      "l2 norm of weights: 5.692451892836737\n",
      "---------------------\n",
      "Iteration Number: 7921\n",
      "Loss: 29.554079204819406\n",
      "l2 norm of gradients: 0.18948725401184618\n",
      "l2 norm of weights: 5.692383489837497\n",
      "---------------------\n",
      "Iteration Number: 7922\n",
      "Loss: 29.552278952192957\n",
      "l2 norm of gradients: 0.18947133235371086\n",
      "l2 norm of weights: 5.692315092038524\n",
      "---------------------\n",
      "Iteration Number: 7923\n",
      "Loss: 29.550479002013617\n",
      "l2 norm of gradients: 0.18945541387045464\n",
      "l2 norm of weights: 5.692246699439071\n",
      "---------------------\n",
      "Iteration Number: 7924\n",
      "Loss: 29.548679354194736\n",
      "l2 norm of gradients: 0.1894394985608398\n",
      "l2 norm of weights: 5.692178312038392\n",
      "---------------------\n",
      "Iteration Number: 7925\n",
      "Loss: 29.54688000864264\n",
      "l2 norm of gradients: 0.18942358642362894\n",
      "l2 norm of weights: 5.692109929835741\n",
      "---------------------\n",
      "Iteration Number: 7926\n",
      "Loss: 29.545080965279098\n",
      "l2 norm of gradients: 0.18940767745758574\n",
      "l2 norm of weights: 5.692041552830374\n",
      "---------------------\n",
      "Iteration Number: 7927\n",
      "Loss: 29.543282224011666\n",
      "l2 norm of gradients: 0.18939177166147436\n",
      "l2 norm of weights: 5.691973181021544\n",
      "---------------------\n",
      "Iteration Number: 7928\n",
      "Loss: 29.54148378476295\n",
      "l2 norm of gradients: 0.18937586903405956\n",
      "l2 norm of weights: 5.691904814408509\n",
      "---------------------\n",
      "Iteration Number: 7929\n",
      "Loss: 29.53968564744148\n",
      "l2 norm of gradients: 0.1893599695741072\n",
      "l2 norm of weights: 5.69183645299052\n",
      "---------------------\n",
      "Iteration Number: 7930\n",
      "Loss: 29.537887811960623\n",
      "l2 norm of gradients: 0.1893440732803834\n",
      "l2 norm of weights: 5.691768096766835\n",
      "---------------------\n",
      "Iteration Number: 7931\n",
      "Loss: 29.53609027823575\n",
      "l2 norm of gradients: 0.18932818015165526\n",
      "l2 norm of weights: 5.691699745736709\n",
      "---------------------\n",
      "Iteration Number: 7932\n",
      "Loss: 29.534293046181112\n",
      "l2 norm of gradients: 0.18931229018669044\n",
      "l2 norm of weights: 5.691631399899396\n",
      "---------------------\n",
      "Iteration Number: 7933\n",
      "Loss: 29.532496115711147\n",
      "l2 norm of gradients: 0.18929640338425735\n",
      "l2 norm of weights: 5.691563059254155\n",
      "---------------------\n",
      "Iteration Number: 7934\n",
      "Loss: 29.53069948673642\n",
      "l2 norm of gradients: 0.1892805197431251\n",
      "l2 norm of weights: 5.691494723800239\n",
      "---------------------\n",
      "Iteration Number: 7935\n",
      "Loss: 29.528903159179485\n",
      "l2 norm of gradients: 0.18926463926206355\n",
      "l2 norm of weights: 5.691426393536906\n",
      "---------------------\n",
      "Iteration Number: 7936\n",
      "Loss: 29.527107132947386\n",
      "l2 norm of gradients: 0.18924876193984316\n",
      "l2 norm of weights: 5.691358068463413\n",
      "---------------------\n",
      "Iteration Number: 7937\n",
      "Loss: 29.525311407957382\n",
      "l2 norm of gradients: 0.18923288777523511\n",
      "l2 norm of weights: 5.691289748579015\n",
      "---------------------\n",
      "Iteration Number: 7938\n",
      "Loss: 29.523515984121744\n",
      "l2 norm of gradients: 0.1892170167670114\n",
      "l2 norm of weights: 5.691221433882969\n",
      "---------------------\n",
      "Iteration Number: 7939\n",
      "Loss: 29.521720861357302\n",
      "l2 norm of gradients: 0.1892011489139445\n",
      "l2 norm of weights: 5.691153124374534\n",
      "---------------------\n",
      "Iteration Number: 7940\n",
      "Loss: 29.51992603957842\n",
      "l2 norm of gradients: 0.18918528421480776\n",
      "l2 norm of weights: 5.691084820052965\n",
      "---------------------\n",
      "Iteration Number: 7941\n",
      "Loss: 29.518131518696297\n",
      "l2 norm of gradients: 0.1891694226683752\n",
      "l2 norm of weights: 5.691016520917519\n",
      "---------------------\n",
      "Iteration Number: 7942\n",
      "Loss: 29.516337298630987\n",
      "l2 norm of gradients: 0.18915356427342145\n",
      "l2 norm of weights: 5.690948226967456\n",
      "---------------------\n",
      "Iteration Number: 7943\n",
      "Loss: 29.51454337929216\n",
      "l2 norm of gradients: 0.1891377090287219\n",
      "l2 norm of weights: 5.690879938202032\n",
      "---------------------\n",
      "Iteration Number: 7944\n",
      "Loss: 29.512749760598172\n",
      "l2 norm of gradients: 0.18912185693305267\n",
      "l2 norm of weights: 5.690811654620506\n",
      "---------------------\n",
      "Iteration Number: 7945\n",
      "Loss: 29.510956442457843\n",
      "l2 norm of gradients: 0.18910600798519048\n",
      "l2 norm of weights: 5.690743376222135\n",
      "---------------------\n",
      "Iteration Number: 7946\n",
      "Loss: 29.50916342479389\n",
      "l2 norm of gradients: 0.18909016218391284\n",
      "l2 norm of weights: 5.690675103006178\n",
      "---------------------\n",
      "Iteration Number: 7947\n",
      "Loss: 29.507370707516355\n",
      "l2 norm of gradients: 0.18907431952799794\n",
      "l2 norm of weights: 5.690606834971895\n",
      "---------------------\n",
      "Iteration Number: 7948\n",
      "Loss: 29.50557829053712\n",
      "l2 norm of gradients: 0.18905848001622447\n",
      "l2 norm of weights: 5.690538572118542\n",
      "---------------------\n",
      "Iteration Number: 7949\n",
      "Loss: 29.50378617377651\n",
      "l2 norm of gradients: 0.1890426436473722\n",
      "l2 norm of weights: 5.690470314445379\n",
      "---------------------\n",
      "Iteration Number: 7950\n",
      "Loss: 29.501994357149194\n",
      "l2 norm of gradients: 0.18902681042022118\n",
      "l2 norm of weights: 5.690402061951666\n",
      "---------------------\n",
      "Iteration Number: 7951\n",
      "Loss: 29.500202840564423\n",
      "l2 norm of gradients: 0.18901098033355246\n",
      "l2 norm of weights: 5.690333814636662\n",
      "---------------------\n",
      "Iteration Number: 7952\n",
      "Loss: 29.49841162394337\n",
      "l2 norm of gradients: 0.18899515338614759\n",
      "l2 norm of weights: 5.690265572499625\n",
      "---------------------\n",
      "Iteration Number: 7953\n",
      "Loss: 29.496620707198332\n",
      "l2 norm of gradients: 0.18897932957678895\n",
      "l2 norm of weights: 5.690197335539817\n",
      "---------------------\n",
      "Iteration Number: 7954\n",
      "Loss: 29.49483009024372\n",
      "l2 norm of gradients: 0.18896350890425953\n",
      "l2 norm of weights: 5.690129103756495\n",
      "---------------------\n",
      "Iteration Number: 7955\n",
      "Loss: 29.493039772995598\n",
      "l2 norm of gradients: 0.18894769136734296\n",
      "l2 norm of weights: 5.690060877148921\n",
      "---------------------\n",
      "Iteration Number: 7956\n",
      "Loss: 29.491249755366674\n",
      "l2 norm of gradients: 0.18893187696482375\n",
      "l2 norm of weights: 5.689992655716356\n",
      "---------------------\n",
      "Iteration Number: 7957\n",
      "Loss: 29.489460037278626\n",
      "l2 norm of gradients: 0.18891606569548686\n",
      "l2 norm of weights: 5.689924439458058\n",
      "---------------------\n",
      "Iteration Number: 7958\n",
      "Loss: 29.487670618637758\n",
      "l2 norm of gradients: 0.18890025755811807\n",
      "l2 norm of weights: 5.68985622837329\n",
      "---------------------\n",
      "Iteration Number: 7959\n",
      "Loss: 29.485881499363977\n",
      "l2 norm of gradients: 0.188884452551504\n",
      "l2 norm of weights: 5.689788022461312\n",
      "---------------------\n",
      "Iteration Number: 7960\n",
      "Loss: 29.484092679374694\n",
      "l2 norm of gradients: 0.18886865067443154\n",
      "l2 norm of weights: 5.689719821721385\n",
      "---------------------\n",
      "Iteration Number: 7961\n",
      "Loss: 29.48230415858036\n",
      "l2 norm of gradients: 0.18885285192568865\n",
      "l2 norm of weights: 5.689651626152769\n",
      "---------------------\n",
      "Iteration Number: 7962\n",
      "Loss: 29.480515936898307\n",
      "l2 norm of gradients: 0.18883705630406392\n",
      "l2 norm of weights: 5.689583435754726\n",
      "---------------------\n",
      "Iteration Number: 7963\n",
      "Loss: 29.478728014244716\n",
      "l2 norm of gradients: 0.18882126380834646\n",
      "l2 norm of weights: 5.68951525052652\n",
      "---------------------\n",
      "Iteration Number: 7964\n",
      "Loss: 29.476940390532935\n",
      "l2 norm of gradients: 0.18880547443732618\n",
      "l2 norm of weights: 5.689447070467408\n",
      "---------------------\n",
      "Iteration Number: 7965\n",
      "Loss: 29.475153065682587\n",
      "l2 norm of gradients: 0.18878968818979375\n",
      "l2 norm of weights: 5.689378895576657\n",
      "---------------------\n",
      "Iteration Number: 7966\n",
      "Loss: 29.47336603960284\n",
      "l2 norm of gradients: 0.18877390506454025\n",
      "l2 norm of weights: 5.689310725853526\n",
      "---------------------\n",
      "Iteration Number: 7967\n",
      "Loss: 29.47157931221574\n",
      "l2 norm of gradients: 0.18875812506035783\n",
      "l2 norm of weights: 5.689242561297277\n",
      "---------------------\n",
      "Iteration Number: 7968\n",
      "Loss: 29.46979288343019\n",
      "l2 norm of gradients: 0.18874234817603908\n",
      "l2 norm of weights: 5.689174401907175\n",
      "---------------------\n",
      "Iteration Number: 7969\n",
      "Loss: 29.46800675316862\n",
      "l2 norm of gradients: 0.18872657441037724\n",
      "l2 norm of weights: 5.689106247682481\n",
      "---------------------\n",
      "Iteration Number: 7970\n",
      "Loss: 29.466220921339954\n",
      "l2 norm of gradients: 0.18871080376216642\n",
      "l2 norm of weights: 5.689038098622458\n",
      "---------------------\n",
      "Iteration Number: 7971\n",
      "Loss: 29.464435387863844\n",
      "l2 norm of gradients: 0.18869503623020129\n",
      "l2 norm of weights: 5.688969954726369\n",
      "---------------------\n",
      "Iteration Number: 7972\n",
      "Loss: 29.462650152656465\n",
      "l2 norm of gradients: 0.18867927181327723\n",
      "l2 norm of weights: 5.6889018159934786\n",
      "---------------------\n",
      "Iteration Number: 7973\n",
      "Loss: 29.46086521562981\n",
      "l2 norm of gradients: 0.1886635105101903\n",
      "l2 norm of weights: 5.6888336824230485\n",
      "---------------------\n",
      "Iteration Number: 7974\n",
      "Loss: 29.459080576705393\n",
      "l2 norm of gradients: 0.18864775231973727\n",
      "l2 norm of weights: 5.688765554014343\n",
      "---------------------\n",
      "Iteration Number: 7975\n",
      "Loss: 29.457296235793542\n",
      "l2 norm of gradients: 0.1886319972407155\n",
      "l2 norm of weights: 5.688697430766625\n",
      "---------------------\n",
      "Iteration Number: 7976\n",
      "Loss: 29.455512192812716\n",
      "l2 norm of gradients: 0.18861624527192317\n",
      "l2 norm of weights: 5.6886293126791605\n",
      "---------------------\n",
      "Iteration Number: 7977\n",
      "Loss: 29.453728447678913\n",
      "l2 norm of gradients: 0.18860049641215906\n",
      "l2 norm of weights: 5.688561199751213\n",
      "---------------------\n",
      "Iteration Number: 7978\n",
      "Loss: 29.451945000305624\n",
      "l2 norm of gradients: 0.1885847506602227\n",
      "l2 norm of weights: 5.6884930919820444\n",
      "---------------------\n",
      "Iteration Number: 7979\n",
      "Loss: 29.45016185061237\n",
      "l2 norm of gradients: 0.1885690080149142\n",
      "l2 norm of weights: 5.688424989370923\n",
      "---------------------\n",
      "Iteration Number: 7980\n",
      "Loss: 29.448378998511643\n",
      "l2 norm of gradients: 0.1885532684750343\n",
      "l2 norm of weights: 5.688356891917112\n",
      "---------------------\n",
      "Iteration Number: 7981\n",
      "Loss: 29.446596443922402\n",
      "l2 norm of gradients: 0.1885375320393847\n",
      "l2 norm of weights: 5.688288799619876\n",
      "---------------------\n",
      "Iteration Number: 7982\n",
      "Loss: 29.444814186758155\n",
      "l2 norm of gradients: 0.18852179870676752\n",
      "l2 norm of weights: 5.68822071247848\n",
      "---------------------\n",
      "Iteration Number: 7983\n",
      "Loss: 29.443032226938254\n",
      "l2 norm of gradients: 0.18850606847598567\n",
      "l2 norm of weights: 5.688152630492191\n",
      "---------------------\n",
      "Iteration Number: 7984\n",
      "Loss: 29.441250564374702\n",
      "l2 norm of gradients: 0.18849034134584267\n",
      "l2 norm of weights: 5.688084553660273\n",
      "---------------------\n",
      "Iteration Number: 7985\n",
      "Loss: 29.439469198984465\n",
      "l2 norm of gradients: 0.18847461731514276\n",
      "l2 norm of weights: 5.688016481981991\n",
      "---------------------\n",
      "Iteration Number: 7986\n",
      "Loss: 29.437688130688784\n",
      "l2 norm of gradients: 0.1884588963826909\n",
      "l2 norm of weights: 5.687948415456613\n",
      "---------------------\n",
      "Iteration Number: 7987\n",
      "Loss: 29.435907359398936\n",
      "l2 norm of gradients: 0.1884431785472927\n",
      "l2 norm of weights: 5.687880354083402\n",
      "---------------------\n",
      "Iteration Number: 7988\n",
      "Loss: 29.434126885029716\n",
      "l2 norm of gradients: 0.18842746380775435\n",
      "l2 norm of weights: 5.687812297861628\n",
      "---------------------\n",
      "Iteration Number: 7989\n",
      "Loss: 29.432346707505083\n",
      "l2 norm of gradients: 0.1884117521628829\n",
      "l2 norm of weights: 5.687744246790555\n",
      "---------------------\n",
      "Iteration Number: 7990\n",
      "Loss: 29.430566826730757\n",
      "l2 norm of gradients: 0.18839604361148593\n",
      "l2 norm of weights: 5.68767620086945\n",
      "---------------------\n",
      "Iteration Number: 7991\n",
      "Loss: 29.428787242634183\n",
      "l2 norm of gradients: 0.1883803381523718\n",
      "l2 norm of weights: 5.68760816009758\n",
      "---------------------\n",
      "Iteration Number: 7992\n",
      "Loss: 29.42700795512236\n",
      "l2 norm of gradients: 0.18836463578434942\n",
      "l2 norm of weights: 5.687540124474212\n",
      "---------------------\n",
      "Iteration Number: 7993\n",
      "Loss: 29.42522896411799\n",
      "l2 norm of gradients: 0.18834893650622853\n",
      "l2 norm of weights: 5.687472093998613\n",
      "---------------------\n",
      "Iteration Number: 7994\n",
      "Loss: 29.423450269531635\n",
      "l2 norm of gradients: 0.1883332403168194\n",
      "l2 norm of weights: 5.687404068670052\n",
      "---------------------\n",
      "Iteration Number: 7995\n",
      "Loss: 29.421671871287145\n",
      "l2 norm of gradients: 0.18831754721493307\n",
      "l2 norm of weights: 5.6873360484877935\n",
      "---------------------\n",
      "Iteration Number: 7996\n",
      "Loss: 29.41989376929842\n",
      "l2 norm of gradients: 0.18830185719938122\n",
      "l2 norm of weights: 5.687268033451106\n",
      "---------------------\n",
      "Iteration Number: 7997\n",
      "Loss: 29.418115963477\n",
      "l2 norm of gradients: 0.1882861702689762\n",
      "l2 norm of weights: 5.687200023559261\n",
      "---------------------\n",
      "Iteration Number: 7998\n",
      "Loss: 29.41633845374704\n",
      "l2 norm of gradients: 0.18827048642253114\n",
      "l2 norm of weights: 5.687132018811522\n",
      "---------------------\n",
      "Iteration Number: 7999\n",
      "Loss: 29.414561240021158\n",
      "l2 norm of gradients: 0.18825480565885966\n",
      "l2 norm of weights: 5.68706401920716\n",
      "---------------------\n",
      "Iteration Number: 8000\n",
      "Loss: 29.412784322216574\n",
      "l2 norm of gradients: 0.18823912797677614\n",
      "l2 norm of weights: 5.686996024745442\n",
      "---------------------\n",
      "Iteration Number: 8001\n",
      "Loss: 29.411007700251155\n",
      "l2 norm of gradients: 0.1882234533750957\n",
      "l2 norm of weights: 5.6869280354256375\n",
      "---------------------\n",
      "Iteration Number: 8002\n",
      "Loss: 29.409231374039603\n",
      "l2 norm of gradients: 0.188207781852634\n",
      "l2 norm of weights: 5.686860051247016\n",
      "---------------------\n",
      "Iteration Number: 8003\n",
      "Loss: 29.407455343499258\n",
      "l2 norm of gradients: 0.18819211340820752\n",
      "l2 norm of weights: 5.686792072208845\n",
      "---------------------\n",
      "Iteration Number: 8004\n",
      "Loss: 29.40567960854938\n",
      "l2 norm of gradients: 0.1881764480406333\n",
      "l2 norm of weights: 5.686724098310394\n",
      "---------------------\n",
      "Iteration Number: 8005\n",
      "Loss: 29.403904169103257\n",
      "l2 norm of gradients: 0.18816078574872905\n",
      "l2 norm of weights: 5.686656129550934\n",
      "---------------------\n",
      "Iteration Number: 8006\n",
      "Loss: 29.40212902508094\n",
      "l2 norm of gradients: 0.18814512653131324\n",
      "l2 norm of weights: 5.6865881659297335\n",
      "---------------------\n",
      "Iteration Number: 8007\n",
      "Loss: 29.40035417639715\n",
      "l2 norm of gradients: 0.18812947038720496\n",
      "l2 norm of weights: 5.686520207446061\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 8008\n",
      "Loss: 29.39857962297081\n",
      "l2 norm of gradients: 0.18811381731522395\n",
      "l2 norm of weights: 5.686452254099189\n",
      "---------------------\n",
      "Iteration Number: 8009\n",
      "Loss: 29.396805364717842\n",
      "l2 norm of gradients: 0.18809816731419063\n",
      "l2 norm of weights: 5.686384305888386\n",
      "---------------------\n",
      "Iteration Number: 8010\n",
      "Loss: 29.395031401557077\n",
      "l2 norm of gradients: 0.18808252038292614\n",
      "l2 norm of weights: 5.6863163628129225\n",
      "---------------------\n",
      "Iteration Number: 8011\n",
      "Loss: 29.393257733403576\n",
      "l2 norm of gradients: 0.18806687652025225\n",
      "l2 norm of weights: 5.686248424872069\n",
      "---------------------\n",
      "Iteration Number: 8012\n",
      "Loss: 29.391484360173408\n",
      "l2 norm of gradients: 0.18805123572499133\n",
      "l2 norm of weights: 5.6861804920650965\n",
      "---------------------\n",
      "Iteration Number: 8013\n",
      "Loss: 29.389711281787296\n",
      "l2 norm of gradients: 0.18803559799596659\n",
      "l2 norm of weights: 5.6861125643912755\n",
      "---------------------\n",
      "Iteration Number: 8014\n",
      "Loss: 29.387938498157467\n",
      "l2 norm of gradients: 0.1880199633320017\n",
      "l2 norm of weights: 5.686044641849877\n",
      "---------------------\n",
      "Iteration Number: 8015\n",
      "Loss: 29.3861660092072\n",
      "l2 norm of gradients: 0.18800433173192121\n",
      "l2 norm of weights: 5.685976724440175\n",
      "---------------------\n",
      "Iteration Number: 8016\n",
      "Loss: 29.384393814851027\n",
      "l2 norm of gradients: 0.18798870319455016\n",
      "l2 norm of weights: 5.685908812161436\n",
      "---------------------\n",
      "Iteration Number: 8017\n",
      "Loss: 29.38262191500649\n",
      "l2 norm of gradients: 0.18797307771871435\n",
      "l2 norm of weights: 5.685840905012935\n",
      "---------------------\n",
      "Iteration Number: 8018\n",
      "Loss: 29.380850309588865\n",
      "l2 norm of gradients: 0.18795745530324026\n",
      "l2 norm of weights: 5.685773002993944\n",
      "---------------------\n",
      "Iteration Number: 8019\n",
      "Loss: 29.379078998519\n",
      "l2 norm of gradients: 0.18794183594695496\n",
      "l2 norm of weights: 5.6857051061037325\n",
      "---------------------\n",
      "Iteration Number: 8020\n",
      "Loss: 29.377307981711354\n",
      "l2 norm of gradients: 0.18792621964868622\n",
      "l2 norm of weights: 5.6856372143415745\n",
      "---------------------\n",
      "Iteration Number: 8021\n",
      "Loss: 29.37553725908513\n",
      "l2 norm of gradients: 0.18791060640726254\n",
      "l2 norm of weights: 5.685569327706742\n",
      "---------------------\n",
      "Iteration Number: 8022\n",
      "Loss: 29.373766830560406\n",
      "l2 norm of gradients: 0.187894996221513\n",
      "l2 norm of weights: 5.685501446198508\n",
      "---------------------\n",
      "Iteration Number: 8023\n",
      "Loss: 29.37199669604926\n",
      "l2 norm of gradients: 0.1878793890902674\n",
      "l2 norm of weights: 5.685433569816144\n",
      "---------------------\n",
      "Iteration Number: 8024\n",
      "Loss: 29.370226855472215\n",
      "l2 norm of gradients: 0.18786378501235618\n",
      "l2 norm of weights: 5.685365698558925\n",
      "---------------------\n",
      "Iteration Number: 8025\n",
      "Loss: 29.368457308745285\n",
      "l2 norm of gradients: 0.18784818398661038\n",
      "l2 norm of weights: 5.685297832426121\n",
      "---------------------\n",
      "Iteration Number: 8026\n",
      "Loss: 29.366688055789115\n",
      "l2 norm of gradients: 0.18783258601186187\n",
      "l2 norm of weights: 5.685229971417009\n",
      "---------------------\n",
      "Iteration Number: 8027\n",
      "Loss: 29.36491909651844\n",
      "l2 norm of gradients: 0.187816991086943\n",
      "l2 norm of weights: 5.68516211553086\n",
      "---------------------\n",
      "Iteration Number: 8028\n",
      "Loss: 29.363150430853953\n",
      "l2 norm of gradients: 0.18780139921068692\n",
      "l2 norm of weights: 5.685094264766947\n",
      "---------------------\n",
      "Iteration Number: 8029\n",
      "Loss: 29.361382058712238\n",
      "l2 norm of gradients: 0.18778581038192732\n",
      "l2 norm of weights: 5.685026419124545\n",
      "---------------------\n",
      "Iteration Number: 8030\n",
      "Loss: 29.35961398000832\n",
      "l2 norm of gradients: 0.18777022459949874\n",
      "l2 norm of weights: 5.684958578602929\n",
      "---------------------\n",
      "Iteration Number: 8031\n",
      "Loss: 29.35784619466512\n",
      "l2 norm of gradients: 0.18775464186223617\n",
      "l2 norm of weights: 5.684890743201371\n",
      "---------------------\n",
      "Iteration Number: 8032\n",
      "Loss: 29.356078702597394\n",
      "l2 norm of gradients: 0.18773906216897537\n",
      "l2 norm of weights: 5.684822912919148\n",
      "---------------------\n",
      "Iteration Number: 8033\n",
      "Loss: 29.354311503723125\n",
      "l2 norm of gradients: 0.1877234855185528\n",
      "l2 norm of weights: 5.684755087755532\n",
      "---------------------\n",
      "Iteration Number: 8034\n",
      "Loss: 29.352544597961007\n",
      "l2 norm of gradients: 0.18770791190980549\n",
      "l2 norm of weights: 5.684687267709799\n",
      "---------------------\n",
      "Iteration Number: 8035\n",
      "Loss: 29.35077798522645\n",
      "l2 norm of gradients: 0.1876923413415712\n",
      "l2 norm of weights: 5.684619452781224\n",
      "---------------------\n",
      "Iteration Number: 8036\n",
      "Loss: 29.349011665442127\n",
      "l2 norm of gradients: 0.18767677381268827\n",
      "l2 norm of weights: 5.684551642969081\n",
      "---------------------\n",
      "Iteration Number: 8037\n",
      "Loss: 29.34724563852276\n",
      "l2 norm of gradients: 0.18766120932199581\n",
      "l2 norm of weights: 5.684483838272647\n",
      "---------------------\n",
      "Iteration Number: 8038\n",
      "Loss: 29.345479904388483\n",
      "l2 norm of gradients: 0.18764564786833346\n",
      "l2 norm of weights: 5.684416038691197\n",
      "---------------------\n",
      "Iteration Number: 8039\n",
      "Loss: 29.343714462957077\n",
      "l2 norm of gradients: 0.18763008945054163\n",
      "l2 norm of weights: 5.684348244224005\n",
      "---------------------\n",
      "Iteration Number: 8040\n",
      "Loss: 29.34194931414579\n",
      "l2 norm of gradients: 0.1876145340674614\n",
      "l2 norm of weights: 5.684280454870349\n",
      "---------------------\n",
      "Iteration Number: 8041\n",
      "Loss: 29.34018445787038\n",
      "l2 norm of gradients: 0.18759898171793438\n",
      "l2 norm of weights: 5.6842126706295035\n",
      "---------------------\n",
      "Iteration Number: 8042\n",
      "Loss: 29.338419894054798\n",
      "l2 norm of gradients: 0.18758343240080294\n",
      "l2 norm of weights: 5.684144891500746\n",
      "---------------------\n",
      "Iteration Number: 8043\n",
      "Loss: 29.33665562261256\n",
      "l2 norm of gradients: 0.1875678861149101\n",
      "l2 norm of weights: 5.684077117483351\n",
      "---------------------\n",
      "Iteration Number: 8044\n",
      "Loss: 29.3348916434651\n",
      "l2 norm of gradients: 0.18755234285909947\n",
      "l2 norm of weights: 5.6840093485765975\n",
      "---------------------\n",
      "Iteration Number: 8045\n",
      "Loss: 29.3331279565311\n",
      "l2 norm of gradients: 0.18753680263221545\n",
      "l2 norm of weights: 5.6839415847797605\n",
      "---------------------\n",
      "Iteration Number: 8046\n",
      "Loss: 29.33136456172398\n",
      "l2 norm of gradients: 0.18752126543310296\n",
      "l2 norm of weights: 5.683873826092117\n",
      "---------------------\n",
      "Iteration Number: 8047\n",
      "Loss: 29.32960145896792\n",
      "l2 norm of gradients: 0.18750573126060766\n",
      "l2 norm of weights: 5.683806072512945\n",
      "---------------------\n",
      "Iteration Number: 8048\n",
      "Loss: 29.327838648177273\n",
      "l2 norm of gradients: 0.18749020011357587\n",
      "l2 norm of weights: 5.683738324041521\n",
      "---------------------\n",
      "Iteration Number: 8049\n",
      "Loss: 29.32607612927367\n",
      "l2 norm of gradients: 0.1874746719908544\n",
      "l2 norm of weights: 5.683670580677123\n",
      "---------------------\n",
      "Iteration Number: 8050\n",
      "Loss: 29.3243139021751\n",
      "l2 norm of gradients: 0.18745914689129098\n",
      "l2 norm of weights: 5.683602842419028\n",
      "---------------------\n",
      "Iteration Number: 8051\n",
      "Loss: 29.322551966798134\n",
      "l2 norm of gradients: 0.18744362481373386\n",
      "l2 norm of weights: 5.683535109266515\n",
      "---------------------\n",
      "Iteration Number: 8052\n",
      "Loss: 29.320790323060002\n",
      "l2 norm of gradients: 0.18742810575703192\n",
      "l2 norm of weights: 5.68346738121886\n",
      "---------------------\n",
      "Iteration Number: 8053\n",
      "Loss: 29.31902897088631\n",
      "l2 norm of gradients: 0.1874125897200347\n",
      "l2 norm of weights: 5.683399658275343\n",
      "---------------------\n",
      "Iteration Number: 8054\n",
      "Loss: 29.317267910189216\n",
      "l2 norm of gradients: 0.18739707670159245\n",
      "l2 norm of weights: 5.683331940435242\n",
      "---------------------\n",
      "Iteration Number: 8055\n",
      "Loss: 29.31550714089\n",
      "l2 norm of gradients: 0.187381566700556\n",
      "l2 norm of weights: 5.683264227697835\n",
      "---------------------\n",
      "Iteration Number: 8056\n",
      "Loss: 29.313746662908322\n",
      "l2 norm of gradients: 0.18736605971577694\n",
      "l2 norm of weights: 5.683196520062401\n",
      "---------------------\n",
      "Iteration Number: 8057\n",
      "Loss: 29.311986476160552\n",
      "l2 norm of gradients: 0.18735055574610743\n",
      "l2 norm of weights: 5.683128817528219\n",
      "---------------------\n",
      "Iteration Number: 8058\n",
      "Loss: 29.31022658056672\n",
      "l2 norm of gradients: 0.1873350547904003\n",
      "l2 norm of weights: 5.683061120094568\n",
      "---------------------\n",
      "Iteration Number: 8059\n",
      "Loss: 29.30846697604551\n",
      "l2 norm of gradients: 0.18731955684750898\n",
      "l2 norm of weights: 5.682993427760728\n",
      "---------------------\n",
      "Iteration Number: 8060\n",
      "Loss: 29.30670766251658\n",
      "l2 norm of gradients: 0.18730406191628762\n",
      "l2 norm of weights: 5.6829257405259765\n",
      "---------------------\n",
      "Iteration Number: 8061\n",
      "Loss: 29.304948639897834\n",
      "l2 norm of gradients: 0.18728856999559107\n",
      "l2 norm of weights: 5.682858058389596\n",
      "---------------------\n",
      "Iteration Number: 8062\n",
      "Loss: 29.303189908108113\n",
      "l2 norm of gradients: 0.1872730810842747\n",
      "l2 norm of weights: 5.682790381350864\n",
      "---------------------\n",
      "Iteration Number: 8063\n",
      "Loss: 29.301431467068628\n",
      "l2 norm of gradients: 0.1872575951811947\n",
      "l2 norm of weights: 5.682722709409061\n",
      "---------------------\n",
      "Iteration Number: 8064\n",
      "Loss: 29.299673316694886\n",
      "l2 norm of gradients: 0.18724211228520768\n",
      "l2 norm of weights: 5.682655042563469\n",
      "---------------------\n",
      "Iteration Number: 8065\n",
      "Loss: 29.297915456909337\n",
      "l2 norm of gradients: 0.18722663239517107\n",
      "l2 norm of weights: 5.682587380813366\n",
      "---------------------\n",
      "Iteration Number: 8066\n",
      "Loss: 29.29615788762876\n",
      "l2 norm of gradients: 0.18721115550994297\n",
      "l2 norm of weights: 5.682519724158032\n",
      "---------------------\n",
      "Iteration Number: 8067\n",
      "Loss: 29.29440060877273\n",
      "l2 norm of gradients: 0.18719568162838202\n",
      "l2 norm of weights: 5.6824520725967504\n",
      "---------------------\n",
      "Iteration Number: 8068\n",
      "Loss: 29.292643620260204\n",
      "l2 norm of gradients: 0.18718021074934754\n",
      "l2 norm of weights: 5.682384426128801\n",
      "---------------------\n",
      "Iteration Number: 8069\n",
      "Loss: 29.29088692201417\n",
      "l2 norm of gradients: 0.1871647428716995\n",
      "l2 norm of weights: 5.682316784753463\n",
      "---------------------\n",
      "Iteration Number: 8070\n",
      "Loss: 29.2891305139482\n",
      "l2 norm of gradients: 0.18714927799429867\n",
      "l2 norm of weights: 5.682249148470022\n",
      "---------------------\n",
      "Iteration Number: 8071\n",
      "Loss: 29.287374395985225\n",
      "l2 norm of gradients: 0.18713381611600619\n",
      "l2 norm of weights: 5.682181517277754\n",
      "---------------------\n",
      "Iteration Number: 8072\n",
      "Loss: 29.28561856804435\n",
      "l2 norm of gradients: 0.187118357235684\n",
      "l2 norm of weights: 5.682113891175945\n",
      "---------------------\n",
      "Iteration Number: 8073\n",
      "Loss: 29.28386303004302\n",
      "l2 norm of gradients: 0.18710290135219484\n",
      "l2 norm of weights: 5.682046270163873\n",
      "---------------------\n",
      "Iteration Number: 8074\n",
      "Loss: 29.282107781900034\n",
      "l2 norm of gradients: 0.18708744846440167\n",
      "l2 norm of weights: 5.681978654240824\n",
      "---------------------\n",
      "Iteration Number: 8075\n",
      "Loss: 29.280352823540547\n",
      "l2 norm of gradients: 0.18707199857116857\n",
      "l2 norm of weights: 5.681911043406078\n",
      "---------------------\n",
      "Iteration Number: 8076\n",
      "Loss: 29.278598154876775\n",
      "l2 norm of gradients: 0.18705655167135998\n",
      "l2 norm of weights: 5.681843437658918\n",
      "---------------------\n",
      "Iteration Number: 8077\n",
      "Loss: 29.276843775834095\n",
      "l2 norm of gradients: 0.18704110776384109\n",
      "l2 norm of weights: 5.681775836998625\n",
      "---------------------\n",
      "Iteration Number: 8078\n",
      "Loss: 29.27508968632799\n",
      "l2 norm of gradients: 0.18702566684747765\n",
      "l2 norm of weights: 5.6817082414244835\n",
      "---------------------\n",
      "Iteration Number: 8079\n",
      "Loss: 29.273335886278744\n",
      "l2 norm of gradients: 0.1870102289211362\n",
      "l2 norm of weights: 5.681640650935775\n",
      "---------------------\n",
      "Iteration Number: 8080\n",
      "Loss: 29.271582375607455\n",
      "l2 norm of gradients: 0.18699479398368377\n",
      "l2 norm of weights: 5.681573065531785\n",
      "---------------------\n",
      "Iteration Number: 8081\n",
      "Loss: 29.269829154231594\n",
      "l2 norm of gradients: 0.18697936203398816\n",
      "l2 norm of weights: 5.681505485211794\n",
      "---------------------\n",
      "Iteration Number: 8082\n",
      "Loss: 29.268076222073965\n",
      "l2 norm of gradients: 0.18696393307091771\n",
      "l2 norm of weights: 5.681437909975085\n",
      "---------------------\n",
      "Iteration Number: 8083\n",
      "Loss: 29.266323579052752\n",
      "l2 norm of gradients: 0.18694850709334146\n",
      "l2 norm of weights: 5.681370339820945\n",
      "---------------------\n",
      "Iteration Number: 8084\n",
      "Loss: 29.264571225086453\n",
      "l2 norm of gradients: 0.18693308410012915\n",
      "l2 norm of weights: 5.681302774748654\n",
      "---------------------\n",
      "Iteration Number: 8085\n",
      "Loss: 29.26281916009655\n",
      "l2 norm of gradients: 0.186917664090151\n",
      "l2 norm of weights: 5.681235214757499\n",
      "---------------------\n",
      "Iteration Number: 8086\n",
      "Loss: 29.2610673840015\n",
      "l2 norm of gradients: 0.18690224706227807\n",
      "l2 norm of weights: 5.6811676598467615\n",
      "---------------------\n",
      "Iteration Number: 8087\n",
      "Loss: 29.259315896722054\n",
      "l2 norm of gradients: 0.1868868330153818\n",
      "l2 norm of weights: 5.681100110015728\n",
      "---------------------\n",
      "Iteration Number: 8088\n",
      "Loss: 29.25756469818043\n",
      "l2 norm of gradients: 0.18687142194833464\n",
      "l2 norm of weights: 5.681032565263681\n",
      "---------------------\n",
      "Iteration Number: 8089\n",
      "Loss: 29.25581378829151\n",
      "l2 norm of gradients: 0.18685601386000944\n",
      "l2 norm of weights: 5.680965025589908\n",
      "---------------------\n",
      "Iteration Number: 8090\n",
      "Loss: 29.25406316697821\n",
      "l2 norm of gradients: 0.18684060874927963\n",
      "l2 norm of weights: 5.680897490993692\n",
      "---------------------\n",
      "Iteration Number: 8091\n",
      "Loss: 29.25231283416143\n",
      "l2 norm of gradients: 0.18682520661501936\n",
      "l2 norm of weights: 5.680829961474316\n",
      "---------------------\n",
      "Iteration Number: 8092\n",
      "Loss: 29.25056278975947\n",
      "l2 norm of gradients: 0.1868098074561036\n",
      "l2 norm of weights: 5.680762437031069\n",
      "---------------------\n",
      "Iteration Number: 8093\n",
      "Loss: 29.24881303369334\n",
      "l2 norm of gradients: 0.1867944112714077\n",
      "l2 norm of weights: 5.680694917663233\n",
      "---------------------\n",
      "Iteration Number: 8094\n",
      "Loss: 29.247063565880378\n",
      "l2 norm of gradients: 0.18677901805980773\n",
      "l2 norm of weights: 5.680627403370096\n",
      "---------------------\n",
      "Iteration Number: 8095\n",
      "Loss: 29.245314386244956\n",
      "l2 norm of gradients: 0.18676362782018047\n",
      "l2 norm of weights: 5.680559894150943\n",
      "---------------------\n",
      "Iteration Number: 8096\n",
      "Loss: 29.24356549470246\n",
      "l2 norm of gradients: 0.1867482405514033\n",
      "l2 norm of weights: 5.68049239000506\n",
      "---------------------\n",
      "Iteration Number: 8097\n",
      "Loss: 29.24181689117875\n",
      "l2 norm of gradients: 0.18673285625235422\n",
      "l2 norm of weights: 5.680424890931733\n",
      "---------------------\n",
      "Iteration Number: 8098\n",
      "Loss: 29.240068575591305\n",
      "l2 norm of gradients: 0.1867174749219118\n",
      "l2 norm of weights: 5.680357396930248\n",
      "---------------------\n",
      "Iteration Number: 8099\n",
      "Loss: 29.238320547861147\n",
      "l2 norm of gradients: 0.18670209655895542\n",
      "l2 norm of weights: 5.680289907999891\n",
      "---------------------\n",
      "Iteration Number: 8100\n",
      "Loss: 29.23657280790634\n",
      "l2 norm of gradients: 0.18668672116236504\n",
      "l2 norm of weights: 5.6802224241399495\n",
      "---------------------\n",
      "Iteration Number: 8101\n",
      "Loss: 29.23482535564832\n",
      "l2 norm of gradients: 0.1866713487310211\n",
      "l2 norm of weights: 5.6801549453497095\n",
      "---------------------\n",
      "Iteration Number: 8102\n",
      "Loss: 29.233078191009316\n",
      "l2 norm of gradients: 0.18665597926380484\n",
      "l2 norm of weights: 5.680087471628459\n",
      "---------------------\n",
      "Iteration Number: 8103\n",
      "Loss: 29.231331313907543\n",
      "l2 norm of gradients: 0.18664061275959823\n",
      "l2 norm of weights: 5.680020002975485\n",
      "---------------------\n",
      "Iteration Number: 8104\n",
      "Loss: 29.22958472426338\n",
      "l2 norm of gradients: 0.1866252492172836\n",
      "l2 norm of weights: 5.679952539390074\n",
      "---------------------\n",
      "Iteration Number: 8105\n",
      "Loss: 29.22783842199854\n",
      "l2 norm of gradients: 0.18660988863574407\n",
      "l2 norm of weights: 5.679885080871513\n",
      "---------------------\n",
      "Iteration Number: 8106\n",
      "Loss: 29.22609240703443\n",
      "l2 norm of gradients: 0.18659453101386342\n",
      "l2 norm of weights: 5.679817627419092\n",
      "---------------------\n",
      "Iteration Number: 8107\n",
      "Loss: 29.224346679289166\n",
      "l2 norm of gradients: 0.18657917635052598\n",
      "l2 norm of weights: 5.679750179032095\n",
      "---------------------\n",
      "Iteration Number: 8108\n",
      "Loss: 29.22260123868476\n",
      "l2 norm of gradients: 0.18656382464461693\n",
      "l2 norm of weights: 5.6796827357098145\n",
      "---------------------\n",
      "Iteration Number: 8109\n",
      "Loss: 29.220856085142998\n",
      "l2 norm of gradients: 0.18654847589502171\n",
      "l2 norm of weights: 5.6796152974515355\n",
      "---------------------\n",
      "Iteration Number: 8110\n",
      "Loss: 29.219111218581943\n",
      "l2 norm of gradients: 0.18653313010062675\n",
      "l2 norm of weights: 5.6795478642565485\n",
      "---------------------\n",
      "Iteration Number: 8111\n",
      "Loss: 29.217366638920474\n",
      "l2 norm of gradients: 0.1865177872603189\n",
      "l2 norm of weights: 5.679480436124141\n",
      "---------------------\n",
      "Iteration Number: 8112\n",
      "Loss: 29.215622346086285\n",
      "l2 norm of gradients: 0.18650244737298574\n",
      "l2 norm of weights: 5.679413013053601\n",
      "---------------------\n",
      "Iteration Number: 8113\n",
      "Loss: 29.21387833999609\n",
      "l2 norm of gradients: 0.18648711043751548\n",
      "l2 norm of weights: 5.679345595044219\n",
      "---------------------\n",
      "Iteration Number: 8114\n",
      "Loss: 29.2121346205697\n",
      "l2 norm of gradients: 0.1864717764527969\n",
      "l2 norm of weights: 5.679278182095283\n",
      "---------------------\n",
      "Iteration Number: 8115\n",
      "Loss: 29.2103911877302\n",
      "l2 norm of gradients: 0.18645644541771947\n",
      "l2 norm of weights: 5.679210774206083\n",
      "---------------------\n",
      "Iteration Number: 8116\n",
      "Loss: 29.20864804139366\n",
      "l2 norm of gradients: 0.18644111733117333\n",
      "l2 norm of weights: 5.679143371375907\n",
      "---------------------\n",
      "Iteration Number: 8117\n",
      "Loss: 29.206905181488658\n",
      "l2 norm of gradients: 0.18642579219204905\n",
      "l2 norm of weights: 5.679075973604046\n",
      "---------------------\n",
      "Iteration Number: 8118\n",
      "Loss: 29.205162607928433\n",
      "l2 norm of gradients: 0.1864104699992382\n",
      "l2 norm of weights: 5.67900858088979\n",
      "---------------------\n",
      "Iteration Number: 8119\n",
      "Loss: 29.203420320642444\n",
      "l2 norm of gradients: 0.1863951507516326\n",
      "l2 norm of weights: 5.6789411932324265\n",
      "---------------------\n",
      "Iteration Number: 8120\n",
      "Loss: 29.201678319544303\n",
      "l2 norm of gradients: 0.18637983444812492\n",
      "l2 norm of weights: 5.6788738106312495\n",
      "---------------------\n",
      "Iteration Number: 8121\n",
      "Loss: 29.199936604557028\n",
      "l2 norm of gradients: 0.18636452108760837\n",
      "l2 norm of weights: 5.678806433085547\n",
      "---------------------\n",
      "Iteration Number: 8122\n",
      "Loss: 29.19819517560125\n",
      "l2 norm of gradients: 0.18634921066897686\n",
      "l2 norm of weights: 5.678739060594609\n",
      "---------------------\n",
      "Iteration Number: 8123\n",
      "Loss: 29.196454032599533\n",
      "l2 norm of gradients: 0.18633390319112492\n",
      "l2 norm of weights: 5.678671693157728\n",
      "---------------------\n",
      "Iteration Number: 8124\n",
      "Loss: 29.194713175476156\n",
      "l2 norm of gradients: 0.18631859865294761\n",
      "l2 norm of weights: 5.678604330774193\n",
      "---------------------\n",
      "Iteration Number: 8125\n",
      "Loss: 29.192972604144266\n",
      "l2 norm of gradients: 0.18630329705334076\n",
      "l2 norm of weights: 5.678536973443296\n",
      "---------------------\n",
      "Iteration Number: 8126\n",
      "Loss: 29.19123231853165\n",
      "l2 norm of gradients: 0.18628799839120075\n",
      "l2 norm of weights: 5.678469621164328\n",
      "---------------------\n",
      "Iteration Number: 8127\n",
      "Loss: 29.18949231855877\n",
      "l2 norm of gradients: 0.18627270266542464\n",
      "l2 norm of weights: 5.67840227393658\n",
      "---------------------\n",
      "Iteration Number: 8128\n",
      "Loss: 29.187752604143736\n",
      "l2 norm of gradients: 0.18625740987491\n",
      "l2 norm of weights: 5.678334931759345\n",
      "---------------------\n",
      "Iteration Number: 8129\n",
      "Loss: 29.186013175209904\n",
      "l2 norm of gradients: 0.18624212001855517\n",
      "l2 norm of weights: 5.6782675946319126\n",
      "---------------------\n",
      "Iteration Number: 8130\n",
      "Loss: 29.184274031677997\n",
      "l2 norm of gradients: 0.18622683309525898\n",
      "l2 norm of weights: 5.678200262553576\n",
      "---------------------\n",
      "Iteration Number: 8131\n",
      "Loss: 29.182535173469812\n",
      "l2 norm of gradients: 0.18621154910392113\n",
      "l2 norm of weights: 5.678132935523627\n",
      "---------------------\n",
      "Iteration Number: 8132\n",
      "Loss: 29.180796600509446\n",
      "l2 norm of gradients: 0.1861962680434416\n",
      "l2 norm of weights: 5.678065613541358\n",
      "---------------------\n",
      "Iteration Number: 8133\n",
      "Loss: 29.179058312711764\n",
      "l2 norm of gradients: 0.18618098991272128\n",
      "l2 norm of weights: 5.67799829660606\n",
      "---------------------\n",
      "Iteration Number: 8134\n",
      "Loss: 29.177320310004752\n",
      "l2 norm of gradients: 0.18616571471066157\n",
      "l2 norm of weights: 5.67793098471703\n",
      "---------------------\n",
      "Iteration Number: 8135\n",
      "Loss: 29.17558259230725\n",
      "l2 norm of gradients: 0.1861504424361645\n",
      "l2 norm of weights: 5.677863677873556\n",
      "---------------------\n",
      "Iteration Number: 8136\n",
      "Loss: 29.17384515953968\n",
      "l2 norm of gradients: 0.18613517308813277\n",
      "l2 norm of weights: 5.677796376074932\n",
      "---------------------\n",
      "Iteration Number: 8137\n",
      "Loss: 29.17210801162683\n",
      "l2 norm of gradients: 0.18611990666546965\n",
      "l2 norm of weights: 5.677729079320452\n",
      "---------------------\n",
      "Iteration Number: 8138\n",
      "Loss: 29.17037114848691\n",
      "l2 norm of gradients: 0.18610464316707903\n",
      "l2 norm of weights: 5.677661787609409\n",
      "---------------------\n",
      "Iteration Number: 8139\n",
      "Loss: 29.168634570042794\n",
      "l2 norm of gradients: 0.18608938259186553\n",
      "l2 norm of weights: 5.677594500941096\n",
      "---------------------\n",
      "Iteration Number: 8140\n",
      "Loss: 29.166898276217335\n",
      "l2 norm of gradients: 0.18607412493873424\n",
      "l2 norm of weights: 5.6775272193148085\n",
      "---------------------\n",
      "Iteration Number: 8141\n",
      "Loss: 29.165162266930672\n",
      "l2 norm of gradients: 0.18605887020659098\n",
      "l2 norm of weights: 5.677459942729837\n",
      "---------------------\n",
      "Iteration Number: 8142\n",
      "Loss: 29.163426542105665\n",
      "l2 norm of gradients: 0.1860436183943422\n",
      "l2 norm of weights: 5.677392671185479\n",
      "---------------------\n",
      "Iteration Number: 8143\n",
      "Loss: 29.161691101663386\n",
      "l2 norm of gradients: 0.1860283695008949\n",
      "l2 norm of weights: 5.677325404681027\n",
      "---------------------\n",
      "Iteration Number: 8144\n",
      "Loss: 29.159955945525713\n",
      "l2 norm of gradients: 0.18601312352515675\n",
      "l2 norm of weights: 5.6772581432157745\n",
      "---------------------\n",
      "Iteration Number: 8145\n",
      "Loss: 29.15822107361459\n",
      "l2 norm of gradients: 0.18599788046603608\n",
      "l2 norm of weights: 5.677190886789017\n",
      "---------------------\n",
      "Iteration Number: 8146\n",
      "Loss: 29.156486485853193\n",
      "l2 norm of gradients: 0.18598264032244172\n",
      "l2 norm of weights: 5.67712363540005\n",
      "---------------------\n",
      "Iteration Number: 8147\n",
      "Loss: 29.15475218216427\n",
      "l2 norm of gradients: 0.18596740309328322\n",
      "l2 norm of weights: 5.677056389048167\n",
      "---------------------\n",
      "Iteration Number: 8148\n",
      "Loss: 29.1530181624621\n",
      "l2 norm of gradients: 0.1859521687774708\n",
      "l2 norm of weights: 5.676989147732663\n",
      "---------------------\n",
      "Iteration Number: 8149\n",
      "Loss: 29.15128442667899\n",
      "l2 norm of gradients: 0.18593693737391517\n",
      "l2 norm of weights: 5.676921911452833\n",
      "---------------------\n",
      "Iteration Number: 8150\n",
      "Loss: 29.149550974730186\n",
      "l2 norm of gradients: 0.1859217088815278\n",
      "l2 norm of weights: 5.676854680207975\n",
      "---------------------\n",
      "Iteration Number: 8151\n",
      "Loss: 29.147817806540928\n",
      "l2 norm of gradients: 0.1859064832992206\n",
      "l2 norm of weights: 5.676787453997381\n",
      "---------------------\n",
      "Iteration Number: 8152\n",
      "Loss: 29.146084922032546\n",
      "l2 norm of gradients: 0.18589126062590627\n",
      "l2 norm of weights: 5.67672023282035\n",
      "---------------------\n",
      "Iteration Number: 8153\n",
      "Loss: 29.144352321127904\n",
      "l2 norm of gradients: 0.18587604086049808\n",
      "l2 norm of weights: 5.676653016676175\n",
      "---------------------\n",
      "Iteration Number: 8154\n",
      "Loss: 29.142620003746334\n",
      "l2 norm of gradients: 0.1858608240019098\n",
      "l2 norm of weights: 5.676585805564153\n",
      "---------------------\n",
      "Iteration Number: 8155\n",
      "Loss: 29.14088796981195\n",
      "l2 norm of gradients: 0.18584561004905606\n",
      "l2 norm of weights: 5.676518599483582\n",
      "---------------------\n",
      "Iteration Number: 8156\n",
      "Loss: 29.13915621924863\n",
      "l2 norm of gradients: 0.18583039900085194\n",
      "l2 norm of weights: 5.676451398433756\n",
      "---------------------\n",
      "Iteration Number: 8157\n",
      "Loss: 29.137424751974642\n",
      "l2 norm of gradients: 0.18581519085621317\n",
      "l2 norm of weights: 5.676384202413972\n",
      "---------------------\n",
      "Iteration Number: 8158\n",
      "Loss: 29.135693567916228\n",
      "l2 norm of gradients: 0.18579998561405606\n",
      "l2 norm of weights: 5.676317011423528\n",
      "---------------------\n",
      "Iteration Number: 8159\n",
      "Loss: 29.13396266699514\n",
      "l2 norm of gradients: 0.18578478327329762\n",
      "l2 norm of weights: 5.676249825461721\n",
      "---------------------\n",
      "Iteration Number: 8160\n",
      "Loss: 29.13223204913071\n",
      "l2 norm of gradients: 0.18576958383285544\n",
      "l2 norm of weights: 5.676182644527846\n",
      "---------------------\n",
      "Iteration Number: 8161\n",
      "Loss: 29.13050171424819\n",
      "l2 norm of gradients: 0.18575438729164773\n",
      "l2 norm of weights: 5.6761154686212025\n",
      "---------------------\n",
      "Iteration Number: 8162\n",
      "Loss: 29.12877166227031\n",
      "l2 norm of gradients: 0.18573919364859334\n",
      "l2 norm of weights: 5.676048297741087\n",
      "---------------------\n",
      "Iteration Number: 8163\n",
      "Loss: 29.12704189311532\n",
      "l2 norm of gradients: 0.18572400290261157\n",
      "l2 norm of weights: 5.675981131886798\n",
      "---------------------\n",
      "Iteration Number: 8164\n",
      "Loss: 29.125312406710737\n",
      "l2 norm of gradients: 0.18570881505262274\n",
      "l2 norm of weights: 5.67591397105763\n",
      "---------------------\n",
      "Iteration Number: 8165\n",
      "Loss: 29.123583202977294\n",
      "l2 norm of gradients: 0.18569363009754725\n",
      "l2 norm of weights: 5.675846815252886\n",
      "---------------------\n",
      "Iteration Number: 8166\n",
      "Loss: 29.121854281837177\n",
      "l2 norm of gradients: 0.18567844803630654\n",
      "l2 norm of weights: 5.67577966447186\n",
      "---------------------\n",
      "Iteration Number: 8167\n",
      "Loss: 29.120125643212457\n",
      "l2 norm of gradients: 0.1856632688678225\n",
      "l2 norm of weights: 5.675712518713853\n",
      "---------------------\n",
      "Iteration Number: 8168\n",
      "Loss: 29.118397287028863\n",
      "l2 norm of gradients: 0.18564809259101758\n",
      "l2 norm of weights: 5.675645377978162\n",
      "---------------------\n",
      "Iteration Number: 8169\n",
      "Loss: 29.116669213204116\n",
      "l2 norm of gradients: 0.185632919204815\n",
      "l2 norm of weights: 5.675578242264086\n",
      "---------------------\n",
      "Iteration Number: 8170\n",
      "Loss: 29.11494142166603\n",
      "l2 norm of gradients: 0.18561774870813855\n",
      "l2 norm of weights: 5.675511111570924\n",
      "---------------------\n",
      "Iteration Number: 8171\n",
      "Loss: 29.11321391233205\n",
      "l2 norm of gradients: 0.18560258109991243\n",
      "l2 norm of weights: 5.675443985897975\n",
      "---------------------\n",
      "Iteration Number: 8172\n",
      "Loss: 29.11148668513106\n",
      "l2 norm of gradients: 0.18558741637906176\n",
      "l2 norm of weights: 5.675376865244539\n",
      "---------------------\n",
      "Iteration Number: 8173\n",
      "Loss: 29.109759739979964\n",
      "l2 norm of gradients: 0.1855722545445121\n",
      "l2 norm of weights: 5.675309749609913\n",
      "---------------------\n",
      "Iteration Number: 8174\n",
      "Loss: 29.10803307680469\n",
      "l2 norm of gradients: 0.18555709559518965\n",
      "l2 norm of weights: 5.6752426389933985\n",
      "---------------------\n",
      "Iteration Number: 8175\n",
      "Loss: 29.10630669552797\n",
      "l2 norm of gradients: 0.18554193953002118\n",
      "l2 norm of weights: 5.675175533394295\n",
      "---------------------\n",
      "Iteration Number: 8176\n",
      "Loss: 29.104580596072463\n",
      "l2 norm of gradients: 0.18552678634793415\n",
      "l2 norm of weights: 5.675108432811902\n",
      "---------------------\n",
      "Iteration Number: 8177\n",
      "Loss: 29.102854778361557\n",
      "l2 norm of gradients: 0.18551163604785667\n",
      "l2 norm of weights: 5.675041337245519\n",
      "---------------------\n",
      "Iteration Number: 8178\n",
      "Loss: 29.101129242316354\n",
      "l2 norm of gradients: 0.1854964886287173\n",
      "l2 norm of weights: 5.674974246694447\n",
      "---------------------\n",
      "Iteration Number: 8179\n",
      "Loss: 29.09940398786256\n",
      "l2 norm of gradients: 0.1854813440894454\n",
      "l2 norm of weights: 5.674907161157986\n",
      "---------------------\n",
      "Iteration Number: 8180\n",
      "Loss: 29.09767901491975\n",
      "l2 norm of gradients: 0.18546620242897074\n",
      "l2 norm of weights: 5.674840080635437\n",
      "---------------------\n",
      "Iteration Number: 8181\n",
      "Loss: 29.09595432341443\n",
      "l2 norm of gradients: 0.18545106364622388\n",
      "l2 norm of weights: 5.6747730051261005\n",
      "---------------------\n",
      "Iteration Number: 8182\n",
      "Loss: 29.09422991326697\n",
      "l2 norm of gradients: 0.18543592774013595\n",
      "l2 norm of weights: 5.6747059346292765\n",
      "---------------------\n",
      "Iteration Number: 8183\n",
      "Loss: 29.09250578440245\n",
      "l2 norm of gradients: 0.18542079470963854\n",
      "l2 norm of weights: 5.674638869144268\n",
      "---------------------\n",
      "Iteration Number: 8184\n",
      "Loss: 29.09078193674636\n",
      "l2 norm of gradients: 0.18540566455366414\n",
      "l2 norm of weights: 5.6745718086703745\n",
      "---------------------\n",
      "Iteration Number: 8185\n",
      "Loss: 29.089058370215263\n",
      "l2 norm of gradients: 0.18539053727114554\n",
      "l2 norm of weights: 5.6745047532068975\n",
      "---------------------\n",
      "Iteration Number: 8186\n",
      "Loss: 29.087335084737123\n",
      "l2 norm of gradients: 0.18537541286101633\n",
      "l2 norm of weights: 5.67443770275314\n",
      "---------------------\n",
      "Iteration Number: 8187\n",
      "Loss: 29.085612080234128\n",
      "l2 norm of gradients: 0.18536029132221063\n",
      "l2 norm of weights: 5.674370657308402\n",
      "---------------------\n",
      "Iteration Number: 8188\n",
      "Loss: 29.083889356631317\n",
      "l2 norm of gradients: 0.18534517265366326\n",
      "l2 norm of weights: 5.6743036168719865\n",
      "---------------------\n",
      "Iteration Number: 8189\n",
      "Loss: 29.082166913850298\n",
      "l2 norm of gradients: 0.18533005685430956\n",
      "l2 norm of weights: 5.674236581443195\n",
      "---------------------\n",
      "Iteration Number: 8190\n",
      "Loss: 29.080444751810337\n",
      "l2 norm of gradients: 0.18531494392308548\n",
      "l2 norm of weights: 5.674169551021332\n",
      "---------------------\n",
      "Iteration Number: 8191\n",
      "Loss: 29.078722870443674\n",
      "l2 norm of gradients: 0.18529983385892762\n",
      "l2 norm of weights: 5.674102525605695\n",
      "---------------------\n",
      "Iteration Number: 8192\n",
      "Loss: 29.077001269667853\n",
      "l2 norm of gradients: 0.18528472666077314\n",
      "l2 norm of weights: 5.674035505195591\n",
      "---------------------\n",
      "Iteration Number: 8193\n",
      "Loss: 29.07527994940581\n",
      "l2 norm of gradients: 0.1852696223275599\n",
      "l2 norm of weights: 5.673968489790322\n",
      "---------------------\n",
      "Iteration Number: 8194\n",
      "Loss: 29.073558909584968\n",
      "l2 norm of gradients: 0.1852545208582262\n",
      "l2 norm of weights: 5.6739014793891895\n",
      "---------------------\n",
      "Iteration Number: 8195\n",
      "Loss: 29.071838150124236\n",
      "l2 norm of gradients: 0.18523942225171122\n",
      "l2 norm of weights: 5.673834473991497\n",
      "---------------------\n",
      "Iteration Number: 8196\n",
      "Loss: 29.07011767094958\n",
      "l2 norm of gradients: 0.18522432650695442\n",
      "l2 norm of weights: 5.673767473596548\n",
      "---------------------\n",
      "Iteration Number: 8197\n",
      "Loss: 29.06839747198888\n",
      "l2 norm of gradients: 0.185209233622896\n",
      "l2 norm of weights: 5.673700478203647\n",
      "---------------------\n",
      "Iteration Number: 8198\n",
      "Loss: 29.066677553157344\n",
      "l2 norm of gradients: 0.185194143598477\n",
      "l2 norm of weights: 5.673633487812096\n",
      "---------------------\n",
      "Iteration Number: 8199\n",
      "Loss: 29.064957914384166\n",
      "l2 norm of gradients: 0.18517905643263863\n",
      "l2 norm of weights: 5.6735665024212\n",
      "---------------------\n",
      "Iteration Number: 8200\n",
      "Loss: 29.063238555591482\n",
      "l2 norm of gradients: 0.185163972124323\n",
      "l2 norm of weights: 5.673499522030263\n",
      "---------------------\n",
      "Iteration Number: 8201\n",
      "Loss: 29.06151947670265\n",
      "l2 norm of gradients: 0.18514889067247278\n",
      "l2 norm of weights: 5.673432546638587\n",
      "---------------------\n",
      "Iteration Number: 8202\n",
      "Loss: 29.059800677642336\n",
      "l2 norm of gradients: 0.1851338120760312\n",
      "l2 norm of weights: 5.673365576245479\n",
      "---------------------\n",
      "Iteration Number: 8203\n",
      "Loss: 29.058082158334827\n",
      "l2 norm of gradients: 0.1851187363339421\n",
      "l2 norm of weights: 5.673298610850243\n",
      "---------------------\n",
      "Iteration Number: 8204\n",
      "Loss: 29.056363918702584\n",
      "l2 norm of gradients: 0.18510366344514995\n",
      "l2 norm of weights: 5.673231650452182\n",
      "---------------------\n",
      "Iteration Number: 8205\n",
      "Loss: 29.054645958669777\n",
      "l2 norm of gradients: 0.1850885934085998\n",
      "l2 norm of weights: 5.673164695050602\n",
      "---------------------\n",
      "Iteration Number: 8206\n",
      "Loss: 29.05292827815897\n",
      "l2 norm of gradients: 0.1850735262232373\n",
      "l2 norm of weights: 5.673097744644807\n",
      "---------------------\n",
      "Iteration Number: 8207\n",
      "Loss: 29.0512108770985\n",
      "l2 norm of gradients: 0.1850584618880087\n",
      "l2 norm of weights: 5.6730307992341045\n",
      "---------------------\n",
      "Iteration Number: 8208\n",
      "Loss: 29.049493755408044\n",
      "l2 norm of gradients: 0.18504340040186093\n",
      "l2 norm of weights: 5.672963858817797\n",
      "---------------------\n",
      "Iteration Number: 8209\n",
      "Loss: 29.047776913012\n",
      "l2 norm of gradients: 0.1850283417637414\n",
      "l2 norm of weights: 5.672896923395191\n",
      "---------------------\n",
      "Iteration Number: 8210\n",
      "Loss: 29.046060349838044\n",
      "l2 norm of gradients: 0.18501328597259817\n",
      "l2 norm of weights: 5.6728299929655925\n",
      "---------------------\n",
      "Iteration Number: 8211\n",
      "Loss: 29.044344065805912\n",
      "l2 norm of gradients: 0.18499823302737992\n",
      "l2 norm of weights: 5.672763067528307\n",
      "---------------------\n",
      "Iteration Number: 8212\n",
      "Loss: 29.042628060840702\n",
      "l2 norm of gradients: 0.18498318292703594\n",
      "l2 norm of weights: 5.672696147082642\n",
      "---------------------\n",
      "Iteration Number: 8213\n",
      "Loss: 29.040912334867034\n",
      "l2 norm of gradients: 0.1849681356705161\n",
      "l2 norm of weights: 5.6726292316279014\n",
      "---------------------\n",
      "Iteration Number: 8214\n",
      "Loss: 29.03919688781054\n",
      "l2 norm of gradients: 0.1849530912567708\n",
      "l2 norm of weights: 5.672562321163392\n",
      "---------------------\n",
      "Iteration Number: 8215\n",
      "Loss: 29.037481719593217\n",
      "l2 norm of gradients: 0.18493804968475125\n",
      "l2 norm of weights: 5.672495415688421\n",
      "---------------------\n",
      "Iteration Number: 8216\n",
      "Loss: 29.03576683013995\n",
      "l2 norm of gradients: 0.18492301095340902\n",
      "l2 norm of weights: 5.672428515202294\n",
      "---------------------\n",
      "Iteration Number: 8217\n",
      "Loss: 29.03405221937345\n",
      "l2 norm of gradients: 0.18490797506169634\n",
      "l2 norm of weights: 5.672361619704319\n",
      "---------------------\n",
      "Iteration Number: 8218\n",
      "Loss: 29.032337887224017\n",
      "l2 norm of gradients: 0.18489294200856624\n",
      "l2 norm of weights: 5.672294729193804\n",
      "---------------------\n",
      "Iteration Number: 8219\n",
      "Loss: 29.03062383360865\n",
      "l2 norm of gradients: 0.184877911792972\n",
      "l2 norm of weights: 5.672227843670053\n",
      "---------------------\n",
      "Iteration Number: 8220\n",
      "Loss: 29.02891005845523\n",
      "l2 norm of gradients: 0.18486288441386783\n",
      "l2 norm of weights: 5.672160963132376\n",
      "---------------------\n",
      "Iteration Number: 8221\n",
      "Loss: 29.027196561687255\n",
      "l2 norm of gradients: 0.18484785987020833\n",
      "l2 norm of weights: 5.67209408758008\n",
      "---------------------\n",
      "Iteration Number: 8222\n",
      "Loss: 29.02548334323024\n",
      "l2 norm of gradients: 0.18483283816094878\n",
      "l2 norm of weights: 5.672027217012471\n",
      "---------------------\n",
      "Iteration Number: 8223\n",
      "Loss: 29.02377040300886\n",
      "l2 norm of gradients: 0.18481781928504504\n",
      "l2 norm of weights: 5.67196035142886\n",
      "---------------------\n",
      "Iteration Number: 8224\n",
      "Loss: 29.02205774094244\n",
      "l2 norm of gradients: 0.18480280324145354\n",
      "l2 norm of weights: 5.671893490828551\n",
      "---------------------\n",
      "Iteration Number: 8225\n",
      "Loss: 29.020345356963993\n",
      "l2 norm of gradients: 0.1847877900291314\n",
      "l2 norm of weights: 5.671826635210855\n",
      "---------------------\n",
      "Iteration Number: 8226\n",
      "Loss: 29.018633250991055\n",
      "l2 norm of gradients: 0.18477277964703617\n",
      "l2 norm of weights: 5.67175978457508\n",
      "---------------------\n",
      "Iteration Number: 8227\n",
      "Loss: 29.016921422953192\n",
      "l2 norm of gradients: 0.1847577720941262\n",
      "l2 norm of weights: 5.6716929389205335\n",
      "---------------------\n",
      "Iteration Number: 8228\n",
      "Loss: 29.015209872770182\n",
      "l2 norm of gradients: 0.18474276736936024\n",
      "l2 norm of weights: 5.671626098246525\n",
      "---------------------\n",
      "Iteration Number: 8229\n",
      "Loss: 29.013498600370784\n",
      "l2 norm of gradients: 0.18472776547169778\n",
      "l2 norm of weights: 5.671559262552364\n",
      "---------------------\n",
      "Iteration Number: 8230\n",
      "Loss: 29.011787605678037\n",
      "l2 norm of gradients: 0.18471276640009887\n",
      "l2 norm of weights: 5.671492431837358\n",
      "---------------------\n",
      "Iteration Number: 8231\n",
      "Loss: 29.01007688861639\n",
      "l2 norm of gradients: 0.1846977701535241\n",
      "l2 norm of weights: 5.671425606100817\n",
      "---------------------\n",
      "Iteration Number: 8232\n",
      "Loss: 29.008366449110138\n",
      "l2 norm of gradients: 0.1846827767309347\n",
      "l2 norm of weights: 5.67135878534205\n",
      "---------------------\n",
      "Iteration Number: 8233\n",
      "Loss: 29.006656287085498\n",
      "l2 norm of gradients: 0.1846677861312925\n",
      "l2 norm of weights: 5.671291969560366\n",
      "---------------------\n",
      "Iteration Number: 8234\n",
      "Loss: 29.004946402464828\n",
      "l2 norm of gradients: 0.18465279835355994\n",
      "l2 norm of weights: 5.671225158755076\n",
      "---------------------\n",
      "Iteration Number: 8235\n",
      "Loss: 29.003236795176246\n",
      "l2 norm of gradients: 0.18463781339669996\n",
      "l2 norm of weights: 5.671158352925491\n",
      "---------------------\n",
      "Iteration Number: 8236\n",
      "Loss: 29.00152746514338\n",
      "l2 norm of gradients: 0.18462283125967618\n",
      "l2 norm of weights: 5.6710915520709175\n",
      "---------------------\n",
      "Iteration Number: 8237\n",
      "Loss: 28.999818412287354\n",
      "l2 norm of gradients: 0.1846078519414528\n",
      "l2 norm of weights: 5.6710247561906675\n",
      "---------------------\n",
      "Iteration Number: 8238\n",
      "Loss: 28.998109636541187\n",
      "l2 norm of gradients: 0.18459287544099462\n",
      "l2 norm of weights: 5.670957965284051\n",
      "---------------------\n",
      "Iteration Number: 8239\n",
      "Loss: 28.996401137822353\n",
      "l2 norm of gradients: 0.18457790175726696\n",
      "l2 norm of weights: 5.67089117935038\n",
      "---------------------\n",
      "Iteration Number: 8240\n",
      "Loss: 28.994692916058565\n",
      "l2 norm of gradients: 0.18456293088923587\n",
      "l2 norm of weights: 5.670824398388963\n",
      "---------------------\n",
      "Iteration Number: 8241\n",
      "Loss: 28.992984971175737\n",
      "l2 norm of gradients: 0.18454796283586786\n",
      "l2 norm of weights: 5.670757622399112\n",
      "---------------------\n",
      "Iteration Number: 8242\n",
      "Loss: 28.991277303097924\n",
      "l2 norm of gradients: 0.18453299759613007\n",
      "l2 norm of weights: 5.670690851380138\n",
      "---------------------\n",
      "Iteration Number: 8243\n",
      "Loss: 28.98956991175136\n",
      "l2 norm of gradients: 0.18451803516899024\n",
      "l2 norm of weights: 5.670624085331352\n",
      "---------------------\n",
      "Iteration Number: 8244\n",
      "Loss: 28.987862797058426\n",
      "l2 norm of gradients: 0.18450307555341675\n",
      "l2 norm of weights: 5.670557324252066\n",
      "---------------------\n",
      "Iteration Number: 8245\n",
      "Loss: 28.986155958945442\n",
      "l2 norm of gradients: 0.1844881187483785\n",
      "l2 norm of weights: 5.670490568141591\n",
      "---------------------\n",
      "Iteration Number: 8246\n",
      "Loss: 28.98444939734131\n",
      "l2 norm of gradients: 0.18447316475284498\n",
      "l2 norm of weights: 5.670423816999238\n",
      "---------------------\n",
      "Iteration Number: 8247\n",
      "Loss: 28.982743112166105\n",
      "l2 norm of gradients: 0.18445821356578632\n",
      "l2 norm of weights: 5.67035707082432\n",
      "---------------------\n",
      "Iteration Number: 8248\n",
      "Loss: 28.981037103345663\n",
      "l2 norm of gradients: 0.18444326518617318\n",
      "l2 norm of weights: 5.670290329616147\n",
      "---------------------\n",
      "Iteration Number: 8249\n",
      "Loss: 28.979331370808207\n",
      "l2 norm of gradients: 0.1844283196129769\n",
      "l2 norm of weights: 5.670223593374034\n",
      "---------------------\n",
      "Iteration Number: 8250\n",
      "Loss: 28.97762591447812\n",
      "l2 norm of gradients: 0.18441337684516934\n",
      "l2 norm of weights: 5.670156862097292\n",
      "---------------------\n",
      "Iteration Number: 8251\n",
      "Loss: 28.975920734278905\n",
      "l2 norm of gradients: 0.18439843688172283\n",
      "l2 norm of weights: 5.670090135785233\n",
      "---------------------\n",
      "Iteration Number: 8252\n",
      "Loss: 28.974215830137304\n",
      "l2 norm of gradients: 0.1843834997216106\n",
      "l2 norm of weights: 5.670023414437171\n",
      "---------------------\n",
      "Iteration Number: 8253\n",
      "Loss: 28.972511201979234\n",
      "l2 norm of gradients: 0.18436856536380625\n",
      "l2 norm of weights: 5.669956698052418\n",
      "---------------------\n",
      "Iteration Number: 8254\n",
      "Loss: 28.97080684972974\n",
      "l2 norm of gradients: 0.18435363380728387\n",
      "l2 norm of weights: 5.669889986630286\n",
      "---------------------\n",
      "Iteration Number: 8255\n",
      "Loss: 28.96910277331287\n",
      "l2 norm of gradients: 0.18433870505101846\n",
      "l2 norm of weights: 5.66982328017009\n",
      "---------------------\n",
      "Iteration Number: 8256\n",
      "Loss: 28.96739897265307\n",
      "l2 norm of gradients: 0.18432377909398523\n",
      "l2 norm of weights: 5.669756578671143\n",
      "---------------------\n",
      "Iteration Number: 8257\n",
      "Loss: 28.965695447682997\n",
      "l2 norm of gradients: 0.1843088559351603\n",
      "l2 norm of weights: 5.669689882132757\n",
      "---------------------\n",
      "Iteration Number: 8258\n",
      "Loss: 28.96399219831864\n",
      "l2 norm of gradients: 0.1842939355735202\n",
      "l2 norm of weights: 5.6696231905542485\n",
      "---------------------\n",
      "Iteration Number: 8259\n",
      "Loss: 28.962289224493645\n",
      "l2 norm of gradients: 0.18427901800804206\n",
      "l2 norm of weights: 5.6695565039349285\n",
      "---------------------\n",
      "Iteration Number: 8260\n",
      "Loss: 28.96058652612913\n",
      "l2 norm of gradients: 0.18426410323770367\n",
      "l2 norm of weights: 5.6694898222741115\n",
      "---------------------\n",
      "Iteration Number: 8261\n",
      "Loss: 28.95888410315176\n",
      "l2 norm of gradients: 0.1842491912614833\n",
      "l2 norm of weights: 5.669423145571113\n",
      "---------------------\n",
      "Iteration Number: 8262\n",
      "Loss: 28.957181955487673\n",
      "l2 norm of gradients: 0.18423428207835993\n",
      "l2 norm of weights: 5.669356473825246\n",
      "---------------------\n",
      "Iteration Number: 8263\n",
      "Loss: 28.955480083061556\n",
      "l2 norm of gradients: 0.18421937568731295\n",
      "l2 norm of weights: 5.669289807035827\n",
      "---------------------\n",
      "Iteration Number: 8264\n",
      "Loss: 28.953778485800342\n",
      "l2 norm of gradients: 0.18420447208732257\n",
      "l2 norm of weights: 5.669223145202168\n",
      "---------------------\n",
      "Iteration Number: 8265\n",
      "Loss: 28.952077163626903\n",
      "l2 norm of gradients: 0.18418957127736937\n",
      "l2 norm of weights: 5.669156488323586\n",
      "---------------------\n",
      "Iteration Number: 8266\n",
      "Loss: 28.95037611647137\n",
      "l2 norm of gradients: 0.18417467325643466\n",
      "l2 norm of weights: 5.669089836399395\n",
      "---------------------\n",
      "Iteration Number: 8267\n",
      "Loss: 28.94867534425756\n",
      "l2 norm of gradients: 0.18415977802350017\n",
      "l2 norm of weights: 5.66902318942891\n",
      "---------------------\n",
      "Iteration Number: 8268\n",
      "Loss: 28.9469748469126\n",
      "l2 norm of gradients: 0.1841448855775485\n",
      "l2 norm of weights: 5.668956547411446\n",
      "---------------------\n",
      "Iteration Number: 8269\n",
      "Loss: 28.94527462435965\n",
      "l2 norm of gradients: 0.1841299959175624\n",
      "l2 norm of weights: 5.66888991034632\n",
      "---------------------\n",
      "Iteration Number: 8270\n",
      "Loss: 28.94357467652712\n",
      "l2 norm of gradients: 0.18411510904252565\n",
      "l2 norm of weights: 5.668823278232847\n",
      "---------------------\n",
      "Iteration Number: 8271\n",
      "Loss: 28.941875003340808\n",
      "l2 norm of gradients: 0.18410022495142236\n",
      "l2 norm of weights: 5.668756651070342\n",
      "---------------------\n",
      "Iteration Number: 8272\n",
      "Loss: 28.94017560472411\n",
      "l2 norm of gradients: 0.18408534364323725\n",
      "l2 norm of weights: 5.668690028858122\n",
      "---------------------\n",
      "Iteration Number: 8273\n",
      "Loss: 28.93847648060531\n",
      "l2 norm of gradients: 0.18407046511695563\n",
      "l2 norm of weights: 5.668623411595503\n",
      "---------------------\n",
      "Iteration Number: 8274\n",
      "Loss: 28.93677763090981\n",
      "l2 norm of gradients: 0.1840555893715635\n",
      "l2 norm of weights: 5.6685567992818\n",
      "---------------------\n",
      "Iteration Number: 8275\n",
      "Loss: 28.935079055566554\n",
      "l2 norm of gradients: 0.18404071640604727\n",
      "l2 norm of weights: 5.668490191916331\n",
      "---------------------\n",
      "Iteration Number: 8276\n",
      "Loss: 28.933380754497453\n",
      "l2 norm of gradients: 0.18402584621939402\n",
      "l2 norm of weights: 5.6684235894984125\n",
      "---------------------\n",
      "Iteration Number: 8277\n",
      "Loss: 28.93168272763043\n",
      "l2 norm of gradients: 0.1840109788105914\n",
      "l2 norm of weights: 5.668356992027361\n",
      "---------------------\n",
      "Iteration Number: 8278\n",
      "Loss: 28.92998497489039\n",
      "l2 norm of gradients: 0.18399611417862766\n",
      "l2 norm of weights: 5.668290399502493\n",
      "---------------------\n",
      "Iteration Number: 8279\n",
      "Loss: 28.928287496205357\n",
      "l2 norm of gradients: 0.18398125232249163\n",
      "l2 norm of weights: 5.668223811923126\n",
      "---------------------\n",
      "Iteration Number: 8280\n",
      "Loss: 28.926590291502876\n",
      "l2 norm of gradients: 0.18396639324117264\n",
      "l2 norm of weights: 5.668157229288577\n",
      "---------------------\n",
      "Iteration Number: 8281\n",
      "Loss: 28.92489336070503\n",
      "l2 norm of gradients: 0.1839515369336607\n",
      "l2 norm of weights: 5.668090651598166\n",
      "---------------------\n",
      "Iteration Number: 8282\n",
      "Loss: 28.92319670374203\n",
      "l2 norm of gradients: 0.18393668339894634\n",
      "l2 norm of weights: 5.668024078851207\n",
      "---------------------\n",
      "Iteration Number: 8283\n",
      "Loss: 28.921500320538108\n",
      "l2 norm of gradients: 0.18392183263602072\n",
      "l2 norm of weights: 5.667957511047019\n",
      "---------------------\n",
      "Iteration Number: 8284\n",
      "Loss: 28.91980421101968\n",
      "l2 norm of gradients: 0.18390698464387548\n",
      "l2 norm of weights: 5.667890948184921\n",
      "---------------------\n",
      "Iteration Number: 8285\n",
      "Loss: 28.91810837511374\n",
      "l2 norm of gradients: 0.18389213942150295\n",
      "l2 norm of weights: 5.66782439026423\n",
      "---------------------\n",
      "Iteration Number: 8286\n",
      "Loss: 28.91641281274593\n",
      "l2 norm of gradients: 0.18387729696789598\n",
      "l2 norm of weights: 5.667757837284265\n",
      "---------------------\n",
      "Iteration Number: 8287\n",
      "Loss: 28.914717523843148\n",
      "l2 norm of gradients: 0.18386245728204798\n",
      "l2 norm of weights: 5.667691289244344\n",
      "---------------------\n",
      "Iteration Number: 8288\n",
      "Loss: 28.913022508333764\n",
      "l2 norm of gradients: 0.18384762036295302\n",
      "l2 norm of weights: 5.667624746143784\n",
      "---------------------\n",
      "Iteration Number: 8289\n",
      "Loss: 28.91132776614272\n",
      "l2 norm of gradients: 0.18383278620960564\n",
      "l2 norm of weights: 5.667558207981908\n",
      "---------------------\n",
      "Iteration Number: 8290\n",
      "Loss: 28.90963329719651\n",
      "l2 norm of gradients: 0.18381795482100105\n",
      "l2 norm of weights: 5.667491674758031\n",
      "---------------------\n",
      "Iteration Number: 8291\n",
      "Loss: 28.907939101420645\n",
      "l2 norm of gradients: 0.1838031261961349\n",
      "l2 norm of weights: 5.667425146471473\n",
      "---------------------\n",
      "Iteration Number: 8292\n",
      "Loss: 28.9062451787431\n",
      "l2 norm of gradients: 0.18378830033400367\n",
      "l2 norm of weights: 5.667358623121555\n",
      "---------------------\n",
      "Iteration Number: 8293\n",
      "Loss: 28.904551529090437\n",
      "l2 norm of gradients: 0.1837734772336041\n",
      "l2 norm of weights: 5.667292104707594\n",
      "---------------------\n",
      "Iteration Number: 8294\n",
      "Loss: 28.90285815239345\n",
      "l2 norm of gradients: 0.1837586568939337\n",
      "l2 norm of weights: 5.667225591228912\n",
      "---------------------\n",
      "Iteration Number: 8295\n",
      "Loss: 28.901165048568753\n",
      "l2 norm of gradients: 0.18374383931399052\n",
      "l2 norm of weights: 5.667159082684827\n",
      "---------------------\n",
      "Iteration Number: 8296\n",
      "Loss: 28.899472217552294\n",
      "l2 norm of gradients: 0.18372902449277323\n",
      "l2 norm of weights: 5.66709257907466\n",
      "---------------------\n",
      "Iteration Number: 8297\n",
      "Loss: 28.89777965926541\n",
      "l2 norm of gradients: 0.18371421242928093\n",
      "l2 norm of weights: 5.667026080397729\n",
      "---------------------\n",
      "Iteration Number: 8298\n",
      "Loss: 28.89608737363859\n",
      "l2 norm of gradients: 0.18369940312251343\n",
      "l2 norm of weights: 5.666959586653357\n",
      "---------------------\n",
      "Iteration Number: 8299\n",
      "Loss: 28.894395360597088\n",
      "l2 norm of gradients: 0.18368459657147107\n",
      "l2 norm of weights: 5.666893097840863\n",
      "---------------------\n",
      "Iteration Number: 8300\n",
      "Loss: 28.89270362006541\n",
      "l2 norm of gradients: 0.18366979277515477\n",
      "l2 norm of weights: 5.666826613959568\n",
      "---------------------\n",
      "Iteration Number: 8301\n",
      "Loss: 28.891012151974554\n",
      "l2 norm of gradients: 0.183654991732566\n",
      "l2 norm of weights: 5.666760135008792\n",
      "---------------------\n",
      "Iteration Number: 8302\n",
      "Loss: 28.889320956250334\n",
      "l2 norm of gradients: 0.18364019344270685\n",
      "l2 norm of weights: 5.666693660987856\n",
      "---------------------\n",
      "Iteration Number: 8303\n",
      "Loss: 28.887630032819\n",
      "l2 norm of gradients: 0.18362539790457985\n",
      "l2 norm of weights: 5.666627191896081\n",
      "---------------------\n",
      "Iteration Number: 8304\n",
      "Loss: 28.885939381607756\n",
      "l2 norm of gradients: 0.1836106051171883\n",
      "l2 norm of weights: 5.666560727732791\n",
      "---------------------\n",
      "Iteration Number: 8305\n",
      "Loss: 28.884249002542767\n",
      "l2 norm of gradients: 0.18359581507953596\n",
      "l2 norm of weights: 5.666494268497303\n",
      "---------------------\n",
      "Iteration Number: 8306\n",
      "Loss: 28.8825588955534\n",
      "l2 norm of gradients: 0.1835810277906271\n",
      "l2 norm of weights: 5.666427814188941\n",
      "---------------------\n",
      "Iteration Number: 8307\n",
      "Loss: 28.880869060566\n",
      "l2 norm of gradients: 0.18356624324946674\n",
      "l2 norm of weights: 5.666361364807026\n",
      "---------------------\n",
      "Iteration Number: 8308\n",
      "Loss: 28.87917949750446\n",
      "l2 norm of gradients: 0.1835514614550603\n",
      "l2 norm of weights: 5.666294920350881\n",
      "---------------------\n",
      "Iteration Number: 8309\n",
      "Loss: 28.877490206299264\n",
      "l2 norm of gradients: 0.18353668240641383\n",
      "l2 norm of weights: 5.666228480819827\n",
      "---------------------\n",
      "Iteration Number: 8310\n",
      "Loss: 28.875801186877464\n",
      "l2 norm of gradients: 0.183521906102534\n",
      "l2 norm of weights: 5.666162046213185\n",
      "---------------------\n",
      "Iteration Number: 8311\n",
      "Loss: 28.874112439164033\n",
      "l2 norm of gradients: 0.18350713254242795\n",
      "l2 norm of weights: 5.66609561653028\n",
      "---------------------\n",
      "Iteration Number: 8312\n",
      "Loss: 28.872423963087588\n",
      "l2 norm of gradients: 0.18349236172510347\n",
      "l2 norm of weights: 5.666029191770433\n",
      "---------------------\n",
      "Iteration Number: 8313\n",
      "Loss: 28.870735758577283\n",
      "l2 norm of gradients: 0.18347759364956892\n",
      "l2 norm of weights: 5.665962771932965\n",
      "---------------------\n",
      "Iteration Number: 8314\n",
      "Loss: 28.869047825557764\n",
      "l2 norm of gradients: 0.18346282831483315\n",
      "l2 norm of weights: 5.6658963570172025\n",
      "---------------------\n",
      "Iteration Number: 8315\n",
      "Loss: 28.86736016395737\n",
      "l2 norm of gradients: 0.18344806571990566\n",
      "l2 norm of weights: 5.665829947022466\n",
      "---------------------\n",
      "Iteration Number: 8316\n",
      "Loss: 28.865672773703572\n",
      "l2 norm of gradients: 0.18343330586379653\n",
      "l2 norm of weights: 5.665763541948081\n",
      "---------------------\n",
      "Iteration Number: 8317\n",
      "Loss: 28.863985654722992\n",
      "l2 norm of gradients: 0.18341854874551627\n",
      "l2 norm of weights: 5.665697141793368\n",
      "---------------------\n",
      "Iteration Number: 8318\n",
      "Loss: 28.86229880694373\n",
      "l2 norm of gradients: 0.18340379436407608\n",
      "l2 norm of weights: 5.665630746557652\n",
      "---------------------\n",
      "Iteration Number: 8319\n",
      "Loss: 28.860612230294638\n",
      "l2 norm of gradients: 0.18338904271848783\n",
      "l2 norm of weights: 5.665564356240255\n",
      "---------------------\n",
      "Iteration Number: 8320\n",
      "Loss: 28.858925924699292\n",
      "l2 norm of gradients: 0.18337429380776366\n",
      "l2 norm of weights: 5.665497970840503\n",
      "---------------------\n",
      "Iteration Number: 8321\n",
      "Loss: 28.857239890089737\n",
      "l2 norm of gradients: 0.1833595476309165\n",
      "l2 norm of weights: 5.66543159035772\n",
      "---------------------\n",
      "Iteration Number: 8322\n",
      "Loss: 28.855554126390963\n",
      "l2 norm of gradients: 0.18334480418695978\n",
      "l2 norm of weights: 5.665365214791228\n",
      "---------------------\n",
      "Iteration Number: 8323\n",
      "Loss: 28.853868633531455\n",
      "l2 norm of gradients: 0.1833300634749076\n",
      "l2 norm of weights: 5.665298844140353\n",
      "---------------------\n",
      "Iteration Number: 8324\n",
      "Loss: 28.8521834114375\n",
      "l2 norm of gradients: 0.18331532549377438\n",
      "l2 norm of weights: 5.665232478404419\n",
      "---------------------\n",
      "Iteration Number: 8325\n",
      "Loss: 28.850498460036277\n",
      "l2 norm of gradients: 0.1833005902425754\n",
      "l2 norm of weights: 5.66516611758275\n",
      "---------------------\n",
      "Iteration Number: 8326\n",
      "Loss: 28.84881377925865\n",
      "l2 norm of gradients: 0.1832858577203262\n",
      "l2 norm of weights: 5.665099761674671\n",
      "---------------------\n",
      "Iteration Number: 8327\n",
      "Loss: 28.847129369032203\n",
      "l2 norm of gradients: 0.18327112792604322\n",
      "l2 norm of weights: 5.665033410679508\n",
      "---------------------\n",
      "Iteration Number: 8328\n",
      "Loss: 28.845445229278447\n",
      "l2 norm of gradients: 0.18325640085874323\n",
      "l2 norm of weights: 5.664967064596585\n",
      "---------------------\n",
      "Iteration Number: 8329\n",
      "Loss: 28.84376135993147\n",
      "l2 norm of gradients: 0.18324167651744358\n",
      "l2 norm of weights: 5.664900723425228\n",
      "---------------------\n",
      "Iteration Number: 8330\n",
      "Loss: 28.842077760918524\n",
      "l2 norm of gradients: 0.18322695490116228\n",
      "l2 norm of weights: 5.664834387164762\n",
      "---------------------\n",
      "Iteration Number: 8331\n",
      "Loss: 28.84039443216444\n",
      "l2 norm of gradients: 0.18321223600891778\n",
      "l2 norm of weights: 5.664768055814513\n",
      "---------------------\n",
      "Iteration Number: 8332\n",
      "Loss: 28.838711373599015\n",
      "l2 norm of gradients: 0.18319751983972932\n",
      "l2 norm of weights: 5.664701729373804\n",
      "---------------------\n",
      "Iteration Number: 8333\n",
      "Loss: 28.837028585150957\n",
      "l2 norm of gradients: 0.18318280639261636\n",
      "l2 norm of weights: 5.664635407841966\n",
      "---------------------\n",
      "Iteration Number: 8334\n",
      "Loss: 28.83534606674588\n",
      "l2 norm of gradients: 0.18316809566659922\n",
      "l2 norm of weights: 5.66456909121832\n",
      "---------------------\n",
      "Iteration Number: 8335\n",
      "Loss: 28.833663818314125\n",
      "l2 norm of gradients: 0.18315338766069866\n",
      "l2 norm of weights: 5.664502779502196\n",
      "---------------------\n",
      "Iteration Number: 8336\n",
      "Loss: 28.83198183978043\n",
      "l2 norm of gradients: 0.183138682373936\n",
      "l2 norm of weights: 5.664436472692918\n",
      "---------------------\n",
      "Iteration Number: 8337\n",
      "Loss: 28.830300131078133\n",
      "l2 norm of gradients: 0.18312397980533315\n",
      "l2 norm of weights: 5.664370170789813\n",
      "---------------------\n",
      "Iteration Number: 8338\n",
      "Loss: 28.828618692128018\n",
      "l2 norm of gradients: 0.18310927995391252\n",
      "l2 norm of weights: 5.66430387379221\n",
      "---------------------\n",
      "Iteration Number: 8339\n",
      "Loss: 28.82693752286347\n",
      "l2 norm of gradients: 0.1830945828186972\n",
      "l2 norm of weights: 5.664237581699433\n",
      "---------------------\n",
      "Iteration Number: 8340\n",
      "Loss: 28.82525662321314\n",
      "l2 norm of gradients: 0.18307988839871076\n",
      "l2 norm of weights: 5.66417129451081\n",
      "---------------------\n",
      "Iteration Number: 8341\n",
      "Loss: 28.82357599310108\n",
      "l2 norm of gradients: 0.18306519669297733\n",
      "l2 norm of weights: 5.664105012225667\n",
      "---------------------\n",
      "Iteration Number: 8342\n",
      "Loss: 28.82189563245718\n",
      "l2 norm of gradients: 0.18305050770052153\n",
      "l2 norm of weights: 5.664038734843333\n",
      "---------------------\n",
      "Iteration Number: 8343\n",
      "Loss: 28.82021554121324\n",
      "l2 norm of gradients: 0.18303582142036876\n",
      "l2 norm of weights: 5.663972462363136\n",
      "---------------------\n",
      "Iteration Number: 8344\n",
      "Loss: 28.81853571928964\n",
      "l2 norm of gradients: 0.18302113785154472\n",
      "l2 norm of weights: 5.663906194784402\n",
      "---------------------\n",
      "Iteration Number: 8345\n",
      "Loss: 28.816856166622422\n",
      "l2 norm of gradients: 0.18300645699307583\n",
      "l2 norm of weights: 5.66383993210646\n",
      "---------------------\n",
      "Iteration Number: 8346\n",
      "Loss: 28.815176883135248\n",
      "l2 norm of gradients: 0.18299177884398904\n",
      "l2 norm of weights: 5.663773674328636\n",
      "---------------------\n",
      "Iteration Number: 8347\n",
      "Loss: 28.813497868758684\n",
      "l2 norm of gradients: 0.18297710340331189\n",
      "l2 norm of weights: 5.663707421450261\n",
      "---------------------\n",
      "Iteration Number: 8348\n",
      "Loss: 28.811819123420097\n",
      "l2 norm of gradients: 0.18296243067007237\n",
      "l2 norm of weights: 5.66364117347066\n",
      "---------------------\n",
      "Iteration Number: 8349\n",
      "Loss: 28.810140647048843\n",
      "l2 norm of gradients: 0.182947760643299\n",
      "l2 norm of weights: 5.6635749303891645\n",
      "---------------------\n",
      "Iteration Number: 8350\n",
      "Loss: 28.808462439571173\n",
      "l2 norm of gradients: 0.18293309332202115\n",
      "l2 norm of weights: 5.663508692205102\n",
      "---------------------\n",
      "Iteration Number: 8351\n",
      "Loss: 28.806784500917335\n",
      "l2 norm of gradients: 0.18291842870526842\n",
      "l2 norm of weights: 5.6634424589178005\n",
      "---------------------\n",
      "Iteration Number: 8352\n",
      "Loss: 28.80510683101556\n",
      "l2 norm of gradients: 0.18290376679207113\n",
      "l2 norm of weights: 5.66337623052659\n",
      "---------------------\n",
      "Iteration Number: 8353\n",
      "Loss: 28.80342942979187\n",
      "l2 norm of gradients: 0.18288910758146007\n",
      "l2 norm of weights: 5.6633100070308\n",
      "---------------------\n",
      "Iteration Number: 8354\n",
      "Loss: 28.801752297180133\n",
      "l2 norm of gradients: 0.18287445107246672\n",
      "l2 norm of weights: 5.663243788429758\n",
      "---------------------\n",
      "Iteration Number: 8355\n",
      "Loss: 28.800075433101632\n",
      "l2 norm of gradients: 0.18285979726412302\n",
      "l2 norm of weights: 5.663177574722795\n",
      "---------------------\n",
      "Iteration Number: 8356\n",
      "Loss: 28.798398837493703\n",
      "l2 norm of gradients: 0.18284514615546135\n",
      "l2 norm of weights: 5.663111365909239\n",
      "---------------------\n",
      "Iteration Number: 8357\n",
      "Loss: 28.79672251027565\n",
      "l2 norm of gradients: 0.1828304977455149\n",
      "l2 norm of weights: 5.663045161988422\n",
      "---------------------\n",
      "Iteration Number: 8358\n",
      "Loss: 28.79504645138352\n",
      "l2 norm of gradients: 0.18281585203331727\n",
      "l2 norm of weights: 5.662978962959672\n",
      "---------------------\n",
      "Iteration Number: 8359\n",
      "Loss: 28.793370660742884\n",
      "l2 norm of gradients: 0.18280120901790262\n",
      "l2 norm of weights: 5.662912768822318\n",
      "---------------------\n",
      "Iteration Number: 8360\n",
      "Loss: 28.791695138281323\n",
      "l2 norm of gradients: 0.18278656869830562\n",
      "l2 norm of weights: 5.662846579575693\n",
      "---------------------\n",
      "Iteration Number: 8361\n",
      "Loss: 28.790019883930768\n",
      "l2 norm of gradients: 0.18277193107356166\n",
      "l2 norm of weights: 5.6627803952191265\n",
      "---------------------\n",
      "Iteration Number: 8362\n",
      "Loss: 28.788344897615968\n",
      "l2 norm of gradients: 0.1827572961427065\n",
      "l2 norm of weights: 5.662714215751948\n",
      "---------------------\n",
      "Iteration Number: 8363\n",
      "Loss: 28.786670179267034\n",
      "l2 norm of gradients: 0.18274266390477653\n",
      "l2 norm of weights: 5.662648041173489\n",
      "---------------------\n",
      "Iteration Number: 8364\n",
      "Loss: 28.784995728816583\n",
      "l2 norm of gradients: 0.18272803435880872\n",
      "l2 norm of weights: 5.66258187148308\n",
      "---------------------\n",
      "Iteration Number: 8365\n",
      "Loss: 28.783321546189615\n",
      "l2 norm of gradients: 0.18271340750384055\n",
      "l2 norm of weights: 5.662515706680052\n",
      "---------------------\n",
      "Iteration Number: 8366\n",
      "Loss: 28.781647631315693\n",
      "l2 norm of gradients: 0.18269878333891007\n",
      "l2 norm of weights: 5.662449546763737\n",
      "---------------------\n",
      "Iteration Number: 8367\n",
      "Loss: 28.779973984123156\n",
      "l2 norm of gradients: 0.1826841618630559\n",
      "l2 norm of weights: 5.662383391733466\n",
      "---------------------\n",
      "Iteration Number: 8368\n",
      "Loss: 28.778300604543407\n",
      "l2 norm of gradients: 0.1826695430753171\n",
      "l2 norm of weights: 5.662317241588569\n",
      "---------------------\n",
      "Iteration Number: 8369\n",
      "Loss: 28.776627492500904\n",
      "l2 norm of gradients: 0.1826549269747335\n",
      "l2 norm of weights: 5.6622510963283785\n",
      "---------------------\n",
      "Iteration Number: 8370\n",
      "Loss: 28.774954647928233\n",
      "l2 norm of gradients: 0.18264031356034527\n",
      "l2 norm of weights: 5.6621849559522275\n",
      "---------------------\n",
      "Iteration Number: 8371\n",
      "Loss: 28.773282070754252\n",
      "l2 norm of gradients: 0.18262570283119325\n",
      "l2 norm of weights: 5.662118820459447\n",
      "---------------------\n",
      "Iteration Number: 8372\n",
      "Loss: 28.771609760908902\n",
      "l2 norm of gradients: 0.1826110947863188\n",
      "l2 norm of weights: 5.662052689849368\n",
      "---------------------\n",
      "Iteration Number: 8373\n",
      "Loss: 28.769937718317227\n",
      "l2 norm of gradients: 0.18259648942476375\n",
      "l2 norm of weights: 5.661986564121324\n",
      "---------------------\n",
      "Iteration Number: 8374\n",
      "Loss: 28.76826594291041\n",
      "l2 norm of gradients: 0.1825818867455707\n",
      "l2 norm of weights: 5.6619204432746475\n",
      "---------------------\n",
      "Iteration Number: 8375\n",
      "Loss: 28.76659443461833\n",
      "l2 norm of gradients: 0.18256728674778253\n",
      "l2 norm of weights: 5.66185432730867\n",
      "---------------------\n",
      "Iteration Number: 8376\n",
      "Loss: 28.764923193371395\n",
      "l2 norm of gradients: 0.18255268943044287\n",
      "l2 norm of weights: 5.6617882162227255\n",
      "---------------------\n",
      "Iteration Number: 8377\n",
      "Loss: 28.76325221909525\n",
      "l2 norm of gradients: 0.18253809479259572\n",
      "l2 norm of weights: 5.661722110016147\n",
      "---------------------\n",
      "Iteration Number: 8378\n",
      "Loss: 28.76158151172306\n",
      "l2 norm of gradients: 0.18252350283328586\n",
      "l2 norm of weights: 5.661656008688266\n",
      "---------------------\n",
      "Iteration Number: 8379\n",
      "Loss: 28.75991107118001\n",
      "l2 norm of gradients: 0.1825089135515584\n",
      "l2 norm of weights: 5.6615899122384175\n",
      "---------------------\n",
      "Iteration Number: 8380\n",
      "Loss: 28.758240897396657\n",
      "l2 norm of gradients: 0.18249432694645915\n",
      "l2 norm of weights: 5.661523820665933\n",
      "---------------------\n",
      "Iteration Number: 8381\n",
      "Loss: 28.756570990306138\n",
      "l2 norm of gradients: 0.1824797430170344\n",
      "l2 norm of weights: 5.661457733970147\n",
      "---------------------\n",
      "Iteration Number: 8382\n",
      "Loss: 28.754901349832355\n",
      "l2 norm of gradients: 0.18246516176233094\n",
      "l2 norm of weights: 5.661391652150395\n",
      "---------------------\n",
      "Iteration Number: 8383\n",
      "Loss: 28.753231975906974\n",
      "l2 norm of gradients: 0.18245058318139623\n",
      "l2 norm of weights: 5.661325575206007\n",
      "---------------------\n",
      "Iteration Number: 8384\n",
      "Loss: 28.751562868460002\n",
      "l2 norm of gradients: 0.18243600727327816\n",
      "l2 norm of weights: 5.6612595031363195\n",
      "---------------------\n",
      "Iteration Number: 8385\n",
      "Loss: 28.749894027421206\n",
      "l2 norm of gradients: 0.18242143403702524\n",
      "l2 norm of weights: 5.661193435940666\n",
      "---------------------\n",
      "Iteration Number: 8386\n",
      "Loss: 28.748225452718486\n",
      "l2 norm of gradients: 0.18240686347168653\n",
      "l2 norm of weights: 5.661127373618381\n",
      "---------------------\n",
      "Iteration Number: 8387\n",
      "Loss: 28.746557144281056\n",
      "l2 norm of gradients: 0.18239229557631154\n",
      "l2 norm of weights: 5.661061316168799\n",
      "---------------------\n",
      "Iteration Number: 8388\n",
      "Loss: 28.744889102040073\n",
      "l2 norm of gradients: 0.18237773034995045\n",
      "l2 norm of weights: 5.660995263591255\n",
      "---------------------\n",
      "Iteration Number: 8389\n",
      "Loss: 28.743221325926893\n",
      "l2 norm of gradients: 0.18236316779165393\n",
      "l2 norm of weights: 5.6609292158850835\n",
      "---------------------\n",
      "Iteration Number: 8390\n",
      "Loss: 28.741553815864396\n",
      "l2 norm of gradients: 0.1823486079004732\n",
      "l2 norm of weights: 5.660863173049618\n",
      "---------------------\n",
      "Iteration Number: 8391\n",
      "Loss: 28.739886571789594\n",
      "l2 norm of gradients: 0.18233405067545994\n",
      "l2 norm of weights: 5.660797135084196\n",
      "---------------------\n",
      "Iteration Number: 8392\n",
      "Loss: 28.738219593627278\n",
      "l2 norm of gradients: 0.1823194961156665\n",
      "l2 norm of weights: 5.6607311019881505\n",
      "---------------------\n",
      "Iteration Number: 8393\n",
      "Loss: 28.73655288131\n",
      "l2 norm of gradients: 0.1823049442201458\n",
      "l2 norm of weights: 5.660665073760819\n",
      "---------------------\n",
      "Iteration Number: 8394\n",
      "Loss: 28.734886434763258\n",
      "l2 norm of gradients: 0.1822903949879511\n",
      "l2 norm of weights: 5.660599050401535\n",
      "---------------------\n",
      "Iteration Number: 8395\n",
      "Loss: 28.733220253922514\n",
      "l2 norm of gradients: 0.1822758484181365\n",
      "l2 norm of weights: 5.660533031909636\n",
      "---------------------\n",
      "Iteration Number: 8396\n",
      "Loss: 28.73155433871371\n",
      "l2 norm of gradients: 0.18226130450975633\n",
      "l2 norm of weights: 5.660467018284456\n",
      "---------------------\n",
      "Iteration Number: 8397\n",
      "Loss: 28.729888689068375\n",
      "l2 norm of gradients: 0.1822467632618657\n",
      "l2 norm of weights: 5.660401009525334\n",
      "---------------------\n",
      "Iteration Number: 8398\n",
      "Loss: 28.72822330491495\n",
      "l2 norm of gradients: 0.18223222467352013\n",
      "l2 norm of weights: 5.660335005631603\n",
      "---------------------\n",
      "Iteration Number: 8399\n",
      "Loss: 28.72655818618275\n",
      "l2 norm of gradients: 0.18221768874377575\n",
      "l2 norm of weights: 5.660269006602601\n",
      "---------------------\n",
      "Iteration Number: 8400\n",
      "Loss: 28.72489333280499\n",
      "l2 norm of gradients: 0.18220315547168914\n",
      "l2 norm of weights: 5.6602030124376626\n",
      "---------------------\n",
      "Iteration Number: 8401\n",
      "Loss: 28.723228744709612\n",
      "l2 norm of gradients: 0.18218862485631765\n",
      "l2 norm of weights: 5.660137023136127\n",
      "---------------------\n",
      "Iteration Number: 8402\n",
      "Loss: 28.721564421823448\n",
      "l2 norm of gradients: 0.18217409689671882\n",
      "l2 norm of weights: 5.66007103869733\n",
      "---------------------\n",
      "Iteration Number: 8403\n",
      "Loss: 28.719900364082058\n",
      "l2 norm of gradients: 0.18215957159195106\n",
      "l2 norm of weights: 5.660005059120609\n",
      "---------------------\n",
      "Iteration Number: 8404\n",
      "Loss: 28.718236571409438\n",
      "l2 norm of gradients: 0.18214504894107314\n",
      "l2 norm of weights: 5.6599390844053\n",
      "---------------------\n",
      "Iteration Number: 8405\n",
      "Loss: 28.716573043741217\n",
      "l2 norm of gradients: 0.18213052894314444\n",
      "l2 norm of weights: 5.6598731145507415\n",
      "---------------------\n",
      "Iteration Number: 8406\n",
      "Loss: 28.71490978100355\n",
      "l2 norm of gradients: 0.1821160115972248\n",
      "l2 norm of weights: 5.659807149556269\n",
      "---------------------\n",
      "Iteration Number: 8407\n",
      "Loss: 28.713246783130288\n",
      "l2 norm of gradients: 0.1821014969023747\n",
      "l2 norm of weights: 5.659741189421223\n",
      "---------------------\n",
      "Iteration Number: 8408\n",
      "Loss: 28.711584050046728\n",
      "l2 norm of gradients: 0.182086984857655\n",
      "l2 norm of weights: 5.659675234144939\n",
      "---------------------\n",
      "Iteration Number: 8409\n",
      "Loss: 28.70992158168758\n",
      "l2 norm of gradients: 0.18207247546212743\n",
      "l2 norm of weights: 5.659609283726756\n",
      "---------------------\n",
      "Iteration Number: 8410\n",
      "Loss: 28.70825937797885\n",
      "l2 norm of gradients: 0.18205796871485394\n",
      "l2 norm of weights: 5.659543338166011\n",
      "---------------------\n",
      "Iteration Number: 8411\n",
      "Loss: 28.706597438855464\n",
      "l2 norm of gradients: 0.18204346461489707\n",
      "l2 norm of weights: 5.659477397462043\n",
      "---------------------\n",
      "Iteration Number: 8412\n",
      "Loss: 28.704935764242602\n",
      "l2 norm of gradients: 0.18202896316131995\n",
      "l2 norm of weights: 5.65941146161419\n",
      "---------------------\n",
      "Iteration Number: 8413\n",
      "Loss: 28.70327435407434\n",
      "l2 norm of gradients: 0.18201446435318627\n",
      "l2 norm of weights: 5.659345530621791\n",
      "---------------------\n",
      "Iteration Number: 8414\n",
      "Loss: 28.701613208277124\n",
      "l2 norm of gradients: 0.18199996818956027\n",
      "l2 norm of weights: 5.659279604484184\n",
      "---------------------\n",
      "Iteration Number: 8415\n",
      "Loss: 28.699952326785997\n",
      "l2 norm of gradients: 0.18198547466950668\n",
      "l2 norm of weights: 5.659213683200709\n",
      "---------------------\n",
      "Iteration Number: 8416\n",
      "Loss: 28.698291709527684\n",
      "l2 norm of gradients: 0.18197098379209076\n",
      "l2 norm of weights: 5.659147766770703\n",
      "---------------------\n",
      "Iteration Number: 8417\n",
      "Loss: 28.69663135643428\n",
      "l2 norm of gradients: 0.18195649555637833\n",
      "l2 norm of weights: 5.659081855193507\n",
      "---------------------\n",
      "Iteration Number: 8418\n",
      "Loss: 28.69497126743625\n",
      "l2 norm of gradients: 0.18194200996143572\n",
      "l2 norm of weights: 5.659015948468459\n",
      "---------------------\n",
      "Iteration Number: 8419\n",
      "Loss: 28.693311442461294\n",
      "l2 norm of gradients: 0.1819275270063299\n",
      "l2 norm of weights: 5.658950046594899\n",
      "---------------------\n",
      "Iteration Number: 8420\n",
      "Loss: 28.69165188144547\n",
      "l2 norm of gradients: 0.18191304669012817\n",
      "l2 norm of weights: 5.658884149572168\n",
      "---------------------\n",
      "Iteration Number: 8421\n",
      "Loss: 28.689992584311305\n",
      "l2 norm of gradients: 0.1818985690118986\n",
      "l2 norm of weights: 5.658818257399603\n",
      "---------------------\n",
      "Iteration Number: 8422\n",
      "Loss: 28.68833355099678\n",
      "l2 norm of gradients: 0.18188409397070965\n",
      "l2 norm of weights: 5.658752370076545\n",
      "---------------------\n",
      "Iteration Number: 8423\n",
      "Loss: 28.68667478142794\n",
      "l2 norm of gradients: 0.1818696215656303\n",
      "l2 norm of weights: 5.658686487602336\n",
      "---------------------\n",
      "Iteration Number: 8424\n",
      "Loss: 28.685016275537542\n",
      "l2 norm of gradients: 0.18185515179573017\n",
      "l2 norm of weights: 5.658620609976313\n",
      "---------------------\n",
      "Iteration Number: 8425\n",
      "Loss: 28.683358033255498\n",
      "l2 norm of gradients: 0.18184068466007938\n",
      "l2 norm of weights: 5.658554737197819\n",
      "---------------------\n",
      "Iteration Number: 8426\n",
      "Loss: 28.681700054512\n",
      "l2 norm of gradients: 0.18182622015774846\n",
      "l2 norm of weights: 5.6584888692661925\n",
      "---------------------\n",
      "Iteration Number: 8427\n",
      "Loss: 28.680042339238465\n",
      "l2 norm of gradients: 0.18181175828780868\n",
      "l2 norm of weights: 5.658423006180775\n",
      "---------------------\n",
      "Iteration Number: 8428\n",
      "Loss: 28.67838488736394\n",
      "l2 norm of gradients: 0.18179729904933173\n",
      "l2 norm of weights: 5.658357147940909\n",
      "---------------------\n",
      "Iteration Number: 8429\n",
      "Loss: 28.676727698822827\n",
      "l2 norm of gradients: 0.1817828424413898\n",
      "l2 norm of weights: 5.658291294545933\n",
      "---------------------\n",
      "Iteration Number: 8430\n",
      "Loss: 28.675070773539648\n",
      "l2 norm of gradients: 0.18176838846305562\n",
      "l2 norm of weights: 5.658225445995188\n",
      "---------------------\n",
      "Iteration Number: 8431\n",
      "Loss: 28.673414111450583\n",
      "l2 norm of gradients: 0.18175393711340262\n",
      "l2 norm of weights: 5.658159602288018\n",
      "---------------------\n",
      "Iteration Number: 8432\n",
      "Loss: 28.6717577124851\n",
      "l2 norm of gradients: 0.1817394883915045\n",
      "l2 norm of weights: 5.6580937634237625\n",
      "---------------------\n",
      "Iteration Number: 8433\n",
      "Loss: 28.67010157657268\n",
      "l2 norm of gradients: 0.18172504229643568\n",
      "l2 norm of weights: 5.658027929401764\n",
      "---------------------\n",
      "Iteration Number: 8434\n",
      "Loss: 28.668445703647105\n",
      "l2 norm of gradients: 0.18171059882727106\n",
      "l2 norm of weights: 5.657962100221363\n",
      "---------------------\n",
      "Iteration Number: 8435\n",
      "Loss: 28.666790093635978\n",
      "l2 norm of gradients: 0.181696157983086\n",
      "l2 norm of weights: 5.657896275881901\n",
      "---------------------\n",
      "Iteration Number: 8436\n",
      "Loss: 28.665134746470684\n",
      "l2 norm of gradients: 0.18168171976295658\n",
      "l2 norm of weights: 5.657830456382723\n",
      "---------------------\n",
      "Iteration Number: 8437\n",
      "Loss: 28.66347966208417\n",
      "l2 norm of gradients: 0.18166728416595915\n",
      "l2 norm of weights: 5.657764641723169\n",
      "---------------------\n",
      "Iteration Number: 8438\n",
      "Loss: 28.66182484040612\n",
      "l2 norm of gradients: 0.1816528511911708\n",
      "l2 norm of weights: 5.657698831902582\n",
      "---------------------\n",
      "Iteration Number: 8439\n",
      "Loss: 28.660170281366163\n",
      "l2 norm of gradients: 0.1816384208376691\n",
      "l2 norm of weights: 5.657633026920304\n",
      "---------------------\n",
      "Iteration Number: 8440\n",
      "Loss: 28.658515984897818\n",
      "l2 norm of gradients: 0.18162399310453203\n",
      "l2 norm of weights: 5.657567226775678\n",
      "---------------------\n",
      "Iteration Number: 8441\n",
      "Loss: 28.656861950929144\n",
      "l2 norm of gradients: 0.1816095679908383\n",
      "l2 norm of weights: 5.657501431468047\n",
      "---------------------\n",
      "Iteration Number: 8442\n",
      "Loss: 28.655208179394698\n",
      "l2 norm of gradients: 0.18159514549566697\n",
      "l2 norm of weights: 5.657435640996753\n",
      "---------------------\n",
      "Iteration Number: 8443\n",
      "Loss: 28.65355467022363\n",
      "l2 norm of gradients: 0.18158072561809774\n",
      "l2 norm of weights: 5.657369855361141\n",
      "---------------------\n",
      "Iteration Number: 8444\n",
      "Loss: 28.651901423346636\n",
      "l2 norm of gradients: 0.18156630835721077\n",
      "l2 norm of weights: 5.657304074560553\n",
      "---------------------\n",
      "Iteration Number: 8445\n",
      "Loss: 28.650248438698846\n",
      "l2 norm of gradients: 0.18155189371208683\n",
      "l2 norm of weights: 5.657238298594333\n",
      "---------------------\n",
      "Iteration Number: 8446\n",
      "Loss: 28.64859571620464\n",
      "l2 norm of gradients: 0.1815374816818071\n",
      "l2 norm of weights: 5.657172527461824\n",
      "---------------------\n",
      "Iteration Number: 8447\n",
      "Loss: 28.646943255800092\n",
      "l2 norm of gradients: 0.18152307226545344\n",
      "l2 norm of weights: 5.65710676116237\n",
      "---------------------\n",
      "Iteration Number: 8448\n",
      "Loss: 28.64529105741418\n",
      "l2 norm of gradients: 0.18150866546210812\n",
      "l2 norm of weights: 5.657040999695314\n",
      "---------------------\n",
      "Iteration Number: 8449\n",
      "Loss: 28.643639120979554\n",
      "l2 norm of gradients: 0.1814942612708539\n",
      "l2 norm of weights: 5.656975243060002\n",
      "---------------------\n",
      "Iteration Number: 8450\n",
      "Loss: 28.641987446429972\n",
      "l2 norm of gradients: 0.1814798596907742\n",
      "l2 norm of weights: 5.656909491255778\n",
      "---------------------\n",
      "Iteration Number: 8451\n",
      "Loss: 28.640336033690634\n",
      "l2 norm of gradients: 0.1814654607209529\n",
      "l2 norm of weights: 5.656843744281985\n",
      "---------------------\n",
      "Iteration Number: 8452\n",
      "Loss: 28.638684882695998\n",
      "l2 norm of gradients: 0.1814510643604744\n",
      "l2 norm of weights: 5.656778002137967\n",
      "---------------------\n",
      "Iteration Number: 8453\n",
      "Loss: 28.63703399337893\n",
      "l2 norm of gradients: 0.18143667060842358\n",
      "l2 norm of weights: 5.656712264823072\n",
      "---------------------\n",
      "Iteration Number: 8454\n",
      "Loss: 28.63538336566726\n",
      "l2 norm of gradients: 0.181422279463886\n",
      "l2 norm of weights: 5.656646532336642\n",
      "---------------------\n",
      "Iteration Number: 8455\n",
      "Loss: 28.63373299949942\n",
      "l2 norm of gradients: 0.18140789092594758\n",
      "l2 norm of weights: 5.656580804678022\n",
      "---------------------\n",
      "Iteration Number: 8456\n",
      "Loss: 28.63208289479729\n",
      "l2 norm of gradients: 0.18139350499369483\n",
      "l2 norm of weights: 5.656515081846558\n",
      "---------------------\n",
      "Iteration Number: 8457\n",
      "Loss: 28.630433051498166\n",
      "l2 norm of gradients: 0.1813791216662148\n",
      "l2 norm of weights: 5.656449363841595\n",
      "---------------------\n",
      "Iteration Number: 8458\n",
      "Loss: 28.62878346953451\n",
      "l2 norm of gradients: 0.18136474094259503\n",
      "l2 norm of weights: 5.6563836506624785\n",
      "---------------------\n",
      "Iteration Number: 8459\n",
      "Loss: 28.62713414883468\n",
      "l2 norm of gradients: 0.18135036282192357\n",
      "l2 norm of weights: 5.656317942308554\n",
      "---------------------\n",
      "Iteration Number: 8460\n",
      "Loss: 28.625485089332233\n",
      "l2 norm of gradients: 0.18133598730328912\n",
      "l2 norm of weights: 5.656252238779167\n",
      "---------------------\n",
      "Iteration Number: 8461\n",
      "Loss: 28.623836290955712\n",
      "l2 norm of gradients: 0.1813216143857807\n",
      "l2 norm of weights: 5.656186540073664\n",
      "---------------------\n",
      "Iteration Number: 8462\n",
      "Loss: 28.62218775364275\n",
      "l2 norm of gradients: 0.18130724406848803\n",
      "l2 norm of weights: 5.65612084619139\n",
      "---------------------\n",
      "Iteration Number: 8463\n",
      "Loss: 28.620539477319163\n",
      "l2 norm of gradients: 0.1812928763505012\n",
      "l2 norm of weights: 5.656055157131693\n",
      "---------------------\n",
      "Iteration Number: 8464\n",
      "Loss: 28.61889146191689\n",
      "l2 norm of gradients: 0.18127851123091104\n",
      "l2 norm of weights: 5.655989472893918\n",
      "---------------------\n",
      "Iteration Number: 8465\n",
      "Loss: 28.617243707374183\n",
      "l2 norm of gradients: 0.18126414870880864\n",
      "l2 norm of weights: 5.655923793477411\n",
      "---------------------\n",
      "Iteration Number: 8466\n",
      "Loss: 28.615596213615493\n",
      "l2 norm of gradients: 0.1812497887832858\n",
      "l2 norm of weights: 5.655858118881521\n",
      "---------------------\n",
      "Iteration Number: 8467\n",
      "Loss: 28.613948980575437\n",
      "l2 norm of gradients: 0.18123543145343476\n",
      "l2 norm of weights: 5.655792449105592\n",
      "---------------------\n",
      "Iteration Number: 8468\n",
      "Loss: 28.612302008183462\n",
      "l2 norm of gradients: 0.1812210767183483\n",
      "l2 norm of weights: 5.655726784148972\n",
      "---------------------\n",
      "Iteration Number: 8469\n",
      "Loss: 28.610655296377715\n",
      "l2 norm of gradients: 0.18120672457711973\n",
      "l2 norm of weights: 5.655661124011009\n",
      "---------------------\n",
      "Iteration Number: 8470\n",
      "Loss: 28.609008845084336\n",
      "l2 norm of gradients: 0.18119237502884283\n",
      "l2 norm of weights: 5.6555954686910495\n",
      "---------------------\n",
      "Iteration Number: 8471\n",
      "Loss: 28.607362654235274\n",
      "l2 norm of gradients: 0.181178028072612\n",
      "l2 norm of weights: 5.655529818188441\n",
      "---------------------\n",
      "Iteration Number: 8472\n",
      "Loss: 28.605716723767745\n",
      "l2 norm of gradients: 0.1811636837075221\n",
      "l2 norm of weights: 5.655464172502531\n",
      "---------------------\n",
      "Iteration Number: 8473\n",
      "Loss: 28.60407105360484\n",
      "l2 norm of gradients: 0.1811493419326684\n",
      "l2 norm of weights: 5.655398531632667\n",
      "---------------------\n",
      "Iteration Number: 8474\n",
      "Loss: 28.602425643687397\n",
      "l2 norm of gradients: 0.18113500274714694\n",
      "l2 norm of weights: 5.655332895578197\n",
      "---------------------\n",
      "Iteration Number: 8475\n",
      "Loss: 28.600780493943848\n",
      "l2 norm of gradients: 0.1811206661500541\n",
      "l2 norm of weights: 5.655267264338469\n",
      "---------------------\n",
      "Iteration Number: 8476\n",
      "Loss: 28.599135604302916\n",
      "l2 norm of gradients: 0.18110633214048677\n",
      "l2 norm of weights: 5.655201637912832\n",
      "---------------------\n",
      "Iteration Number: 8477\n",
      "Loss: 28.597490974701643\n",
      "l2 norm of gradients: 0.18109200071754245\n",
      "l2 norm of weights: 5.655136016300633\n",
      "---------------------\n",
      "Iteration Number: 8478\n",
      "Loss: 28.59584660506917\n",
      "l2 norm of gradients: 0.1810776718803191\n",
      "l2 norm of weights: 5.655070399501221\n",
      "---------------------\n",
      "Iteration Number: 8479\n",
      "Loss: 28.59420249534082\n",
      "l2 norm of gradients: 0.18106334562791515\n",
      "l2 norm of weights: 5.655004787513945\n",
      "---------------------\n",
      "Iteration Number: 8480\n",
      "Loss: 28.592558645445255\n",
      "l2 norm of gradients: 0.18104902195942973\n",
      "l2 norm of weights: 5.654939180338153\n",
      "---------------------\n",
      "Iteration Number: 8481\n",
      "Loss: 28.59091505531523\n",
      "l2 norm of gradients: 0.18103470087396223\n",
      "l2 norm of weights: 5.654873577973195\n",
      "---------------------\n",
      "Iteration Number: 8482\n",
      "Loss: 28.589271724884373\n",
      "l2 norm of gradients: 0.18102038237061277\n",
      "l2 norm of weights: 5.654807980418419\n",
      "---------------------\n",
      "Iteration Number: 8483\n",
      "Loss: 28.58762865408193\n",
      "l2 norm of gradients: 0.1810060664484819\n",
      "l2 norm of weights: 5.654742387673174\n",
      "---------------------\n",
      "Iteration Number: 8484\n",
      "Loss: 28.5859858428449\n",
      "l2 norm of gradients: 0.1809917531066707\n",
      "l2 norm of weights: 5.654676799736811\n",
      "---------------------\n",
      "Iteration Number: 8485\n",
      "Loss: 28.584343291103192\n",
      "l2 norm of gradients: 0.18097744234428076\n",
      "l2 norm of weights: 5.6546112166086795\n",
      "---------------------\n",
      "Iteration Number: 8486\n",
      "Loss: 28.58270099878753\n",
      "l2 norm of gradients: 0.18096313416041412\n",
      "l2 norm of weights: 5.654545638288127\n",
      "---------------------\n",
      "Iteration Number: 8487\n",
      "Loss: 28.581058965832217\n",
      "l2 norm of gradients: 0.18094882855417352\n",
      "l2 norm of weights: 5.654480064774505\n",
      "---------------------\n",
      "Iteration Number: 8488\n",
      "Loss: 28.57941719216796\n",
      "l2 norm of gradients: 0.180934525524662\n",
      "l2 norm of weights: 5.6544144960671625\n",
      "---------------------\n",
      "Iteration Number: 8489\n",
      "Loss: 28.57777567773084\n",
      "l2 norm of gradients: 0.18092022507098315\n",
      "l2 norm of weights: 5.654348932165451\n",
      "---------------------\n",
      "Iteration Number: 8490\n",
      "Loss: 28.576134422449407\n",
      "l2 norm of gradients: 0.18090592719224127\n",
      "l2 norm of weights: 5.654283373068719\n",
      "---------------------\n",
      "Iteration Number: 8491\n",
      "Loss: 28.574493426255827\n",
      "l2 norm of gradients: 0.18089163188754095\n",
      "l2 norm of weights: 5.65421781877632\n",
      "---------------------\n",
      "Iteration Number: 8492\n",
      "Loss: 28.57285268908478\n",
      "l2 norm of gradients: 0.18087733915598747\n",
      "l2 norm of weights: 5.654152269287601\n",
      "---------------------\n",
      "Iteration Number: 8493\n",
      "Loss: 28.57121221086745\n",
      "l2 norm of gradients: 0.18086304899668637\n",
      "l2 norm of weights: 5.654086724601913\n",
      "---------------------\n",
      "Iteration Number: 8494\n",
      "Loss: 28.569571991539338\n",
      "l2 norm of gradients: 0.18084876140874404\n",
      "l2 norm of weights: 5.65402118471861\n",
      "---------------------\n",
      "Iteration Number: 8495\n",
      "Loss: 28.56793203102797\n",
      "l2 norm of gradients: 0.1808344763912671\n",
      "l2 norm of weights: 5.653955649637041\n",
      "---------------------\n",
      "Iteration Number: 8496\n",
      "Loss: 28.566292329269547\n",
      "l2 norm of gradients: 0.18082019394336282\n",
      "l2 norm of weights: 5.653890119356556\n",
      "---------------------\n",
      "Iteration Number: 8497\n",
      "Loss: 28.564652886197173\n",
      "l2 norm of gradients: 0.18080591406413904\n",
      "l2 norm of weights: 5.653824593876509\n",
      "---------------------\n",
      "Iteration Number: 8498\n",
      "Loss: 28.563013701740072\n",
      "l2 norm of gradients: 0.18079163675270385\n",
      "l2 norm of weights: 5.65375907319625\n",
      "---------------------\n",
      "Iteration Number: 8499\n",
      "Loss: 28.561374775833393\n",
      "l2 norm of gradients: 0.1807773620081662\n",
      "l2 norm of weights: 5.653693557315131\n",
      "---------------------\n",
      "Iteration Number: 8500\n",
      "Loss: 28.55973610840865\n",
      "l2 norm of gradients: 0.18076308982963527\n",
      "l2 norm of weights: 5.653628046232503\n",
      "---------------------\n",
      "Iteration Number: 8501\n",
      "Loss: 28.55809769939946\n",
      "l2 norm of gradients: 0.18074882021622082\n",
      "l2 norm of weights: 5.653562539947718\n",
      "---------------------\n",
      "Iteration Number: 8502\n",
      "Loss: 28.55645954873879\n",
      "l2 norm of gradients: 0.18073455316703324\n",
      "l2 norm of weights: 5.65349703846013\n",
      "---------------------\n",
      "Iteration Number: 8503\n",
      "Loss: 28.55482165635723\n",
      "l2 norm of gradients: 0.18072028868118337\n",
      "l2 norm of weights: 5.653431541769089\n",
      "---------------------\n",
      "Iteration Number: 8504\n",
      "Loss: 28.553184022189555\n",
      "l2 norm of gradients: 0.18070602675778244\n",
      "l2 norm of weights: 5.653366049873947\n",
      "---------------------\n",
      "Iteration Number: 8505\n",
      "Loss: 28.55154664616987\n",
      "l2 norm of gradients: 0.1806917673959424\n",
      "l2 norm of weights: 5.653300562774059\n",
      "---------------------\n",
      "Iteration Number: 8506\n",
      "Loss: 28.5499095282289\n",
      "l2 norm of gradients: 0.18067751059477552\n",
      "l2 norm of weights: 5.653235080468776\n",
      "---------------------\n",
      "Iteration Number: 8507\n",
      "Loss: 28.548272668299564\n",
      "l2 norm of gradients: 0.18066325635339467\n",
      "l2 norm of weights: 5.653169602957452\n",
      "---------------------\n",
      "Iteration Number: 8508\n",
      "Loss: 28.546636066314598\n",
      "l2 norm of gradients: 0.18064900467091322\n",
      "l2 norm of weights: 5.653104130239438\n",
      "---------------------\n",
      "Iteration Number: 8509\n",
      "Loss: 28.54499972220909\n",
      "l2 norm of gradients: 0.18063475554644506\n",
      "l2 norm of weights: 5.653038662314089\n",
      "---------------------\n",
      "Iteration Number: 8510\n",
      "Loss: 28.543363635911998\n",
      "l2 norm of gradients: 0.1806205089791045\n",
      "l2 norm of weights: 5.6529731991807575\n",
      "---------------------\n",
      "Iteration Number: 8511\n",
      "Loss: 28.541727807360804\n",
      "l2 norm of gradients: 0.18060626496800655\n",
      "l2 norm of weights: 5.652907740838797\n",
      "---------------------\n",
      "Iteration Number: 8512\n",
      "Loss: 28.54009223648554\n",
      "l2 norm of gradients: 0.18059202351226653\n",
      "l2 norm of weights: 5.652842287287561\n",
      "---------------------\n",
      "Iteration Number: 8513\n",
      "Loss: 28.538456923221162\n",
      "l2 norm of gradients: 0.18057778461100027\n",
      "l2 norm of weights: 5.652776838526404\n",
      "---------------------\n",
      "Iteration Number: 8514\n",
      "Loss: 28.536821867499278\n",
      "l2 norm of gradients: 0.1805635482633243\n",
      "l2 norm of weights: 5.652711394554679\n",
      "---------------------\n",
      "Iteration Number: 8515\n",
      "Loss: 28.535187069252142\n",
      "l2 norm of gradients: 0.1805493144683555\n",
      "l2 norm of weights: 5.65264595537174\n",
      "---------------------\n",
      "Iteration Number: 8516\n",
      "Loss: 28.533552528416067\n",
      "l2 norm of gradients: 0.1805350832252113\n",
      "l2 norm of weights: 5.652580520976941\n",
      "---------------------\n",
      "Iteration Number: 8517\n",
      "Loss: 28.531918244922444\n",
      "l2 norm of gradients: 0.18052085453300956\n",
      "l2 norm of weights: 5.6525150913696365\n",
      "---------------------\n",
      "Iteration Number: 8518\n",
      "Loss: 28.530284218700626\n",
      "l2 norm of gradients: 0.1805066283908688\n",
      "l2 norm of weights: 5.652449666549182\n",
      "---------------------\n",
      "Iteration Number: 8519\n",
      "Loss: 28.52865044969009\n",
      "l2 norm of gradients: 0.18049240479790793\n",
      "l2 norm of weights: 5.652384246514931\n",
      "---------------------\n",
      "Iteration Number: 8520\n",
      "Loss: 28.527016937822875\n",
      "l2 norm of gradients: 0.18047818375324626\n",
      "l2 norm of weights: 5.652318831266238\n",
      "---------------------\n",
      "Iteration Number: 8521\n",
      "Loss: 28.525383683029165\n",
      "l2 norm of gradients: 0.18046396525600397\n",
      "l2 norm of weights: 5.6522534208024595\n",
      "---------------------\n",
      "Iteration Number: 8522\n",
      "Loss: 28.523750685242206\n",
      "l2 norm of gradients: 0.1804497493053014\n",
      "l2 norm of weights: 5.652188015122949\n",
      "---------------------\n",
      "Iteration Number: 8523\n",
      "Loss: 28.522117944400957\n",
      "l2 norm of gradients: 0.1804355359002595\n",
      "l2 norm of weights: 5.652122614227063\n",
      "---------------------\n",
      "Iteration Number: 8524\n",
      "Loss: 28.520485460432482\n",
      "l2 norm of gradients: 0.18042132503999972\n",
      "l2 norm of weights: 5.652057218114155\n",
      "---------------------\n",
      "Iteration Number: 8525\n",
      "Loss: 28.518853233272793\n",
      "l2 norm of gradients: 0.18040711672364398\n",
      "l2 norm of weights: 5.651991826783583\n",
      "---------------------\n",
      "Iteration Number: 8526\n",
      "Loss: 28.517221262853663\n",
      "l2 norm of gradients: 0.18039291095031487\n",
      "l2 norm of weights: 5.651926440234701\n",
      "---------------------\n",
      "Iteration Number: 8527\n",
      "Loss: 28.515589549111095\n",
      "l2 norm of gradients: 0.18037870771913528\n",
      "l2 norm of weights: 5.651861058466864\n",
      "---------------------\n",
      "Iteration Number: 8528\n",
      "Loss: 28.513958091977155\n",
      "l2 norm of gradients: 0.18036450702922865\n",
      "l2 norm of weights: 5.65179568147943\n",
      "---------------------\n",
      "Iteration Number: 8529\n",
      "Loss: 28.512326891384337\n",
      "l2 norm of gradients: 0.18035030887971903\n",
      "l2 norm of weights: 5.651730309271754\n",
      "---------------------\n",
      "Iteration Number: 8530\n",
      "Loss: 28.510695947268655\n",
      "l2 norm of gradients: 0.1803361132697308\n",
      "l2 norm of weights: 5.651664941843192\n",
      "---------------------\n",
      "Iteration Number: 8531\n",
      "Loss: 28.50906525956095\n",
      "l2 norm of gradients: 0.180321920198389\n",
      "l2 norm of weights: 5.651599579193101\n",
      "---------------------\n",
      "Iteration Number: 8532\n",
      "Loss: 28.50743482819615\n",
      "l2 norm of gradients: 0.18030772966481912\n",
      "l2 norm of weights: 5.651534221320838\n",
      "---------------------\n",
      "Iteration Number: 8533\n",
      "Loss: 28.505804653106264\n",
      "l2 norm of gradients: 0.18029354166814707\n",
      "l2 norm of weights: 5.651468868225758\n",
      "---------------------\n",
      "Iteration Number: 8534\n",
      "Loss: 28.504174734229288\n",
      "l2 norm of gradients: 0.1802793562074994\n",
      "l2 norm of weights: 5.65140351990722\n",
      "---------------------\n",
      "Iteration Number: 8535\n",
      "Loss: 28.502545071495327\n",
      "l2 norm of gradients: 0.18026517328200303\n",
      "l2 norm of weights: 5.651338176364579\n",
      "---------------------\n",
      "Iteration Number: 8536\n",
      "Loss: 28.500915664837727\n",
      "l2 norm of gradients: 0.18025099289078547\n",
      "l2 norm of weights: 5.651272837597194\n",
      "---------------------\n",
      "Iteration Number: 8537\n",
      "Loss: 28.499286514189624\n",
      "l2 norm of gradients: 0.18023681503297476\n",
      "l2 norm of weights: 5.65120750360442\n",
      "---------------------\n",
      "Iteration Number: 8538\n",
      "Loss: 28.497657619489154\n",
      "l2 norm of gradients: 0.18022263970769917\n",
      "l2 norm of weights: 5.651142174385616\n",
      "---------------------\n",
      "Iteration Number: 8539\n",
      "Loss: 28.49602898066346\n",
      "l2 norm of gradients: 0.1802084669140879\n",
      "l2 norm of weights: 5.65107684994014\n",
      "---------------------\n",
      "Iteration Number: 8540\n",
      "Loss: 28.494400597652575\n",
      "l2 norm of gradients: 0.18019429665127024\n",
      "l2 norm of weights: 5.651011530267349\n",
      "---------------------\n",
      "Iteration Number: 8541\n",
      "Loss: 28.492772470386612\n",
      "l2 norm of gradients: 0.18018012891837634\n",
      "l2 norm of weights: 5.650946215366601\n",
      "---------------------\n",
      "Iteration Number: 8542\n",
      "Loss: 28.491144598801228\n",
      "l2 norm of gradients: 0.18016596371453658\n",
      "l2 norm of weights: 5.650880905237254\n",
      "---------------------\n",
      "Iteration Number: 8543\n",
      "Loss: 28.489516982826913\n",
      "l2 norm of gradients: 0.18015180103888187\n",
      "l2 norm of weights: 5.6508155998786656\n",
      "---------------------\n",
      "Iteration Number: 8544\n",
      "Loss: 28.487889622401546\n",
      "l2 norm of gradients: 0.18013764089054377\n",
      "l2 norm of weights: 5.650750299290196\n",
      "---------------------\n",
      "Iteration Number: 8545\n",
      "Loss: 28.486262517458574\n",
      "l2 norm of gradients: 0.18012348326865427\n",
      "l2 norm of weights: 5.6506850034712\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 8546\n",
      "Loss: 28.48463566793038\n",
      "l2 norm of gradients: 0.1801093281723457\n",
      "l2 norm of weights: 5.650619712421041\n",
      "---------------------\n",
      "Iteration Number: 8547\n",
      "Loss: 28.48300907375083\n",
      "l2 norm of gradients: 0.18009517560075106\n",
      "l2 norm of weights: 5.650554426139074\n",
      "---------------------\n",
      "Iteration Number: 8548\n",
      "Loss: 28.481382734853774\n",
      "l2 norm of gradients: 0.18008102555300387\n",
      "l2 norm of weights: 5.650489144624659\n",
      "---------------------\n",
      "Iteration Number: 8549\n",
      "Loss: 28.479756651175265\n",
      "l2 norm of gradients: 0.180066878028238\n",
      "l2 norm of weights: 5.650423867877157\n",
      "---------------------\n",
      "Iteration Number: 8550\n",
      "Loss: 28.478130822646012\n",
      "l2 norm of gradients: 0.18005273302558794\n",
      "l2 norm of weights: 5.6503585958959235\n",
      "---------------------\n",
      "Iteration Number: 8551\n",
      "Loss: 28.47650524920451\n",
      "l2 norm of gradients: 0.18003859054418858\n",
      "l2 norm of weights: 5.65029332868032\n",
      "---------------------\n",
      "Iteration Number: 8552\n",
      "Loss: 28.47487993078083\n",
      "l2 norm of gradients: 0.18002445058317545\n",
      "l2 norm of weights: 5.650228066229706\n",
      "---------------------\n",
      "Iteration Number: 8553\n",
      "Loss: 28.47325486730926\n",
      "l2 norm of gradients: 0.18001031314168436\n",
      "l2 norm of weights: 5.650162808543442\n",
      "---------------------\n",
      "Iteration Number: 8554\n",
      "Loss: 28.4716300587275\n",
      "l2 norm of gradients: 0.17999617821885178\n",
      "l2 norm of weights: 5.650097555620886\n",
      "---------------------\n",
      "Iteration Number: 8555\n",
      "Loss: 28.47000550496747\n",
      "l2 norm of gradients: 0.1799820458138146\n",
      "l2 norm of weights: 5.650032307461399\n",
      "---------------------\n",
      "Iteration Number: 8556\n",
      "Loss: 28.468381205961123\n",
      "l2 norm of gradients: 0.17996791592571035\n",
      "l2 norm of weights: 5.649967064064341\n",
      "---------------------\n",
      "Iteration Number: 8557\n",
      "Loss: 28.466757161646196\n",
      "l2 norm of gradients: 0.17995378855367672\n",
      "l2 norm of weights: 5.649901825429071\n",
      "---------------------\n",
      "Iteration Number: 8558\n",
      "Loss: 28.465133371956984\n",
      "l2 norm of gradients: 0.1799396636968523\n",
      "l2 norm of weights: 5.649836591554951\n",
      "---------------------\n",
      "Iteration Number: 8559\n",
      "Loss: 28.463509836826812\n",
      "l2 norm of gradients: 0.17992554135437586\n",
      "l2 norm of weights: 5.649771362441341\n",
      "---------------------\n",
      "Iteration Number: 8560\n",
      "Loss: 28.46188655618736\n",
      "l2 norm of gradients: 0.17991142152538683\n",
      "l2 norm of weights: 5.649706138087601\n",
      "---------------------\n",
      "Iteration Number: 8561\n",
      "Loss: 28.460263529975812\n",
      "l2 norm of gradients: 0.1798973042090251\n",
      "l2 norm of weights: 5.649640918493093\n",
      "---------------------\n",
      "Iteration Number: 8562\n",
      "Loss: 28.458640758125586\n",
      "l2 norm of gradients: 0.17988318940443096\n",
      "l2 norm of weights: 5.6495757036571765\n",
      "---------------------\n",
      "Iteration Number: 8563\n",
      "Loss: 28.457018240572353\n",
      "l2 norm of gradients: 0.17986907711074535\n",
      "l2 norm of weights: 5.649510493579215\n",
      "---------------------\n",
      "Iteration Number: 8564\n",
      "Loss: 28.455395977249392\n",
      "l2 norm of gradients: 0.1798549673271096\n",
      "l2 norm of weights: 5.649445288258567\n",
      "---------------------\n",
      "Iteration Number: 8565\n",
      "Loss: 28.453773968091063\n",
      "l2 norm of gradients: 0.17984086005266547\n",
      "l2 norm of weights: 5.649380087694595\n",
      "---------------------\n",
      "Iteration Number: 8566\n",
      "Loss: 28.452152213032495\n",
      "l2 norm of gradients: 0.17982675528655537\n",
      "l2 norm of weights: 5.649314891886662\n",
      "---------------------\n",
      "Iteration Number: 8567\n",
      "Loss: 28.450530712005907\n",
      "l2 norm of gradients: 0.17981265302792207\n",
      "l2 norm of weights: 5.649249700834127\n",
      "---------------------\n",
      "Iteration Number: 8568\n",
      "Loss: 28.448909464949203\n",
      "l2 norm of gradients: 0.17979855327590893\n",
      "l2 norm of weights: 5.649184514536354\n",
      "---------------------\n",
      "Iteration Number: 8569\n",
      "Loss: 28.44728847179269\n",
      "l2 norm of gradients: 0.17978445602965967\n",
      "l2 norm of weights: 5.649119332992704\n",
      "---------------------\n",
      "Iteration Number: 8570\n",
      "Loss: 28.445667732477425\n",
      "l2 norm of gradients: 0.17977036128831864\n",
      "l2 norm of weights: 5.649054156202539\n",
      "---------------------\n",
      "Iteration Number: 8571\n",
      "Loss: 28.44404724693128\n",
      "l2 norm of gradients: 0.17975626905103068\n",
      "l2 norm of weights: 5.648988984165222\n",
      "---------------------\n",
      "Iteration Number: 8572\n",
      "Loss: 28.442427015093678\n",
      "l2 norm of gradients: 0.17974217931694086\n",
      "l2 norm of weights: 5.648923816880115\n",
      "---------------------\n",
      "Iteration Number: 8573\n",
      "Loss: 28.440807036896476\n",
      "l2 norm of gradients: 0.17972809208519513\n",
      "l2 norm of weights: 5.64885865434658\n",
      "---------------------\n",
      "Iteration Number: 8574\n",
      "Loss: 28.43918731227395\n",
      "l2 norm of gradients: 0.17971400735493961\n",
      "l2 norm of weights: 5.6487934965639806\n",
      "---------------------\n",
      "Iteration Number: 8575\n",
      "Loss: 28.43756784116268\n",
      "l2 norm of gradients: 0.17969992512532107\n",
      "l2 norm of weights: 5.64872834353168\n",
      "---------------------\n",
      "Iteration Number: 8576\n",
      "Loss: 28.435948623496763\n",
      "l2 norm of gradients: 0.17968584539548674\n",
      "l2 norm of weights: 5.648663195249039\n",
      "---------------------\n",
      "Iteration Number: 8577\n",
      "Loss: 28.434329659211304\n",
      "l2 norm of gradients: 0.17967176816458427\n",
      "l2 norm of weights: 5.648598051715424\n",
      "---------------------\n",
      "Iteration Number: 8578\n",
      "Loss: 28.4327109482404\n",
      "l2 norm of gradients: 0.1796576934317619\n",
      "l2 norm of weights: 5.648532912930196\n",
      "---------------------\n",
      "Iteration Number: 8579\n",
      "Loss: 28.431092490518346\n",
      "l2 norm of gradients: 0.17964362119616828\n",
      "l2 norm of weights: 5.6484677788927184\n",
      "---------------------\n",
      "Iteration Number: 8580\n",
      "Loss: 28.429474285981527\n",
      "l2 norm of gradients: 0.17962955145695259\n",
      "l2 norm of weights: 5.648402649602357\n",
      "---------------------\n",
      "Iteration Number: 8581\n",
      "Loss: 28.427856334561632\n",
      "l2 norm of gradients: 0.17961548421326448\n",
      "l2 norm of weights: 5.648337525058473\n",
      "---------------------\n",
      "Iteration Number: 8582\n",
      "Loss: 28.426238636197976\n",
      "l2 norm of gradients: 0.17960141946425406\n",
      "l2 norm of weights: 5.648272405260431\n",
      "---------------------\n",
      "Iteration Number: 8583\n",
      "Loss: 28.424621190824514\n",
      "l2 norm of gradients: 0.17958735720907198\n",
      "l2 norm of weights: 5.648207290207595\n",
      "---------------------\n",
      "Iteration Number: 8584\n",
      "Loss: 28.423003998372664\n",
      "l2 norm of gradients: 0.17957329744686928\n",
      "l2 norm of weights: 5.64814217989933\n",
      "---------------------\n",
      "Iteration Number: 8585\n",
      "Loss: 28.42138705878022\n",
      "l2 norm of gradients: 0.17955924017679764\n",
      "l2 norm of weights: 5.648077074335\n",
      "---------------------\n",
      "Iteration Number: 8586\n",
      "Loss: 28.419770371981834\n",
      "l2 norm of gradients: 0.1795451853980091\n",
      "l2 norm of weights: 5.648011973513968\n",
      "---------------------\n",
      "Iteration Number: 8587\n",
      "Loss: 28.418153937913413\n",
      "l2 norm of gradients: 0.17953113310965615\n",
      "l2 norm of weights: 5.647946877435602\n",
      "---------------------\n",
      "Iteration Number: 8588\n",
      "Loss: 28.416537756507367\n",
      "l2 norm of gradients: 0.1795170833108919\n",
      "l2 norm of weights: 5.647881786099263\n",
      "---------------------\n",
      "Iteration Number: 8589\n",
      "Loss: 28.414921827701107\n",
      "l2 norm of gradients: 0.17950303600086984\n",
      "l2 norm of weights: 5.647816699504317\n",
      "---------------------\n",
      "Iteration Number: 8590\n",
      "Loss: 28.413306151426823\n",
      "l2 norm of gradients: 0.179488991178744\n",
      "l2 norm of weights: 5.647751617650131\n",
      "---------------------\n",
      "Iteration Number: 8591\n",
      "Loss: 28.411690727624666\n",
      "l2 norm of gradients: 0.17947494884366888\n",
      "l2 norm of weights: 5.647686540536068\n",
      "---------------------\n",
      "Iteration Number: 8592\n",
      "Loss: 28.410075556225436\n",
      "l2 norm of gradients: 0.17946090899479944\n",
      "l2 norm of weights: 5.647621468161494\n",
      "---------------------\n",
      "Iteration Number: 8593\n",
      "Loss: 28.408460637165984\n",
      "l2 norm of gradients: 0.17944687163129114\n",
      "l2 norm of weights: 5.647556400525774\n",
      "---------------------\n",
      "Iteration Number: 8594\n",
      "Loss: 28.40684597037942\n",
      "l2 norm of gradients: 0.17943283675229985\n",
      "l2 norm of weights: 5.647491337628274\n",
      "---------------------\n",
      "Iteration Number: 8595\n",
      "Loss: 28.405231555804843\n",
      "l2 norm of gradients: 0.17941880435698204\n",
      "l2 norm of weights: 5.647426279468361\n",
      "---------------------\n",
      "Iteration Number: 8596\n",
      "Loss: 28.403617393373583\n",
      "l2 norm of gradients: 0.17940477444449465\n",
      "l2 norm of weights: 5.647361226045398\n",
      "---------------------\n",
      "Iteration Number: 8597\n",
      "Loss: 28.402003483022593\n",
      "l2 norm of gradients: 0.17939074701399496\n",
      "l2 norm of weights: 5.647296177358754\n",
      "---------------------\n",
      "Iteration Number: 8598\n",
      "Loss: 28.40038982468795\n",
      "l2 norm of gradients: 0.1793767220646409\n",
      "l2 norm of weights: 5.647231133407794\n",
      "---------------------\n",
      "Iteration Number: 8599\n",
      "Loss: 28.398776418305413\n",
      "l2 norm of gradients: 0.17936269959559079\n",
      "l2 norm of weights: 5.647166094191884\n",
      "---------------------\n",
      "Iteration Number: 8600\n",
      "Loss: 28.397163263807535\n",
      "l2 norm of gradients: 0.17934867960600342\n",
      "l2 norm of weights: 5.647101059710391\n",
      "---------------------\n",
      "Iteration Number: 8601\n",
      "Loss: 28.39555036113279\n",
      "l2 norm of gradients: 0.17933466209503812\n",
      "l2 norm of weights: 5.64703602996268\n",
      "---------------------\n",
      "Iteration Number: 8602\n",
      "Loss: 28.393937710212345\n",
      "l2 norm of gradients: 0.17932064706185472\n",
      "l2 norm of weights: 5.646971004948121\n",
      "---------------------\n",
      "Iteration Number: 8603\n",
      "Loss: 28.392325310983615\n",
      "l2 norm of gradients: 0.17930663450561338\n",
      "l2 norm of weights: 5.646905984666079\n",
      "---------------------\n",
      "Iteration Number: 8604\n",
      "Loss: 28.390713163387158\n",
      "l2 norm of gradients: 0.1792926244254749\n",
      "l2 norm of weights: 5.646840969115921\n",
      "---------------------\n",
      "Iteration Number: 8605\n",
      "Loss: 28.389101267350583\n",
      "l2 norm of gradients: 0.17927861682060045\n",
      "l2 norm of weights: 5.646775958297013\n",
      "---------------------\n",
      "Iteration Number: 8606\n",
      "Loss: 28.387489622813515\n",
      "l2 norm of gradients: 0.17926461169015176\n",
      "l2 norm of weights: 5.646710952208726\n",
      "---------------------\n",
      "Iteration Number: 8607\n",
      "Loss: 28.38587822971073\n",
      "l2 norm of gradients: 0.17925060903329096\n",
      "l2 norm of weights: 5.646645950850423\n",
      "---------------------\n",
      "Iteration Number: 8608\n",
      "Loss: 28.384267087977324\n",
      "l2 norm of gradients: 0.17923660884918072\n",
      "l2 norm of weights: 5.646580954221475\n",
      "---------------------\n",
      "Iteration Number: 8609\n",
      "Loss: 28.382656197551462\n",
      "l2 norm of gradients: 0.17922261113698423\n",
      "l2 norm of weights: 5.646515962321249\n",
      "---------------------\n",
      "Iteration Number: 8610\n",
      "Loss: 28.381045558365777\n",
      "l2 norm of gradients: 0.1792086158958649\n",
      "l2 norm of weights: 5.646450975149112\n",
      "---------------------\n",
      "Iteration Number: 8611\n",
      "Loss: 28.37943517035446\n",
      "l2 norm of gradients: 0.179194623124987\n",
      "l2 norm of weights: 5.646385992704433\n",
      "---------------------\n",
      "Iteration Number: 8612\n",
      "Loss: 28.377825033456922\n",
      "l2 norm of gradients: 0.179180632823515\n",
      "l2 norm of weights: 5.646321014986579\n",
      "---------------------\n",
      "Iteration Number: 8613\n",
      "Loss: 28.37621514760957\n",
      "l2 norm of gradients: 0.17916664499061397\n",
      "l2 norm of weights: 5.64625604199492\n",
      "---------------------\n",
      "Iteration Number: 8614\n",
      "Loss: 28.37460551274178\n",
      "l2 norm of gradients: 0.17915265962544935\n",
      "l2 norm of weights: 5.646191073728823\n",
      "---------------------\n",
      "Iteration Number: 8615\n",
      "Loss: 28.37299612879661\n",
      "l2 norm of gradients: 0.17913867672718714\n",
      "l2 norm of weights: 5.646126110187658\n",
      "---------------------\n",
      "Iteration Number: 8616\n",
      "Loss: 28.371386995705528\n",
      "l2 norm of gradients: 0.1791246962949939\n",
      "l2 norm of weights: 5.646061151370793\n",
      "---------------------\n",
      "Iteration Number: 8617\n",
      "Loss: 28.36977811340363\n",
      "l2 norm of gradients: 0.17911071832803638\n",
      "l2 norm of weights: 5.645996197277595\n",
      "---------------------\n",
      "Iteration Number: 8618\n",
      "Loss: 28.368169481829405\n",
      "l2 norm of gradients: 0.1790967428254821\n",
      "l2 norm of weights: 5.645931247907438\n",
      "---------------------\n",
      "Iteration Number: 8619\n",
      "Loss: 28.366561100918318\n",
      "l2 norm of gradients: 0.1790827697864989\n",
      "l2 norm of weights: 5.6458663032596865\n",
      "---------------------\n",
      "Iteration Number: 8620\n",
      "Loss: 28.36495297060785\n",
      "l2 norm of gradients: 0.17906879921025512\n",
      "l2 norm of weights: 5.645801363333713\n",
      "---------------------\n",
      "Iteration Number: 8621\n",
      "Loss: 28.36334509082822\n",
      "l2 norm of gradients: 0.17905483109591963\n",
      "l2 norm of weights: 5.645736428128885\n",
      "---------------------\n",
      "Iteration Number: 8622\n",
      "Loss: 28.36173746151907\n",
      "l2 norm of gradients: 0.17904086544266168\n",
      "l2 norm of weights: 5.645671497644572\n",
      "---------------------\n",
      "Iteration Number: 8623\n",
      "Loss: 28.360130082617935\n",
      "l2 norm of gradients: 0.17902690224965107\n",
      "l2 norm of weights: 5.645606571880145\n",
      "---------------------\n",
      "Iteration Number: 8624\n",
      "Loss: 28.35852295405575\n",
      "l2 norm of gradients: 0.17901294151605804\n",
      "l2 norm of weights: 5.645541650834974\n",
      "---------------------\n",
      "Iteration Number: 8625\n",
      "Loss: 28.35691607577433\n",
      "l2 norm of gradients: 0.17899898324105332\n",
      "l2 norm of weights: 5.645476734508429\n",
      "---------------------\n",
      "Iteration Number: 8626\n",
      "Loss: 28.355309447704773\n",
      "l2 norm of gradients: 0.17898502742380804\n",
      "l2 norm of weights: 5.645411822899878\n",
      "---------------------\n",
      "Iteration Number: 8627\n",
      "Loss: 28.353703069786043\n",
      "l2 norm of gradients: 0.17897107406349388\n",
      "l2 norm of weights: 5.645346916008695\n",
      "---------------------\n",
      "Iteration Number: 8628\n",
      "Loss: 28.352096941954112\n",
      "l2 norm of gradients: 0.178957123159283\n",
      "l2 norm of weights: 5.645282013834247\n",
      "---------------------\n",
      "Iteration Number: 8629\n",
      "Loss: 28.350491064144272\n",
      "l2 norm of gradients: 0.1789431747103479\n",
      "l2 norm of weights: 5.645217116375908\n",
      "---------------------\n",
      "Iteration Number: 8630\n",
      "Loss: 28.348885436293358\n",
      "l2 norm of gradients: 0.1789292287158618\n",
      "l2 norm of weights: 5.645152223633046\n",
      "---------------------\n",
      "Iteration Number: 8631\n",
      "Loss: 28.34728005833615\n",
      "l2 norm of gradients: 0.1789152851749981\n",
      "l2 norm of weights: 5.645087335605033\n",
      "---------------------\n",
      "Iteration Number: 8632\n",
      "Loss: 28.345674930210166\n",
      "l2 norm of gradients: 0.17890134408693092\n",
      "l2 norm of weights: 5.64502245229124\n",
      "---------------------\n",
      "Iteration Number: 8633\n",
      "Loss: 28.34407005184827\n",
      "l2 norm of gradients: 0.17888740545083467\n",
      "l2 norm of weights: 5.6449575736910385\n",
      "---------------------\n",
      "Iteration Number: 8634\n",
      "Loss: 28.34246542319034\n",
      "l2 norm of gradients: 0.17887346926588432\n",
      "l2 norm of weights: 5.644892699803799\n",
      "---------------------\n",
      "Iteration Number: 8635\n",
      "Loss: 28.34086104417414\n",
      "l2 norm of gradients: 0.1788595355312553\n",
      "l2 norm of weights: 5.644827830628895\n",
      "---------------------\n",
      "Iteration Number: 8636\n",
      "Loss: 28.33925691472931\n",
      "l2 norm of gradients: 0.1788456042461235\n",
      "l2 norm of weights: 5.644762966165696\n",
      "---------------------\n",
      "Iteration Number: 8637\n",
      "Loss: 28.337653034799764\n",
      "l2 norm of gradients: 0.17883167540966521\n",
      "l2 norm of weights: 5.644698106413574\n",
      "---------------------\n",
      "Iteration Number: 8638\n",
      "Loss: 28.336049404316885\n",
      "l2 norm of gradients: 0.1788177490210573\n",
      "l2 norm of weights: 5.644633251371902\n",
      "---------------------\n",
      "Iteration Number: 8639\n",
      "Loss: 28.334446023217694\n",
      "l2 norm of gradients: 0.1788038250794771\n",
      "l2 norm of weights: 5.6445684010400505\n",
      "---------------------\n",
      "Iteration Number: 8640\n",
      "Loss: 28.332842891438702\n",
      "l2 norm of gradients: 0.17878990358410224\n",
      "l2 norm of weights: 5.644503555417394\n",
      "---------------------\n",
      "Iteration Number: 8641\n",
      "Loss: 28.33124000891898\n",
      "l2 norm of gradients: 0.17877598453411112\n",
      "l2 norm of weights: 5.644438714503304\n",
      "---------------------\n",
      "Iteration Number: 8642\n",
      "Loss: 28.329637375591503\n",
      "l2 norm of gradients: 0.17876206792868224\n",
      "l2 norm of weights: 5.644373878297151\n",
      "---------------------\n",
      "Iteration Number: 8643\n",
      "Loss: 28.328034991393324\n",
      "l2 norm of gradients: 0.17874815376699493\n",
      "l2 norm of weights: 5.64430904679831\n",
      "---------------------\n",
      "Iteration Number: 8644\n",
      "Loss: 28.32643285626111\n",
      "l2 norm of gradients: 0.17873424204822871\n",
      "l2 norm of weights: 5.6442442200061524\n",
      "---------------------\n",
      "Iteration Number: 8645\n",
      "Loss: 28.324830970135007\n",
      "l2 norm of gradients: 0.17872033277156368\n",
      "l2 norm of weights: 5.644179397920053\n",
      "---------------------\n",
      "Iteration Number: 8646\n",
      "Loss: 28.323229332946596\n",
      "l2 norm of gradients: 0.17870642593618044\n",
      "l2 norm of weights: 5.644114580539382\n",
      "---------------------\n",
      "Iteration Number: 8647\n",
      "Loss: 28.32162794463258\n",
      "l2 norm of gradients: 0.17869252154125995\n",
      "l2 norm of weights: 5.644049767863515\n",
      "---------------------\n",
      "Iteration Number: 8648\n",
      "Loss: 28.320026805130226\n",
      "l2 norm of gradients: 0.17867861958598374\n",
      "l2 norm of weights: 5.643984959891824\n",
      "---------------------\n",
      "Iteration Number: 8649\n",
      "Loss: 28.318425914379805\n",
      "l2 norm of gradients: 0.1786647200695337\n",
      "l2 norm of weights: 5.643920156623683\n",
      "---------------------\n",
      "Iteration Number: 8650\n",
      "Loss: 28.316825272314187\n",
      "l2 norm of gradients: 0.1786508229910923\n",
      "l2 norm of weights: 5.643855358058467\n",
      "---------------------\n",
      "Iteration Number: 8651\n",
      "Loss: 28.315224878870996\n",
      "l2 norm of gradients: 0.1786369283498424\n",
      "l2 norm of weights: 5.643790564195546\n",
      "---------------------\n",
      "Iteration Number: 8652\n",
      "Loss: 28.313624733986153\n",
      "l2 norm of gradients: 0.17862303614496733\n",
      "l2 norm of weights: 5.643725775034298\n",
      "---------------------\n",
      "Iteration Number: 8653\n",
      "Loss: 28.312024837596994\n",
      "l2 norm of gradients: 0.1786091463756509\n",
      "l2 norm of weights: 5.643660990574094\n",
      "---------------------\n",
      "Iteration Number: 8654\n",
      "Loss: 28.31042518964058\n",
      "l2 norm of gradients: 0.1785952590410774\n",
      "l2 norm of weights: 5.643596210814311\n",
      "---------------------\n",
      "Iteration Number: 8655\n",
      "Loss: 28.308825790053234\n",
      "l2 norm of gradients: 0.1785813741404315\n",
      "l2 norm of weights: 5.643531435754321\n",
      "---------------------\n",
      "Iteration Number: 8656\n",
      "Loss: 28.307226638771688\n",
      "l2 norm of gradients: 0.17856749167289845\n",
      "l2 norm of weights: 5.6434666653934995\n",
      "---------------------\n",
      "Iteration Number: 8657\n",
      "Loss: 28.305627735730816\n",
      "l2 norm of gradients: 0.17855361163766387\n",
      "l2 norm of weights: 5.643401899731221\n",
      "---------------------\n",
      "Iteration Number: 8658\n",
      "Loss: 28.304029080871338\n",
      "l2 norm of gradients: 0.17853973403391385\n",
      "l2 norm of weights: 5.64333713876686\n",
      "---------------------\n",
      "Iteration Number: 8659\n",
      "Loss: 28.302430674128594\n",
      "l2 norm of gradients: 0.17852585886083505\n",
      "l2 norm of weights: 5.6432723824997915\n",
      "---------------------\n",
      "Iteration Number: 8660\n",
      "Loss: 28.30083251543701\n",
      "l2 norm of gradients: 0.17851198611761449\n",
      "l2 norm of weights: 5.64320763092939\n",
      "---------------------\n",
      "Iteration Number: 8661\n",
      "Loss: 28.299234604736817\n",
      "l2 norm of gradients: 0.1784981158034396\n",
      "l2 norm of weights: 5.643142884055032\n",
      "---------------------\n",
      "Iteration Number: 8662\n",
      "Loss: 28.29763694196287\n",
      "l2 norm of gradients: 0.17848424791749837\n",
      "l2 norm of weights: 5.643078141876092\n",
      "---------------------\n",
      "Iteration Number: 8663\n",
      "Loss: 28.29603952705475\n",
      "l2 norm of gradients: 0.17847038245897914\n",
      "l2 norm of weights: 5.643013404391946\n",
      "---------------------\n",
      "Iteration Number: 8664\n",
      "Loss: 28.294442359945315\n",
      "l2 norm of gradients: 0.17845651942707094\n",
      "l2 norm of weights: 5.642948671601968\n",
      "---------------------\n",
      "Iteration Number: 8665\n",
      "Loss: 28.292845440574705\n",
      "l2 norm of gradients: 0.17844265882096302\n",
      "l2 norm of weights: 5.642883943505537\n",
      "---------------------\n",
      "Iteration Number: 8666\n",
      "Loss: 28.2912487688786\n",
      "l2 norm of gradients: 0.1784288006398452\n",
      "l2 norm of weights: 5.642819220102026\n",
      "---------------------\n",
      "Iteration Number: 8667\n",
      "Loss: 28.289652344793392\n",
      "l2 norm of gradients: 0.17841494488290768\n",
      "l2 norm of weights: 5.642754501390811\n",
      "---------------------\n",
      "Iteration Number: 8668\n",
      "Loss: 28.288056168257494\n",
      "l2 norm of gradients: 0.17840109154934133\n",
      "l2 norm of weights: 5.642689787371269\n",
      "---------------------\n",
      "Iteration Number: 8669\n",
      "Loss: 28.286460239207265\n",
      "l2 norm of gradients: 0.17838724063833714\n",
      "l2 norm of weights: 5.6426250780427765\n",
      "---------------------\n",
      "Iteration Number: 8670\n",
      "Loss: 28.284864557580025\n",
      "l2 norm of gradients: 0.17837339214908682\n",
      "l2 norm of weights: 5.64256037340471\n",
      "---------------------\n",
      "Iteration Number: 8671\n",
      "Loss: 28.283269123313723\n",
      "l2 norm of gradients: 0.17835954608078242\n",
      "l2 norm of weights: 5.6424956734564455\n",
      "---------------------\n",
      "Iteration Number: 8672\n",
      "Loss: 28.281673936344003\n",
      "l2 norm of gradients: 0.17834570243261655\n",
      "l2 norm of weights: 5.6424309781973605\n",
      "---------------------\n",
      "Iteration Number: 8673\n",
      "Loss: 28.280078996608875\n",
      "l2 norm of gradients: 0.17833186120378217\n",
      "l2 norm of weights: 5.642366287626831\n",
      "---------------------\n",
      "Iteration Number: 8674\n",
      "Loss: 28.27848430404615\n",
      "l2 norm of gradients: 0.17831802239347272\n",
      "l2 norm of weights: 5.642301601744234\n",
      "---------------------\n",
      "Iteration Number: 8675\n",
      "Loss: 28.276889858591243\n",
      "l2 norm of gradients: 0.17830418600088216\n",
      "l2 norm of weights: 5.642236920548948\n",
      "---------------------\n",
      "Iteration Number: 8676\n",
      "Loss: 28.275295660181484\n",
      "l2 norm of gradients: 0.1782903520252048\n",
      "l2 norm of weights: 5.642172244040348\n",
      "---------------------\n",
      "Iteration Number: 8677\n",
      "Loss: 28.273701708759475\n",
      "l2 norm of gradients: 0.1782765204656355\n",
      "l2 norm of weights: 5.642107572217814\n",
      "---------------------\n",
      "Iteration Number: 8678\n",
      "Loss: 28.272108004254434\n",
      "l2 norm of gradients: 0.1782626913213696\n",
      "l2 norm of weights: 5.642042905080722\n",
      "---------------------\n",
      "Iteration Number: 8679\n",
      "Loss: 28.270514546608457\n",
      "l2 norm of gradients: 0.17824886459160275\n",
      "l2 norm of weights: 5.64197824262845\n",
      "---------------------\n",
      "Iteration Number: 8680\n",
      "Loss: 28.268921335757337\n",
      "l2 norm of gradients: 0.17823504027553116\n",
      "l2 norm of weights: 5.641913584860376\n",
      "---------------------\n",
      "Iteration Number: 8681\n",
      "Loss: 28.267328371640634\n",
      "l2 norm of gradients: 0.17822121837235155\n",
      "l2 norm of weights: 5.641848931775877\n",
      "---------------------\n",
      "Iteration Number: 8682\n",
      "Loss: 28.2657356541926\n",
      "l2 norm of gradients: 0.1782073988812609\n",
      "l2 norm of weights: 5.641784283374333\n",
      "---------------------\n",
      "Iteration Number: 8683\n",
      "Loss: 28.264143183353212\n",
      "l2 norm of gradients: 0.1781935818014568\n",
      "l2 norm of weights: 5.641719639655122\n",
      "---------------------\n",
      "Iteration Number: 8684\n",
      "Loss: 28.262550959057872\n",
      "l2 norm of gradients: 0.17817976713213735\n",
      "l2 norm of weights: 5.64165500061762\n",
      "---------------------\n",
      "Iteration Number: 8685\n",
      "Loss: 28.26095898124678\n",
      "l2 norm of gradients: 0.1781659548725009\n",
      "l2 norm of weights: 5.641590366261208\n",
      "---------------------\n",
      "Iteration Number: 8686\n",
      "Loss: 28.259367249854442\n",
      "l2 norm of gradients: 0.1781521450217464\n",
      "l2 norm of weights: 5.641525736585264\n",
      "---------------------\n",
      "Iteration Number: 8687\n",
      "Loss: 28.257775764819268\n",
      "l2 norm of gradients: 0.17813833757907319\n",
      "l2 norm of weights: 5.641461111589166\n",
      "---------------------\n",
      "Iteration Number: 8688\n",
      "Loss: 28.2561845260806\n",
      "l2 norm of gradients: 0.17812453254368116\n",
      "l2 norm of weights: 5.641396491272295\n",
      "---------------------\n",
      "Iteration Number: 8689\n",
      "Loss: 28.25459353357421\n",
      "l2 norm of gradients: 0.17811072991477048\n",
      "l2 norm of weights: 5.641331875634028\n",
      "---------------------\n",
      "Iteration Number: 8690\n",
      "Loss: 28.253002787238486\n",
      "l2 norm of gradients: 0.17809692969154195\n",
      "l2 norm of weights: 5.641267264673745\n",
      "---------------------\n",
      "Iteration Number: 8691\n",
      "Loss: 28.251412287009384\n",
      "l2 norm of gradients: 0.17808313187319674\n",
      "l2 norm of weights: 5.641202658390825\n",
      "---------------------\n",
      "Iteration Number: 8692\n",
      "Loss: 28.24982203282689\n",
      "l2 norm of gradients: 0.1780693364589364\n",
      "l2 norm of weights: 5.641138056784649\n",
      "---------------------\n",
      "Iteration Number: 8693\n",
      "Loss: 28.24823202462719\n",
      "l2 norm of gradients: 0.17805554344796307\n",
      "l2 norm of weights: 5.641073459854595\n",
      "---------------------\n",
      "Iteration Number: 8694\n",
      "Loss: 28.246642262349233\n",
      "l2 norm of gradients: 0.17804175283947926\n",
      "l2 norm of weights: 5.641008867600044\n",
      "---------------------\n",
      "Iteration Number: 8695\n",
      "Loss: 28.245052745928284\n",
      "l2 norm of gradients: 0.17802796463268794\n",
      "l2 norm of weights: 5.640944280020374\n",
      "---------------------\n",
      "Iteration Number: 8696\n",
      "Loss: 28.24346347530583\n",
      "l2 norm of gradients: 0.17801417882679252\n",
      "l2 norm of weights: 5.6408796971149675\n",
      "---------------------\n",
      "Iteration Number: 8697\n",
      "Loss: 28.241874450415285\n",
      "l2 norm of gradients: 0.1780003954209969\n",
      "l2 norm of weights: 5.640815118883204\n",
      "---------------------\n",
      "Iteration Number: 8698\n",
      "Loss: 28.240285671198496\n",
      "l2 norm of gradients: 0.1779866144145054\n",
      "l2 norm of weights: 5.640750545324463\n",
      "---------------------\n",
      "Iteration Number: 8699\n",
      "Loss: 28.238697137590954\n",
      "l2 norm of gradients: 0.1779728358065228\n",
      "l2 norm of weights: 5.640685976438125\n",
      "---------------------\n",
      "Iteration Number: 8700\n",
      "Loss: 28.23710884953177\n",
      "l2 norm of gradients: 0.1779590595962542\n",
      "l2 norm of weights: 5.640621412223572\n",
      "---------------------\n",
      "Iteration Number: 8701\n",
      "Loss: 28.235520806956348\n",
      "l2 norm of gradients: 0.17794528578290542\n",
      "l2 norm of weights: 5.6405568526801835\n",
      "---------------------\n",
      "Iteration Number: 8702\n",
      "Loss: 28.233933009805344\n",
      "l2 norm of gradients: 0.17793151436568252\n",
      "l2 norm of weights: 5.640492297807341\n",
      "---------------------\n",
      "Iteration Number: 8703\n",
      "Loss: 28.232345458016354\n",
      "l2 norm of gradients: 0.1779177453437921\n",
      "l2 norm of weights: 5.6404277476044244\n",
      "---------------------\n",
      "Iteration Number: 8704\n",
      "Loss: 28.230758151525766\n",
      "l2 norm of gradients: 0.1779039787164411\n",
      "l2 norm of weights: 5.640363202070818\n",
      "---------------------\n",
      "Iteration Number: 8705\n",
      "Loss: 28.22917109027283\n",
      "l2 norm of gradients: 0.17789021448283698\n",
      "l2 norm of weights: 5.640298661205899\n",
      "---------------------\n",
      "Iteration Number: 8706\n",
      "Loss: 28.22758427419459\n",
      "l2 norm of gradients: 0.1778764526421877\n",
      "l2 norm of weights: 5.640234125009052\n",
      "---------------------\n",
      "Iteration Number: 8707\n",
      "Loss: 28.22599770322938\n",
      "l2 norm of gradients: 0.1778626931937016\n",
      "l2 norm of weights: 5.640169593479658\n",
      "---------------------\n",
      "Iteration Number: 8708\n",
      "Loss: 28.22441137731468\n",
      "l2 norm of gradients: 0.17784893613658742\n",
      "l2 norm of weights: 5.640105066617097\n",
      "---------------------\n",
      "Iteration Number: 8709\n",
      "Loss: 28.222825296391058\n",
      "l2 norm of gradients: 0.17783518147005445\n",
      "l2 norm of weights: 5.640040544420753\n",
      "---------------------\n",
      "Iteration Number: 8710\n",
      "Loss: 28.22123946039363\n",
      "l2 norm of gradients: 0.17782142919331237\n",
      "l2 norm of weights: 5.6399760268900065\n",
      "---------------------\n",
      "Iteration Number: 8711\n",
      "Loss: 28.2196538692621\n",
      "l2 norm of gradients: 0.1778076793055713\n",
      "l2 norm of weights: 5.639911514024241\n",
      "---------------------\n",
      "Iteration Number: 8712\n",
      "Loss: 28.218068522932658\n",
      "l2 norm of gradients: 0.17779393180604178\n",
      "l2 norm of weights: 5.639847005822837\n",
      "---------------------\n",
      "Iteration Number: 8713\n",
      "Loss: 28.216483421346375\n",
      "l2 norm of gradients: 0.1777801866939349\n",
      "l2 norm of weights: 5.639782502285179\n",
      "---------------------\n",
      "Iteration Number: 8714\n",
      "Loss: 28.21489856444077\n",
      "l2 norm of gradients: 0.17776644396846208\n",
      "l2 norm of weights: 5.639718003410649\n",
      "---------------------\n",
      "Iteration Number: 8715\n",
      "Loss: 28.213313952152046\n",
      "l2 norm of gradients: 0.17775270362883522\n",
      "l2 norm of weights: 5.639653509198628\n",
      "---------------------\n",
      "Iteration Number: 8716\n",
      "Loss: 28.211729584420958\n",
      "l2 norm of gradients: 0.17773896567426667\n",
      "l2 norm of weights: 5.639589019648501\n",
      "---------------------\n",
      "Iteration Number: 8717\n",
      "Loss: 28.210145461183618\n",
      "l2 norm of gradients: 0.1777252301039693\n",
      "l2 norm of weights: 5.63952453475965\n",
      "---------------------\n",
      "Iteration Number: 8718\n",
      "Loss: 28.208561582379943\n",
      "l2 norm of gradients: 0.17771149691715626\n",
      "l2 norm of weights: 5.639460054531458\n",
      "---------------------\n",
      "Iteration Number: 8719\n",
      "Loss: 28.206977947945976\n",
      "l2 norm of gradients: 0.17769776611304125\n",
      "l2 norm of weights: 5.63939557896331\n",
      "---------------------\n",
      "Iteration Number: 8720\n",
      "Loss: 28.205394557822927\n",
      "l2 norm of gradients: 0.17768403769083838\n",
      "l2 norm of weights: 5.639331108054586\n",
      "---------------------\n",
      "Iteration Number: 8721\n",
      "Loss: 28.2038114119465\n",
      "l2 norm of gradients: 0.17767031164976224\n",
      "l2 norm of weights: 5.639266641804673\n",
      "---------------------\n",
      "Iteration Number: 8722\n",
      "Loss: 28.202228510257996\n",
      "l2 norm of gradients: 0.17765658798902784\n",
      "l2 norm of weights: 5.639202180212952\n",
      "---------------------\n",
      "Iteration Number: 8723\n",
      "Loss: 28.20064585269262\n",
      "l2 norm of gradients: 0.17764286670785057\n",
      "l2 norm of weights: 5.639137723278809\n",
      "---------------------\n",
      "Iteration Number: 8724\n",
      "Loss: 28.199063439191672\n",
      "l2 norm of gradients: 0.1776291478054464\n",
      "l2 norm of weights: 5.639073271001627\n",
      "---------------------\n",
      "Iteration Number: 8725\n",
      "Loss: 28.197481269689764\n",
      "l2 norm of gradients: 0.17761543128103152\n",
      "l2 norm of weights: 5.63900882338079\n",
      "---------------------\n",
      "Iteration Number: 8726\n",
      "Loss: 28.19589934412936\n",
      "l2 norm of gradients: 0.17760171713382286\n",
      "l2 norm of weights: 5.6389443804156825\n",
      "---------------------\n",
      "Iteration Number: 8727\n",
      "Loss: 28.19431766244917\n",
      "l2 norm of gradients: 0.1775880053630375\n",
      "l2 norm of weights: 5.638879942105688\n",
      "---------------------\n",
      "Iteration Number: 8728\n",
      "Loss: 28.19273622458385\n",
      "l2 norm of gradients: 0.17757429596789315\n",
      "l2 norm of weights: 5.638815508450192\n",
      "---------------------\n",
      "Iteration Number: 8729\n",
      "Loss: 28.191155030473862\n",
      "l2 norm of gradients: 0.1775605889476079\n",
      "l2 norm of weights: 5.638751079448579\n",
      "---------------------\n",
      "Iteration Number: 8730\n",
      "Loss: 28.18957408005923\n",
      "l2 norm of gradients: 0.17754688430140023\n",
      "l2 norm of weights: 5.6386866551002335\n",
      "---------------------\n",
      "Iteration Number: 8731\n",
      "Loss: 28.18799337327554\n",
      "l2 norm of gradients: 0.17753318202848914\n",
      "l2 norm of weights: 5.63862223540454\n",
      "---------------------\n",
      "Iteration Number: 8732\n",
      "Loss: 28.18641291006495\n",
      "l2 norm of gradients: 0.17751948212809404\n",
      "l2 norm of weights: 5.6385578203608855\n",
      "---------------------\n",
      "Iteration Number: 8733\n",
      "Loss: 28.184832690362835\n",
      "l2 norm of gradients: 0.17750578459943472\n",
      "l2 norm of weights: 5.638493409968652\n",
      "---------------------\n",
      "Iteration Number: 8734\n",
      "Loss: 28.18325271410991\n",
      "l2 norm of gradients: 0.17749208944173148\n",
      "l2 norm of weights: 5.638429004227228\n",
      "---------------------\n",
      "Iteration Number: 8735\n",
      "Loss: 28.181672981242954\n",
      "l2 norm of gradients: 0.17747839665420503\n",
      "l2 norm of weights: 5.638364603135997\n",
      "---------------------\n",
      "Iteration Number: 8736\n",
      "Loss: 28.180093491705012\n",
      "l2 norm of gradients: 0.17746470623607652\n",
      "l2 norm of weights: 5.638300206694345\n",
      "---------------------\n",
      "Iteration Number: 8737\n",
      "Loss: 28.1785142454286\n",
      "l2 norm of gradients: 0.17745101818656756\n",
      "l2 norm of weights: 5.638235814901659\n",
      "---------------------\n",
      "Iteration Number: 8738\n",
      "Loss: 28.176935242357466\n",
      "l2 norm of gradients: 0.17743733250490015\n",
      "l2 norm of weights: 5.638171427757324\n",
      "---------------------\n",
      "Iteration Number: 8739\n",
      "Loss: 28.17535648242828\n",
      "l2 norm of gradients: 0.17742364919029674\n",
      "l2 norm of weights: 5.638107045260724\n",
      "---------------------\n",
      "Iteration Number: 8740\n",
      "Loss: 28.173777965581028\n",
      "l2 norm of gradients: 0.1774099682419803\n",
      "l2 norm of weights: 5.6380426674112485\n",
      "---------------------\n",
      "Iteration Number: 8741\n",
      "Loss: 28.172199691752816\n",
      "l2 norm of gradients: 0.17739628965917403\n",
      "l2 norm of weights: 5.637978294208281\n",
      "---------------------\n",
      "Iteration Number: 8742\n",
      "Loss: 28.170621660882706\n",
      "l2 norm of gradients: 0.17738261344110176\n",
      "l2 norm of weights: 5.637913925651211\n",
      "---------------------\n",
      "Iteration Number: 8743\n",
      "Loss: 28.169043872908905\n",
      "l2 norm of gradients: 0.1773689395869877\n",
      "l2 norm of weights: 5.637849561739422\n",
      "---------------------\n",
      "Iteration Number: 8744\n",
      "Loss: 28.16746632777499\n",
      "l2 norm of gradients: 0.17735526809605656\n",
      "l2 norm of weights: 5.637785202472302\n",
      "---------------------\n",
      "Iteration Number: 8745\n",
      "Loss: 28.16588902541379\n",
      "l2 norm of gradients: 0.1773415989675333\n",
      "l2 norm of weights: 5.637720847849239\n",
      "---------------------\n",
      "Iteration Number: 8746\n",
      "Loss: 28.164311965767162\n",
      "l2 norm of gradients: 0.1773279322006434\n",
      "l2 norm of weights: 5.637656497869617\n",
      "---------------------\n",
      "Iteration Number: 8747\n",
      "Loss: 28.162735148775663\n",
      "l2 norm of gradients: 0.17731426779461293\n",
      "l2 norm of weights: 5.637592152532826\n",
      "---------------------\n",
      "Iteration Number: 8748\n",
      "Loss: 28.161158574375932\n",
      "l2 norm of gradients: 0.17730060574866813\n",
      "l2 norm of weights: 5.637527811838252\n",
      "---------------------\n",
      "Iteration Number: 8749\n",
      "Loss: 28.15958224250504\n",
      "l2 norm of gradients: 0.1772869460620359\n",
      "l2 norm of weights: 5.6374634757852835\n",
      "---------------------\n",
      "Iteration Number: 8750\n",
      "Loss: 28.158006153107227\n",
      "l2 norm of gradients: 0.1772732887339434\n",
      "l2 norm of weights: 5.637399144373306\n",
      "---------------------\n",
      "Iteration Number: 8751\n",
      "Loss: 28.156430306118843\n",
      "l2 norm of gradients: 0.17725963376361834\n",
      "l2 norm of weights: 5.637334817601708\n",
      "---------------------\n",
      "Iteration Number: 8752\n",
      "Loss: 28.154854701476324\n",
      "l2 norm of gradients: 0.1772459811502888\n",
      "l2 norm of weights: 5.6372704954698785\n",
      "---------------------\n",
      "Iteration Number: 8753\n",
      "Loss: 28.15327933912381\n",
      "l2 norm of gradients: 0.17723233089318327\n",
      "l2 norm of weights: 5.637206177977204\n",
      "---------------------\n",
      "Iteration Number: 8754\n",
      "Loss: 28.151704218998727\n",
      "l2 norm of gradients: 0.17721868299153076\n",
      "l2 norm of weights: 5.6371418651230725\n",
      "---------------------\n",
      "Iteration Number: 8755\n",
      "Loss: 28.15012934103788\n",
      "l2 norm of gradients: 0.17720503744456076\n",
      "l2 norm of weights: 5.637077556906873\n",
      "---------------------\n",
      "Iteration Number: 8756\n",
      "Loss: 28.148554705180906\n",
      "l2 norm of gradients: 0.1771913942515029\n",
      "l2 norm of weights: 5.637013253327995\n",
      "---------------------\n",
      "Iteration Number: 8757\n",
      "Loss: 28.14698031136904\n",
      "l2 norm of gradients: 0.1771777534115876\n",
      "l2 norm of weights: 5.636948954385823\n",
      "---------------------\n",
      "Iteration Number: 8758\n",
      "Loss: 28.145406159541917\n",
      "l2 norm of gradients: 0.17716411492404543\n",
      "l2 norm of weights: 5.63688466007975\n",
      "---------------------\n",
      "Iteration Number: 8759\n",
      "Loss: 28.143832249637814\n",
      "l2 norm of gradients: 0.17715047878810755\n",
      "l2 norm of weights: 5.636820370409162\n",
      "---------------------\n",
      "Iteration Number: 8760\n",
      "Loss: 28.142258581595623\n",
      "l2 norm of gradients: 0.1771368450030055\n",
      "l2 norm of weights: 5.63675608537345\n",
      "---------------------\n",
      "Iteration Number: 8761\n",
      "Loss: 28.140685155351157\n",
      "l2 norm of gradients: 0.17712321356797128\n",
      "l2 norm of weights: 5.636691804972001\n",
      "---------------------\n",
      "Iteration Number: 8762\n",
      "Loss: 28.13911197085108\n",
      "l2 norm of gradients: 0.17710958448223724\n",
      "l2 norm of weights: 5.636627529204205\n",
      "---------------------\n",
      "Iteration Number: 8763\n",
      "Loss: 28.137539028030417\n",
      "l2 norm of gradients: 0.17709595774503625\n",
      "l2 norm of weights: 5.636563258069452\n",
      "---------------------\n",
      "Iteration Number: 8764\n",
      "Loss: 28.135966326829532\n",
      "l2 norm of gradients: 0.1770823333556015\n",
      "l2 norm of weights: 5.63649899156713\n",
      "---------------------\n",
      "Iteration Number: 8765\n",
      "Loss: 28.13439386718681\n",
      "l2 norm of gradients: 0.1770687113131668\n",
      "l2 norm of weights: 5.636434729696629\n",
      "---------------------\n",
      "Iteration Number: 8766\n",
      "Loss: 28.132821649042867\n",
      "l2 norm of gradients: 0.17705509161696614\n",
      "l2 norm of weights: 5.6363704724573385\n",
      "---------------------\n",
      "Iteration Number: 8767\n",
      "Loss: 28.131249672334068\n",
      "l2 norm of gradients: 0.17704147426623415\n",
      "l2 norm of weights: 5.63630621984865\n",
      "---------------------\n",
      "Iteration Number: 8768\n",
      "Loss: 28.129677937004047\n",
      "l2 norm of gradients: 0.17702785926020573\n",
      "l2 norm of weights: 5.636241971869953\n",
      "---------------------\n",
      "Iteration Number: 8769\n",
      "Loss: 28.128106442991946\n",
      "l2 norm of gradients: 0.17701424659811626\n",
      "l2 norm of weights: 5.636177728520635\n",
      "---------------------\n",
      "Iteration Number: 8770\n",
      "Loss: 28.12653519023366\n",
      "l2 norm of gradients: 0.17700063627920168\n",
      "l2 norm of weights: 5.636113489800088\n",
      "---------------------\n",
      "Iteration Number: 8771\n",
      "Loss: 28.124964178672254\n",
      "l2 norm of gradients: 0.1769870283026981\n",
      "l2 norm of weights: 5.636049255707705\n",
      "---------------------\n",
      "Iteration Number: 8772\n",
      "Loss: 28.123393408245796\n",
      "l2 norm of gradients: 0.17697342266784227\n",
      "l2 norm of weights: 5.635985026242872\n",
      "---------------------\n",
      "Iteration Number: 8773\n",
      "Loss: 28.121822878893962\n",
      "l2 norm of gradients: 0.17695981937387126\n",
      "l2 norm of weights: 5.635920801404981\n",
      "---------------------\n",
      "Iteration Number: 8774\n",
      "Loss: 28.120252590557374\n",
      "l2 norm of gradients: 0.1769462184200226\n",
      "l2 norm of weights: 5.635856581193425\n",
      "---------------------\n",
      "Iteration Number: 8775\n",
      "Loss: 28.118682543173392\n",
      "l2 norm of gradients: 0.1769326198055342\n",
      "l2 norm of weights: 5.6357923656075934\n",
      "---------------------\n",
      "Iteration Number: 8776\n",
      "Loss: 28.11711273668258\n",
      "l2 norm of gradients: 0.1769190235296445\n",
      "l2 norm of weights: 5.635728154646877\n",
      "---------------------\n",
      "Iteration Number: 8777\n",
      "Loss: 28.11554317102464\n",
      "l2 norm of gradients: 0.1769054295915922\n",
      "l2 norm of weights: 5.6356639483106665\n",
      "---------------------\n",
      "Iteration Number: 8778\n",
      "Loss: 28.113973846141484\n",
      "l2 norm of gradients: 0.17689183799061664\n",
      "l2 norm of weights: 5.635599746598355\n",
      "---------------------\n",
      "Iteration Number: 8779\n",
      "Loss: 28.11240476197151\n",
      "l2 norm of gradients: 0.17687824872595742\n",
      "l2 norm of weights: 5.635535549509331\n",
      "---------------------\n",
      "Iteration Number: 8780\n",
      "Loss: 28.110835918451958\n",
      "l2 norm of gradients: 0.17686466179685445\n",
      "l2 norm of weights: 5.635471357042991\n",
      "---------------------\n",
      "Iteration Number: 8781\n",
      "Loss: 28.109267315525194\n",
      "l2 norm of gradients: 0.17685107720254842\n",
      "l2 norm of weights: 5.6354071691987215\n",
      "---------------------\n",
      "Iteration Number: 8782\n",
      "Loss: 28.107698953130967\n",
      "l2 norm of gradients: 0.17683749494228018\n",
      "l2 norm of weights: 5.635342985975917\n",
      "---------------------\n",
      "Iteration Number: 8783\n",
      "Loss: 28.10613083120727\n",
      "l2 norm of gradients: 0.176823915015291\n",
      "l2 norm of weights: 5.635278807373969\n",
      "---------------------\n",
      "Iteration Number: 8784\n",
      "Loss: 28.104562949695993\n",
      "l2 norm of gradients: 0.1768103374208227\n",
      "l2 norm of weights: 5.635214633392271\n",
      "---------------------\n",
      "Iteration Number: 8785\n",
      "Loss: 28.10299530853681\n",
      "l2 norm of gradients: 0.1767967621581174\n",
      "l2 norm of weights: 5.635150464030212\n",
      "---------------------\n",
      "Iteration Number: 8786\n",
      "Loss: 28.10142790766779\n",
      "l2 norm of gradients: 0.17678318922641778\n",
      "l2 norm of weights: 5.635086299287187\n",
      "---------------------\n",
      "Iteration Number: 8787\n",
      "Loss: 28.09986074703251\n",
      "l2 norm of gradients: 0.1767696186249668\n",
      "l2 norm of weights: 5.635022139162588\n",
      "---------------------\n",
      "Iteration Number: 8788\n",
      "Loss: 28.09829382656726\n",
      "l2 norm of gradients: 0.17675605035300787\n",
      "l2 norm of weights: 5.634957983655809\n",
      "---------------------\n",
      "Iteration Number: 8789\n",
      "Loss: 28.096727146211848\n",
      "l2 norm of gradients: 0.17674248440978488\n",
      "l2 norm of weights: 5.634893832766241\n",
      "---------------------\n",
      "Iteration Number: 8790\n",
      "Loss: 28.095160705907347\n",
      "l2 norm of gradients: 0.1767289207945421\n",
      "l2 norm of weights: 5.634829686493276\n",
      "---------------------\n",
      "Iteration Number: 8791\n",
      "Loss: 28.093594505595526\n",
      "l2 norm of gradients: 0.17671535950652428\n",
      "l2 norm of weights: 5.634765544836309\n",
      "---------------------\n",
      "Iteration Number: 8792\n",
      "Loss: 28.09202854521346\n",
      "l2 norm of gradients: 0.17670180054497645\n",
      "l2 norm of weights: 5.634701407794733\n",
      "---------------------\n",
      "Iteration Number: 8793\n",
      "Loss: 28.090462824703557\n",
      "l2 norm of gradients: 0.17668824390914423\n",
      "l2 norm of weights: 5.6346372753679415\n",
      "---------------------\n",
      "Iteration Number: 8794\n",
      "Loss: 28.088897344004668\n",
      "l2 norm of gradients: 0.17667468959827343\n",
      "l2 norm of weights: 5.634573147555328\n",
      "---------------------\n",
      "Iteration Number: 8795\n",
      "Loss: 28.087332103057634\n",
      "l2 norm of gradients: 0.1766611376116106\n",
      "l2 norm of weights: 5.634509024356284\n",
      "---------------------\n",
      "Iteration Number: 8796\n",
      "Loss: 28.085767101799057\n",
      "l2 norm of gradients: 0.1766475879484024\n",
      "l2 norm of weights: 5.634444905770205\n",
      "---------------------\n",
      "Iteration Number: 8797\n",
      "Loss: 28.084202340175054\n",
      "l2 norm of gradients: 0.1766340406078961\n",
      "l2 norm of weights: 5.634380791796485\n",
      "---------------------\n",
      "Iteration Number: 8798\n",
      "Loss: 28.082637818122382\n",
      "l2 norm of gradients: 0.17662049558933932\n",
      "l2 norm of weights: 5.634316682434518\n",
      "---------------------\n",
      "Iteration Number: 8799\n",
      "Loss: 28.08107353558152\n",
      "l2 norm of gradients: 0.17660695289198006\n",
      "l2 norm of weights: 5.634252577683698\n",
      "---------------------\n",
      "Iteration Number: 8800\n",
      "Loss: 28.07950949249347\n",
      "l2 norm of gradients: 0.17659341251506683\n",
      "l2 norm of weights: 5.634188477543419\n",
      "---------------------\n",
      "Iteration Number: 8801\n",
      "Loss: 28.077945688794923\n",
      "l2 norm of gradients: 0.17657987445784845\n",
      "l2 norm of weights: 5.634124382013076\n",
      "---------------------\n",
      "Iteration Number: 8802\n",
      "Loss: 28.07638212443155\n",
      "l2 norm of gradients: 0.17656633871957428\n",
      "l2 norm of weights: 5.634060291092063\n",
      "---------------------\n",
      "Iteration Number: 8803\n",
      "Loss: 28.07481879933947\n",
      "l2 norm of gradients: 0.17655280529949396\n",
      "l2 norm of weights: 5.633996204779774\n",
      "---------------------\n",
      "Iteration Number: 8804\n",
      "Loss: 28.073255713463652\n",
      "l2 norm of gradients: 0.17653927419685767\n",
      "l2 norm of weights: 5.633932123075606\n",
      "---------------------\n",
      "Iteration Number: 8805\n",
      "Loss: 28.071692866738996\n",
      "l2 norm of gradients: 0.1765257454109159\n",
      "l2 norm of weights: 5.633868045978953\n",
      "---------------------\n",
      "Iteration Number: 8806\n",
      "Loss: 28.07013025910711\n",
      "l2 norm of gradients: 0.17651221894091962\n",
      "l2 norm of weights: 5.633803973489209\n",
      "---------------------\n",
      "Iteration Number: 8807\n",
      "Loss: 28.06856789051067\n",
      "l2 norm of gradients: 0.17649869478612015\n",
      "l2 norm of weights: 5.633739905605769\n",
      "---------------------\n",
      "Iteration Number: 8808\n",
      "Loss: 28.06700576088885\n",
      "l2 norm of gradients: 0.17648517294576938\n",
      "l2 norm of weights: 5.6336758423280315\n",
      "---------------------\n",
      "Iteration Number: 8809\n",
      "Loss: 28.065443870180047\n",
      "l2 norm of gradients: 0.1764716534191194\n",
      "l2 norm of weights: 5.633611783655387\n",
      "---------------------\n",
      "Iteration Number: 8810\n",
      "Loss: 28.063882218329365\n",
      "l2 norm of gradients: 0.17645813620542286\n",
      "l2 norm of weights: 5.633547729587236\n",
      "---------------------\n",
      "Iteration Number: 8811\n",
      "Loss: 28.06232080527362\n",
      "l2 norm of gradients: 0.1764446213039328\n",
      "l2 norm of weights: 5.633483680122972\n",
      "---------------------\n",
      "Iteration Number: 8812\n",
      "Loss: 28.060759630953186\n",
      "l2 norm of gradients: 0.17643110871390258\n",
      "l2 norm of weights: 5.633419635261991\n",
      "---------------------\n",
      "Iteration Number: 8813\n",
      "Loss: 28.059198695311373\n",
      "l2 norm of gradients: 0.17641759843458615\n",
      "l2 norm of weights: 5.633355595003689\n",
      "---------------------\n",
      "Iteration Number: 8814\n",
      "Loss: 28.05763799828394\n",
      "l2 norm of gradients: 0.17640409046523767\n",
      "l2 norm of weights: 5.633291559347462\n",
      "---------------------\n",
      "Iteration Number: 8815\n",
      "Loss: 28.056077539816553\n",
      "l2 norm of gradients: 0.17639058480511186\n",
      "l2 norm of weights: 5.633227528292707\n",
      "---------------------\n",
      "Iteration Number: 8816\n",
      "Loss: 28.05451731984871\n",
      "l2 norm of gradients: 0.1763770814534638\n",
      "l2 norm of weights: 5.63316350183882\n",
      "---------------------\n",
      "Iteration Number: 8817\n",
      "Loss: 28.052957338317764\n",
      "l2 norm of gradients: 0.17636358040954897\n",
      "l2 norm of weights: 5.633099479985198\n",
      "---------------------\n",
      "Iteration Number: 8818\n",
      "Loss: 28.051397595166694\n",
      "l2 norm of gradients: 0.17635008167262328\n",
      "l2 norm of weights: 5.633035462731237\n",
      "---------------------\n",
      "Iteration Number: 8819\n",
      "Loss: 28.0498380903361\n",
      "l2 norm of gradients: 0.17633658524194307\n",
      "l2 norm of weights: 5.632971450076334\n",
      "---------------------\n",
      "Iteration Number: 8820\n",
      "Loss: 28.04827882376667\n",
      "l2 norm of gradients: 0.176323091116765\n",
      "l2 norm of weights: 5.632907442019886\n",
      "---------------------\n",
      "Iteration Number: 8821\n",
      "Loss: 28.04671979539891\n",
      "l2 norm of gradients: 0.17630959929634626\n",
      "l2 norm of weights: 5.63284343856129\n",
      "---------------------\n",
      "Iteration Number: 8822\n",
      "Loss: 28.04516100517552\n",
      "l2 norm of gradients: 0.1762961097799444\n",
      "l2 norm of weights: 5.632779439699943\n",
      "---------------------\n",
      "Iteration Number: 8823\n",
      "Loss: 28.043602453032555\n",
      "l2 norm of gradients: 0.17628262256681732\n",
      "l2 norm of weights: 5.6327154454352435\n",
      "---------------------\n",
      "Iteration Number: 8824\n",
      "Loss: 28.04204413891564\n",
      "l2 norm of gradients: 0.17626913765622343\n",
      "l2 norm of weights: 5.632651455766589\n",
      "---------------------\n",
      "Iteration Number: 8825\n",
      "Loss: 28.040486062760174\n",
      "l2 norm of gradients: 0.17625565504742144\n",
      "l2 norm of weights: 5.632587470693375\n",
      "---------------------\n",
      "Iteration Number: 8826\n",
      "Loss: 28.038928224511555\n",
      "l2 norm of gradients: 0.17624217473967066\n",
      "l2 norm of weights: 5.632523490215001\n",
      "---------------------\n",
      "Iteration Number: 8827\n",
      "Loss: 28.037370624109577\n",
      "l2 norm of gradients: 0.17622869673223052\n",
      "l2 norm of weights: 5.632459514330865\n",
      "---------------------\n",
      "Iteration Number: 8828\n",
      "Loss: 28.03581326149522\n",
      "l2 norm of gradients: 0.17621522102436113\n",
      "l2 norm of weights: 5.632395543040364\n",
      "---------------------\n",
      "Iteration Number: 8829\n",
      "Loss: 28.03425613660746\n",
      "l2 norm of gradients: 0.1762017476153228\n",
      "l2 norm of weights: 5.632331576342896\n",
      "---------------------\n",
      "Iteration Number: 8830\n",
      "Loss: 28.032699249388077\n",
      "l2 norm of gradients: 0.1761882765043765\n",
      "l2 norm of weights: 5.632267614237862\n",
      "---------------------\n",
      "Iteration Number: 8831\n",
      "Loss: 28.031142599779503\n",
      "l2 norm of gradients: 0.17617480769078323\n",
      "l2 norm of weights: 5.6322036567246565\n",
      "---------------------\n",
      "Iteration Number: 8832\n",
      "Loss: 28.029586187721783\n",
      "l2 norm of gradients: 0.17616134117380478\n",
      "l2 norm of weights: 5.6321397038026815\n",
      "---------------------\n",
      "Iteration Number: 8833\n",
      "Loss: 28.028030013154186\n",
      "l2 norm of gradients: 0.17614787695270312\n",
      "l2 norm of weights: 5.632075755471333\n",
      "---------------------\n",
      "Iteration Number: 8834\n",
      "Loss: 28.0264740760207\n",
      "l2 norm of gradients: 0.17613441502674068\n",
      "l2 norm of weights: 5.6320118117300115\n",
      "---------------------\n",
      "Iteration Number: 8835\n",
      "Loss: 28.02491837625868\n",
      "l2 norm of gradients: 0.1761209553951803\n",
      "l2 norm of weights: 5.631947872578116\n",
      "---------------------\n",
      "Iteration Number: 8836\n",
      "Loss: 28.023362913814577\n",
      "l2 norm of gradients: 0.1761074980572852\n",
      "l2 norm of weights: 5.6318839380150445\n",
      "---------------------\n",
      "Iteration Number: 8837\n",
      "Loss: 28.021807688622765\n",
      "l2 norm of gradients: 0.17609404301231915\n",
      "l2 norm of weights: 5.631820008040196\n",
      "---------------------\n",
      "Iteration Number: 8838\n",
      "Loss: 28.020252700629392\n",
      "l2 norm of gradients: 0.17608059025954606\n",
      "l2 norm of weights: 5.631756082652972\n",
      "---------------------\n",
      "Iteration Number: 8839\n",
      "Loss: 28.018697949773745\n",
      "l2 norm of gradients: 0.1760671397982305\n",
      "l2 norm of weights: 5.6316921618527696\n",
      "---------------------\n",
      "Iteration Number: 8840\n",
      "Loss: 28.017143435995663\n",
      "l2 norm of gradients: 0.17605369162763726\n",
      "l2 norm of weights: 5.631628245638989\n",
      "---------------------\n",
      "Iteration Number: 8841\n",
      "Loss: 28.015589159240314\n",
      "l2 norm of gradients: 0.17604024574703164\n",
      "l2 norm of weights: 5.631564334011032\n",
      "---------------------\n",
      "Iteration Number: 8842\n",
      "Loss: 28.014035119443708\n",
      "l2 norm of gradients: 0.1760268021556793\n",
      "l2 norm of weights: 5.631500426968295\n",
      "---------------------\n",
      "Iteration Number: 8843\n",
      "Loss: 28.01248131654988\n",
      "l2 norm of gradients: 0.17601336085284633\n",
      "l2 norm of weights: 5.631436524510182\n",
      "---------------------\n",
      "Iteration Number: 8844\n",
      "Loss: 28.010927750499288\n",
      "l2 norm of gradients: 0.17599992183779922\n",
      "l2 norm of weights: 5.631372626636089\n",
      "---------------------\n",
      "Iteration Number: 8845\n",
      "Loss: 28.00937442123504\n",
      "l2 norm of gradients: 0.1759864851098048\n",
      "l2 norm of weights: 5.631308733345419\n",
      "---------------------\n",
      "Iteration Number: 8846\n",
      "Loss: 28.007821328694117\n",
      "l2 norm of gradients: 0.17597305066813035\n",
      "l2 norm of weights: 5.631244844637572\n",
      "---------------------\n",
      "Iteration Number: 8847\n",
      "Loss: 28.00626847282425\n",
      "l2 norm of gradients: 0.1759596185120436\n",
      "l2 norm of weights: 5.631180960511948\n",
      "---------------------\n",
      "Iteration Number: 8848\n",
      "Loss: 28.004715853559492\n",
      "l2 norm of gradients: 0.17594618864081266\n",
      "l2 norm of weights: 5.631117080967948\n",
      "---------------------\n",
      "Iteration Number: 8849\n",
      "Loss: 28.003163470845262\n",
      "l2 norm of gradients: 0.17593276105370595\n",
      "l2 norm of weights: 5.631053206004972\n",
      "---------------------\n",
      "Iteration Number: 8850\n",
      "Loss: 28.001611324623894\n",
      "l2 norm of gradients: 0.17591933574999236\n",
      "l2 norm of weights: 5.630989335622422\n",
      "---------------------\n",
      "Iteration Number: 8851\n",
      "Loss: 28.000059414834627\n",
      "l2 norm of gradients: 0.17590591272894118\n",
      "l2 norm of weights: 5.630925469819699\n",
      "---------------------\n",
      "Iteration Number: 8852\n",
      "Loss: 27.99850774141747\n",
      "l2 norm of gradients: 0.17589249198982215\n",
      "l2 norm of weights: 5.630861608596203\n",
      "---------------------\n",
      "Iteration Number: 8853\n",
      "Loss: 27.996956304316825\n",
      "l2 norm of gradients: 0.1758790735319053\n",
      "l2 norm of weights: 5.630797751951338\n",
      "---------------------\n",
      "Iteration Number: 8854\n",
      "Loss: 27.995405103472805\n",
      "l2 norm of gradients: 0.17586565735446114\n",
      "l2 norm of weights: 5.630733899884502\n",
      "---------------------\n",
      "Iteration Number: 8855\n",
      "Loss: 27.993854138827345\n",
      "l2 norm of gradients: 0.1758522434567605\n",
      "l2 norm of weights: 5.630670052395099\n",
      "---------------------\n",
      "Iteration Number: 8856\n",
      "Loss: 27.99230341032211\n",
      "l2 norm of gradients: 0.17583883183807475\n",
      "l2 norm of weights: 5.63060620948253\n",
      "---------------------\n",
      "Iteration Number: 8857\n",
      "Loss: 27.990752917896845\n",
      "l2 norm of gradients: 0.1758254224976755\n",
      "l2 norm of weights: 5.630542371146197\n",
      "---------------------\n",
      "Iteration Number: 8858\n",
      "Loss: 27.989202661494136\n",
      "l2 norm of gradients: 0.17581201543483493\n",
      "l2 norm of weights: 5.6304785373855015\n",
      "---------------------\n",
      "Iteration Number: 8859\n",
      "Loss: 27.987652641057043\n",
      "l2 norm of gradients: 0.1757986106488254\n",
      "l2 norm of weights: 5.630414708199846\n",
      "---------------------\n",
      "Iteration Number: 8860\n",
      "Loss: 27.98610285652439\n",
      "l2 norm of gradients: 0.17578520813891985\n",
      "l2 norm of weights: 5.630350883588633\n",
      "---------------------\n",
      "Iteration Number: 8861\n",
      "Loss: 27.984553307839278\n",
      "l2 norm of gradients: 0.1757718079043916\n",
      "l2 norm of weights: 5.630287063551265\n",
      "---------------------\n",
      "Iteration Number: 8862\n",
      "Loss: 27.98300399494427\n",
      "l2 norm of gradients: 0.1757584099445142\n",
      "l2 norm of weights: 5.630223248087144\n",
      "---------------------\n",
      "Iteration Number: 8863\n",
      "Loss: 27.981454917780724\n",
      "l2 norm of gradients: 0.1757450142585618\n",
      "l2 norm of weights: 5.630159437195672\n",
      "---------------------\n",
      "Iteration Number: 8864\n",
      "Loss: 27.979906076286422\n",
      "l2 norm of gradients: 0.17573162084580882\n",
      "l2 norm of weights: 5.630095630876253\n",
      "---------------------\n",
      "Iteration Number: 8865\n",
      "Loss: 27.97835747040685\n",
      "l2 norm of gradients: 0.1757182297055302\n",
      "l2 norm of weights: 5.63003182912829\n",
      "---------------------\n",
      "Iteration Number: 8866\n",
      "Loss: 27.976809100083397\n",
      "l2 norm of gradients: 0.17570484083700114\n",
      "l2 norm of weights: 5.629968031951185\n",
      "---------------------\n",
      "Iteration Number: 8867\n",
      "Loss: 27.97526096525834\n",
      "l2 norm of gradients: 0.17569145423949725\n",
      "l2 norm of weights: 5.629904239344342\n",
      "---------------------\n",
      "Iteration Number: 8868\n",
      "Loss: 27.9737130658701\n",
      "l2 norm of gradients: 0.17567806991229462\n",
      "l2 norm of weights: 5.629840451307164\n",
      "---------------------\n",
      "Iteration Number: 8869\n",
      "Loss: 27.97216540186307\n",
      "l2 norm of gradients: 0.17566468785466968\n",
      "l2 norm of weights: 5.629776667839054\n",
      "---------------------\n",
      "Iteration Number: 8870\n",
      "Loss: 27.970617973179593\n",
      "l2 norm of gradients: 0.1756513080658993\n",
      "l2 norm of weights: 5.629712888939416\n",
      "---------------------\n",
      "Iteration Number: 8871\n",
      "Loss: 27.969070779759836\n",
      "l2 norm of gradients: 0.1756379305452607\n",
      "l2 norm of weights: 5.629649114607655\n",
      "---------------------\n",
      "Iteration Number: 8872\n",
      "Loss: 27.96752382154565\n",
      "l2 norm of gradients: 0.17562455529203144\n",
      "l2 norm of weights: 5.629585344843171\n",
      "---------------------\n",
      "Iteration Number: 8873\n",
      "Loss: 27.965977098479698\n",
      "l2 norm of gradients: 0.1756111823054896\n",
      "l2 norm of weights: 5.629521579645373\n",
      "---------------------\n",
      "Iteration Number: 8874\n",
      "Loss: 27.96443061050675\n",
      "l2 norm of gradients: 0.17559781158491358\n",
      "l2 norm of weights: 5.629457819013661\n",
      "---------------------\n",
      "Iteration Number: 8875\n",
      "Loss: 27.962884357562686\n",
      "l2 norm of gradients: 0.17558444312958213\n",
      "l2 norm of weights: 5.6293940629474415\n",
      "---------------------\n",
      "Iteration Number: 8876\n",
      "Loss: 27.961338339591997\n",
      "l2 norm of gradients: 0.17557107693877452\n",
      "l2 norm of weights: 5.629330311446117\n",
      "---------------------\n",
      "Iteration Number: 8877\n",
      "Loss: 27.959792556537288\n",
      "l2 norm of gradients: 0.17555771301177034\n",
      "l2 norm of weights: 5.629266564509095\n",
      "---------------------\n",
      "Iteration Number: 8878\n",
      "Loss: 27.958247008338866\n",
      "l2 norm of gradients: 0.1755443513478495\n",
      "l2 norm of weights: 5.6292028221357775\n",
      "---------------------\n",
      "Iteration Number: 8879\n",
      "Loss: 27.95670169494225\n",
      "l2 norm of gradients: 0.17553099194629235\n",
      "l2 norm of weights: 5.62913908432557\n",
      "---------------------\n",
      "Iteration Number: 8880\n",
      "Loss: 27.95515661628565\n",
      "l2 norm of gradients: 0.17551763480637975\n",
      "l2 norm of weights: 5.629075351077876\n",
      "---------------------\n",
      "Iteration Number: 8881\n",
      "Loss: 27.95361177231406\n",
      "l2 norm of gradients: 0.1755042799273928\n",
      "l2 norm of weights: 5.629011622392103\n",
      "---------------------\n",
      "Iteration Number: 8882\n",
      "Loss: 27.95206716296718\n",
      "l2 norm of gradients: 0.17549092730861307\n",
      "l2 norm of weights: 5.628947898267656\n",
      "---------------------\n",
      "Iteration Number: 8883\n",
      "Loss: 27.950522788188074\n",
      "l2 norm of gradients: 0.17547757694932245\n",
      "l2 norm of weights: 5.628884178703937\n",
      "---------------------\n",
      "Iteration Number: 8884\n",
      "Loss: 27.948978647919215\n",
      "l2 norm of gradients: 0.1754642288488033\n",
      "l2 norm of weights: 5.628820463700356\n",
      "---------------------\n",
      "Iteration Number: 8885\n",
      "Loss: 27.947434742101827\n",
      "l2 norm of gradients: 0.17545088300633832\n",
      "l2 norm of weights: 5.628756753256314\n",
      "---------------------\n",
      "Iteration Number: 8886\n",
      "Loss: 27.94589107067828\n",
      "l2 norm of gradients: 0.17543753942121063\n",
      "l2 norm of weights: 5.62869304737122\n",
      "---------------------\n",
      "Iteration Number: 8887\n",
      "Loss: 27.944347633590812\n",
      "l2 norm of gradients: 0.17542419809270365\n",
      "l2 norm of weights: 5.6286293460444785\n",
      "---------------------\n",
      "Iteration Number: 8888\n",
      "Loss: 27.942804430780757\n",
      "l2 norm of gradients: 0.1754108590201013\n",
      "l2 norm of weights: 5.628565649275495\n",
      "---------------------\n",
      "Iteration Number: 8889\n",
      "Loss: 27.941261462193694\n",
      "l2 norm of gradients: 0.17539752220268792\n",
      "l2 norm of weights: 5.628501957063677\n",
      "---------------------\n",
      "Iteration Number: 8890\n",
      "Loss: 27.93971872776809\n",
      "l2 norm of gradients: 0.17538418763974808\n",
      "l2 norm of weights: 5.628438269408429\n",
      "---------------------\n",
      "Iteration Number: 8891\n",
      "Loss: 27.93817622744541\n",
      "l2 norm of gradients: 0.1753708553305669\n",
      "l2 norm of weights: 5.6283745863091585\n",
      "---------------------\n",
      "Iteration Number: 8892\n",
      "Loss: 27.936633961171815\n",
      "l2 norm of gradients: 0.17535752527442972\n",
      "l2 norm of weights: 5.628310907765271\n",
      "---------------------\n",
      "Iteration Number: 8893\n",
      "Loss: 27.935091928888657\n",
      "l2 norm of gradients: 0.1753441974706224\n",
      "l2 norm of weights: 5.6282472337761735\n",
      "---------------------\n",
      "Iteration Number: 8894\n",
      "Loss: 27.933550130534346\n",
      "l2 norm of gradients: 0.17533087191843114\n",
      "l2 norm of weights: 5.628183564341273\n",
      "---------------------\n",
      "Iteration Number: 8895\n",
      "Loss: 27.932008566056272\n",
      "l2 norm of gradients: 0.17531754861714255\n",
      "l2 norm of weights: 5.628119899459977\n",
      "---------------------\n",
      "Iteration Number: 8896\n",
      "Loss: 27.930467235395458\n",
      "l2 norm of gradients: 0.1753042275660436\n",
      "l2 norm of weights: 5.62805623913169\n",
      "---------------------\n",
      "Iteration Number: 8897\n",
      "Loss: 27.928926138490564\n",
      "l2 norm of gradients: 0.17529090876442163\n",
      "l2 norm of weights: 5.627992583355822\n",
      "---------------------\n",
      "Iteration Number: 8898\n",
      "Loss: 27.927385275288714\n",
      "l2 norm of gradients: 0.1752775922115645\n",
      "l2 norm of weights: 5.627928932131777\n",
      "---------------------\n",
      "Iteration Number: 8899\n",
      "Loss: 27.925844645731242\n",
      "l2 norm of gradients: 0.17526427790676016\n",
      "l2 norm of weights: 5.627865285458966\n",
      "---------------------\n",
      "Iteration Number: 8900\n",
      "Loss: 27.92430424976087\n",
      "l2 norm of gradients: 0.17525096584929725\n",
      "l2 norm of weights: 5.6278016433367934\n",
      "---------------------\n",
      "Iteration Number: 8901\n",
      "Loss: 27.922764087316587\n",
      "l2 norm of gradients: 0.1752376560384647\n",
      "l2 norm of weights: 5.627738005764669\n",
      "---------------------\n",
      "Iteration Number: 8902\n",
      "Loss: 27.92122415834342\n",
      "l2 norm of gradients: 0.17522434847355167\n",
      "l2 norm of weights: 5.627674372741999\n",
      "---------------------\n",
      "Iteration Number: 8903\n",
      "Loss: 27.919684462784893\n",
      "l2 norm of gradients: 0.17521104315384797\n",
      "l2 norm of weights: 5.627610744268193\n",
      "---------------------\n",
      "Iteration Number: 8904\n",
      "Loss: 27.918145000580758\n",
      "l2 norm of gradients: 0.17519774007864355\n",
      "l2 norm of weights: 5.627547120342657\n",
      "---------------------\n",
      "Iteration Number: 8905\n",
      "Loss: 27.91660577167697\n",
      "l2 norm of gradients: 0.17518443924722893\n",
      "l2 norm of weights: 5.627483500964798\n",
      "---------------------\n",
      "Iteration Number: 8906\n",
      "Loss: 27.91506677601367\n",
      "l2 norm of gradients: 0.17517114065889494\n",
      "l2 norm of weights: 5.627419886134029\n",
      "---------------------\n",
      "Iteration Number: 8907\n",
      "Loss: 27.913528013533355\n",
      "l2 norm of gradients: 0.1751578443129327\n",
      "l2 norm of weights: 5.627356275849754\n",
      "---------------------\n",
      "Iteration Number: 8908\n",
      "Loss: 27.911989484179962\n",
      "l2 norm of gradients: 0.17514455020863381\n",
      "l2 norm of weights: 5.627292670111385\n",
      "---------------------\n",
      "Iteration Number: 8909\n",
      "Loss: 27.910451187895905\n",
      "l2 norm of gradients: 0.17513125834529034\n",
      "l2 norm of weights: 5.627229068918327\n",
      "---------------------\n",
      "Iteration Number: 8910\n",
      "Loss: 27.908913124622828\n",
      "l2 norm of gradients: 0.1751179687221945\n",
      "l2 norm of weights: 5.627165472269991\n",
      "---------------------\n",
      "Iteration Number: 8911\n",
      "Loss: 27.90737529430261\n",
      "l2 norm of gradients: 0.17510468133863913\n",
      "l2 norm of weights: 5.627101880165785\n",
      "---------------------\n",
      "Iteration Number: 8912\n",
      "Loss: 27.9058376968835\n",
      "l2 norm of gradients: 0.1750913961939173\n",
      "l2 norm of weights: 5.627038292605119\n",
      "---------------------\n",
      "Iteration Number: 8913\n",
      "Loss: 27.904300332299023\n",
      "l2 norm of gradients: 0.17507811328732253\n",
      "l2 norm of weights: 5.626974709587401\n",
      "---------------------\n",
      "Iteration Number: 8914\n",
      "Loss: 27.902763200500715\n",
      "l2 norm of gradients: 0.17506483261814867\n",
      "l2 norm of weights: 5.626911131112041\n",
      "---------------------\n",
      "Iteration Number: 8915\n",
      "Loss: 27.901226301425844\n",
      "l2 norm of gradients: 0.17505155418569\n",
      "l2 norm of weights: 5.626847557178449\n",
      "---------------------\n",
      "Iteration Number: 8916\n",
      "Loss: 27.899689635019698\n",
      "l2 norm of gradients: 0.1750382779892411\n",
      "l2 norm of weights: 5.626783987786034\n",
      "---------------------\n",
      "Iteration Number: 8917\n",
      "Loss: 27.898153201224737\n",
      "l2 norm of gradients: 0.17502500402809706\n",
      "l2 norm of weights: 5.626720422934205\n",
      "---------------------\n",
      "Iteration Number: 8918\n",
      "Loss: 27.896616999981426\n",
      "l2 norm of gradients: 0.17501173230155323\n",
      "l2 norm of weights: 5.626656862622372\n",
      "---------------------\n",
      "Iteration Number: 8919\n",
      "Loss: 27.89508103123722\n",
      "l2 norm of gradients: 0.17499846280890538\n",
      "l2 norm of weights: 5.626593306849946\n",
      "---------------------\n",
      "Iteration Number: 8920\n",
      "Loss: 27.893545294929886\n",
      "l2 norm of gradients: 0.17498519554944966\n",
      "l2 norm of weights: 5.626529755616335\n",
      "---------------------\n",
      "Iteration Number: 8921\n",
      "Loss: 27.892009791005748\n",
      "l2 norm of gradients: 0.1749719305224826\n",
      "l2 norm of weights: 5.626466208920951\n",
      "---------------------\n",
      "Iteration Number: 8922\n",
      "Loss: 27.89047451940534\n",
      "l2 norm of gradients: 0.17495866772730115\n",
      "l2 norm of weights: 5.626402666763205\n",
      "---------------------\n",
      "Iteration Number: 8923\n",
      "Loss: 27.888939480074992\n",
      "l2 norm of gradients: 0.17494540716320253\n",
      "l2 norm of weights: 5.626339129142505\n",
      "---------------------\n",
      "Iteration Number: 8924\n",
      "Loss: 27.88740467295466\n",
      "l2 norm of gradients: 0.1749321488294845\n",
      "l2 norm of weights: 5.626275596058264\n",
      "---------------------\n",
      "Iteration Number: 8925\n",
      "Loss: 27.885870097989006\n",
      "l2 norm of gradients: 0.17491889272544492\n",
      "l2 norm of weights: 5.62621206750989\n",
      "---------------------\n",
      "Iteration Number: 8926\n",
      "Loss: 27.88433575512013\n",
      "l2 norm of gradients: 0.17490563885038235\n",
      "l2 norm of weights: 5.626148543496797\n",
      "---------------------\n",
      "Iteration Number: 8927\n",
      "Loss: 27.882801644289493\n",
      "l2 norm of gradients: 0.1748923872035956\n",
      "l2 norm of weights: 5.626085024018393\n",
      "---------------------\n",
      "Iteration Number: 8928\n",
      "Loss: 27.881267765443667\n",
      "l2 norm of gradients: 0.17487913778438374\n",
      "l2 norm of weights: 5.626021509074091\n",
      "---------------------\n",
      "Iteration Number: 8929\n",
      "Loss: 27.879734118522457\n",
      "l2 norm of gradients: 0.17486589059204635\n",
      "l2 norm of weights: 5.625957998663302\n",
      "---------------------\n",
      "Iteration Number: 8930\n",
      "Loss: 27.878200703471293\n",
      "l2 norm of gradients: 0.17485264562588343\n",
      "l2 norm of weights: 5.625894492785436\n",
      "---------------------\n",
      "Iteration Number: 8931\n",
      "Loss: 27.8766675202312\n",
      "l2 norm of gradients: 0.17483940288519512\n",
      "l2 norm of weights: 5.625830991439907\n",
      "---------------------\n",
      "Iteration Number: 8932\n",
      "Loss: 27.875134568748805\n",
      "l2 norm of gradients: 0.17482616236928222\n",
      "l2 norm of weights: 5.625767494626124\n",
      "---------------------\n",
      "Iteration Number: 8933\n",
      "Loss: 27.873601848962178\n",
      "l2 norm of gradients: 0.1748129240774457\n",
      "l2 norm of weights: 5.625704002343499\n",
      "---------------------\n",
      "Iteration Number: 8934\n",
      "Loss: 27.87206936081974\n",
      "l2 norm of gradients: 0.17479968800898704\n",
      "l2 norm of weights: 5.625640514591446\n",
      "---------------------\n",
      "Iteration Number: 8935\n",
      "Loss: 27.8705371042603\n",
      "l2 norm of gradients: 0.17478645416320798\n",
      "l2 norm of weights: 5.6255770313693745\n",
      "---------------------\n",
      "Iteration Number: 8936\n",
      "Loss: 27.86900507922909\n",
      "l2 norm of gradients: 0.1747732225394107\n",
      "l2 norm of weights: 5.625513552676699\n",
      "---------------------\n",
      "Iteration Number: 8937\n",
      "Loss: 27.867473285669508\n",
      "l2 norm of gradients: 0.17475999313689772\n",
      "l2 norm of weights: 5.625450078512829\n",
      "---------------------\n",
      "Iteration Number: 8938\n",
      "Loss: 27.865941723525083\n",
      "l2 norm of gradients: 0.174746765954972\n",
      "l2 norm of weights: 5.62538660887718\n",
      "---------------------\n",
      "Iteration Number: 8939\n",
      "Loss: 27.8644103927371\n",
      "l2 norm of gradients: 0.17473354099293675\n",
      "l2 norm of weights: 5.625323143769162\n",
      "---------------------\n",
      "Iteration Number: 8940\n",
      "Loss: 27.8628792932504\n",
      "l2 norm of gradients: 0.17472031825009568\n",
      "l2 norm of weights: 5.6252596831881885\n",
      "---------------------\n",
      "Iteration Number: 8941\n",
      "Loss: 27.86134842500973\n",
      "l2 norm of gradients: 0.17470709772575282\n",
      "l2 norm of weights: 5.625196227133672\n",
      "---------------------\n",
      "Iteration Number: 8942\n",
      "Loss: 27.859817787955457\n",
      "l2 norm of gradients: 0.17469387941921252\n",
      "l2 norm of weights: 5.625132775605025\n",
      "---------------------\n",
      "Iteration Number: 8943\n",
      "Loss: 27.858287382032895\n",
      "l2 norm of gradients: 0.17468066332977963\n",
      "l2 norm of weights: 5.625069328601663\n",
      "---------------------\n",
      "Iteration Number: 8944\n",
      "Loss: 27.856757207182795\n",
      "l2 norm of gradients: 0.17466744945675922\n",
      "l2 norm of weights: 5.625005886122996\n",
      "---------------------\n",
      "Iteration Number: 8945\n",
      "Loss: 27.855227263351967\n",
      "l2 norm of gradients: 0.17465423779945682\n",
      "l2 norm of weights: 5.624942448168439\n",
      "---------------------\n",
      "Iteration Number: 8946\n",
      "Loss: 27.853697550482984\n",
      "l2 norm of gradients: 0.17464102835717832\n",
      "l2 norm of weights: 5.624879014737405\n",
      "---------------------\n",
      "Iteration Number: 8947\n",
      "Loss: 27.852168068517777\n",
      "l2 norm of gradients: 0.17462782112923\n",
      "l2 norm of weights: 5.624815585829307\n",
      "---------------------\n",
      "Iteration Number: 8948\n",
      "Loss: 27.850638817400547\n",
      "l2 norm of gradients: 0.17461461611491844\n",
      "l2 norm of weights: 5.624752161443559\n",
      "---------------------\n",
      "Iteration Number: 8949\n",
      "Loss: 27.849109797075833\n",
      "l2 norm of gradients: 0.17460141331355064\n",
      "l2 norm of weights: 5.624688741579575\n",
      "---------------------\n",
      "Iteration Number: 8950\n",
      "Loss: 27.847581007485974\n",
      "l2 norm of gradients: 0.17458821272443395\n",
      "l2 norm of weights: 5.624625326236769\n",
      "---------------------\n",
      "Iteration Number: 8951\n",
      "Loss: 27.84605244857454\n",
      "l2 norm of gradients: 0.17457501434687614\n",
      "l2 norm of weights: 5.624561915414555\n",
      "---------------------\n",
      "Iteration Number: 8952\n",
      "Loss: 27.84452412028619\n",
      "l2 norm of gradients: 0.17456181818018526\n",
      "l2 norm of weights: 5.624498509112347\n",
      "---------------------\n",
      "Iteration Number: 8953\n",
      "Loss: 27.842996022562104\n",
      "l2 norm of gradients: 0.17454862422366985\n",
      "l2 norm of weights: 5.6244351073295595\n",
      "---------------------\n",
      "Iteration Number: 8954\n",
      "Loss: 27.841468155348\n",
      "l2 norm of gradients: 0.17453543247663864\n",
      "l2 norm of weights: 5.624371710065605\n",
      "---------------------\n",
      "Iteration Number: 8955\n",
      "Loss: 27.839940518587174\n",
      "l2 norm of gradients: 0.1745222429384009\n",
      "l2 norm of weights: 5.624308317319901\n",
      "---------------------\n",
      "Iteration Number: 8956\n",
      "Loss: 27.838413112221957\n",
      "l2 norm of gradients: 0.17450905560826618\n",
      "l2 norm of weights: 5.624244929091861\n",
      "---------------------\n",
      "Iteration Number: 8957\n",
      "Loss: 27.836885936198872\n",
      "l2 norm of gradients: 0.1744958704855444\n",
      "l2 norm of weights: 5.6241815453809005\n",
      "---------------------\n",
      "Iteration Number: 8958\n",
      "Loss: 27.83535899045996\n",
      "l2 norm of gradients: 0.17448268756954588\n",
      "l2 norm of weights: 5.624118166186433\n",
      "---------------------\n",
      "Iteration Number: 8959\n",
      "Loss: 27.83383227494656\n",
      "l2 norm of gradients: 0.1744695068595813\n",
      "l2 norm of weights: 5.624054791507873\n",
      "---------------------\n",
      "Iteration Number: 8960\n",
      "Loss: 27.832305789605222\n",
      "l2 norm of gradients: 0.1744563283549617\n",
      "l2 norm of weights: 5.623991421344638\n",
      "---------------------\n",
      "Iteration Number: 8961\n",
      "Loss: 27.830779534378287\n",
      "l2 norm of gradients: 0.17444315205499844\n",
      "l2 norm of weights: 5.623928055696141\n",
      "---------------------\n",
      "Iteration Number: 8962\n",
      "Loss: 27.829253509210723\n",
      "l2 norm of gradients: 0.17442997795900334\n",
      "l2 norm of weights: 5.6238646945618\n",
      "---------------------\n",
      "Iteration Number: 8963\n",
      "Loss: 27.827727714047292\n",
      "l2 norm of gradients: 0.17441680606628845\n",
      "l2 norm of weights: 5.623801337941029\n",
      "---------------------\n",
      "Iteration Number: 8964\n",
      "Loss: 27.826202148829125\n",
      "l2 norm of gradients: 0.17440363637616632\n",
      "l2 norm of weights: 5.623737985833243\n",
      "---------------------\n",
      "Iteration Number: 8965\n",
      "Loss: 27.824676813500147\n",
      "l2 norm of gradients: 0.17439046888794982\n",
      "l2 norm of weights: 5.623674638237859\n",
      "---------------------\n",
      "Iteration Number: 8966\n",
      "Loss: 27.823151708006677\n",
      "l2 norm of gradients: 0.1743773036009521\n",
      "l2 norm of weights: 5.6236112951542925\n",
      "---------------------\n",
      "Iteration Number: 8967\n",
      "Loss: 27.82162683228898\n",
      "l2 norm of gradients: 0.17436414051448682\n",
      "l2 norm of weights: 5.62354795658196\n",
      "---------------------\n",
      "Iteration Number: 8968\n",
      "Loss: 27.820102186296772\n",
      "l2 norm of gradients: 0.17435097962786794\n",
      "l2 norm of weights: 5.623484622520277\n",
      "---------------------\n",
      "Iteration Number: 8969\n",
      "Loss: 27.81857776996798\n",
      "l2 norm of gradients: 0.1743378209404097\n",
      "l2 norm of weights: 5.6234212929686604\n",
      "---------------------\n",
      "Iteration Number: 8970\n",
      "Loss: 27.817053583249336\n",
      "l2 norm of gradients: 0.17432466445142683\n",
      "l2 norm of weights: 5.623357967926526\n",
      "---------------------\n",
      "Iteration Number: 8971\n",
      "Loss: 27.815529626081602\n",
      "l2 norm of gradients: 0.17431151016023436\n",
      "l2 norm of weights: 5.623294647393291\n",
      "---------------------\n",
      "Iteration Number: 8972\n",
      "Loss: 27.81400589841546\n",
      "l2 norm of gradients: 0.17429835806614769\n",
      "l2 norm of weights: 5.623231331368372\n",
      "---------------------\n",
      "Iteration Number: 8973\n",
      "Loss: 27.812482400189335\n",
      "l2 norm of gradients: 0.1742852081684825\n",
      "l2 norm of weights: 5.623168019851185\n",
      "---------------------\n",
      "Iteration Number: 8974\n",
      "Loss: 27.81095913134819\n",
      "l2 norm of gradients: 0.17427206046655508\n",
      "l2 norm of weights: 5.623104712841149\n",
      "---------------------\n",
      "Iteration Number: 8975\n",
      "Loss: 27.809436091836695\n",
      "l2 norm of gradients: 0.17425891495968177\n",
      "l2 norm of weights: 5.623041410337678\n",
      "---------------------\n",
      "Iteration Number: 8976\n",
      "Loss: 27.807913281600005\n",
      "l2 norm of gradients: 0.17424577164717953\n",
      "l2 norm of weights: 5.622978112340193\n",
      "---------------------\n",
      "Iteration Number: 8977\n",
      "Loss: 27.80639070057929\n",
      "l2 norm of gradients: 0.17423263052836546\n",
      "l2 norm of weights: 5.622914818848109\n",
      "---------------------\n",
      "Iteration Number: 8978\n",
      "Loss: 27.804868348721552\n",
      "l2 norm of gradients: 0.17421949160255717\n",
      "l2 norm of weights: 5.622851529860843\n",
      "---------------------\n",
      "Iteration Number: 8979\n",
      "Loss: 27.803346225969864\n",
      "l2 norm of gradients: 0.17420635486907263\n",
      "l2 norm of weights: 5.622788245377813\n",
      "---------------------\n",
      "Iteration Number: 8980\n",
      "Loss: 27.801824332266122\n",
      "l2 norm of gradients: 0.17419322032723003\n",
      "l2 norm of weights: 5.6227249653984375\n",
      "---------------------\n",
      "Iteration Number: 8981\n",
      "Loss: 27.800302667558004\n",
      "l2 norm of gradients: 0.17418008797634812\n",
      "l2 norm of weights: 5.6226616899221336\n",
      "---------------------\n",
      "Iteration Number: 8982\n",
      "Loss: 27.798781231787594\n",
      "l2 norm of gradients: 0.17416695781574584\n",
      "l2 norm of weights: 5.62259841894832\n",
      "---------------------\n",
      "Iteration Number: 8983\n",
      "Loss: 27.797260024902307\n",
      "l2 norm of gradients: 0.17415382984474256\n",
      "l2 norm of weights: 5.622535152476414\n",
      "---------------------\n",
      "Iteration Number: 8984\n",
      "Loss: 27.795739046841053\n",
      "l2 norm of gradients: 0.17414070406265802\n",
      "l2 norm of weights: 5.622471890505834\n",
      "---------------------\n",
      "Iteration Number: 8985\n",
      "Loss: 27.79421829755185\n",
      "l2 norm of gradients: 0.1741275804688123\n",
      "l2 norm of weights: 5.622408633035999\n",
      "---------------------\n",
      "Iteration Number: 8986\n",
      "Loss: 27.792697776975803\n",
      "l2 norm of gradients: 0.17411445906252582\n",
      "l2 norm of weights: 5.622345380066327\n",
      "---------------------\n",
      "Iteration Number: 8987\n",
      "Loss: 27.791177485062267\n",
      "l2 norm of gradients: 0.17410133984311932\n",
      "l2 norm of weights: 5.622282131596236\n",
      "---------------------\n",
      "Iteration Number: 8988\n",
      "Loss: 27.789657421751127\n",
      "l2 norm of gradients: 0.17408822280991412\n",
      "l2 norm of weights: 5.622218887625146\n",
      "---------------------\n",
      "Iteration Number: 8989\n",
      "Loss: 27.788137586988015\n",
      "l2 norm of gradients: 0.17407510796223155\n",
      "l2 norm of weights: 5.622155648152475\n",
      "---------------------\n",
      "Iteration Number: 8990\n",
      "Loss: 27.78661798071772\n",
      "l2 norm of gradients: 0.17406199529939356\n",
      "l2 norm of weights: 5.622092413177643\n",
      "---------------------\n",
      "Iteration Number: 8991\n",
      "Loss: 27.78509860288324\n",
      "l2 norm of gradients: 0.17404888482072234\n",
      "l2 norm of weights: 5.622029182700066\n",
      "---------------------\n",
      "Iteration Number: 8992\n",
      "Loss: 27.783579453430946\n",
      "l2 norm of gradients: 0.17403577652554053\n",
      "l2 norm of weights: 5.621965956719167\n",
      "---------------------\n",
      "Iteration Number: 8993\n",
      "Loss: 27.782060532303458\n",
      "l2 norm of gradients: 0.174022670413171\n",
      "l2 norm of weights: 5.6219027352343645\n",
      "---------------------\n",
      "Iteration Number: 8994\n",
      "Loss: 27.780541839445693\n",
      "l2 norm of gradients: 0.17400956648293706\n",
      "l2 norm of weights: 5.621839518245076\n",
      "---------------------\n",
      "Iteration Number: 8995\n",
      "Loss: 27.77902337480399\n",
      "l2 norm of gradients: 0.17399646473416233\n",
      "l2 norm of weights: 5.621776305750722\n",
      "---------------------\n",
      "Iteration Number: 8996\n",
      "Loss: 27.777505138320468\n",
      "l2 norm of gradients: 0.17398336516617086\n",
      "l2 norm of weights: 5.621713097750724\n",
      "---------------------\n",
      "Iteration Number: 8997\n",
      "Loss: 27.77598712994028\n",
      "l2 norm of gradients: 0.17397026777828692\n",
      "l2 norm of weights: 5.6216498942444995\n",
      "---------------------\n",
      "Iteration Number: 8998\n",
      "Loss: 27.774469349607806\n",
      "l2 norm of gradients: 0.1739571725698353\n",
      "l2 norm of weights: 5.6215866952314695\n",
      "---------------------\n",
      "Iteration Number: 8999\n",
      "Loss: 27.77295179726775\n",
      "l2 norm of gradients: 0.17394407954014104\n",
      "l2 norm of weights: 5.621523500711055\n",
      "---------------------\n",
      "Iteration Number: 9000\n",
      "Loss: 27.771434472864\n",
      "l2 norm of gradients: 0.1739309886885295\n",
      "l2 norm of weights: 5.621460310682674\n",
      "---------------------\n",
      "Iteration Number: 9001\n",
      "Loss: 27.769917376341898\n",
      "l2 norm of gradients: 0.17391790001432644\n",
      "l2 norm of weights: 5.621397125145749\n",
      "---------------------\n",
      "Iteration Number: 9002\n",
      "Loss: 27.768400507648558\n",
      "l2 norm of gradients: 0.1739048135168581\n",
      "l2 norm of weights: 5.621333944099699\n",
      "---------------------\n",
      "Iteration Number: 9003\n",
      "Loss: 27.76688386672265\n",
      "l2 norm of gradients: 0.17389172919545082\n",
      "l2 norm of weights: 5.621270767543946\n",
      "---------------------\n",
      "Iteration Number: 9004\n",
      "Loss: 27.76536745351431\n",
      "l2 norm of gradients: 0.1738786470494315\n",
      "l2 norm of weights: 5.621207595477909\n",
      "---------------------\n",
      "Iteration Number: 9005\n",
      "Loss: 27.763851267965673\n",
      "l2 norm of gradients: 0.1738655670781273\n",
      "l2 norm of weights: 5.621144427901011\n",
      "---------------------\n",
      "Iteration Number: 9006\n",
      "Loss: 27.762335310020223\n",
      "l2 norm of gradients: 0.17385248928086572\n",
      "l2 norm of weights: 5.621081264812671\n",
      "---------------------\n",
      "Iteration Number: 9007\n",
      "Loss: 27.760819579624325\n",
      "l2 norm of gradients: 0.17383941365697467\n",
      "l2 norm of weights: 5.621018106212311\n",
      "---------------------\n",
      "Iteration Number: 9008\n",
      "Loss: 27.759304076721993\n",
      "l2 norm of gradients: 0.17382634020578233\n",
      "l2 norm of weights: 5.6209549520993525\n",
      "---------------------\n",
      "Iteration Number: 9009\n",
      "Loss: 27.757788801262368\n",
      "l2 norm of gradients: 0.17381326892661733\n",
      "l2 norm of weights: 5.620891802473216\n",
      "---------------------\n",
      "Iteration Number: 9010\n",
      "Loss: 27.75627375318103\n",
      "l2 norm of gradients: 0.17380019981880854\n",
      "l2 norm of weights: 5.620828657333324\n",
      "---------------------\n",
      "Iteration Number: 9011\n",
      "Loss: 27.754758932430907\n",
      "l2 norm of gradients: 0.17378713288168532\n",
      "l2 norm of weights: 5.620765516679097\n",
      "---------------------\n",
      "Iteration Number: 9012\n",
      "Loss: 27.753244338953188\n",
      "l2 norm of gradients: 0.17377406811457727\n",
      "l2 norm of weights: 5.620702380509958\n",
      "---------------------\n",
      "Iteration Number: 9013\n",
      "Loss: 27.75172997269323\n",
      "l2 norm of gradients: 0.17376100551681425\n",
      "l2 norm of weights: 5.620639248825328\n",
      "---------------------\n",
      "Iteration Number: 9014\n",
      "Loss: 27.75021583359734\n",
      "l2 norm of gradients: 0.17374794508772676\n",
      "l2 norm of weights: 5.620576121624629\n",
      "---------------------\n",
      "Iteration Number: 9015\n",
      "Loss: 27.74870192160804\n",
      "l2 norm of gradients: 0.17373488682664545\n",
      "l2 norm of weights: 5.620512998907284\n",
      "---------------------\n",
      "Iteration Number: 9016\n",
      "Loss: 27.747188236669476\n",
      "l2 norm of gradients: 0.17372183073290126\n",
      "l2 norm of weights: 5.620449880672713\n",
      "---------------------\n",
      "Iteration Number: 9017\n",
      "Loss: 27.745674778731736\n",
      "l2 norm of gradients: 0.1737087768058256\n",
      "l2 norm of weights: 5.620386766920341\n",
      "---------------------\n",
      "Iteration Number: 9018\n",
      "Loss: 27.744161547734258\n",
      "l2 norm of gradients: 0.17369572504475017\n",
      "l2 norm of weights: 5.62032365764959\n",
      "---------------------\n",
      "Iteration Number: 9019\n",
      "Loss: 27.74264854362422\n",
      "l2 norm of gradients: 0.17368267544900712\n",
      "l2 norm of weights: 5.620260552859881\n",
      "---------------------\n",
      "Iteration Number: 9020\n",
      "Loss: 27.741135766345764\n",
      "l2 norm of gradients: 0.17366962801792876\n",
      "l2 norm of weights: 5.620197452550638\n",
      "---------------------\n",
      "Iteration Number: 9021\n",
      "Loss: 27.739623215844944\n",
      "l2 norm of gradients: 0.17365658275084792\n",
      "l2 norm of weights: 5.620134356721284\n",
      "---------------------\n",
      "Iteration Number: 9022\n",
      "Loss: 27.73811089206535\n",
      "l2 norm of gradients: 0.17364353964709764\n",
      "l2 norm of weights: 5.620071265371241\n",
      "---------------------\n",
      "Iteration Number: 9023\n",
      "Loss: 27.736598794953654\n",
      "l2 norm of gradients: 0.17363049870601144\n",
      "l2 norm of weights: 5.6200081784999325\n",
      "---------------------\n",
      "Iteration Number: 9024\n",
      "Loss: 27.735086924455025\n",
      "l2 norm of gradients: 0.17361745992692307\n",
      "l2 norm of weights: 5.6199450961067825\n",
      "---------------------\n",
      "Iteration Number: 9025\n",
      "Loss: 27.73357528051233\n",
      "l2 norm of gradients: 0.17360442330916673\n",
      "l2 norm of weights: 5.619882018191214\n",
      "---------------------\n",
      "Iteration Number: 9026\n",
      "Loss: 27.732063863073588\n",
      "l2 norm of gradients: 0.17359138885207687\n",
      "l2 norm of weights: 5.61981894475265\n",
      "---------------------\n",
      "Iteration Number: 9027\n",
      "Loss: 27.73055267207959\n",
      "l2 norm of gradients: 0.17357835655498835\n",
      "l2 norm of weights: 5.6197558757905135\n",
      "---------------------\n",
      "Iteration Number: 9028\n",
      "Loss: 27.72904170747874\n",
      "l2 norm of gradients: 0.1735653264172363\n",
      "l2 norm of weights: 5.619692811304231\n",
      "---------------------\n",
      "Iteration Number: 9029\n",
      "Loss: 27.727530969217046\n",
      "l2 norm of gradients: 0.17355229843815626\n",
      "l2 norm of weights: 5.619629751293223\n",
      "---------------------\n",
      "Iteration Number: 9030\n",
      "Loss: 27.726020457236718\n",
      "l2 norm of gradients: 0.17353927261708416\n",
      "l2 norm of weights: 5.619566695756915\n",
      "---------------------\n",
      "Iteration Number: 9031\n",
      "Loss: 27.724510171486056\n",
      "l2 norm of gradients: 0.1735262489533562\n",
      "l2 norm of weights: 5.619503644694731\n",
      "---------------------\n",
      "Iteration Number: 9032\n",
      "Loss: 27.723000111906245\n",
      "l2 norm of gradients: 0.17351322744630884\n",
      "l2 norm of weights: 5.619440598106096\n",
      "---------------------\n",
      "Iteration Number: 9033\n",
      "Loss: 27.72149027844585\n",
      "l2 norm of gradients: 0.17350020809527905\n",
      "l2 norm of weights: 5.619377555990433\n",
      "---------------------\n",
      "Iteration Number: 9034\n",
      "Loss: 27.71998067104815\n",
      "l2 norm of gradients: 0.17348719089960407\n",
      "l2 norm of weights: 5.619314518347168\n",
      "---------------------\n",
      "Iteration Number: 9035\n",
      "Loss: 27.71847128966077\n",
      "l2 norm of gradients: 0.1734741758586215\n",
      "l2 norm of weights: 5.619251485175724\n",
      "---------------------\n",
      "Iteration Number: 9036\n",
      "Loss: 27.71696213422723\n",
      "l2 norm of gradients: 0.1734611629716692\n",
      "l2 norm of weights: 5.619188456475526\n",
      "---------------------\n",
      "Iteration Number: 9037\n",
      "Loss: 27.715453204692665\n",
      "l2 norm of gradients: 0.17344815223808546\n",
      "l2 norm of weights: 5.619125432245999\n",
      "---------------------\n",
      "Iteration Number: 9038\n",
      "Loss: 27.71394450100266\n",
      "l2 norm of gradients: 0.17343514365720897\n",
      "l2 norm of weights: 5.61906241248657\n",
      "---------------------\n",
      "Iteration Number: 9039\n",
      "Loss: 27.712436023103123\n",
      "l2 norm of gradients: 0.17342213722837857\n",
      "l2 norm of weights: 5.618999397196661\n",
      "---------------------\n",
      "Iteration Number: 9040\n",
      "Loss: 27.710927770937946\n",
      "l2 norm of gradients: 0.1734091329509337\n",
      "l2 norm of weights: 5.618936386375699\n",
      "---------------------\n",
      "Iteration Number: 9041\n",
      "Loss: 27.70941974445626\n",
      "l2 norm of gradients: 0.1733961308242137\n",
      "l2 norm of weights: 5.618873380023109\n",
      "---------------------\n",
      "Iteration Number: 9042\n",
      "Loss: 27.707911943598457\n",
      "l2 norm of gradients: 0.17338313084755885\n",
      "l2 norm of weights: 5.618810378138315\n",
      "---------------------\n",
      "Iteration Number: 9043\n",
      "Loss: 27.706404368313045\n",
      "l2 norm of gradients: 0.17337013302030935\n",
      "l2 norm of weights: 5.6187473807207455\n",
      "---------------------\n",
      "Iteration Number: 9044\n",
      "Loss: 27.70489701854542\n",
      "l2 norm of gradients: 0.1733571373418058\n",
      "l2 norm of weights: 5.618684387769824\n",
      "---------------------\n",
      "Iteration Number: 9045\n",
      "Loss: 27.7033898942392\n",
      "l2 norm of gradients: 0.17334414381138927\n",
      "l2 norm of weights: 5.618621399284978\n",
      "---------------------\n",
      "Iteration Number: 9046\n",
      "Loss: 27.701882995340142\n",
      "l2 norm of gradients: 0.17333115242840103\n",
      "l2 norm of weights: 5.618558415265631\n",
      "---------------------\n",
      "Iteration Number: 9047\n",
      "Loss: 27.70037632179621\n",
      "l2 norm of gradients: 0.1733181631921828\n",
      "l2 norm of weights: 5.618495435711212\n",
      "---------------------\n",
      "Iteration Number: 9048\n",
      "Loss: 27.698869873550322\n",
      "l2 norm of gradients: 0.17330517610207652\n",
      "l2 norm of weights: 5.618432460621145\n",
      "---------------------\n",
      "Iteration Number: 9049\n",
      "Loss: 27.69736365054835\n",
      "l2 norm of gradients: 0.1732921911574246\n",
      "l2 norm of weights: 5.6183694899948575\n",
      "---------------------\n",
      "Iteration Number: 9050\n",
      "Loss: 27.69585765273797\n",
      "l2 norm of gradients: 0.1732792083575697\n",
      "l2 norm of weights: 5.618306523831775\n",
      "---------------------\n",
      "Iteration Number: 9051\n",
      "Loss: 27.69435188006301\n",
      "l2 norm of gradients: 0.17326622770185482\n",
      "l2 norm of weights: 5.618243562131325\n",
      "---------------------\n",
      "Iteration Number: 9052\n",
      "Loss: 27.69284633246854\n",
      "l2 norm of gradients: 0.17325324918962337\n",
      "l2 norm of weights: 5.618180604892935\n",
      "---------------------\n",
      "Iteration Number: 9053\n",
      "Loss: 27.6913410099015\n",
      "l2 norm of gradients: 0.17324027282021895\n",
      "l2 norm of weights: 5.6181176521160285\n",
      "---------------------\n",
      "Iteration Number: 9054\n",
      "Loss: 27.689835912307643\n",
      "l2 norm of gradients: 0.1732272985929857\n",
      "l2 norm of weights: 5.618054703800036\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 9055\n",
      "Loss: 27.688331039630615\n",
      "l2 norm of gradients: 0.17321432650726792\n",
      "l2 norm of weights: 5.617991759944384\n",
      "---------------------\n",
      "Iteration Number: 9056\n",
      "Loss: 27.686826391817952\n",
      "l2 norm of gradients: 0.1732013565624103\n",
      "l2 norm of weights: 5.617928820548498\n",
      "---------------------\n",
      "Iteration Number: 9057\n",
      "Loss: 27.685321968816364\n",
      "l2 norm of gradients: 0.17318838875775797\n",
      "l2 norm of weights: 5.617865885611806\n",
      "---------------------\n",
      "Iteration Number: 9058\n",
      "Loss: 27.68381777056781\n",
      "l2 norm of gradients: 0.17317542309265618\n",
      "l2 norm of weights: 5.617802955133736\n",
      "---------------------\n",
      "Iteration Number: 9059\n",
      "Loss: 27.68231379702203\n",
      "l2 norm of gradients: 0.1731624595664507\n",
      "l2 norm of weights: 5.6177400291137145\n",
      "---------------------\n",
      "Iteration Number: 9060\n",
      "Loss: 27.68081004812071\n",
      "l2 norm of gradients: 0.17314949817848763\n",
      "l2 norm of weights: 5.617677107551171\n",
      "---------------------\n",
      "Iteration Number: 9061\n",
      "Loss: 27.679306523812315\n",
      "l2 norm of gradients: 0.17313653892811323\n",
      "l2 norm of weights: 5.617614190445532\n",
      "---------------------\n",
      "Iteration Number: 9062\n",
      "Loss: 27.67780322404407\n",
      "l2 norm of gradients: 0.17312358181467427\n",
      "l2 norm of weights: 5.617551277796226\n",
      "---------------------\n",
      "Iteration Number: 9063\n",
      "Loss: 27.6763001487593\n",
      "l2 norm of gradients: 0.17311062683751788\n",
      "l2 norm of weights: 5.617488369602681\n",
      "---------------------\n",
      "Iteration Number: 9064\n",
      "Loss: 27.67479729790379\n",
      "l2 norm of gradients: 0.1730976739959913\n",
      "l2 norm of weights: 5.617425465864324\n",
      "---------------------\n",
      "Iteration Number: 9065\n",
      "Loss: 27.673294671425086\n",
      "l2 norm of gradients: 0.1730847232894423\n",
      "l2 norm of weights: 5.617362566580584\n",
      "---------------------\n",
      "Iteration Number: 9066\n",
      "Loss: 27.67179226926641\n",
      "l2 norm of gradients: 0.17307177471721893\n",
      "l2 norm of weights: 5.617299671750891\n",
      "---------------------\n",
      "Iteration Number: 9067\n",
      "Loss: 27.670290091378327\n",
      "l2 norm of gradients: 0.17305882827866956\n",
      "l2 norm of weights: 5.617236781374673\n",
      "---------------------\n",
      "Iteration Number: 9068\n",
      "Loss: 27.66878813770108\n",
      "l2 norm of gradients: 0.1730458839731429\n",
      "l2 norm of weights: 5.617173895451356\n",
      "---------------------\n",
      "Iteration Number: 9069\n",
      "Loss: 27.66728640818384\n",
      "l2 norm of gradients: 0.17303294179998804\n",
      "l2 norm of weights: 5.617111013980372\n",
      "---------------------\n",
      "Iteration Number: 9070\n",
      "Loss: 27.665784902772042\n",
      "l2 norm of gradients: 0.17302000175855428\n",
      "l2 norm of weights: 5.617048136961148\n",
      "---------------------\n",
      "Iteration Number: 9071\n",
      "Loss: 27.66428362141193\n",
      "l2 norm of gradients: 0.17300706384819134\n",
      "l2 norm of weights: 5.616985264393113\n",
      "---------------------\n",
      "Iteration Number: 9072\n",
      "Loss: 27.66278256405093\n",
      "l2 norm of gradients: 0.17299412806824926\n",
      "l2 norm of weights: 5.616922396275698\n",
      "---------------------\n",
      "Iteration Number: 9073\n",
      "Loss: 27.66128173063157\n",
      "l2 norm of gradients: 0.17298119441807844\n",
      "l2 norm of weights: 5.6168595326083315\n",
      "---------------------\n",
      "Iteration Number: 9074\n",
      "Loss: 27.65978112109968\n",
      "l2 norm of gradients: 0.17296826289702955\n",
      "l2 norm of weights: 5.616796673390442\n",
      "---------------------\n",
      "Iteration Number: 9075\n",
      "Loss: 27.65828073540608\n",
      "l2 norm of gradients: 0.1729553335044536\n",
      "l2 norm of weights: 5.6167338186214595\n",
      "---------------------\n",
      "Iteration Number: 9076\n",
      "Loss: 27.656780573493148\n",
      "l2 norm of gradients: 0.17294240623970197\n",
      "l2 norm of weights: 5.616670968300814\n",
      "---------------------\n",
      "Iteration Number: 9077\n",
      "Loss: 27.655280635308397\n",
      "l2 norm of gradients: 0.1729294811021264\n",
      "l2 norm of weights: 5.616608122427935\n",
      "---------------------\n",
      "Iteration Number: 9078\n",
      "Loss: 27.65378092079657\n",
      "l2 norm of gradients: 0.17291655809107878\n",
      "l2 norm of weights: 5.616545281002252\n",
      "---------------------\n",
      "Iteration Number: 9079\n",
      "Loss: 27.652281429904335\n",
      "l2 norm of gradients: 0.17290363720591156\n",
      "l2 norm of weights: 5.616482444023196\n",
      "---------------------\n",
      "Iteration Number: 9080\n",
      "Loss: 27.650782162579034\n",
      "l2 norm of gradients: 0.17289071844597734\n",
      "l2 norm of weights: 5.616419611490198\n",
      "---------------------\n",
      "Iteration Number: 9081\n",
      "Loss: 27.64928311876534\n",
      "l2 norm of gradients: 0.1728778018106292\n",
      "l2 norm of weights: 5.6163567834026855\n",
      "---------------------\n",
      "Iteration Number: 9082\n",
      "Loss: 27.647784298411892\n",
      "l2 norm of gradients: 0.1728648872992204\n",
      "l2 norm of weights: 5.61629395976009\n",
      "---------------------\n",
      "Iteration Number: 9083\n",
      "Loss: 27.646285701459522\n",
      "l2 norm of gradients: 0.17285197491110463\n",
      "l2 norm of weights: 5.616231140561843\n",
      "---------------------\n",
      "Iteration Number: 9084\n",
      "Loss: 27.644787327859632\n",
      "l2 norm of gradients: 0.17283906464563586\n",
      "l2 norm of weights: 5.616168325807374\n",
      "---------------------\n",
      "Iteration Number: 9085\n",
      "Loss: 27.643289177558295\n",
      "l2 norm of gradients: 0.17282615650216834\n",
      "l2 norm of weights: 5.616105515496114\n",
      "---------------------\n",
      "Iteration Number: 9086\n",
      "Loss: 27.64179125049862\n",
      "l2 norm of gradients: 0.17281325048005686\n",
      "l2 norm of weights: 5.616042709627495\n",
      "---------------------\n",
      "Iteration Number: 9087\n",
      "Loss: 27.640293546629294\n",
      "l2 norm of gradients: 0.17280034657865626\n",
      "l2 norm of weights: 5.615979908200948\n",
      "---------------------\n",
      "Iteration Number: 9088\n",
      "Loss: 27.638796065895345\n",
      "l2 norm of gradients: 0.17278744479732186\n",
      "l2 norm of weights: 5.615917111215901\n",
      "---------------------\n",
      "Iteration Number: 9089\n",
      "Loss: 27.63729880824513\n",
      "l2 norm of gradients: 0.1727745451354093\n",
      "l2 norm of weights: 5.615854318671789\n",
      "---------------------\n",
      "Iteration Number: 9090\n",
      "Loss: 27.635801773621537\n",
      "l2 norm of gradients: 0.17276164759227453\n",
      "l2 norm of weights: 5.615791530568041\n",
      "---------------------\n",
      "Iteration Number: 9091\n",
      "Loss: 27.634304961974077\n",
      "l2 norm of gradients: 0.17274875216727373\n",
      "l2 norm of weights: 5.61572874690409\n",
      "---------------------\n",
      "Iteration Number: 9092\n",
      "Loss: 27.632808373246924\n",
      "l2 norm of gradients: 0.1727358588597636\n",
      "l2 norm of weights: 5.615665967679367\n",
      "---------------------\n",
      "Iteration Number: 9093\n",
      "Loss: 27.631312007388384\n",
      "l2 norm of gradients: 0.172722967669101\n",
      "l2 norm of weights: 5.615603192893303\n",
      "---------------------\n",
      "Iteration Number: 9094\n",
      "Loss: 27.629815864343392\n",
      "l2 norm of gradients: 0.17271007859464316\n",
      "l2 norm of weights: 5.615540422545331\n",
      "---------------------\n",
      "Iteration Number: 9095\n",
      "Loss: 27.6283199440611\n",
      "l2 norm of gradients: 0.17269719163574765\n",
      "l2 norm of weights: 5.615477656634883\n",
      "---------------------\n",
      "Iteration Number: 9096\n",
      "Loss: 27.62682424648372\n",
      "l2 norm of gradients: 0.17268430679177238\n",
      "l2 norm of weights: 5.61541489516139\n",
      "---------------------\n",
      "Iteration Number: 9097\n",
      "Loss: 27.625328771560056\n",
      "l2 norm of gradients: 0.17267142406207556\n",
      "l2 norm of weights: 5.615352138124285\n",
      "---------------------\n",
      "Iteration Number: 9098\n",
      "Loss: 27.62383351923875\n",
      "l2 norm of gradients: 0.1726585434460157\n",
      "l2 norm of weights: 5.615289385523001\n",
      "---------------------\n",
      "Iteration Number: 9099\n",
      "Loss: 27.622338489462578\n",
      "l2 norm of gradients: 0.17264566494295167\n",
      "l2 norm of weights: 5.615226637356969\n",
      "---------------------\n",
      "Iteration Number: 9100\n",
      "Loss: 27.620843682179565\n",
      "l2 norm of gradients: 0.17263278855224265\n",
      "l2 norm of weights: 5.615163893625622\n",
      "---------------------\n",
      "Iteration Number: 9101\n",
      "Loss: 27.61934909733573\n",
      "l2 norm of gradients: 0.1726199142732481\n",
      "l2 norm of weights: 5.615101154328394\n",
      "---------------------\n",
      "Iteration Number: 9102\n",
      "Loss: 27.617854734878694\n",
      "l2 norm of gradients: 0.172607042105328\n",
      "l2 norm of weights: 5.615038419464716\n",
      "---------------------\n",
      "Iteration Number: 9103\n",
      "Loss: 27.61636059475456\n",
      "l2 norm of gradients: 0.17259417204784225\n",
      "l2 norm of weights: 5.614975689034022\n",
      "---------------------\n",
      "Iteration Number: 9104\n",
      "Loss: 27.614866676910655\n",
      "l2 norm of gradients: 0.17258130410015154\n",
      "l2 norm of weights: 5.614912963035745\n",
      "---------------------\n",
      "Iteration Number: 9105\n",
      "Loss: 27.613372981292407\n",
      "l2 norm of gradients: 0.17256843826161652\n",
      "l2 norm of weights: 5.614850241469318\n",
      "---------------------\n",
      "Iteration Number: 9106\n",
      "Loss: 27.611879507846794\n",
      "l2 norm of gradients: 0.17255557453159834\n",
      "l2 norm of weights: 5.614787524334175\n",
      "---------------------\n",
      "Iteration Number: 9107\n",
      "Loss: 27.610386256519725\n",
      "l2 norm of gradients: 0.17254271290945844\n",
      "l2 norm of weights: 5.614724811629748\n",
      "---------------------\n",
      "Iteration Number: 9108\n",
      "Loss: 27.608893227261103\n",
      "l2 norm of gradients: 0.17252985339455854\n",
      "l2 norm of weights: 5.614662103355472\n",
      "---------------------\n",
      "Iteration Number: 9109\n",
      "Loss: 27.60740042001309\n",
      "l2 norm of gradients: 0.17251699598626072\n",
      "l2 norm of weights: 5.614599399510779\n",
      "---------------------\n",
      "Iteration Number: 9110\n",
      "Loss: 27.605907834725564\n",
      "l2 norm of gradients: 0.17250414068392733\n",
      "l2 norm of weights: 5.614536700095105\n",
      "---------------------\n",
      "Iteration Number: 9111\n",
      "Loss: 27.604415471344744\n",
      "l2 norm of gradients: 0.17249128748692116\n",
      "l2 norm of weights: 5.6144740051078825\n",
      "---------------------\n",
      "Iteration Number: 9112\n",
      "Loss: 27.6029233298177\n",
      "l2 norm of gradients: 0.17247843639460514\n",
      "l2 norm of weights: 5.614411314548546\n",
      "---------------------\n",
      "Iteration Number: 9113\n",
      "Loss: 27.601431410090157\n",
      "l2 norm of gradients: 0.17246558740634269\n",
      "l2 norm of weights: 5.614348628416529\n",
      "---------------------\n",
      "Iteration Number: 9114\n",
      "Loss: 27.599939712109524\n",
      "l2 norm of gradients: 0.17245274052149742\n",
      "l2 norm of weights: 5.6142859467112665\n",
      "---------------------\n",
      "Iteration Number: 9115\n",
      "Loss: 27.598448235822012\n",
      "l2 norm of gradients: 0.17243989573943336\n",
      "l2 norm of weights: 5.614223269432194\n",
      "---------------------\n",
      "Iteration Number: 9116\n",
      "Loss: 27.59695698117643\n",
      "l2 norm of gradients: 0.17242705305951478\n",
      "l2 norm of weights: 5.614160596578744\n",
      "---------------------\n",
      "Iteration Number: 9117\n",
      "Loss: 27.59546594811652\n",
      "l2 norm of gradients: 0.17241421248110625\n",
      "l2 norm of weights: 5.614097928150351\n",
      "---------------------\n",
      "Iteration Number: 9118\n",
      "Loss: 27.59397513659235\n",
      "l2 norm of gradients: 0.17240137400357278\n",
      "l2 norm of weights: 5.614035264146452\n",
      "---------------------\n",
      "Iteration Number: 9119\n",
      "Loss: 27.592484546548185\n",
      "l2 norm of gradients: 0.1723885376262795\n",
      "l2 norm of weights: 5.61397260456648\n",
      "---------------------\n",
      "Iteration Number: 9120\n",
      "Loss: 27.590994177931474\n",
      "l2 norm of gradients: 0.17237570334859212\n",
      "l2 norm of weights: 5.61390994940987\n",
      "---------------------\n",
      "Iteration Number: 9121\n",
      "Loss: 27.589504030689422\n",
      "l2 norm of gradients: 0.17236287116987645\n",
      "l2 norm of weights: 5.613847298676059\n",
      "---------------------\n",
      "Iteration Number: 9122\n",
      "Loss: 27.588014104771652\n",
      "l2 norm of gradients: 0.17235004108949867\n",
      "l2 norm of weights: 5.6137846523644805\n",
      "---------------------\n",
      "Iteration Number: 9123\n",
      "Loss: 27.586524400122723\n",
      "l2 norm of gradients: 0.1723372131068253\n",
      "l2 norm of weights: 5.61372201047457\n",
      "---------------------\n",
      "Iteration Number: 9124\n",
      "Loss: 27.58503491668865\n",
      "l2 norm of gradients: 0.1723243872212232\n",
      "l2 norm of weights: 5.613659373005763\n",
      "---------------------\n",
      "Iteration Number: 9125\n",
      "Loss: 27.583545654417044\n",
      "l2 norm of gradients: 0.17231156343205944\n",
      "l2 norm of weights: 5.613596739957497\n",
      "---------------------\n",
      "Iteration Number: 9126\n",
      "Loss: 27.582056613256533\n",
      "l2 norm of gradients: 0.17229874173870152\n",
      "l2 norm of weights: 5.613534111329205\n",
      "---------------------\n",
      "Iteration Number: 9127\n",
      "Loss: 27.5805677931534\n",
      "l2 norm of gradients: 0.17228592214051722\n",
      "l2 norm of weights: 5.613471487120325\n",
      "---------------------\n",
      "Iteration Number: 9128\n",
      "Loss: 27.579079194054664\n",
      "l2 norm of gradients: 0.17227310463687465\n",
      "l2 norm of weights: 5.613408867330292\n",
      "---------------------\n",
      "Iteration Number: 9129\n",
      "Loss: 27.577590815907875\n",
      "l2 norm of gradients: 0.1722602892271421\n",
      "l2 norm of weights: 5.613346251958541\n",
      "---------------------\n",
      "Iteration Number: 9130\n",
      "Loss: 27.57610265865742\n",
      "l2 norm of gradients: 0.1722474759106884\n",
      "l2 norm of weights: 5.613283641004511\n",
      "---------------------\n",
      "Iteration Number: 9131\n",
      "Loss: 27.574614722252793\n",
      "l2 norm of gradients: 0.17223466468688248\n",
      "l2 norm of weights: 5.613221034467636\n",
      "---------------------\n",
      "Iteration Number: 9132\n",
      "Loss: 27.573127006642626\n",
      "l2 norm of gradients: 0.17222185555509378\n",
      "l2 norm of weights: 5.613158432347353\n",
      "---------------------\n",
      "Iteration Number: 9133\n",
      "Loss: 27.57163951177119\n",
      "l2 norm of gradients: 0.17220904851469182\n",
      "l2 norm of weights: 5.613095834643099\n",
      "---------------------\n",
      "Iteration Number: 9134\n",
      "Loss: 27.570152237587322\n",
      "l2 norm of gradients: 0.17219624356504665\n",
      "l2 norm of weights: 5.613033241354311\n",
      "---------------------\n",
      "Iteration Number: 9135\n",
      "Loss: 27.568665184036348\n",
      "l2 norm of gradients: 0.17218344070552852\n",
      "l2 norm of weights: 5.612970652480426\n",
      "---------------------\n",
      "Iteration Number: 9136\n",
      "Loss: 27.56717835106817\n",
      "l2 norm of gradients: 0.172170639935508\n",
      "l2 norm of weights: 5.612908068020879\n",
      "---------------------\n",
      "Iteration Number: 9137\n",
      "Loss: 27.56569173863002\n",
      "l2 norm of gradients: 0.172157841254356\n",
      "l2 norm of weights: 5.612845487975109\n",
      "---------------------\n",
      "Iteration Number: 9138\n",
      "Loss: 27.56420534666568\n",
      "l2 norm of gradients: 0.17214504466144373\n",
      "l2 norm of weights: 5.612782912342552\n",
      "---------------------\n",
      "Iteration Number: 9139\n",
      "Loss: 27.56271917512701\n",
      "l2 norm of gradients: 0.17213225015614272\n",
      "l2 norm of weights: 5.612720341122646\n",
      "---------------------\n",
      "Iteration Number: 9140\n",
      "Loss: 27.56123322395777\n",
      "l2 norm of gradients: 0.17211945773782475\n",
      "l2 norm of weights: 5.61265777431483\n",
      "---------------------\n",
      "Iteration Number: 9141\n",
      "Loss: 27.5597474931071\n",
      "l2 norm of gradients: 0.17210666740586195\n",
      "l2 norm of weights: 5.612595211918538\n",
      "---------------------\n",
      "Iteration Number: 9142\n",
      "Loss: 27.558261982522367\n",
      "l2 norm of gradients: 0.17209387915962684\n",
      "l2 norm of weights: 5.612532653933209\n",
      "---------------------\n",
      "Iteration Number: 9143\n",
      "Loss: 27.556776692148375\n",
      "l2 norm of gradients: 0.17208109299849214\n",
      "l2 norm of weights: 5.612470100358283\n",
      "---------------------\n",
      "Iteration Number: 9144\n",
      "Loss: 27.555291621936615\n",
      "l2 norm of gradients: 0.1720683089218309\n",
      "l2 norm of weights: 5.612407551193195\n",
      "---------------------\n",
      "Iteration Number: 9145\n",
      "Loss: 27.553806771831777\n",
      "l2 norm of gradients: 0.17205552692901652\n",
      "l2 norm of weights: 5.612345006437385\n",
      "---------------------\n",
      "Iteration Number: 9146\n",
      "Loss: 27.55232214177933\n",
      "l2 norm of gradients: 0.17204274701942265\n",
      "l2 norm of weights: 5.6122824660902895\n",
      "---------------------\n",
      "Iteration Number: 9147\n",
      "Loss: 27.550837731732162\n",
      "l2 norm of gradients: 0.17202996919242328\n",
      "l2 norm of weights: 5.612219930151349\n",
      "---------------------\n",
      "Iteration Number: 9148\n",
      "Loss: 27.5493535416353\n",
      "l2 norm of gradients: 0.17201719344739275\n",
      "l2 norm of weights: 5.61215739862\n",
      "---------------------\n",
      "Iteration Number: 9149\n",
      "Loss: 27.547869571433104\n",
      "l2 norm of gradients: 0.17200441978370565\n",
      "l2 norm of weights: 5.612094871495681\n",
      "---------------------\n",
      "Iteration Number: 9150\n",
      "Loss: 27.54638582107879\n",
      "l2 norm of gradients: 0.17199164820073687\n",
      "l2 norm of weights: 5.6120323487778325\n",
      "---------------------\n",
      "Iteration Number: 9151\n",
      "Loss: 27.54490229051547\n",
      "l2 norm of gradients: 0.17197887869786166\n",
      "l2 norm of weights: 5.611969830465891\n",
      "---------------------\n",
      "Iteration Number: 9152\n",
      "Loss: 27.543418979692124\n",
      "l2 norm of gradients: 0.1719661112744555\n",
      "l2 norm of weights: 5.6119073165592965\n",
      "---------------------\n",
      "Iteration Number: 9153\n",
      "Loss: 27.541935888556154\n",
      "l2 norm of gradients: 0.17195334592989428\n",
      "l2 norm of weights: 5.611844807057488\n",
      "---------------------\n",
      "Iteration Number: 9154\n",
      "Loss: 27.540453017056823\n",
      "l2 norm of gradients: 0.1719405826635541\n",
      "l2 norm of weights: 5.611782301959905\n",
      "---------------------\n",
      "Iteration Number: 9155\n",
      "Loss: 27.538970365138486\n",
      "l2 norm of gradients: 0.17192782147481148\n",
      "l2 norm of weights: 5.611719801265984\n",
      "---------------------\n",
      "Iteration Number: 9156\n",
      "Loss: 27.53748793275085\n",
      "l2 norm of gradients: 0.17191506236304305\n",
      "l2 norm of weights: 5.611657304975169\n",
      "---------------------\n",
      "Iteration Number: 9157\n",
      "Loss: 27.53600571984222\n",
      "l2 norm of gradients: 0.17190230532762596\n",
      "l2 norm of weights: 5.611594813086896\n",
      "---------------------\n",
      "Iteration Number: 9158\n",
      "Loss: 27.534523726358625\n",
      "l2 norm of gradients: 0.17188955036793752\n",
      "l2 norm of weights: 5.611532325600605\n",
      "---------------------\n",
      "Iteration Number: 9159\n",
      "Loss: 27.5330419522475\n",
      "l2 norm of gradients: 0.17187679748335538\n",
      "l2 norm of weights: 5.611469842515737\n",
      "---------------------\n",
      "Iteration Number: 9160\n",
      "Loss: 27.53156039745659\n",
      "l2 norm of gradients: 0.1718640466732576\n",
      "l2 norm of weights: 5.611407363831731\n",
      "---------------------\n",
      "Iteration Number: 9161\n",
      "Loss: 27.530079061935865\n",
      "l2 norm of gradients: 0.17185129793702236\n",
      "l2 norm of weights: 5.611344889548026\n",
      "---------------------\n",
      "Iteration Number: 9162\n",
      "Loss: 27.528597945631585\n",
      "l2 norm of gradients: 0.1718385512740283\n",
      "l2 norm of weights: 5.6112824196640645\n",
      "---------------------\n",
      "Iteration Number: 9163\n",
      "Loss: 27.52711704849098\n",
      "l2 norm of gradients: 0.17182580668365433\n",
      "l2 norm of weights: 5.611219954179285\n",
      "---------------------\n",
      "Iteration Number: 9164\n",
      "Loss: 27.525636370462863\n",
      "l2 norm of gradients: 0.1718130641652795\n",
      "l2 norm of weights: 5.611157493093128\n",
      "---------------------\n",
      "Iteration Number: 9165\n",
      "Loss: 27.524155911494113\n",
      "l2 norm of gradients: 0.17180032371828338\n",
      "l2 norm of weights: 5.611095036405034\n",
      "---------------------\n",
      "Iteration Number: 9166\n",
      "Loss: 27.52267567153232\n",
      "l2 norm of gradients: 0.1717875853420458\n",
      "l2 norm of weights: 5.611032584114443\n",
      "---------------------\n",
      "Iteration Number: 9167\n",
      "Loss: 27.52119565052609\n",
      "l2 norm of gradients: 0.17177484903594675\n",
      "l2 norm of weights: 5.610970136220796\n",
      "---------------------\n",
      "Iteration Number: 9168\n",
      "Loss: 27.51971584842308\n",
      "l2 norm of gradients: 0.17176211479936673\n",
      "l2 norm of weights: 5.6109076927235355\n",
      "---------------------\n",
      "Iteration Number: 9169\n",
      "Loss: 27.51823626517134\n",
      "l2 norm of gradients: 0.1717493826316864\n",
      "l2 norm of weights: 5.6108452536221\n",
      "---------------------\n",
      "Iteration Number: 9170\n",
      "Loss: 27.51675690071789\n",
      "l2 norm of gradients: 0.1717366525322867\n",
      "l2 norm of weights: 5.610782818915931\n",
      "---------------------\n",
      "Iteration Number: 9171\n",
      "Loss: 27.515277755012864\n",
      "l2 norm of gradients: 0.171723924500549\n",
      "l2 norm of weights: 5.610720388604471\n",
      "---------------------\n",
      "Iteration Number: 9172\n",
      "Loss: 27.513798828000244\n",
      "l2 norm of gradients: 0.17171119853585487\n",
      "l2 norm of weights: 5.610657962687159\n",
      "---------------------\n",
      "Iteration Number: 9173\n",
      "Loss: 27.512320119632516\n",
      "l2 norm of gradients: 0.17169847463758617\n",
      "l2 norm of weights: 5.6105955411634385\n",
      "---------------------\n",
      "Iteration Number: 9174\n",
      "Loss: 27.5108416298545\n",
      "l2 norm of gradients: 0.17168575280512519\n",
      "l2 norm of weights: 5.6105331240327505\n",
      "---------------------\n",
      "Iteration Number: 9175\n",
      "Loss: 27.50936335861487\n",
      "l2 norm of gradients: 0.17167303303785436\n",
      "l2 norm of weights: 5.610470711294536\n",
      "---------------------\n",
      "Iteration Number: 9176\n",
      "Loss: 27.50788530586209\n",
      "l2 norm of gradients: 0.1716603153351565\n",
      "l2 norm of weights: 5.610408302948236\n",
      "---------------------\n",
      "Iteration Number: 9177\n",
      "Loss: 27.50640747154425\n",
      "l2 norm of gradients: 0.1716475996964147\n",
      "l2 norm of weights: 5.610345898993295\n",
      "---------------------\n",
      "Iteration Number: 9178\n",
      "Loss: 27.504929855608943\n",
      "l2 norm of gradients: 0.17163488612101235\n",
      "l2 norm of weights: 5.610283499429152\n",
      "---------------------\n",
      "Iteration Number: 9179\n",
      "Loss: 27.50345245800361\n",
      "l2 norm of gradients: 0.17162217460833318\n",
      "l2 norm of weights: 5.610221104255252\n",
      "---------------------\n",
      "Iteration Number: 9180\n",
      "Loss: 27.501975278677513\n",
      "l2 norm of gradients: 0.17160946515776113\n",
      "l2 norm of weights: 5.6101587134710345\n",
      "---------------------\n",
      "Iteration Number: 9181\n",
      "Loss: 27.500498317578586\n",
      "l2 norm of gradients: 0.17159675776868055\n",
      "l2 norm of weights: 5.610096327075943\n",
      "---------------------\n",
      "Iteration Number: 9182\n",
      "Loss: 27.499021574653135\n",
      "l2 norm of gradients: 0.171584052440476\n",
      "l2 norm of weights: 5.6100339450694205\n",
      "---------------------\n",
      "Iteration Number: 9183\n",
      "Loss: 27.497545049851052\n",
      "l2 norm of gradients: 0.1715713491725323\n",
      "l2 norm of weights: 5.609971567450908\n",
      "---------------------\n",
      "Iteration Number: 9184\n",
      "Loss: 27.496068743122066\n",
      "l2 norm of gradients: 0.17155864796423478\n",
      "l2 norm of weights: 5.60990919421985\n",
      "---------------------\n",
      "Iteration Number: 9185\n",
      "Loss: 27.4945926544101\n",
      "l2 norm of gradients: 0.17154594881496885\n",
      "l2 norm of weights: 5.609846825375688\n",
      "---------------------\n",
      "Iteration Number: 9186\n",
      "Loss: 27.493116783666906\n",
      "l2 norm of gradients: 0.1715332517241202\n",
      "l2 norm of weights: 5.609784460917867\n",
      "---------------------\n",
      "Iteration Number: 9187\n",
      "Loss: 27.49164113083982\n",
      "l2 norm of gradients: 0.17152055669107505\n",
      "l2 norm of weights: 5.6097221008458265\n",
      "---------------------\n",
      "Iteration Number: 9188\n",
      "Loss: 27.490165695877188\n",
      "l2 norm of gradients: 0.17150786371521964\n",
      "l2 norm of weights: 5.6096597451590124\n",
      "---------------------\n",
      "Iteration Number: 9189\n",
      "Loss: 27.488690478725463\n",
      "l2 norm of gradients: 0.17149517279594073\n",
      "l2 norm of weights: 5.609597393856867\n",
      "---------------------\n",
      "Iteration Number: 9190\n",
      "Loss: 27.48721547933321\n",
      "l2 norm of gradients: 0.17148248393262527\n",
      "l2 norm of weights: 5.609535046938834\n",
      "---------------------\n",
      "Iteration Number: 9191\n",
      "Loss: 27.48574069765162\n",
      "l2 norm of gradients: 0.17146979712466048\n",
      "l2 norm of weights: 5.609472704404357\n",
      "---------------------\n",
      "Iteration Number: 9192\n",
      "Loss: 27.484266133626992\n",
      "l2 norm of gradients: 0.17145711237143393\n",
      "l2 norm of weights: 5.609410366252879\n",
      "---------------------\n",
      "Iteration Number: 9193\n",
      "Loss: 27.482791787206732\n",
      "l2 norm of gradients: 0.1714444296723335\n",
      "l2 norm of weights: 5.609348032483845\n",
      "---------------------\n",
      "Iteration Number: 9194\n",
      "Loss: 27.481317658341418\n",
      "l2 norm of gradients: 0.17143174902674724\n",
      "l2 norm of weights: 5.609285703096697\n",
      "---------------------\n",
      "Iteration Number: 9195\n",
      "Loss: 27.47984374697579\n",
      "l2 norm of gradients: 0.17141907043406368\n",
      "l2 norm of weights: 5.6092233780908805\n",
      "---------------------\n",
      "Iteration Number: 9196\n",
      "Loss: 27.478370053062754\n",
      "l2 norm of gradients: 0.17140639389367146\n",
      "l2 norm of weights: 5.6091610574658395\n",
      "---------------------\n",
      "Iteration Number: 9197\n",
      "Loss: 27.476896576547396\n",
      "l2 norm of gradients: 0.1713937194049597\n",
      "l2 norm of weights: 5.609098741221017\n",
      "---------------------\n",
      "Iteration Number: 9198\n",
      "Loss: 27.475423317379303\n",
      "l2 norm of gradients: 0.17138104696731757\n",
      "l2 norm of weights: 5.609036429355859\n",
      "---------------------\n",
      "Iteration Number: 9199\n",
      "Loss: 27.473950275506972\n",
      "l2 norm of gradients: 0.17136837658013485\n",
      "l2 norm of weights: 5.608974121869809\n",
      "---------------------\n",
      "Iteration Number: 9200\n",
      "Loss: 27.472477450877815\n",
      "l2 norm of gradients: 0.17135570824280133\n",
      "l2 norm of weights: 5.608911818762311\n",
      "---------------------\n",
      "Iteration Number: 9201\n",
      "Loss: 27.471004843442973\n",
      "l2 norm of gradients: 0.17134304195470723\n",
      "l2 norm of weights: 5.608849520032812\n",
      "---------------------\n",
      "Iteration Number: 9202\n",
      "Loss: 27.469532453147142\n",
      "l2 norm of gradients: 0.171330377715243\n",
      "l2 norm of weights: 5.6087872256807545\n",
      "---------------------\n",
      "Iteration Number: 9203\n",
      "Loss: 27.468060279940975\n",
      "l2 norm of gradients: 0.17131771552379943\n",
      "l2 norm of weights: 5.608724935705584\n",
      "---------------------\n",
      "Iteration Number: 9204\n",
      "Loss: 27.46658832377386\n",
      "l2 norm of gradients: 0.17130505537976765\n",
      "l2 norm of weights: 5.608662650106745\n",
      "---------------------\n",
      "Iteration Number: 9205\n",
      "Loss: 27.465116584593464\n",
      "l2 norm of gradients: 0.17129239728253898\n",
      "l2 norm of weights: 5.6086003688836845\n",
      "---------------------\n",
      "Iteration Number: 9206\n",
      "Loss: 27.46364506234731\n",
      "l2 norm of gradients: 0.171279741231505\n",
      "l2 norm of weights: 5.608538092035847\n",
      "---------------------\n",
      "Iteration Number: 9207\n",
      "Loss: 27.462173756983404\n",
      "l2 norm of gradients: 0.1712670872260577\n",
      "l2 norm of weights: 5.608475819562677\n",
      "---------------------\n",
      "Iteration Number: 9208\n",
      "Loss: 27.460702668454758\n",
      "l2 norm of gradients: 0.17125443526558937\n",
      "l2 norm of weights: 5.60841355146362\n",
      "---------------------\n",
      "Iteration Number: 9209\n",
      "Loss: 27.459231796703545\n",
      "l2 norm of gradients: 0.17124178534949247\n",
      "l2 norm of weights: 5.608351287738123\n",
      "---------------------\n",
      "Iteration Number: 9210\n",
      "Loss: 27.457761141684216\n",
      "l2 norm of gradients: 0.17122913747715976\n",
      "l2 norm of weights: 5.608289028385631\n",
      "---------------------\n",
      "Iteration Number: 9211\n",
      "Loss: 27.45629070334177\n",
      "l2 norm of gradients: 0.17121649164798441\n",
      "l2 norm of weights: 5.6082267734055895\n",
      "---------------------\n",
      "Iteration Number: 9212\n",
      "Loss: 27.45482048162619\n",
      "l2 norm of gradients: 0.17120384786135973\n",
      "l2 norm of weights: 5.608164522797445\n",
      "---------------------\n",
      "Iteration Number: 9213\n",
      "Loss: 27.453350476485937\n",
      "l2 norm of gradients: 0.17119120611667954\n",
      "l2 norm of weights: 5.608102276560643\n",
      "---------------------\n",
      "Iteration Number: 9214\n",
      "Loss: 27.45188068786954\n",
      "l2 norm of gradients: 0.17117856641333767\n",
      "l2 norm of weights: 5.608040034694632\n",
      "---------------------\n",
      "Iteration Number: 9215\n",
      "Loss: 27.45041111572721\n",
      "l2 norm of gradients: 0.1711659287507284\n",
      "l2 norm of weights: 5.607977797198855\n",
      "---------------------\n",
      "Iteration Number: 9216\n",
      "Loss: 27.448941760005013\n",
      "l2 norm of gradients: 0.17115329312824623\n",
      "l2 norm of weights: 5.60791556407276\n",
      "---------------------\n",
      "Iteration Number: 9217\n",
      "Loss: 27.447472620653645\n",
      "l2 norm of gradients: 0.17114065954528615\n",
      "l2 norm of weights: 5.607853335315794\n",
      "---------------------\n",
      "Iteration Number: 9218\n",
      "Loss: 27.446003697621645\n",
      "l2 norm of gradients: 0.1711280280012431\n",
      "l2 norm of weights: 5.607791110927404\n",
      "---------------------\n",
      "Iteration Number: 9219\n",
      "Loss: 27.444534990856233\n",
      "l2 norm of gradients: 0.1711153984955125\n",
      "l2 norm of weights: 5.607728890907035\n",
      "---------------------\n",
      "Iteration Number: 9220\n",
      "Loss: 27.4430665003087\n",
      "l2 norm of gradients: 0.17110277102749014\n",
      "l2 norm of weights: 5.607666675254136\n",
      "---------------------\n",
      "Iteration Number: 9221\n",
      "Loss: 27.441598225925418\n",
      "l2 norm of gradients: 0.1710901455965719\n",
      "l2 norm of weights: 5.607604463968153\n",
      "---------------------\n",
      "Iteration Number: 9222\n",
      "Loss: 27.440130167658594\n",
      "l2 norm of gradients: 0.17107752220215408\n",
      "l2 norm of weights: 5.607542257048532\n",
      "---------------------\n",
      "Iteration Number: 9223\n",
      "Loss: 27.438662325452345\n",
      "l2 norm of gradients: 0.17106490084363318\n",
      "l2 norm of weights: 5.607480054494723\n",
      "---------------------\n",
      "Iteration Number: 9224\n",
      "Loss: 27.437194699260232\n",
      "l2 norm of gradients: 0.1710522815204061\n",
      "l2 norm of weights: 5.607417856306172\n",
      "---------------------\n",
      "Iteration Number: 9225\n",
      "Loss: 27.435727289028108\n",
      "l2 norm of gradients: 0.17103966423186992\n",
      "l2 norm of weights: 5.607355662482325\n",
      "---------------------\n",
      "Iteration Number: 9226\n",
      "Loss: 27.4342600947058\n",
      "l2 norm of gradients: 0.17102704897742207\n",
      "l2 norm of weights: 5.607293473022633\n",
      "---------------------\n",
      "Iteration Number: 9227\n",
      "Loss: 27.43279311624185\n",
      "l2 norm of gradients: 0.17101443575646022\n",
      "l2 norm of weights: 5.607231287926541\n",
      "---------------------\n",
      "Iteration Number: 9228\n",
      "Loss: 27.431326353586897\n",
      "l2 norm of gradients: 0.1710018245683823\n",
      "l2 norm of weights: 5.6071691071934975\n",
      "---------------------\n",
      "Iteration Number: 9229\n",
      "Loss: 27.42985980668776\n",
      "l2 norm of gradients: 0.1709892154125866\n",
      "l2 norm of weights: 5.607106930822951\n",
      "---------------------\n",
      "Iteration Number: 9230\n",
      "Loss: 27.428393475494584\n",
      "l2 norm of gradients: 0.17097660828847167\n",
      "l2 norm of weights: 5.607044758814349\n",
      "---------------------\n",
      "Iteration Number: 9231\n",
      "Loss: 27.42692735995512\n",
      "l2 norm of gradients: 0.1709640031954363\n",
      "l2 norm of weights: 5.6069825911671405\n",
      "---------------------\n",
      "Iteration Number: 9232\n",
      "Loss: 27.425461460022017\n",
      "l2 norm of gradients: 0.17095140013287963\n",
      "l2 norm of weights: 5.606920427880773\n",
      "---------------------\n",
      "Iteration Number: 9233\n",
      "Loss: 27.423995775639426\n",
      "l2 norm of gradients: 0.17093879910020102\n",
      "l2 norm of weights: 5.606858268954696\n",
      "---------------------\n",
      "Iteration Number: 9234\n",
      "Loss: 27.42253030675951\n",
      "l2 norm of gradients: 0.17092620009680012\n",
      "l2 norm of weights: 5.606796114388357\n",
      "---------------------\n",
      "Iteration Number: 9235\n",
      "Loss: 27.421065053329897\n",
      "l2 norm of gradients: 0.17091360312207696\n",
      "l2 norm of weights: 5.606733964181205\n",
      "---------------------\n",
      "Iteration Number: 9236\n",
      "Loss: 27.4196000153013\n",
      "l2 norm of gradients: 0.17090100817543172\n",
      "l2 norm of weights: 5.606671818332689\n",
      "---------------------\n",
      "Iteration Number: 9237\n",
      "Loss: 27.41813519261885\n",
      "l2 norm of gradients: 0.1708884152562649\n",
      "l2 norm of weights: 5.606609676842258\n",
      "---------------------\n",
      "Iteration Number: 9238\n",
      "Loss: 27.416670585237693\n",
      "l2 norm of gradients: 0.17087582436397733\n",
      "l2 norm of weights: 5.606547539709362\n",
      "---------------------\n",
      "Iteration Number: 9239\n",
      "Loss: 27.41520619310331\n",
      "l2 norm of gradients: 0.17086323549797003\n",
      "l2 norm of weights: 5.606485406933448\n",
      "---------------------\n",
      "Iteration Number: 9240\n",
      "Loss: 27.41374201616539\n",
      "l2 norm of gradients: 0.1708506486576445\n",
      "l2 norm of weights: 5.606423278513966\n",
      "---------------------\n",
      "Iteration Number: 9241\n",
      "Loss: 27.412278054375275\n",
      "l2 norm of gradients: 0.1708380638424022\n",
      "l2 norm of weights: 5.606361154450366\n",
      "---------------------\n",
      "Iteration Number: 9242\n",
      "Loss: 27.41081430767833\n",
      "l2 norm of gradients: 0.1708254810516452\n",
      "l2 norm of weights: 5.606299034742097\n",
      "---------------------\n",
      "Iteration Number: 9243\n",
      "Loss: 27.409350776025345\n",
      "l2 norm of gradients: 0.17081290028477558\n",
      "l2 norm of weights: 5.60623691938861\n",
      "---------------------\n",
      "Iteration Number: 9244\n",
      "Loss: 27.407887459365924\n",
      "l2 norm of gradients: 0.17080032154119593\n",
      "l2 norm of weights: 5.606174808389352\n",
      "---------------------\n",
      "Iteration Number: 9245\n",
      "Loss: 27.40642435764818\n",
      "l2 norm of gradients: 0.17078774482030895\n",
      "l2 norm of weights: 5.6061127017437755\n",
      "---------------------\n",
      "Iteration Number: 9246\n",
      "Loss: 27.404961470824183\n",
      "l2 norm of gradients: 0.1707751701215177\n",
      "l2 norm of weights: 5.606050599451329\n",
      "---------------------\n",
      "Iteration Number: 9247\n",
      "Loss: 27.403498798841024\n",
      "l2 norm of gradients: 0.17076259744422542\n",
      "l2 norm of weights: 5.605988501511463\n",
      "---------------------\n",
      "Iteration Number: 9248\n",
      "Loss: 27.4020363416479\n",
      "l2 norm of gradients: 0.17075002678783585\n",
      "l2 norm of weights: 5.605926407923628\n",
      "---------------------\n",
      "Iteration Number: 9249\n",
      "Loss: 27.400574099194465\n",
      "l2 norm of gradients: 0.17073745815175276\n",
      "l2 norm of weights: 5.605864318687274\n",
      "---------------------\n",
      "Iteration Number: 9250\n",
      "Loss: 27.399112071430668\n",
      "l2 norm of gradients: 0.1707248915353803\n",
      "l2 norm of weights: 5.605802233801851\n",
      "---------------------\n",
      "Iteration Number: 9251\n",
      "Loss: 27.397650258307216\n",
      "l2 norm of gradients: 0.170712326938123\n",
      "l2 norm of weights: 5.605740153266812\n",
      "---------------------\n",
      "Iteration Number: 9252\n",
      "Loss: 27.396188659769845\n",
      "l2 norm of gradients: 0.17069976435938547\n",
      "l2 norm of weights: 5.6056780770816035\n",
      "---------------------\n",
      "Iteration Number: 9253\n",
      "Loss: 27.39472727577041\n",
      "l2 norm of gradients: 0.1706872037985727\n",
      "l2 norm of weights: 5.605616005245679\n",
      "---------------------\n",
      "Iteration Number: 9254\n",
      "Loss: 27.393266106258636\n",
      "l2 norm of gradients: 0.17067464525509\n",
      "l2 norm of weights: 5.605553937758489\n",
      "---------------------\n",
      "Iteration Number: 9255\n",
      "Loss: 27.391805151180677\n",
      "l2 norm of gradients: 0.17066208872834285\n",
      "l2 norm of weights: 5.605491874619485\n",
      "---------------------\n",
      "Iteration Number: 9256\n",
      "Loss: 27.39034441049126\n",
      "l2 norm of gradients: 0.17064953421773713\n",
      "l2 norm of weights: 5.605429815828117\n",
      "---------------------\n",
      "Iteration Number: 9257\n",
      "Loss: 27.388883884136682\n",
      "l2 norm of gradients: 0.17063698172267885\n",
      "l2 norm of weights: 5.605367761383837\n",
      "---------------------\n",
      "Iteration Number: 9258\n",
      "Loss: 27.387423572066474\n",
      "l2 norm of gradients: 0.17062443124257448\n",
      "l2 norm of weights: 5.605305711286096\n",
      "---------------------\n",
      "Iteration Number: 9259\n",
      "Loss: 27.385963474229904\n",
      "l2 norm of gradients: 0.17061188277683062\n",
      "l2 norm of weights: 5.6052436655343465\n",
      "---------------------\n",
      "Iteration Number: 9260\n",
      "Loss: 27.38450359057766\n",
      "l2 norm of gradients: 0.17059933632485416\n",
      "l2 norm of weights: 5.605181624128039\n",
      "---------------------\n",
      "Iteration Number: 9261\n",
      "Loss: 27.383043921058366\n",
      "l2 norm of gradients: 0.1705867918860523\n",
      "l2 norm of weights: 5.605119587066625\n",
      "---------------------\n",
      "Iteration Number: 9262\n",
      "Loss: 27.381584465621813\n",
      "l2 norm of gradients: 0.17057424945983252\n",
      "l2 norm of weights: 5.605057554349557\n",
      "---------------------\n",
      "Iteration Number: 9263\n",
      "Loss: 27.380125224217394\n",
      "l2 norm of gradients: 0.17056170904560253\n",
      "l2 norm of weights: 5.604995525976287\n",
      "---------------------\n",
      "Iteration Number: 9264\n",
      "Loss: 27.37866619679724\n",
      "l2 norm of gradients: 0.1705491706427704\n",
      "l2 norm of weights: 5.604933501946267\n",
      "---------------------\n",
      "Iteration Number: 9265\n",
      "Loss: 27.3772073833071\n",
      "l2 norm of gradients: 0.17053663425074436\n",
      "l2 norm of weights: 5.6048714822589485\n",
      "---------------------\n",
      "Iteration Number: 9266\n",
      "Loss: 27.375748783697613\n",
      "l2 norm of gradients: 0.17052409986893302\n",
      "l2 norm of weights: 5.6048094669137845\n",
      "---------------------\n",
      "Iteration Number: 9267\n",
      "Loss: 27.374290397920365\n",
      "l2 norm of gradients: 0.17051156749674523\n",
      "l2 norm of weights: 5.6047474559102275\n",
      "---------------------\n",
      "Iteration Number: 9268\n",
      "Loss: 27.372832225924235\n",
      "l2 norm of gradients: 0.17049903713359\n",
      "l2 norm of weights: 5.60468544924773\n",
      "---------------------\n",
      "Iteration Number: 9269\n",
      "Loss: 27.37137426765701\n",
      "l2 norm of gradients: 0.1704865087788768\n",
      "l2 norm of weights: 5.604623446925744\n",
      "---------------------\n",
      "Iteration Number: 9270\n",
      "Loss: 27.36991652307114\n",
      "l2 norm of gradients: 0.17047398243201528\n",
      "l2 norm of weights: 5.6045614489437225\n",
      "---------------------\n",
      "Iteration Number: 9271\n",
      "Loss: 27.36845899211382\n",
      "l2 norm of gradients: 0.1704614580924153\n",
      "l2 norm of weights: 5.604499455301119\n",
      "---------------------\n",
      "Iteration Number: 9272\n",
      "Loss: 27.367001674736276\n",
      "l2 norm of gradients: 0.1704489357594872\n",
      "l2 norm of weights: 5.6044374659973855\n",
      "---------------------\n",
      "Iteration Number: 9273\n",
      "Loss: 27.365544570888936\n",
      "l2 norm of gradients: 0.17043641543264126\n",
      "l2 norm of weights: 5.604375481031976\n",
      "---------------------\n",
      "Iteration Number: 9274\n",
      "Loss: 27.364087680519095\n",
      "l2 norm of gradients: 0.1704238971112883\n",
      "l2 norm of weights: 5.604313500404343\n",
      "---------------------\n",
      "Iteration Number: 9275\n",
      "Loss: 27.36263100358059\n",
      "l2 norm of gradients: 0.17041138079483942\n",
      "l2 norm of weights: 5.604251524113941\n",
      "---------------------\n",
      "Iteration Number: 9276\n",
      "Loss: 27.361174540019633\n",
      "l2 norm of gradients: 0.17039886648270575\n",
      "l2 norm of weights: 5.604189552160222\n",
      "---------------------\n",
      "Iteration Number: 9277\n",
      "Loss: 27.3597182897865\n",
      "l2 norm of gradients: 0.17038635417429893\n",
      "l2 norm of weights: 5.604127584542641\n",
      "---------------------\n",
      "Iteration Number: 9278\n",
      "Loss: 27.358262252832006\n",
      "l2 norm of gradients: 0.1703738438690308\n",
      "l2 norm of weights: 5.604065621260651\n",
      "---------------------\n",
      "Iteration Number: 9279\n",
      "Loss: 27.356806429107124\n",
      "l2 norm of gradients: 0.17036133556631342\n",
      "l2 norm of weights: 5.604003662313705\n",
      "---------------------\n",
      "Iteration Number: 9280\n",
      "Loss: 27.35535081855909\n",
      "l2 norm of gradients: 0.1703488292655591\n",
      "l2 norm of weights: 5.603941707701258\n",
      "---------------------\n",
      "Iteration Number: 9281\n",
      "Loss: 27.35389542113812\n",
      "l2 norm of gradients: 0.17033632496618054\n",
      "l2 norm of weights: 5.603879757422764\n",
      "---------------------\n",
      "Iteration Number: 9282\n",
      "Loss: 27.352440236796287\n",
      "l2 norm of gradients: 0.17032382266759064\n",
      "l2 norm of weights: 5.603817811477676\n",
      "---------------------\n",
      "Iteration Number: 9283\n",
      "Loss: 27.35098526548245\n",
      "l2 norm of gradients: 0.17031132236920252\n",
      "l2 norm of weights: 5.60375586986545\n",
      "---------------------\n",
      "Iteration Number: 9284\n",
      "Loss: 27.349530507148195\n",
      "l2 norm of gradients: 0.17029882407042965\n",
      "l2 norm of weights: 5.603693932585539\n",
      "---------------------\n",
      "Iteration Number: 9285\n",
      "Loss: 27.348075961738164\n",
      "l2 norm of gradients: 0.17028632777068575\n",
      "l2 norm of weights: 5.603631999637399\n",
      "---------------------\n",
      "Iteration Number: 9286\n",
      "Loss: 27.34662162920886\n",
      "l2 norm of gradients: 0.17027383346938477\n",
      "l2 norm of weights: 5.603570071020482\n",
      "---------------------\n",
      "Iteration Number: 9287\n",
      "Loss: 27.34516750950595\n",
      "l2 norm of gradients: 0.17026134116594088\n",
      "l2 norm of weights: 5.603508146734245\n",
      "---------------------\n",
      "Iteration Number: 9288\n",
      "Loss: 27.343713602580863\n",
      "l2 norm of gradients: 0.17024885085976874\n",
      "l2 norm of weights: 5.603446226778142\n",
      "---------------------\n",
      "Iteration Number: 9289\n",
      "Loss: 27.342259908384808\n",
      "l2 norm of gradients: 0.170236362550283\n",
      "l2 norm of weights: 5.603384311151628\n",
      "---------------------\n",
      "Iteration Number: 9290\n",
      "Loss: 27.340806426865775\n",
      "l2 norm of gradients: 0.17022387623689875\n",
      "l2 norm of weights: 5.603322399854158\n",
      "---------------------\n",
      "Iteration Number: 9291\n",
      "Loss: 27.33935315797558\n",
      "l2 norm of gradients: 0.1702113919190313\n",
      "l2 norm of weights: 5.603260492885188\n",
      "---------------------\n",
      "Iteration Number: 9292\n",
      "Loss: 27.337900101663642\n",
      "l2 norm of gradients: 0.1701989095960962\n",
      "l2 norm of weights: 5.6031985902441726\n",
      "---------------------\n",
      "Iteration Number: 9293\n",
      "Loss: 27.336447257878635\n",
      "l2 norm of gradients: 0.17018642926750932\n",
      "l2 norm of weights: 5.603136691930568\n",
      "---------------------\n",
      "Iteration Number: 9294\n",
      "Loss: 27.334994626572243\n",
      "l2 norm of gradients: 0.1701739509326867\n",
      "l2 norm of weights: 5.603074797943828\n",
      "---------------------\n",
      "Iteration Number: 9295\n",
      "Loss: 27.333542207696805\n",
      "l2 norm of gradients: 0.1701614745910448\n",
      "l2 norm of weights: 5.603012908283409\n",
      "---------------------\n",
      "Iteration Number: 9296\n",
      "Loss: 27.332090001197198\n",
      "l2 norm of gradients: 0.17014900024200022\n",
      "l2 norm of weights: 5.602951022948767\n",
      "---------------------\n",
      "Iteration Number: 9297\n",
      "Loss: 27.330638007026458\n",
      "l2 norm of gradients: 0.17013652788496988\n",
      "l2 norm of weights: 5.602889141939358\n",
      "---------------------\n",
      "Iteration Number: 9298\n",
      "Loss: 27.329186225138006\n",
      "l2 norm of gradients: 0.17012405751937087\n",
      "l2 norm of weights: 5.602827265254638\n",
      "---------------------\n",
      "Iteration Number: 9299\n",
      "Loss: 27.327734655476036\n",
      "l2 norm of gradients: 0.17011158914462068\n",
      "l2 norm of weights: 5.602765392894064\n",
      "---------------------\n",
      "Iteration Number: 9300\n",
      "Loss: 27.32628329799436\n",
      "l2 norm of gradients: 0.170099122760137\n",
      "l2 norm of weights: 5.602703524857089\n",
      "---------------------\n",
      "Iteration Number: 9301\n",
      "Loss: 27.324832152644156\n",
      "l2 norm of gradients: 0.17008665836533773\n",
      "l2 norm of weights: 5.602641661143173\n",
      "---------------------\n",
      "Iteration Number: 9302\n",
      "Loss: 27.32338121937289\n",
      "l2 norm of gradients: 0.1700741959596412\n",
      "l2 norm of weights: 5.60257980175177\n",
      "---------------------\n",
      "Iteration Number: 9303\n",
      "Loss: 27.321930498130907\n",
      "l2 norm of gradients: 0.1700617355424658\n",
      "l2 norm of weights: 5.6025179466823385\n",
      "---------------------\n",
      "Iteration Number: 9304\n",
      "Loss: 27.320479988869927\n",
      "l2 norm of gradients: 0.1700492771132303\n",
      "l2 norm of weights: 5.602456095934334\n",
      "---------------------\n",
      "Iteration Number: 9305\n",
      "Loss: 27.319029691542216\n",
      "l2 norm of gradients: 0.1700368206713537\n",
      "l2 norm of weights: 5.602394249507213\n",
      "---------------------\n",
      "Iteration Number: 9306\n",
      "Loss: 27.3175796060936\n",
      "l2 norm of gradients: 0.1700243662162553\n",
      "l2 norm of weights: 5.602332407400433\n",
      "---------------------\n",
      "Iteration Number: 9307\n",
      "Loss: 27.316129732477396\n",
      "l2 norm of gradients: 0.17001191374735464\n",
      "l2 norm of weights: 5.602270569613451\n",
      "---------------------\n",
      "Iteration Number: 9308\n",
      "Loss: 27.31468007064486\n",
      "l2 norm of gradients: 0.16999946326407148\n",
      "l2 norm of weights: 5.602208736145725\n",
      "---------------------\n",
      "Iteration Number: 9309\n",
      "Loss: 27.313230620543553\n",
      "l2 norm of gradients: 0.16998701476582587\n",
      "l2 norm of weights: 5.60214690699671\n",
      "---------------------\n",
      "Iteration Number: 9310\n",
      "Loss: 27.311781382125226\n",
      "l2 norm of gradients: 0.16997456825203816\n",
      "l2 norm of weights: 5.602085082165865\n",
      "---------------------\n",
      "Iteration Number: 9311\n",
      "Loss: 27.31033235533888\n",
      "l2 norm of gradients: 0.1699621237221289\n",
      "l2 norm of weights: 5.602023261652647\n",
      "---------------------\n",
      "Iteration Number: 9312\n",
      "Loss: 27.30888354013749\n",
      "l2 norm of gradients: 0.169949681175519\n",
      "l2 norm of weights: 5.6019614454565145\n",
      "---------------------\n",
      "Iteration Number: 9313\n",
      "Loss: 27.307434936472625\n",
      "l2 norm of gradients: 0.1699372406116295\n",
      "l2 norm of weights: 5.601899633576924\n",
      "---------------------\n",
      "Iteration Number: 9314\n",
      "Loss: 27.30598654428918\n",
      "l2 norm of gradients: 0.16992480202988167\n",
      "l2 norm of weights: 5.601837826013333\n",
      "---------------------\n",
      "Iteration Number: 9315\n",
      "Loss: 27.304538363544125\n",
      "l2 norm of gradients: 0.16991236542969732\n",
      "l2 norm of weights: 5.6017760227652005\n",
      "---------------------\n",
      "Iteration Number: 9316\n",
      "Loss: 27.30309039418328\n",
      "l2 norm of gradients: 0.16989993081049817\n",
      "l2 norm of weights: 5.601714223831984\n",
      "---------------------\n",
      "Iteration Number: 9317\n",
      "Loss: 27.301642636158228\n",
      "l2 norm of gradients: 0.1698874981717064\n",
      "l2 norm of weights: 5.601652429213143\n",
      "---------------------\n",
      "Iteration Number: 9318\n",
      "Loss: 27.30019508942176\n",
      "l2 norm of gradients: 0.16987506751274442\n",
      "l2 norm of weights: 5.601590638908134\n",
      "---------------------\n",
      "Iteration Number: 9319\n",
      "Loss: 27.298747753919663\n",
      "l2 norm of gradients: 0.16986263883303493\n",
      "l2 norm of weights: 5.601528852916416\n",
      "---------------------\n",
      "Iteration Number: 9320\n",
      "Loss: 27.29730062960944\n",
      "l2 norm of gradients: 0.16985021213200077\n",
      "l2 norm of weights: 5.601467071237448\n",
      "---------------------\n",
      "Iteration Number: 9321\n",
      "Loss: 27.295853716436078\n",
      "l2 norm of gradients: 0.16983778740906516\n",
      "l2 norm of weights: 5.6014052938706875\n",
      "---------------------\n",
      "Iteration Number: 9322\n",
      "Loss: 27.294407014352586\n",
      "l2 norm of gradients: 0.16982536466365153\n",
      "l2 norm of weights: 5.601343520815594\n",
      "---------------------\n",
      "Iteration Number: 9323\n",
      "Loss: 27.292960523306764\n",
      "l2 norm of gradients: 0.1698129438951835\n",
      "l2 norm of weights: 5.6012817520716265\n",
      "---------------------\n",
      "Iteration Number: 9324\n",
      "Loss: 27.291514243254596\n",
      "l2 norm of gradients: 0.1698005251030851\n",
      "l2 norm of weights: 5.601219987638244\n",
      "---------------------\n",
      "Iteration Number: 9325\n",
      "Loss: 27.29006817414181\n",
      "l2 norm of gradients: 0.16978810828678048\n",
      "l2 norm of weights: 5.6011582275149046\n",
      "---------------------\n",
      "Iteration Number: 9326\n",
      "Loss: 27.288622315922535\n",
      "l2 norm of gradients: 0.16977569344569413\n",
      "l2 norm of weights: 5.6010964717010685\n",
      "---------------------\n",
      "Iteration Number: 9327\n",
      "Loss: 27.287176668546447\n",
      "l2 norm of gradients: 0.16976328057925077\n",
      "l2 norm of weights: 5.6010347201961945\n",
      "---------------------\n",
      "Iteration Number: 9328\n",
      "Loss: 27.285731231961318\n",
      "l2 norm of gradients: 0.1697508696868753\n",
      "l2 norm of weights: 5.600972972999743\n",
      "---------------------\n",
      "Iteration Number: 9329\n",
      "Loss: 27.28428600612209\n",
      "l2 norm of gradients: 0.16973846076799304\n",
      "l2 norm of weights: 5.600911230111172\n",
      "---------------------\n",
      "Iteration Number: 9330\n",
      "Loss: 27.282840990977146\n",
      "l2 norm of gradients: 0.16972605382202943\n",
      "l2 norm of weights: 5.600849491529942\n",
      "---------------------\n",
      "Iteration Number: 9331\n",
      "Loss: 27.28139618647686\n",
      "l2 norm of gradients: 0.16971364884841017\n",
      "l2 norm of weights: 5.600787757255513\n",
      "---------------------\n",
      "Iteration Number: 9332\n",
      "Loss: 27.279951592575237\n",
      "l2 norm of gradients: 0.16970124584656132\n",
      "l2 norm of weights: 5.600726027287345\n",
      "---------------------\n",
      "Iteration Number: 9333\n",
      "Loss: 27.27850720921871\n",
      "l2 norm of gradients: 0.16968884481590915\n",
      "l2 norm of weights: 5.600664301624896\n",
      "---------------------\n",
      "Iteration Number: 9334\n",
      "Loss: 27.277063036362108\n",
      "l2 norm of gradients: 0.16967644575588003\n",
      "l2 norm of weights: 5.600602580267628\n",
      "---------------------\n",
      "Iteration Number: 9335\n",
      "Loss: 27.2756190739542\n",
      "l2 norm of gradients: 0.16966404866590093\n",
      "l2 norm of weights: 5.600540863215002\n",
      "---------------------\n",
      "Iteration Number: 9336\n",
      "Loss: 27.274175321945936\n",
      "l2 norm of gradients: 0.16965165354539863\n",
      "l2 norm of weights: 5.600479150466477\n",
      "---------------------\n",
      "Iteration Number: 9337\n",
      "Loss: 27.272731780288034\n",
      "l2 norm of gradients: 0.16963926039380053\n",
      "l2 norm of weights: 5.600417442021513\n",
      "---------------------\n",
      "Iteration Number: 9338\n",
      "Loss: 27.27128844893272\n",
      "l2 norm of gradients: 0.16962686921053413\n",
      "l2 norm of weights: 5.60035573787957\n",
      "---------------------\n",
      "Iteration Number: 9339\n",
      "Loss: 27.26984532782906\n",
      "l2 norm of gradients: 0.1696144799950272\n",
      "l2 norm of weights: 5.6002940380401105\n",
      "---------------------\n",
      "Iteration Number: 9340\n",
      "Loss: 27.268402416930314\n",
      "l2 norm of gradients: 0.16960209274670773\n",
      "l2 norm of weights: 5.600232342502595\n",
      "---------------------\n",
      "Iteration Number: 9341\n",
      "Loss: 27.266959716185717\n",
      "l2 norm of gradients: 0.16958970746500404\n",
      "l2 norm of weights: 5.600170651266485\n",
      "---------------------\n",
      "Iteration Number: 9342\n",
      "Loss: 27.265517225546038\n",
      "l2 norm of gradients: 0.16957732414934465\n",
      "l2 norm of weights: 5.6001089643312385\n",
      "---------------------\n",
      "Iteration Number: 9343\n",
      "Loss: 27.264074944962424\n",
      "l2 norm of gradients: 0.16956494279915835\n",
      "l2 norm of weights: 5.60004728169632\n",
      "---------------------\n",
      "Iteration Number: 9344\n",
      "Loss: 27.262632874387034\n",
      "l2 norm of gradients: 0.16955256341387417\n",
      "l2 norm of weights: 5.599985603361189\n",
      "---------------------\n",
      "Iteration Number: 9345\n",
      "Loss: 27.261191013770283\n",
      "l2 norm of gradients: 0.1695401859929214\n",
      "l2 norm of weights: 5.599923929325307\n",
      "---------------------\n",
      "Iteration Number: 9346\n",
      "Loss: 27.259749363062447\n",
      "l2 norm of gradients: 0.16952781053572952\n",
      "l2 norm of weights: 5.599862259588135\n",
      "---------------------\n",
      "Iteration Number: 9347\n",
      "Loss: 27.258307922213522\n",
      "l2 norm of gradients: 0.16951543704172842\n",
      "l2 norm of weights: 5.599800594149136\n",
      "---------------------\n",
      "Iteration Number: 9348\n",
      "Loss: 27.256866691178352\n",
      "l2 norm of gradients: 0.16950306551034808\n",
      "l2 norm of weights: 5.599738933007771\n",
      "---------------------\n",
      "Iteration Number: 9349\n",
      "Loss: 27.25542566990569\n",
      "l2 norm of gradients: 0.1694906959410188\n",
      "l2 norm of weights: 5.599677276163501\n",
      "---------------------\n",
      "Iteration Number: 9350\n",
      "Loss: 27.25398485834714\n",
      "l2 norm of gradients: 0.16947832833317109\n",
      "l2 norm of weights: 5.599615623615789\n",
      "---------------------\n",
      "Iteration Number: 9351\n",
      "Loss: 27.252544256453106\n",
      "l2 norm of gradients: 0.16946596268623576\n",
      "l2 norm of weights: 5.599553975364096\n",
      "---------------------\n",
      "Iteration Number: 9352\n",
      "Loss: 27.251103864174365\n",
      "l2 norm of gradients: 0.16945359899964388\n",
      "l2 norm of weights: 5.599492331407886\n",
      "---------------------\n",
      "Iteration Number: 9353\n",
      "Loss: 27.24966368146396\n",
      "l2 norm of gradients: 0.16944123727282673\n",
      "l2 norm of weights: 5.599430691746619\n",
      "---------------------\n",
      "Iteration Number: 9354\n",
      "Loss: 27.248223708272473\n",
      "l2 norm of gradients: 0.16942887750521582\n",
      "l2 norm of weights: 5.599369056379759\n",
      "---------------------\n",
      "Iteration Number: 9355\n",
      "Loss: 27.246783944548433\n",
      "l2 norm of gradients: 0.16941651969624297\n",
      "l2 norm of weights: 5.599307425306767\n",
      "---------------------\n",
      "Iteration Number: 9356\n",
      "Loss: 27.245344390248086\n",
      "l2 norm of gradients: 0.16940416384534018\n",
      "l2 norm of weights: 5.599245798527107\n",
      "---------------------\n",
      "Iteration Number: 9357\n",
      "Loss: 27.24390504531924\n",
      "l2 norm of gradients: 0.1693918099519398\n",
      "l2 norm of weights: 5.599184176040241\n",
      "---------------------\n",
      "Iteration Number: 9358\n",
      "Loss: 27.242465909712276\n",
      "l2 norm of gradients: 0.16937945801547433\n",
      "l2 norm of weights: 5.599122557845633\n",
      "---------------------\n",
      "Iteration Number: 9359\n",
      "Loss: 27.24102698338044\n",
      "l2 norm of gradients: 0.16936710803537652\n",
      "l2 norm of weights: 5.599060943942744\n",
      "---------------------\n",
      "Iteration Number: 9360\n",
      "Loss: 27.23958826627347\n",
      "l2 norm of gradients: 0.16935476001107938\n",
      "l2 norm of weights: 5.598999334331038\n",
      "---------------------\n",
      "Iteration Number: 9361\n",
      "Loss: 27.23814975834396\n",
      "l2 norm of gradients: 0.16934241394201632\n",
      "l2 norm of weights: 5.598937729009978\n",
      "---------------------\n",
      "Iteration Number: 9362\n",
      "Loss: 27.236711459543777\n",
      "l2 norm of gradients: 0.16933006982762072\n",
      "l2 norm of weights: 5.598876127979028\n",
      "---------------------\n",
      "Iteration Number: 9363\n",
      "Loss: 27.235273369822675\n",
      "l2 norm of gradients: 0.16931772766732642\n",
      "l2 norm of weights: 5.59881453123765\n",
      "---------------------\n",
      "Iteration Number: 9364\n",
      "Loss: 27.233835489134755\n",
      "l2 norm of gradients: 0.1693053874605674\n",
      "l2 norm of weights: 5.598752938785308\n",
      "---------------------\n",
      "Iteration Number: 9365\n",
      "Loss: 27.23239781742778\n",
      "l2 norm of gradients: 0.16929304920677793\n",
      "l2 norm of weights: 5.598691350621467\n",
      "---------------------\n",
      "Iteration Number: 9366\n",
      "Loss: 27.23096035465511\n",
      "l2 norm of gradients: 0.16928071290539257\n",
      "l2 norm of weights: 5.598629766745589\n",
      "---------------------\n",
      "Iteration Number: 9367\n",
      "Loss: 27.229523100765903\n",
      "l2 norm of gradients: 0.16926837855584603\n",
      "l2 norm of weights: 5.598568187157138\n",
      "---------------------\n",
      "Iteration Number: 9368\n",
      "Loss: 27.22808605571598\n",
      "l2 norm of gradients: 0.16925604615757334\n",
      "l2 norm of weights: 5.598506611855578\n",
      "---------------------\n",
      "Iteration Number: 9369\n",
      "Loss: 27.226649219452955\n",
      "l2 norm of gradients: 0.16924371571000968\n",
      "l2 norm of weights: 5.598445040840375\n",
      "---------------------\n",
      "Iteration Number: 9370\n",
      "Loss: 27.225212591930692\n",
      "l2 norm of gradients: 0.1692313872125906\n",
      "l2 norm of weights: 5.59838347411099\n",
      "---------------------\n",
      "Iteration Number: 9371\n",
      "Loss: 27.223776173097747\n",
      "l2 norm of gradients: 0.1692190606647518\n",
      "l2 norm of weights: 5.59832191166689\n",
      "---------------------\n",
      "Iteration Number: 9372\n",
      "Loss: 27.222339962907537\n",
      "l2 norm of gradients: 0.1692067360659293\n",
      "l2 norm of weights: 5.598260353507537\n",
      "---------------------\n",
      "Iteration Number: 9373\n",
      "Loss: 27.220903961313425\n",
      "l2 norm of gradients: 0.16919441341555932\n",
      "l2 norm of weights: 5.598198799632398\n",
      "---------------------\n",
      "Iteration Number: 9374\n",
      "Loss: 27.219468168262445\n",
      "l2 norm of gradients: 0.16918209271307832\n",
      "l2 norm of weights: 5.598137250040935\n",
      "---------------------\n",
      "Iteration Number: 9375\n",
      "Loss: 27.218032583710322\n",
      "l2 norm of gradients: 0.16916977395792296\n",
      "l2 norm of weights: 5.598075704732615\n",
      "---------------------\n",
      "Iteration Number: 9376\n",
      "Loss: 27.21659720760544\n",
      "l2 norm of gradients: 0.1691574571495303\n",
      "l2 norm of weights: 5.598014163706901\n",
      "---------------------\n",
      "Iteration Number: 9377\n",
      "Loss: 27.215162039901692\n",
      "l2 norm of gradients: 0.16914514228733743\n",
      "l2 norm of weights: 5.59795262696326\n",
      "---------------------\n",
      "Iteration Number: 9378\n",
      "Loss: 27.21372708054807\n",
      "l2 norm of gradients: 0.16913282937078186\n",
      "l2 norm of weights: 5.597891094501156\n",
      "---------------------\n",
      "Iteration Number: 9379\n",
      "Loss: 27.21229232950068\n",
      "l2 norm of gradients: 0.16912051839930126\n",
      "l2 norm of weights: 5.597829566320053\n",
      "---------------------\n",
      "Iteration Number: 9380\n",
      "Loss: 27.210857786706182\n",
      "l2 norm of gradients: 0.16910820937233356\n",
      "l2 norm of weights: 5.5977680424194185\n",
      "---------------------\n",
      "Iteration Number: 9381\n",
      "Loss: 27.2094234521187\n",
      "l2 norm of gradients: 0.1690959022893169\n",
      "l2 norm of weights: 5.597706522798717\n",
      "---------------------\n",
      "Iteration Number: 9382\n",
      "Loss: 27.207989325689752\n",
      "l2 norm of gradients: 0.16908359714968973\n",
      "l2 norm of weights: 5.5976450074574124\n",
      "---------------------\n",
      "Iteration Number: 9383\n",
      "Loss: 27.206555407370217\n",
      "l2 norm of gradients: 0.16907129395289067\n",
      "l2 norm of weights: 5.597583496394972\n",
      "---------------------\n",
      "Iteration Number: 9384\n",
      "Loss: 27.205121697113345\n",
      "l2 norm of gradients: 0.16905899269835864\n",
      "l2 norm of weights: 5.597521989610862\n",
      "---------------------\n",
      "Iteration Number: 9385\n",
      "Loss: 27.203688194868864\n",
      "l2 norm of gradients: 0.16904669338553274\n",
      "l2 norm of weights: 5.597460487104548\n",
      "---------------------\n",
      "Iteration Number: 9386\n",
      "Loss: 27.202254900590916\n",
      "l2 norm of gradients: 0.1690343960138524\n",
      "l2 norm of weights: 5.5973989888754945\n",
      "---------------------\n",
      "Iteration Number: 9387\n",
      "Loss: 27.200821814228103\n",
      "l2 norm of gradients: 0.16902210058275713\n",
      "l2 norm of weights: 5.597337494923169\n",
      "---------------------\n",
      "Iteration Number: 9388\n",
      "Loss: 27.199388935735666\n",
      "l2 norm of gradients: 0.1690098070916869\n",
      "l2 norm of weights: 5.5972760052470365\n",
      "---------------------\n",
      "Iteration Number: 9389\n",
      "Loss: 27.197956265062423\n",
      "l2 norm of gradients: 0.1689975155400818\n",
      "l2 norm of weights: 5.597214519846566\n",
      "---------------------\n",
      "Iteration Number: 9390\n",
      "Loss: 27.19652380216187\n",
      "l2 norm of gradients: 0.16898522592738208\n",
      "l2 norm of weights: 5.5971530387212205\n",
      "---------------------\n",
      "Iteration Number: 9391\n",
      "Loss: 27.19509154698397\n",
      "l2 norm of gradients: 0.16897293825302837\n",
      "l2 norm of weights: 5.597091561870468\n",
      "---------------------\n",
      "Iteration Number: 9392\n",
      "Loss: 27.19365949948174\n",
      "l2 norm of gradients: 0.1689606525164615\n",
      "l2 norm of weights: 5.597030089293775\n",
      "---------------------\n",
      "Iteration Number: 9393\n",
      "Loss: 27.192227659608754\n",
      "l2 norm of gradients: 0.16894836871712246\n",
      "l2 norm of weights: 5.5969686209906095\n",
      "---------------------\n",
      "Iteration Number: 9394\n",
      "Loss: 27.19079602731509\n",
      "l2 norm of gradients: 0.1689360868544526\n",
      "l2 norm of weights: 5.5969071569604365\n",
      "---------------------\n",
      "Iteration Number: 9395\n",
      "Loss: 27.189364602552722\n",
      "l2 norm of gradients: 0.16892380692789344\n",
      "l2 norm of weights: 5.596845697202723\n",
      "---------------------\n",
      "Iteration Number: 9396\n",
      "Loss: 27.187933385272302\n",
      "l2 norm of gradients: 0.16891152893688674\n",
      "l2 norm of weights: 5.5967842417169384\n",
      "---------------------\n",
      "Iteration Number: 9397\n",
      "Loss: 27.18650237542746\n",
      "l2 norm of gradients: 0.16889925288087454\n",
      "l2 norm of weights: 5.596722790502547\n",
      "---------------------\n",
      "Iteration Number: 9398\n",
      "Loss: 27.18507157296896\n",
      "l2 norm of gradients: 0.16888697875929903\n",
      "l2 norm of weights: 5.596661343559018\n",
      "---------------------\n",
      "Iteration Number: 9399\n",
      "Loss: 27.183640977851088\n",
      "l2 norm of gradients: 0.16887470657160272\n",
      "l2 norm of weights: 5.596599900885818\n",
      "---------------------\n",
      "Iteration Number: 9400\n",
      "Loss: 27.18221059002472\n",
      "l2 norm of gradients: 0.16886243631722833\n",
      "l2 norm of weights: 5.596538462482415\n",
      "---------------------\n",
      "Iteration Number: 9401\n",
      "Loss: 27.18078040943926\n",
      "l2 norm of gradients: 0.16885016799561875\n",
      "l2 norm of weights: 5.596477028348276\n",
      "---------------------\n",
      "Iteration Number: 9402\n",
      "Loss: 27.17935043604882\n",
      "l2 norm of gradients: 0.1688379016062173\n",
      "l2 norm of weights: 5.596415598482869\n",
      "---------------------\n",
      "Iteration Number: 9403\n",
      "Loss: 27.177920669807094\n",
      "l2 norm of gradients: 0.16882563714846732\n",
      "l2 norm of weights: 5.596354172885662\n",
      "---------------------\n",
      "Iteration Number: 9404\n",
      "Loss: 27.176491110663054\n",
      "l2 norm of gradients: 0.16881337462181253\n",
      "l2 norm of weights: 5.596292751556123\n",
      "---------------------\n",
      "Iteration Number: 9405\n",
      "Loss: 27.175061758568756\n",
      "l2 norm of gradients: 0.16880111402569678\n",
      "l2 norm of weights: 5.59623133449372\n",
      "---------------------\n",
      "Iteration Number: 9406\n",
      "Loss: 27.173632613480542\n",
      "l2 norm of gradients: 0.1687888553595642\n",
      "l2 norm of weights: 5.596169921697921\n",
      "---------------------\n",
      "Iteration Number: 9407\n",
      "Loss: 27.172203675343\n",
      "l2 norm of gradients: 0.1687765986228592\n",
      "l2 norm of weights: 5.596108513168196\n",
      "---------------------\n",
      "Iteration Number: 9408\n",
      "Loss: 27.17077494411692\n",
      "l2 norm of gradients: 0.1687643438150264\n",
      "l2 norm of weights: 5.596047108904011\n",
      "---------------------\n",
      "Iteration Number: 9409\n",
      "Loss: 27.169346419749242\n",
      "l2 norm of gradients: 0.1687520909355106\n",
      "l2 norm of weights: 5.595985708904835\n",
      "---------------------\n",
      "Iteration Number: 9410\n",
      "Loss: 27.167918102189475\n",
      "l2 norm of gradients: 0.1687398399837569\n",
      "l2 norm of weights: 5.595924313170137\n",
      "---------------------\n",
      "Iteration Number: 9411\n",
      "Loss: 27.166489991396134\n",
      "l2 norm of gradients: 0.16872759095921058\n",
      "l2 norm of weights: 5.5958629216993865\n",
      "---------------------\n",
      "Iteration Number: 9412\n",
      "Loss: 27.165062087319527\n",
      "l2 norm of gradients: 0.1687153438613172\n",
      "l2 norm of weights: 5.595801534492051\n",
      "---------------------\n",
      "Iteration Number: 9413\n",
      "Loss: 27.16363438991024\n",
      "l2 norm of gradients: 0.1687030986895226\n",
      "l2 norm of weights: 5.595740151547601\n",
      "---------------------\n",
      "Iteration Number: 9414\n",
      "Loss: 27.162206899119912\n",
      "l2 norm of gradients: 0.1686908554432727\n",
      "l2 norm of weights: 5.595678772865504\n",
      "---------------------\n",
      "Iteration Number: 9415\n",
      "Loss: 27.16077961490143\n",
      "l2 norm of gradients: 0.16867861412201376\n",
      "l2 norm of weights: 5.595617398445231\n",
      "---------------------\n",
      "Iteration Number: 9416\n",
      "Loss: 27.159352537208754\n",
      "l2 norm of gradients: 0.1686663747251924\n",
      "l2 norm of weights: 5.59555602828625\n",
      "---------------------\n",
      "Iteration Number: 9417\n",
      "Loss: 27.15792566599295\n",
      "l2 norm of gradients: 0.16865413725225512\n",
      "l2 norm of weights: 5.59549466238803\n",
      "---------------------\n",
      "Iteration Number: 9418\n",
      "Loss: 27.15649900120618\n",
      "l2 norm of gradients: 0.16864190170264898\n",
      "l2 norm of weights: 5.5954333007500425\n",
      "---------------------\n",
      "Iteration Number: 9419\n",
      "Loss: 27.155072542798045\n",
      "l2 norm of gradients: 0.16862966807582114\n",
      "l2 norm of weights: 5.595371943371755\n",
      "---------------------\n",
      "Iteration Number: 9420\n",
      "Loss: 27.153646290725828\n",
      "l2 norm of gradients: 0.16861743637121898\n",
      "l2 norm of weights: 5.595310590252638\n",
      "---------------------\n",
      "Iteration Number: 9421\n",
      "Loss: 27.152220244939524\n",
      "l2 norm of gradients: 0.1686052065882902\n",
      "l2 norm of weights: 5.595249241392162\n",
      "---------------------\n",
      "Iteration Number: 9422\n",
      "Loss: 27.150794405389995\n",
      "l2 norm of gradients: 0.16859297872648263\n",
      "l2 norm of weights: 5.595187896789796\n",
      "---------------------\n",
      "Iteration Number: 9423\n",
      "Loss: 27.149368772033583\n",
      "l2 norm of gradients: 0.1685807527852444\n",
      "l2 norm of weights: 5.5951265564450114\n",
      "---------------------\n",
      "Iteration Number: 9424\n",
      "Loss: 27.147943344817303\n",
      "l2 norm of gradients: 0.1685685287640238\n",
      "l2 norm of weights: 5.595065220357276\n",
      "---------------------\n",
      "Iteration Number: 9425\n",
      "Loss: 27.146518123697206\n",
      "l2 norm of gradients: 0.1685563066622694\n",
      "l2 norm of weights: 5.595003888526064\n",
      "---------------------\n",
      "Iteration Number: 9426\n",
      "Loss: 27.145093108626195\n",
      "l2 norm of gradients: 0.16854408647943003\n",
      "l2 norm of weights: 5.594942560950841\n",
      "---------------------\n",
      "Iteration Number: 9427\n",
      "Loss: 27.14366829955318\n",
      "l2 norm of gradients: 0.16853186821495475\n",
      "l2 norm of weights: 5.594881237631081\n",
      "---------------------\n",
      "Iteration Number: 9428\n",
      "Loss: 27.14224369643289\n",
      "l2 norm of gradients: 0.16851965186829274\n",
      "l2 norm of weights: 5.5948199185662535\n",
      "---------------------\n",
      "Iteration Number: 9429\n",
      "Loss: 27.14081929921747\n",
      "l2 norm of gradients: 0.16850743743889351\n",
      "l2 norm of weights: 5.59475860375583\n",
      "---------------------\n",
      "Iteration Number: 9430\n",
      "Loss: 27.13939510786023\n",
      "l2 norm of gradients: 0.1684952249262068\n",
      "l2 norm of weights: 5.59469729319928\n",
      "---------------------\n",
      "Iteration Number: 9431\n",
      "Loss: 27.13797112231318\n",
      "l2 norm of gradients: 0.1684830143296825\n",
      "l2 norm of weights: 5.594635986896075\n",
      "---------------------\n",
      "Iteration Number: 9432\n",
      "Loss: 27.13654734252709\n",
      "l2 norm of gradients: 0.16847080564877082\n",
      "l2 norm of weights: 5.594574684845687\n",
      "---------------------\n",
      "Iteration Number: 9433\n",
      "Loss: 27.13512376845648\n",
      "l2 norm of gradients: 0.16845859888292217\n",
      "l2 norm of weights: 5.594513387047586\n",
      "---------------------\n",
      "Iteration Number: 9434\n",
      "Loss: 27.133700400052472\n",
      "l2 norm of gradients: 0.16844639403158715\n",
      "l2 norm of weights: 5.5944520935012445\n",
      "---------------------\n",
      "Iteration Number: 9435\n",
      "Loss: 27.132277237268898\n",
      "l2 norm of gradients: 0.16843419109421665\n",
      "l2 norm of weights: 5.5943908042061326\n",
      "---------------------\n",
      "Iteration Number: 9436\n",
      "Loss: 27.130854280058394\n",
      "l2 norm of gradients: 0.16842199007026176\n",
      "l2 norm of weights: 5.594329519161723\n",
      "---------------------\n",
      "Iteration Number: 9437\n",
      "Loss: 27.129431528371487\n",
      "l2 norm of gradients: 0.16840979095917383\n",
      "l2 norm of weights: 5.594268238367485\n",
      "---------------------\n",
      "Iteration Number: 9438\n",
      "Loss: 27.12800898216305\n",
      "l2 norm of gradients: 0.16839759376040428\n",
      "l2 norm of weights: 5.594206961822894\n",
      "---------------------\n",
      "Iteration Number: 9439\n",
      "Loss: 27.126586641384385\n",
      "l2 norm of gradients: 0.16838539847340495\n",
      "l2 norm of weights: 5.594145689527418\n",
      "---------------------\n",
      "Iteration Number: 9440\n",
      "Loss: 27.125164505988312\n",
      "l2 norm of gradients: 0.16837320509762788\n",
      "l2 norm of weights: 5.594084421480533\n",
      "---------------------\n",
      "Iteration Number: 9441\n",
      "Loss: 27.123742575928397\n",
      "l2 norm of gradients: 0.16836101363252526\n",
      "l2 norm of weights: 5.594023157681708\n",
      "---------------------\n",
      "Iteration Number: 9442\n",
      "Loss: 27.122320851156026\n",
      "l2 norm of gradients: 0.16834882407754953\n",
      "l2 norm of weights: 5.5939618981304156\n",
      "---------------------\n",
      "Iteration Number: 9443\n",
      "Loss: 27.120899331624333\n",
      "l2 norm of gradients: 0.16833663643215332\n",
      "l2 norm of weights: 5.59390064282613\n",
      "---------------------\n",
      "Iteration Number: 9444\n",
      "Loss: 27.11947801728732\n",
      "l2 norm of gradients: 0.16832445069578963\n",
      "l2 norm of weights: 5.593839391768321\n",
      "---------------------\n",
      "Iteration Number: 9445\n",
      "Loss: 27.11805690809446\n",
      "l2 norm of gradients: 0.16831226686791156\n",
      "l2 norm of weights: 5.593778144956463\n",
      "---------------------\n",
      "Iteration Number: 9446\n",
      "Loss: 27.116636004002114\n",
      "l2 norm of gradients: 0.16830008494797244\n",
      "l2 norm of weights: 5.593716902390027\n",
      "---------------------\n",
      "Iteration Number: 9447\n",
      "Loss: 27.115215304960056\n",
      "l2 norm of gradients: 0.16828790493542584\n",
      "l2 norm of weights: 5.593655664068487\n",
      "---------------------\n",
      "Iteration Number: 9448\n",
      "Loss: 27.11379481092399\n",
      "l2 norm of gradients: 0.16827572682972555\n",
      "l2 norm of weights: 5.593594429991317\n",
      "---------------------\n",
      "Iteration Number: 9449\n",
      "Loss: 27.112374521844483\n",
      "l2 norm of gradients: 0.16826355063032564\n",
      "l2 norm of weights: 5.593533200157987\n",
      "---------------------\n",
      "Iteration Number: 9450\n",
      "Loss: 27.110954437673755\n",
      "l2 norm of gradients: 0.16825137633668036\n",
      "l2 norm of weights: 5.593471974567972\n",
      "---------------------\n",
      "Iteration Number: 9451\n",
      "Loss: 27.109534558365162\n",
      "l2 norm of gradients: 0.16823920394824418\n",
      "l2 norm of weights: 5.593410753220744\n",
      "---------------------\n",
      "Iteration Number: 9452\n",
      "Loss: 27.10811488387252\n",
      "l2 norm of gradients: 0.1682270334644718\n",
      "l2 norm of weights: 5.593349536115778\n",
      "---------------------\n",
      "Iteration Number: 9453\n",
      "Loss: 27.106695414149094\n",
      "l2 norm of gradients: 0.16821486488481815\n",
      "l2 norm of weights: 5.593288323252546\n",
      "---------------------\n",
      "Iteration Number: 9454\n",
      "Loss: 27.10527614914699\n",
      "l2 norm of gradients: 0.16820269820873834\n",
      "l2 norm of weights: 5.593227114630522\n",
      "---------------------\n",
      "Iteration Number: 9455\n",
      "Loss: 27.103857088818167\n",
      "l2 norm of gradients: 0.16819053343568782\n",
      "l2 norm of weights: 5.59316591024918\n",
      "---------------------\n",
      "Iteration Number: 9456\n",
      "Loss: 27.102438233117393\n",
      "l2 norm of gradients: 0.16817837056512214\n",
      "l2 norm of weights: 5.593104710107992\n",
      "---------------------\n",
      "Iteration Number: 9457\n",
      "Loss: 27.10101958199599\n",
      "l2 norm of gradients: 0.1681662095964971\n",
      "l2 norm of weights: 5.593043514206434\n",
      "---------------------\n",
      "Iteration Number: 9458\n",
      "Loss: 27.09960113540689\n",
      "l2 norm of gradients: 0.1681540505292688\n",
      "l2 norm of weights: 5.592982322543979\n",
      "---------------------\n",
      "Iteration Number: 9459\n",
      "Loss: 27.098182893303157\n",
      "l2 norm of gradients: 0.16814189336289342\n",
      "l2 norm of weights: 5.592921135120101\n",
      "---------------------\n",
      "Iteration Number: 9460\n",
      "Loss: 27.096764855638803\n",
      "l2 norm of gradients: 0.16812973809682752\n",
      "l2 norm of weights: 5.592859951934274\n",
      "---------------------\n",
      "Iteration Number: 9461\n",
      "Loss: 27.0953470223649\n",
      "l2 norm of gradients: 0.16811758473052774\n",
      "l2 norm of weights: 5.592798772985972\n",
      "---------------------\n",
      "Iteration Number: 9462\n",
      "Loss: 27.093929393435584\n",
      "l2 norm of gradients: 0.1681054332634511\n",
      "l2 norm of weights: 5.592737598274671\n",
      "---------------------\n",
      "Iteration Number: 9463\n",
      "Loss: 27.092511968804992\n",
      "l2 norm of gradients: 0.16809328369505466\n",
      "l2 norm of weights: 5.592676427799844\n",
      "---------------------\n",
      "Iteration Number: 9464\n",
      "Loss: 27.091094748424155\n",
      "l2 norm of gradients: 0.16808113602479582\n",
      "l2 norm of weights: 5.592615261560966\n",
      "---------------------\n",
      "Iteration Number: 9465\n",
      "Loss: 27.089677732246653\n",
      "l2 norm of gradients: 0.16806899025213223\n",
      "l2 norm of weights: 5.592554099557511\n",
      "---------------------\n",
      "Iteration Number: 9466\n",
      "Loss: 27.0882609202264\n",
      "l2 norm of gradients: 0.1680568463765216\n",
      "l2 norm of weights: 5.592492941788955\n",
      "---------------------\n",
      "Iteration Number: 9467\n",
      "Loss: 27.086844312317105\n",
      "l2 norm of gradients: 0.16804470439742206\n",
      "l2 norm of weights: 5.592431788254773\n",
      "---------------------\n",
      "Iteration Number: 9468\n",
      "Loss: 27.085427908468063\n",
      "l2 norm of gradients: 0.16803256431429175\n",
      "l2 norm of weights: 5.5923706389544385\n",
      "---------------------\n",
      "Iteration Number: 9469\n",
      "Loss: 27.084011708635828\n",
      "l2 norm of gradients: 0.16802042612658927\n",
      "l2 norm of weights: 5.592309493887428\n",
      "---------------------\n",
      "Iteration Number: 9470\n",
      "Loss: 27.082595712771926\n",
      "l2 norm of gradients: 0.16800828983377322\n",
      "l2 norm of weights: 5.592248353053216\n",
      "---------------------\n",
      "Iteration Number: 9471\n",
      "Loss: 27.081179920831353\n",
      "l2 norm of gradients: 0.1679961554353026\n",
      "l2 norm of weights: 5.592187216451279\n",
      "---------------------\n",
      "Iteration Number: 9472\n",
      "Loss: 27.079764332765606\n",
      "l2 norm of gradients: 0.16798402293063647\n",
      "l2 norm of weights: 5.592126084081091\n",
      "---------------------\n",
      "Iteration Number: 9473\n",
      "Loss: 27.078348948528813\n",
      "l2 norm of gradients: 0.16797189231923412\n",
      "l2 norm of weights: 5.592064955942128\n",
      "---------------------\n",
      "Iteration Number: 9474\n",
      "Loss: 27.076933768071733\n",
      "l2 norm of gradients: 0.1679597636005553\n",
      "l2 norm of weights: 5.592003832033867\n",
      "---------------------\n",
      "Iteration Number: 9475\n",
      "Loss: 27.075518791350966\n",
      "l2 norm of gradients: 0.16794763677405966\n",
      "l2 norm of weights: 5.591942712355783\n",
      "---------------------\n",
      "Iteration Number: 9476\n",
      "Loss: 27.074104018316614\n",
      "l2 norm of gradients: 0.16793551183920724\n",
      "l2 norm of weights: 5.59188159690735\n",
      "---------------------\n",
      "Iteration Number: 9477\n",
      "Loss: 27.072689448925892\n",
      "l2 norm of gradients: 0.1679233887954583\n",
      "l2 norm of weights: 5.5918204856880465\n",
      "---------------------\n",
      "Iteration Number: 9478\n",
      "Loss: 27.071275083129066\n",
      "l2 norm of gradients: 0.16791126764227315\n",
      "l2 norm of weights: 5.591759378697348\n",
      "---------------------\n",
      "Iteration Number: 9479\n",
      "Loss: 27.069860920877424\n",
      "l2 norm of gradients: 0.16789914837911263\n",
      "l2 norm of weights: 5.591698275934731\n",
      "---------------------\n",
      "Iteration Number: 9480\n",
      "Loss: 27.068446962128498\n",
      "l2 norm of gradients: 0.1678870310054375\n",
      "l2 norm of weights: 5.5916371773996705\n",
      "---------------------\n",
      "Iteration Number: 9481\n",
      "Loss: 27.067033206832622\n",
      "l2 norm of gradients: 0.16787491552070885\n",
      "l2 norm of weights: 5.5915760830916446\n",
      "---------------------\n",
      "Iteration Number: 9482\n",
      "Loss: 27.0656196549446\n",
      "l2 norm of gradients: 0.1678628019243881\n",
      "l2 norm of weights: 5.591514993010128\n",
      "---------------------\n",
      "Iteration Number: 9483\n",
      "Loss: 27.064206306419077\n",
      "l2 norm of gradients: 0.16785069021593665\n",
      "l2 norm of weights: 5.5914539071546\n",
      "---------------------\n",
      "Iteration Number: 9484\n",
      "Loss: 27.062793161206844\n",
      "l2 norm of gradients: 0.1678385803948163\n",
      "l2 norm of weights: 5.591392825524536\n",
      "---------------------\n",
      "Iteration Number: 9485\n",
      "Loss: 27.061380219261768\n",
      "l2 norm of gradients: 0.167826472460489\n",
      "l2 norm of weights: 5.591331748119411\n",
      "---------------------\n",
      "Iteration Number: 9486\n",
      "Loss: 27.059967480538354\n",
      "l2 norm of gradients: 0.1678143664124169\n",
      "l2 norm of weights: 5.591270674938705\n",
      "---------------------\n",
      "Iteration Number: 9487\n",
      "Loss: 27.058554944987822\n",
      "l2 norm of gradients: 0.16780226225006245\n",
      "l2 norm of weights: 5.591209605981893\n",
      "---------------------\n",
      "Iteration Number: 9488\n",
      "Loss: 27.057142612565237\n",
      "l2 norm of gradients: 0.16779015997288824\n",
      "l2 norm of weights: 5.591148541248455\n",
      "---------------------\n",
      "Iteration Number: 9489\n",
      "Loss: 27.055730483224046\n",
      "l2 norm of gradients: 0.16777805958035702\n",
      "l2 norm of weights: 5.591087480737865\n",
      "---------------------\n",
      "Iteration Number: 9490\n",
      "Loss: 27.054318556918254\n",
      "l2 norm of gradients: 0.1677659610719319\n",
      "l2 norm of weights: 5.591026424449603\n",
      "---------------------\n",
      "Iteration Number: 9491\n",
      "Loss: 27.052906833599337\n",
      "l2 norm of gradients: 0.1677538644470761\n",
      "l2 norm of weights: 5.5909653723831445\n",
      "---------------------\n",
      "Iteration Number: 9492\n",
      "Loss: 27.05149531322221\n",
      "l2 norm of gradients: 0.1677417697052531\n",
      "l2 norm of weights: 5.590904324537968\n",
      "---------------------\n",
      "Iteration Number: 9493\n",
      "Loss: 27.050083995739463\n",
      "l2 norm of gradients: 0.16772967684592657\n",
      "l2 norm of weights: 5.5908432809135515\n",
      "---------------------\n",
      "Iteration Number: 9494\n",
      "Loss: 27.04867288110656\n",
      "l2 norm of gradients: 0.1677175858685604\n",
      "l2 norm of weights: 5.590782241509372\n",
      "---------------------\n",
      "Iteration Number: 9495\n",
      "Loss: 27.047261969274892\n",
      "l2 norm of gradients: 0.16770549677261867\n",
      "l2 norm of weights: 5.590721206324909\n",
      "---------------------\n",
      "Iteration Number: 9496\n",
      "Loss: 27.045851260197665\n",
      "l2 norm of gradients: 0.16769340955756573\n",
      "l2 norm of weights: 5.59066017535964\n",
      "---------------------\n",
      "Iteration Number: 9497\n",
      "Loss: 27.044440753828656\n",
      "l2 norm of gradients: 0.16768132422286613\n",
      "l2 norm of weights: 5.590599148613043\n",
      "---------------------\n",
      "Iteration Number: 9498\n",
      "Loss: 27.043030450125173\n",
      "l2 norm of gradients: 0.1676692407679846\n",
      "l2 norm of weights: 5.5905381260845965\n",
      "---------------------\n",
      "Iteration Number: 9499\n",
      "Loss: 27.04162034903519\n",
      "l2 norm of gradients: 0.16765715919238608\n",
      "l2 norm of weights: 5.590477107773777\n",
      "---------------------\n",
      "Iteration Number: 9500\n",
      "Loss: 27.040210450515666\n",
      "l2 norm of gradients: 0.16764507949553575\n",
      "l2 norm of weights: 5.590416093680066\n",
      "---------------------\n",
      "Iteration Number: 9501\n",
      "Loss: 27.03880075451996\n",
      "l2 norm of gradients: 0.16763300167689899\n",
      "l2 norm of weights: 5.59035508380294\n",
      "---------------------\n",
      "Iteration Number: 9502\n",
      "Loss: 27.037391261000195\n",
      "l2 norm of gradients: 0.1676209257359414\n",
      "l2 norm of weights: 5.59029407814188\n",
      "---------------------\n",
      "Iteration Number: 9503\n",
      "Loss: 27.035981969912488\n",
      "l2 norm of gradients: 0.16760885167212877\n",
      "l2 norm of weights: 5.590233076696363\n",
      "---------------------\n",
      "Iteration Number: 9504\n",
      "Loss: 27.034572881207364\n",
      "l2 norm of gradients: 0.16759677948492718\n",
      "l2 norm of weights: 5.590172079465868\n",
      "---------------------\n",
      "Iteration Number: 9505\n",
      "Loss: 27.033163994842045\n",
      "l2 norm of gradients: 0.1675847091738028\n",
      "l2 norm of weights: 5.590111086449874\n",
      "---------------------\n",
      "Iteration Number: 9506\n",
      "Loss: 27.03175531076565\n",
      "l2 norm of gradients: 0.1675726407382221\n",
      "l2 norm of weights: 5.590050097647861\n",
      "---------------------\n",
      "Iteration Number: 9507\n",
      "Loss: 27.03034682893733\n",
      "l2 norm of gradients: 0.1675605741776517\n",
      "l2 norm of weights: 5.589989113059309\n",
      "---------------------\n",
      "Iteration Number: 9508\n",
      "Loss: 27.02893854930611\n",
      "l2 norm of gradients: 0.16754850949155842\n",
      "l2 norm of weights: 5.589928132683695\n",
      "---------------------\n",
      "Iteration Number: 9509\n",
      "Loss: 27.02753047182773\n",
      "l2 norm of gradients: 0.16753644667940948\n",
      "l2 norm of weights: 5.5898671565205\n",
      "---------------------\n",
      "Iteration Number: 9510\n",
      "Loss: 27.02612259645733\n",
      "l2 norm of gradients: 0.16752438574067205\n",
      "l2 norm of weights: 5.589806184569204\n",
      "---------------------\n",
      "Iteration Number: 9511\n",
      "Loss: 27.024714923144767\n",
      "l2 norm of gradients: 0.16751232667481364\n",
      "l2 norm of weights: 5.589745216829285\n",
      "---------------------\n",
      "Iteration Number: 9512\n",
      "Loss: 27.023307451847327\n",
      "l2 norm of gradients: 0.16750026948130195\n",
      "l2 norm of weights: 5.5896842533002244\n",
      "---------------------\n",
      "Iteration Number: 9513\n",
      "Loss: 27.02190018251863\n",
      "l2 norm of gradients: 0.1674882141596049\n",
      "l2 norm of weights: 5.589623293981502\n",
      "---------------------\n",
      "Iteration Number: 9514\n",
      "Loss: 27.020493115110938\n",
      "l2 norm of gradients: 0.1674761607091906\n",
      "l2 norm of weights: 5.589562338872597\n",
      "---------------------\n",
      "Iteration Number: 9515\n",
      "Loss: 27.019086249579626\n",
      "l2 norm of gradients: 0.16746410912952742\n",
      "l2 norm of weights: 5.58950138797299\n",
      "---------------------\n",
      "Iteration Number: 9516\n",
      "Loss: 27.017679585876657\n",
      "l2 norm of gradients: 0.16745205942008382\n",
      "l2 norm of weights: 5.589440441282161\n",
      "---------------------\n",
      "Iteration Number: 9517\n",
      "Loss: 27.016273123955905\n",
      "l2 norm of gradients: 0.16744001158032865\n",
      "l2 norm of weights: 5.589379498799592\n",
      "---------------------\n",
      "Iteration Number: 9518\n",
      "Loss: 27.01486686377298\n",
      "l2 norm of gradients: 0.16742796560973083\n",
      "l2 norm of weights: 5.589318560524761\n",
      "---------------------\n",
      "Iteration Number: 9519\n",
      "Loss: 27.01346080528202\n",
      "l2 norm of gradients: 0.1674159215077594\n",
      "l2 norm of weights: 5.58925762645715\n",
      "---------------------\n",
      "Iteration Number: 9520\n",
      "Loss: 27.012054948434656\n",
      "l2 norm of gradients: 0.16740387927388395\n",
      "l2 norm of weights: 5.589196696596239\n",
      "---------------------\n",
      "Iteration Number: 9521\n",
      "Loss: 27.010649293186724\n",
      "l2 norm of gradients: 0.1673918389075739\n",
      "l2 norm of weights: 5.5891357709415095\n",
      "---------------------\n",
      "Iteration Number: 9522\n",
      "Loss: 27.00924383949067\n",
      "l2 norm of gradients: 0.16737980040829906\n",
      "l2 norm of weights: 5.589074849492442\n",
      "---------------------\n",
      "Iteration Number: 9523\n",
      "Loss: 27.007838587302135\n",
      "l2 norm of gradients: 0.16736776377552945\n",
      "l2 norm of weights: 5.589013932248518\n",
      "---------------------\n",
      "Iteration Number: 9524\n",
      "Loss: 27.00643353657496\n",
      "l2 norm of gradients: 0.1673557290087353\n",
      "l2 norm of weights: 5.588953019209217\n",
      "---------------------\n",
      "Iteration Number: 9525\n",
      "Loss: 27.00502868726028\n",
      "l2 norm of gradients: 0.16734369610738695\n",
      "l2 norm of weights: 5.588892110374022\n",
      "---------------------\n",
      "Iteration Number: 9526\n",
      "Loss: 27.00362403931567\n",
      "l2 norm of gradients: 0.16733166507095504\n",
      "l2 norm of weights: 5.588831205742414\n",
      "---------------------\n",
      "Iteration Number: 9527\n",
      "Loss: 27.002219592692704\n",
      "l2 norm of gradients: 0.16731963589891038\n",
      "l2 norm of weights: 5.588770305313874\n",
      "---------------------\n",
      "Iteration Number: 9528\n",
      "Loss: 27.000815347348382\n",
      "l2 norm of gradients: 0.16730760859072402\n",
      "l2 norm of weights: 5.588709409087884\n",
      "---------------------\n",
      "Iteration Number: 9529\n",
      "Loss: 26.999411303233767\n",
      "l2 norm of gradients: 0.1672955831458672\n",
      "l2 norm of weights: 5.588648517063925\n",
      "---------------------\n",
      "Iteration Number: 9530\n",
      "Loss: 26.99800746030223\n",
      "l2 norm of gradients: 0.1672835595638113\n",
      "l2 norm of weights: 5.58858762924148\n",
      "---------------------\n",
      "Iteration Number: 9531\n",
      "Loss: 26.996603818512042\n",
      "l2 norm of gradients: 0.16727153784402804\n",
      "l2 norm of weights: 5.588526745620029\n",
      "---------------------\n",
      "Iteration Number: 9532\n",
      "Loss: 26.995200377813624\n",
      "l2 norm of gradients: 0.16725951798598923\n",
      "l2 norm of weights: 5.588465866199055\n",
      "---------------------\n",
      "Iteration Number: 9533\n",
      "Loss: 26.993797138163174\n",
      "l2 norm of gradients: 0.16724749998916688\n",
      "l2 norm of weights: 5.58840499097804\n",
      "---------------------\n",
      "Iteration Number: 9534\n",
      "Loss: 26.992394099514218\n",
      "l2 norm of gradients: 0.1672354838530333\n",
      "l2 norm of weights: 5.588344119956467\n",
      "---------------------\n",
      "Iteration Number: 9535\n",
      "Loss: 26.990991261819406\n",
      "l2 norm of gradients: 0.16722346957706094\n",
      "l2 norm of weights: 5.588283253133817\n",
      "---------------------\n",
      "Iteration Number: 9536\n",
      "Loss: 26.989588625034795\n",
      "l2 norm of gradients: 0.16721145716072242\n",
      "l2 norm of weights: 5.588222390509573\n",
      "---------------------\n",
      "Iteration Number: 9537\n",
      "Loss: 26.988186189113282\n",
      "l2 norm of gradients: 0.16719944660349068\n",
      "l2 norm of weights: 5.588161532083217\n",
      "---------------------\n",
      "Iteration Number: 9538\n",
      "Loss: 26.986783954010924\n",
      "l2 norm of gradients: 0.16718743790483873\n",
      "l2 norm of weights: 5.588100677854232\n",
      "---------------------\n",
      "Iteration Number: 9539\n",
      "Loss: 26.985381919679195\n",
      "l2 norm of gradients: 0.16717543106423993\n",
      "l2 norm of weights: 5.588039827822102\n",
      "---------------------\n",
      "Iteration Number: 9540\n",
      "Loss: 26.98398008607438\n",
      "l2 norm of gradients: 0.16716342608116766\n",
      "l2 norm of weights: 5.587978981986308\n",
      "---------------------\n",
      "Iteration Number: 9541\n",
      "Loss: 26.982578453150754\n",
      "l2 norm of gradients: 0.16715142295509566\n",
      "l2 norm of weights: 5.587918140346333\n",
      "---------------------\n",
      "Iteration Number: 9542\n",
      "Loss: 26.98117702086018\n",
      "l2 norm of gradients: 0.16713942168549784\n",
      "l2 norm of weights: 5.587857302901662\n",
      "---------------------\n",
      "Iteration Number: 9543\n",
      "Loss: 26.979775789160392\n",
      "l2 norm of gradients: 0.16712742227184818\n",
      "l2 norm of weights: 5.587796469651775\n",
      "---------------------\n",
      "Iteration Number: 9544\n",
      "Loss: 26.97837475800484\n",
      "l2 norm of gradients: 0.16711542471362112\n",
      "l2 norm of weights: 5.587735640596158\n",
      "---------------------\n",
      "Iteration Number: 9545\n",
      "Loss: 26.976973927345703\n",
      "l2 norm of gradients: 0.167103429010291\n",
      "l2 norm of weights: 5.587674815734294\n",
      "---------------------\n",
      "Iteration Number: 9546\n",
      "Loss: 26.975573297138027\n",
      "l2 norm of gradients: 0.16709143516133265\n",
      "l2 norm of weights: 5.587613995065665\n",
      "---------------------\n",
      "Iteration Number: 9547\n",
      "Loss: 26.974172867337465\n",
      "l2 norm of gradients: 0.16707944316622086\n",
      "l2 norm of weights: 5.587553178589756\n",
      "---------------------\n",
      "Iteration Number: 9548\n",
      "Loss: 26.972772637896306\n",
      "l2 norm of gradients: 0.1670674530244308\n",
      "l2 norm of weights: 5.58749236630605\n",
      "---------------------\n",
      "Iteration Number: 9549\n",
      "Loss: 26.971372608772555\n",
      "l2 norm of gradients: 0.1670554647354377\n",
      "l2 norm of weights: 5.58743155821403\n",
      "---------------------\n",
      "Iteration Number: 9550\n",
      "Loss: 26.969972779917466\n",
      "l2 norm of gradients: 0.1670434782987171\n",
      "l2 norm of weights: 5.5873707543131825\n",
      "---------------------\n",
      "Iteration Number: 9551\n",
      "Loss: 26.968573151285888\n",
      "l2 norm of gradients: 0.1670314937137447\n",
      "l2 norm of weights: 5.587309954602989\n",
      "---------------------\n",
      "Iteration Number: 9552\n",
      "Loss: 26.9671737228315\n",
      "l2 norm of gradients: 0.16701951097999643\n",
      "l2 norm of weights: 5.587249159082934\n",
      "---------------------\n",
      "Iteration Number: 9553\n",
      "Loss: 26.965774494511674\n",
      "l2 norm of gradients: 0.16700753009694835\n",
      "l2 norm of weights: 5.5871883677525025\n",
      "---------------------\n",
      "Iteration Number: 9554\n",
      "Loss: 26.964375466277655\n",
      "l2 norm of gradients: 0.16699555106407674\n",
      "l2 norm of weights: 5.587127580611179\n",
      "---------------------\n",
      "Iteration Number: 9555\n",
      "Loss: 26.962976638085333\n",
      "l2 norm of gradients: 0.16698357388085816\n",
      "l2 norm of weights: 5.587066797658447\n",
      "---------------------\n",
      "Iteration Number: 9556\n",
      "Loss: 26.96157800988989\n",
      "l2 norm of gradients: 0.16697159854676927\n",
      "l2 norm of weights: 5.587006018893792\n",
      "---------------------\n",
      "Iteration Number: 9557\n",
      "Loss: 26.96017958164478\n",
      "l2 norm of gradients: 0.166959625061287\n",
      "l2 norm of weights: 5.5869452443166985\n",
      "---------------------\n",
      "Iteration Number: 9558\n",
      "Loss: 26.95878135330371\n",
      "l2 norm of gradients: 0.16694765342388843\n",
      "l2 norm of weights: 5.58688447392665\n",
      "---------------------\n",
      "Iteration Number: 9559\n",
      "Loss: 26.95738332482357\n",
      "l2 norm of gradients: 0.1669356836340508\n",
      "l2 norm of weights: 5.586823707723133\n",
      "---------------------\n",
      "Iteration Number: 9560\n",
      "Loss: 26.955985496156856\n",
      "l2 norm of gradients: 0.16692371569125175\n",
      "l2 norm of weights: 5.586762945705632\n",
      "---------------------\n",
      "Iteration Number: 9561\n",
      "Loss: 26.954587867257956\n",
      "l2 norm of gradients: 0.16691174959496885\n",
      "l2 norm of weights: 5.586702187873631\n",
      "---------------------\n",
      "Iteration Number: 9562\n",
      "Loss: 26.953190438084043\n",
      "l2 norm of gradients: 0.16689978534468008\n",
      "l2 norm of weights: 5.586641434226617\n",
      "---------------------\n",
      "Iteration Number: 9563\n",
      "Loss: 26.951793208587198\n",
      "l2 norm of gradients: 0.16688782293986346\n",
      "l2 norm of weights: 5.5865806847640735\n",
      "---------------------\n",
      "Iteration Number: 9564\n",
      "Loss: 26.950396178722343\n",
      "l2 norm of gradients: 0.16687586237999727\n",
      "l2 norm of weights: 5.586519939485487\n",
      "---------------------\n",
      "Iteration Number: 9565\n",
      "Loss: 26.94899934844477\n",
      "l2 norm of gradients: 0.16686390366456008\n",
      "l2 norm of weights: 5.586459198390343\n",
      "---------------------\n",
      "Iteration Number: 9566\n",
      "Loss: 26.9476027177095\n",
      "l2 norm of gradients: 0.16685194679303053\n",
      "l2 norm of weights: 5.586398461478126\n",
      "---------------------\n",
      "Iteration Number: 9567\n",
      "Loss: 26.946206286471607\n",
      "l2 norm of gradients: 0.1668399917648875\n",
      "l2 norm of weights: 5.586337728748324\n",
      "---------------------\n",
      "Iteration Number: 9568\n",
      "Loss: 26.944810054680488\n",
      "l2 norm of gradients: 0.16682803857961004\n",
      "l2 norm of weights: 5.58627700020042\n",
      "---------------------\n",
      "Iteration Number: 9569\n",
      "Loss: 26.943414022298498\n",
      "l2 norm of gradients: 0.16681608723667748\n",
      "l2 norm of weights: 5.586216275833902\n",
      "---------------------\n",
      "Iteration Number: 9570\n",
      "Loss: 26.942018189275306\n",
      "l2 norm of gradients: 0.16680413773556926\n",
      "l2 norm of weights: 5.586155555648255\n",
      "---------------------\n",
      "Iteration Number: 9571\n",
      "Loss: 26.94062255556874\n",
      "l2 norm of gradients: 0.1667921900757651\n",
      "l2 norm of weights: 5.586094839642967\n",
      "---------------------\n",
      "Iteration Number: 9572\n",
      "Loss: 26.93922712112992\n",
      "l2 norm of gradients: 0.16678024425674481\n",
      "l2 norm of weights: 5.586034127817522\n",
      "---------------------\n",
      "Iteration Number: 9573\n",
      "Loss: 26.937831885917426\n",
      "l2 norm of gradients: 0.16676830027798845\n",
      "l2 norm of weights: 5.585973420171406\n",
      "---------------------\n",
      "Iteration Number: 9574\n",
      "Loss: 26.936436849882387\n",
      "l2 norm of gradients: 0.16675635813897632\n",
      "l2 norm of weights: 5.585912716704107\n",
      "---------------------\n",
      "Iteration Number: 9575\n",
      "Loss: 26.935042012983875\n",
      "l2 norm of gradients: 0.16674441783918884\n",
      "l2 norm of weights: 5.585852017415111\n",
      "---------------------\n",
      "Iteration Number: 9576\n",
      "Loss: 26.93364737517188\n",
      "l2 norm of gradients: 0.16673247937810665\n",
      "l2 norm of weights: 5.585791322303906\n",
      "---------------------\n",
      "Iteration Number: 9577\n",
      "Loss: 26.932252936404833\n",
      "l2 norm of gradients: 0.1667205427552106\n",
      "l2 norm of weights: 5.585730631369977\n",
      "---------------------\n",
      "Iteration Number: 9578\n",
      "Loss: 26.930858696634935\n",
      "l2 norm of gradients: 0.16670860796998174\n",
      "l2 norm of weights: 5.58566994461281\n",
      "---------------------\n",
      "Iteration Number: 9579\n",
      "Loss: 26.929464655819704\n",
      "l2 norm of gradients: 0.16669667502190133\n",
      "l2 norm of weights: 5.585609262031896\n",
      "---------------------\n",
      "Iteration Number: 9580\n",
      "Loss: 26.92807081391238\n",
      "l2 norm of gradients: 0.1666847439104507\n",
      "l2 norm of weights: 5.585548583626718\n",
      "---------------------\n",
      "Iteration Number: 9581\n",
      "Loss: 26.926677170866313\n",
      "l2 norm of gradients: 0.16667281463511155\n",
      "l2 norm of weights: 5.585487909396765\n",
      "---------------------\n",
      "Iteration Number: 9582\n",
      "Loss: 26.925283726639726\n",
      "l2 norm of gradients: 0.1666608871953657\n",
      "l2 norm of weights: 5.585427239341524\n",
      "---------------------\n",
      "Iteration Number: 9583\n",
      "Loss: 26.923890481185154\n",
      "l2 norm of gradients: 0.16664896159069514\n",
      "l2 norm of weights: 5.585366573460482\n",
      "---------------------\n",
      "Iteration Number: 9584\n",
      "Loss: 26.922497434460013\n",
      "l2 norm of gradients: 0.16663703782058206\n",
      "l2 norm of weights: 5.585305911753128\n",
      "---------------------\n",
      "Iteration Number: 9585\n",
      "Loss: 26.921104586414852\n",
      "l2 norm of gradients: 0.16662511588450887\n",
      "l2 norm of weights: 5.585245254218949\n",
      "---------------------\n",
      "Iteration Number: 9586\n",
      "Loss: 26.91971193700891\n",
      "l2 norm of gradients: 0.16661319578195813\n",
      "l2 norm of weights: 5.585184600857431\n",
      "---------------------\n",
      "Iteration Number: 9587\n",
      "Loss: 26.918319486193074\n",
      "l2 norm of gradients: 0.1666012775124127\n",
      "l2 norm of weights: 5.585123951668064\n",
      "---------------------\n",
      "Iteration Number: 9588\n",
      "Loss: 26.916927233927314\n",
      "l2 norm of gradients: 0.1665893610753554\n",
      "l2 norm of weights: 5.585063306650335\n",
      "---------------------\n",
      "Iteration Number: 9589\n",
      "Loss: 26.915535180163907\n",
      "l2 norm of gradients: 0.1665774464702695\n",
      "l2 norm of weights: 5.585002665803732\n",
      "---------------------\n",
      "Iteration Number: 9590\n",
      "Loss: 26.914143324856624\n",
      "l2 norm of gradients: 0.1665655336966384\n",
      "l2 norm of weights: 5.5849420291277445\n",
      "---------------------\n",
      "Iteration Number: 9591\n",
      "Loss: 26.912751667961363\n",
      "l2 norm of gradients: 0.16655362275394564\n",
      "l2 norm of weights: 5.584881396621858\n",
      "---------------------\n",
      "Iteration Number: 9592\n",
      "Loss: 26.91136020943496\n",
      "l2 norm of gradients: 0.1665417136416749\n",
      "l2 norm of weights: 5.584820768285564\n",
      "---------------------\n",
      "Iteration Number: 9593\n",
      "Loss: 26.909968949229093\n",
      "l2 norm of gradients: 0.1665298063593101\n",
      "l2 norm of weights: 5.5847601441183485\n",
      "---------------------\n",
      "Iteration Number: 9594\n",
      "Loss: 26.90857788730312\n",
      "l2 norm of gradients: 0.16651790090633542\n",
      "l2 norm of weights: 5.584699524119702\n",
      "---------------------\n",
      "Iteration Number: 9595\n",
      "Loss: 26.90718702360894\n",
      "l2 norm of gradients: 0.1665059972822351\n",
      "l2 norm of weights: 5.5846389082891115\n",
      "---------------------\n",
      "Iteration Number: 9596\n",
      "Loss: 26.905796358103366\n",
      "l2 norm of gradients: 0.16649409548649383\n",
      "l2 norm of weights: 5.584578296626066\n",
      "---------------------\n",
      "Iteration Number: 9597\n",
      "Loss: 26.904405890738737\n",
      "l2 norm of gradients: 0.16648219551859614\n",
      "l2 norm of weights: 5.584517689130056\n",
      "---------------------\n",
      "Iteration Number: 9598\n",
      "Loss: 26.903015621472655\n",
      "l2 norm of gradients: 0.16647029737802693\n",
      "l2 norm of weights: 5.58445708580057\n",
      "---------------------\n",
      "Iteration Number: 9599\n",
      "Loss: 26.901625550259958\n",
      "l2 norm of gradients: 0.16645840106427134\n",
      "l2 norm of weights: 5.584396486637095\n",
      "---------------------\n",
      "Iteration Number: 9600\n",
      "Loss: 26.900235677054614\n",
      "l2 norm of gradients: 0.16644650657681465\n",
      "l2 norm of weights: 5.584335891639122\n",
      "---------------------\n",
      "Iteration Number: 9601\n",
      "Loss: 26.898846001814544\n",
      "l2 norm of gradients: 0.16643461391514225\n",
      "l2 norm of weights: 5.584275300806141\n",
      "---------------------\n",
      "Iteration Number: 9602\n",
      "Loss: 26.897456524491357\n",
      "l2 norm of gradients: 0.1664227230787398\n",
      "l2 norm of weights: 5.58421471413764\n",
      "---------------------\n",
      "Iteration Number: 9603\n",
      "Loss: 26.896067245043728\n",
      "l2 norm of gradients: 0.16641083406709317\n",
      "l2 norm of weights: 5.584154131633109\n",
      "---------------------\n",
      "Iteration Number: 9604\n",
      "Loss: 26.894678163423613\n",
      "l2 norm of gradients: 0.1663989468796884\n",
      "l2 norm of weights: 5.584093553292036\n",
      "---------------------\n",
      "Iteration Number: 9605\n",
      "Loss: 26.893289279587954\n",
      "l2 norm of gradients: 0.16638706151601168\n",
      "l2 norm of weights: 5.584032979113915\n",
      "---------------------\n",
      "Iteration Number: 9606\n",
      "Loss: 26.89190059349193\n",
      "l2 norm of gradients: 0.1663751779755494\n",
      "l2 norm of weights: 5.583972409098232\n",
      "---------------------\n",
      "Iteration Number: 9607\n",
      "Loss: 26.890512105091716\n",
      "l2 norm of gradients: 0.16636329625778823\n",
      "l2 norm of weights: 5.583911843244479\n",
      "---------------------\n",
      "Iteration Number: 9608\n",
      "Loss: 26.8891238143414\n",
      "l2 norm of gradients: 0.16635141636221484\n",
      "l2 norm of weights: 5.583851281552144\n",
      "---------------------\n",
      "Iteration Number: 9609\n",
      "Loss: 26.88773572119492\n",
      "l2 norm of gradients: 0.1663395382883163\n",
      "l2 norm of weights: 5.583790724020719\n",
      "---------------------\n",
      "Iteration Number: 9610\n",
      "Loss: 26.886347825609622\n",
      "l2 norm of gradients: 0.16632766203557967\n",
      "l2 norm of weights: 5.5837301706496945\n",
      "---------------------\n",
      "Iteration Number: 9611\n",
      "Loss: 26.88496012754071\n",
      "l2 norm of gradients: 0.16631578760349244\n",
      "l2 norm of weights: 5.58366962143856\n",
      "---------------------\n",
      "Iteration Number: 9612\n",
      "Loss: 26.883572626941955\n",
      "l2 norm of gradients: 0.166303914991542\n",
      "l2 norm of weights: 5.5836090763868045\n",
      "---------------------\n",
      "Iteration Number: 9613\n",
      "Loss: 26.882185323769704\n",
      "l2 norm of gradients: 0.16629204419921614\n",
      "l2 norm of weights: 5.583548535493922\n",
      "---------------------\n",
      "Iteration Number: 9614\n",
      "Loss: 26.88079821798134\n",
      "l2 norm of gradients: 0.16628017522600283\n",
      "l2 norm of weights: 5.5834879987593995\n",
      "---------------------\n",
      "Iteration Number: 9615\n",
      "Loss: 26.87941130952977\n",
      "l2 norm of gradients: 0.16626830807138995\n",
      "l2 norm of weights: 5.58342746618273\n",
      "---------------------\n",
      "Iteration Number: 9616\n",
      "Loss: 26.87802459837083\n",
      "l2 norm of gradients: 0.16625644273486603\n",
      "l2 norm of weights: 5.5833669377634045\n",
      "---------------------\n",
      "Iteration Number: 9617\n",
      "Loss: 26.87663808446008\n",
      "l2 norm of gradients: 0.16624457921591942\n",
      "l2 norm of weights: 5.583306413500914\n",
      "---------------------\n",
      "Iteration Number: 9618\n",
      "Loss: 26.875251767751628\n",
      "l2 norm of gradients: 0.1662327175140388\n",
      "l2 norm of weights: 5.583245893394748\n",
      "---------------------\n",
      "Iteration Number: 9619\n",
      "Loss: 26.87386564820338\n",
      "l2 norm of gradients: 0.16622085762871303\n",
      "l2 norm of weights: 5.5831853774444\n",
      "---------------------\n",
      "Iteration Number: 9620\n",
      "Loss: 26.872479725770102\n",
      "l2 norm of gradients: 0.16620899955943108\n",
      "l2 norm of weights: 5.583124865649358\n",
      "---------------------\n",
      "Iteration Number: 9621\n",
      "Loss: 26.8710940004067\n",
      "l2 norm of gradients: 0.16619714330568222\n",
      "l2 norm of weights: 5.583064358009117\n",
      "---------------------\n",
      "Iteration Number: 9622\n",
      "Loss: 26.869708472067387\n",
      "l2 norm of gradients: 0.16618528886695583\n",
      "l2 norm of weights: 5.583003854523167\n",
      "---------------------\n",
      "Iteration Number: 9623\n",
      "Loss: 26.868323140710192\n",
      "l2 norm of gradients: 0.16617343624274145\n",
      "l2 norm of weights: 5.5829433551909995\n",
      "---------------------\n",
      "Iteration Number: 9624\n",
      "Loss: 26.86693800628875\n",
      "l2 norm of gradients: 0.16616158543252893\n",
      "l2 norm of weights: 5.5828828600121065\n",
      "---------------------\n",
      "Iteration Number: 9625\n",
      "Loss: 26.86555306875953\n",
      "l2 norm of gradients: 0.1661497364358082\n",
      "l2 norm of weights: 5.582822368985979\n",
      "---------------------\n",
      "Iteration Number: 9626\n",
      "Loss: 26.86416832807952\n",
      "l2 norm of gradients: 0.16613788925206938\n",
      "l2 norm of weights: 5.582761882112111\n",
      "---------------------\n",
      "Iteration Number: 9627\n",
      "Loss: 26.862783784200097\n",
      "l2 norm of gradients: 0.16612604388080277\n",
      "l2 norm of weights: 5.582701399389992\n",
      "---------------------\n",
      "Iteration Number: 9628\n",
      "Loss: 26.8613994370819\n",
      "l2 norm of gradients: 0.1661142003214989\n",
      "l2 norm of weights: 5.582640920819117\n",
      "---------------------\n",
      "Iteration Number: 9629\n",
      "Loss: 26.860015286675363\n",
      "l2 norm of gradients: 0.16610235857364847\n",
      "l2 norm of weights: 5.582580446398975\n",
      "---------------------\n",
      "Iteration Number: 9630\n",
      "Loss: 26.85863133294099\n",
      "l2 norm of gradients: 0.16609051863674243\n",
      "l2 norm of weights: 5.582519976129062\n",
      "---------------------\n",
      "Iteration Number: 9631\n",
      "Loss: 26.857247575830527\n",
      "l2 norm of gradients: 0.16607868051027166\n",
      "l2 norm of weights: 5.582459510008866\n",
      "---------------------\n",
      "Iteration Number: 9632\n",
      "Loss: 26.855864015302657\n",
      "l2 norm of gradients: 0.16606684419372758\n",
      "l2 norm of weights: 5.582399048037884\n",
      "---------------------\n",
      "Iteration Number: 9633\n",
      "Loss: 26.854480651309864\n",
      "l2 norm of gradients: 0.1660550096866015\n",
      "l2 norm of weights: 5.582338590215605\n",
      "---------------------\n",
      "Iteration Number: 9634\n",
      "Loss: 26.853097483812487\n",
      "l2 norm of gradients: 0.16604317698838508\n",
      "l2 norm of weights: 5.582278136541525\n",
      "---------------------\n",
      "Iteration Number: 9635\n",
      "Loss: 26.851714512760033\n",
      "l2 norm of gradients: 0.16603134609857018\n",
      "l2 norm of weights: 5.5822176870151345\n",
      "---------------------\n",
      "Iteration Number: 9636\n",
      "Loss: 26.850331738112995\n",
      "l2 norm of gradients: 0.1660195170166486\n",
      "l2 norm of weights: 5.582157241635928\n",
      "---------------------\n",
      "Iteration Number: 9637\n",
      "Loss: 26.848949159826393\n",
      "l2 norm of gradients: 0.16600768974211264\n",
      "l2 norm of weights: 5.582096800403398\n",
      "---------------------\n",
      "Iteration Number: 9638\n",
      "Loss: 26.84756677785417\n",
      "l2 norm of gradients: 0.16599586427445462\n",
      "l2 norm of weights: 5.582036363317037\n",
      "---------------------\n",
      "Iteration Number: 9639\n",
      "Loss: 26.846184592152337\n",
      "l2 norm of gradients: 0.16598404061316702\n",
      "l2 norm of weights: 5.581975930376339\n",
      "---------------------\n",
      "Iteration Number: 9640\n",
      "Loss: 26.844802602677966\n",
      "l2 norm of gradients: 0.16597221875774257\n",
      "l2 norm of weights: 5.581915501580799\n",
      "---------------------\n",
      "Iteration Number: 9641\n",
      "Loss: 26.843420809386934\n",
      "l2 norm of gradients: 0.16596039870767412\n",
      "l2 norm of weights: 5.581855076929908\n",
      "---------------------\n",
      "Iteration Number: 9642\n",
      "Loss: 26.842039212233736\n",
      "l2 norm of gradients: 0.1659485804624548\n",
      "l2 norm of weights: 5.581794656423159\n",
      "---------------------\n",
      "Iteration Number: 9643\n",
      "Loss: 26.840657811173408\n",
      "l2 norm of gradients: 0.16593676402157786\n",
      "l2 norm of weights: 5.581734240060049\n",
      "---------------------\n",
      "Iteration Number: 9644\n",
      "Loss: 26.83927660616431\n",
      "l2 norm of gradients: 0.16592494938453667\n",
      "l2 norm of weights: 5.58167382784007\n",
      "---------------------\n",
      "Iteration Number: 9645\n",
      "Loss: 26.837895597160912\n",
      "l2 norm of gradients: 0.16591313655082485\n",
      "l2 norm of weights: 5.581613419762715\n",
      "---------------------\n",
      "Iteration Number: 9646\n",
      "Loss: 26.836514784119814\n",
      "l2 norm of gradients: 0.16590132551993622\n",
      "l2 norm of weights: 5.581553015827479\n",
      "---------------------\n",
      "Iteration Number: 9647\n",
      "Loss: 26.835134166996994\n",
      "l2 norm of gradients: 0.16588951629136472\n",
      "l2 norm of weights: 5.581492616033857\n",
      "---------------------\n",
      "Iteration Number: 9648\n",
      "Loss: 26.8337537457459\n",
      "l2 norm of gradients: 0.16587770886460454\n",
      "l2 norm of weights: 5.581432220381342\n",
      "---------------------\n",
      "Iteration Number: 9649\n",
      "Loss: 26.83237352032535\n",
      "l2 norm of gradients: 0.16586590323915\n",
      "l2 norm of weights: 5.581371828869429\n",
      "---------------------\n",
      "Iteration Number: 9650\n",
      "Loss: 26.830993490687128\n",
      "l2 norm of gradients: 0.16585409941449558\n",
      "l2 norm of weights: 5.581311441497612\n",
      "---------------------\n",
      "Iteration Number: 9651\n",
      "Loss: 26.829613656793413\n",
      "l2 norm of gradients: 0.16584229739013603\n",
      "l2 norm of weights: 5.581251058265385\n",
      "---------------------\n",
      "Iteration Number: 9652\n",
      "Loss: 26.828234018596078\n",
      "l2 norm of gradients: 0.16583049716556617\n",
      "l2 norm of weights: 5.581190679172244\n",
      "---------------------\n",
      "Iteration Number: 9653\n",
      "Loss: 26.826854576051005\n",
      "l2 norm of gradients: 0.16581869874028105\n",
      "l2 norm of weights: 5.581130304217684\n",
      "---------------------\n",
      "Iteration Number: 9654\n",
      "Loss: 26.825475329115058\n",
      "l2 norm of gradients: 0.16580690211377594\n",
      "l2 norm of weights: 5.5810699334011975\n",
      "---------------------\n",
      "Iteration Number: 9655\n",
      "Loss: 26.82409627774536\n",
      "l2 norm of gradients: 0.1657951072855462\n",
      "l2 norm of weights: 5.581009566722282\n",
      "---------------------\n",
      "Iteration Number: 9656\n",
      "Loss: 26.822717421895508\n",
      "l2 norm of gradients: 0.16578331425508747\n",
      "l2 norm of weights: 5.58094920418043\n",
      "---------------------\n",
      "Iteration Number: 9657\n",
      "Loss: 26.821338761521783\n",
      "l2 norm of gradients: 0.16577152302189543\n",
      "l2 norm of weights: 5.58088884577514\n",
      "---------------------\n",
      "Iteration Number: 9658\n",
      "Loss: 26.819960296583286\n",
      "l2 norm of gradients: 0.16575973358546614\n",
      "l2 norm of weights: 5.580828491505906\n",
      "---------------------\n",
      "Iteration Number: 9659\n",
      "Loss: 26.81858202703247\n",
      "l2 norm of gradients: 0.16574794594529563\n",
      "l2 norm of weights: 5.580768141372221\n",
      "---------------------\n",
      "Iteration Number: 9660\n",
      "Loss: 26.81720395282628\n",
      "l2 norm of gradients: 0.16573616010088021\n",
      "l2 norm of weights: 5.580707795373583\n",
      "---------------------\n",
      "Iteration Number: 9661\n",
      "Loss: 26.815826073923088\n",
      "l2 norm of gradients: 0.16572437605171644\n",
      "l2 norm of weights: 5.580647453509488\n",
      "---------------------\n",
      "Iteration Number: 9662\n",
      "Loss: 26.81444839027574\n",
      "l2 norm of gradients: 0.16571259379730088\n",
      "l2 norm of weights: 5.580587115779429\n",
      "---------------------\n",
      "Iteration Number: 9663\n",
      "Loss: 26.813070901842565\n",
      "l2 norm of gradients: 0.16570081333713038\n",
      "l2 norm of weights: 5.580526782182904\n",
      "---------------------\n",
      "Iteration Number: 9664\n",
      "Loss: 26.81169360857859\n",
      "l2 norm of gradients: 0.165689034670702\n",
      "l2 norm of weights: 5.580466452719409\n",
      "---------------------\n",
      "Iteration Number: 9665\n",
      "Loss: 26.81031651044042\n",
      "l2 norm of gradients: 0.16567725779751286\n",
      "l2 norm of weights: 5.580406127388438\n",
      "---------------------\n",
      "Iteration Number: 9666\n",
      "Loss: 26.8089396073851\n",
      "l2 norm of gradients: 0.16566548271706036\n",
      "l2 norm of weights: 5.58034580618949\n",
      "---------------------\n",
      "Iteration Number: 9667\n",
      "Loss: 26.807562899365195\n",
      "l2 norm of gradients: 0.165653709428842\n",
      "l2 norm of weights: 5.580285489122058\n",
      "---------------------\n",
      "Iteration Number: 9668\n",
      "Loss: 26.806186386340983\n",
      "l2 norm of gradients: 0.1656419379323556\n",
      "l2 norm of weights: 5.580225176185642\n",
      "---------------------\n",
      "Iteration Number: 9669\n",
      "Loss: 26.80481006826732\n",
      "l2 norm of gradients: 0.16563016822709895\n",
      "l2 norm of weights: 5.580164867379735\n",
      "---------------------\n",
      "Iteration Number: 9670\n",
      "Loss: 26.80343394510008\n",
      "l2 norm of gradients: 0.16561840031257014\n",
      "l2 norm of weights: 5.580104562703835\n",
      "---------------------\n",
      "Iteration Number: 9671\n",
      "Loss: 26.802058016795755\n",
      "l2 norm of gradients: 0.1656066341882674\n",
      "l2 norm of weights: 5.580044262157439\n",
      "---------------------\n",
      "Iteration Number: 9672\n",
      "Loss: 26.800682283309325\n",
      "l2 norm of gradients: 0.16559486985368918\n",
      "l2 norm of weights: 5.579983965740043\n",
      "---------------------\n",
      "Iteration Number: 9673\n",
      "Loss: 26.799306744599228\n",
      "l2 norm of gradients: 0.1655831073083341\n",
      "l2 norm of weights: 5.579923673451144\n",
      "---------------------\n",
      "Iteration Number: 9674\n",
      "Loss: 26.79793140062054\n",
      "l2 norm of gradients: 0.16557134655170083\n",
      "l2 norm of weights: 5.579863385290239\n",
      "---------------------\n",
      "Iteration Number: 9675\n",
      "Loss: 26.79655625133058\n",
      "l2 norm of gradients: 0.16555958758328845\n",
      "l2 norm of weights: 5.579803101256825\n",
      "---------------------\n",
      "Iteration Number: 9676\n",
      "Loss: 26.795181296682895\n",
      "l2 norm of gradients: 0.16554783040259602\n",
      "l2 norm of weights: 5.5797428213504\n",
      "---------------------\n",
      "Iteration Number: 9677\n",
      "Loss: 26.793806536636037\n",
      "l2 norm of gradients: 0.1655360750091228\n",
      "l2 norm of weights: 5.579682545570459\n",
      "---------------------\n",
      "Iteration Number: 9678\n",
      "Loss: 26.792431971145884\n",
      "l2 norm of gradients: 0.16552432140236828\n",
      "l2 norm of weights: 5.579622273916502\n",
      "---------------------\n",
      "Iteration Number: 9679\n",
      "Loss: 26.791057600169257\n",
      "l2 norm of gradients: 0.16551256958183208\n",
      "l2 norm of weights: 5.5795620063880245\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 9680\n",
      "Loss: 26.78968342366266\n",
      "l2 norm of gradients: 0.16550081954701404\n",
      "l2 norm of weights: 5.579501742984526\n",
      "---------------------\n",
      "Iteration Number: 9681\n",
      "Loss: 26.78830944157998\n",
      "l2 norm of gradients: 0.16548907129741416\n",
      "l2 norm of weights: 5.5794414837055015\n",
      "---------------------\n",
      "Iteration Number: 9682\n",
      "Loss: 26.78693565388123\n",
      "l2 norm of gradients: 0.16547732483253264\n",
      "l2 norm of weights: 5.579381228550451\n",
      "---------------------\n",
      "Iteration Number: 9683\n",
      "Loss: 26.785562060519503\n",
      "l2 norm of gradients: 0.1654655801518697\n",
      "l2 norm of weights: 5.579320977518871\n",
      "---------------------\n",
      "Iteration Number: 9684\n",
      "Loss: 26.784188661455982\n",
      "l2 norm of gradients: 0.165453837254926\n",
      "l2 norm of weights: 5.57926073061026\n",
      "---------------------\n",
      "Iteration Number: 9685\n",
      "Loss: 26.78281545664012\n",
      "l2 norm of gradients: 0.16544209614120214\n",
      "l2 norm of weights: 5.579200487824117\n",
      "---------------------\n",
      "Iteration Number: 9686\n",
      "Loss: 26.78144244603344\n",
      "l2 norm of gradients: 0.165430356810199\n",
      "l2 norm of weights: 5.579140249159939\n",
      "---------------------\n",
      "Iteration Number: 9687\n",
      "Loss: 26.780069629591086\n",
      "l2 norm of gradients: 0.16541861926141752\n",
      "l2 norm of weights: 5.579080014617224\n",
      "---------------------\n",
      "Iteration Number: 9688\n",
      "Loss: 26.778697007270573\n",
      "l2 norm of gradients: 0.16540688349435903\n",
      "l2 norm of weights: 5.579019784195471\n",
      "---------------------\n",
      "Iteration Number: 9689\n",
      "Loss: 26.777324579026402\n",
      "l2 norm of gradients: 0.16539514950852488\n",
      "l2 norm of weights: 5.578959557894179\n",
      "---------------------\n",
      "Iteration Number: 9690\n",
      "Loss: 26.775952344815863\n",
      "l2 norm of gradients: 0.16538341730341655\n",
      "l2 norm of weights: 5.578899335712846\n",
      "---------------------\n",
      "Iteration Number: 9691\n",
      "Loss: 26.774580304596125\n",
      "l2 norm of gradients: 0.16537168687853585\n",
      "l2 norm of weights: 5.578839117650969\n",
      "---------------------\n",
      "Iteration Number: 9692\n",
      "Loss: 26.773208458322472\n",
      "l2 norm of gradients: 0.16535995823338454\n",
      "l2 norm of weights: 5.57877890370805\n",
      "---------------------\n",
      "Iteration Number: 9693\n",
      "Loss: 26.77183680595251\n",
      "l2 norm of gradients: 0.16534823136746488\n",
      "l2 norm of weights: 5.578718693883586\n",
      "---------------------\n",
      "Iteration Number: 9694\n",
      "Loss: 26.77046534744183\n",
      "l2 norm of gradients: 0.1653365062802789\n",
      "l2 norm of weights: 5.578658488177077\n",
      "---------------------\n",
      "Iteration Number: 9695\n",
      "Loss: 26.769094082749156\n",
      "l2 norm of gradients: 0.16532478297132908\n",
      "l2 norm of weights: 5.57859828658802\n",
      "---------------------\n",
      "Iteration Number: 9696\n",
      "Loss: 26.76772301182763\n",
      "l2 norm of gradients: 0.16531306144011806\n",
      "l2 norm of weights: 5.578538089115917\n",
      "---------------------\n",
      "Iteration Number: 9697\n",
      "Loss: 26.76635213463793\n",
      "l2 norm of gradients: 0.16530134168614852\n",
      "l2 norm of weights: 5.578477895760266\n",
      "---------------------\n",
      "Iteration Number: 9698\n",
      "Loss: 26.764981451133213\n",
      "l2 norm of gradients: 0.16528962370892336\n",
      "l2 norm of weights: 5.578417706520566\n",
      "---------------------\n",
      "Iteration Number: 9699\n",
      "Loss: 26.763610961271503\n",
      "l2 norm of gradients: 0.16527790750794577\n",
      "l2 norm of weights: 5.578357521396316\n",
      "---------------------\n",
      "Iteration Number: 9700\n",
      "Loss: 26.76224066501065\n",
      "l2 norm of gradients: 0.16526619308271887\n",
      "l2 norm of weights: 5.578297340387017\n",
      "---------------------\n",
      "Iteration Number: 9701\n",
      "Loss: 26.760870562303943\n",
      "l2 norm of gradients: 0.1652544804327462\n",
      "l2 norm of weights: 5.578237163492169\n",
      "---------------------\n",
      "Iteration Number: 9702\n",
      "Loss: 26.759500653111893\n",
      "l2 norm of gradients: 0.1652427695575313\n",
      "l2 norm of weights: 5.5781769907112695\n",
      "---------------------\n",
      "Iteration Number: 9703\n",
      "Loss: 26.758130937388707\n",
      "l2 norm of gradients: 0.16523106045657793\n",
      "l2 norm of weights: 5.578116822043821\n",
      "---------------------\n",
      "Iteration Number: 9704\n",
      "Loss: 26.756761415092214\n",
      "l2 norm of gradients: 0.1652193531293901\n",
      "l2 norm of weights: 5.578056657489322\n",
      "---------------------\n",
      "Iteration Number: 9705\n",
      "Loss: 26.755392086178162\n",
      "l2 norm of gradients: 0.1652076475754718\n",
      "l2 norm of weights: 5.577996497047273\n",
      "---------------------\n",
      "Iteration Number: 9706\n",
      "Loss: 26.754022950604423\n",
      "l2 norm of gradients: 0.1651959437943274\n",
      "l2 norm of weights: 5.577936340717175\n",
      "---------------------\n",
      "Iteration Number: 9707\n",
      "Loss: 26.752654008327287\n",
      "l2 norm of gradients: 0.16518424178546132\n",
      "l2 norm of weights: 5.577876188498526\n",
      "---------------------\n",
      "Iteration Number: 9708\n",
      "Loss: 26.75128525930335\n",
      "l2 norm of gradients: 0.16517254154837815\n",
      "l2 norm of weights: 5.577816040390829\n",
      "---------------------\n",
      "Iteration Number: 9709\n",
      "Loss: 26.74991670348943\n",
      "l2 norm of gradients: 0.1651608430825827\n",
      "l2 norm of weights: 5.577755896393583\n",
      "---------------------\n",
      "Iteration Number: 9710\n",
      "Loss: 26.748548340843165\n",
      "l2 norm of gradients: 0.1651491463875799\n",
      "l2 norm of weights: 5.577695756506291\n",
      "---------------------\n",
      "Iteration Number: 9711\n",
      "Loss: 26.74718017131905\n",
      "l2 norm of gradients: 0.1651374514628749\n",
      "l2 norm of weights: 5.57763562072845\n",
      "---------------------\n",
      "Iteration Number: 9712\n",
      "Loss: 26.74581219487518\n",
      "l2 norm of gradients: 0.16512575830797296\n",
      "l2 norm of weights: 5.577575489059563\n",
      "---------------------\n",
      "Iteration Number: 9713\n",
      "Loss: 26.74444441147028\n",
      "l2 norm of gradients: 0.16511406692237957\n",
      "l2 norm of weights: 5.577515361499131\n",
      "---------------------\n",
      "Iteration Number: 9714\n",
      "Loss: 26.743076821060004\n",
      "l2 norm of gradients: 0.1651023773056003\n",
      "l2 norm of weights: 5.577455238046655\n",
      "---------------------\n",
      "Iteration Number: 9715\n",
      "Loss: 26.741709423599463\n",
      "l2 norm of gradients: 0.16509068945714095\n",
      "l2 norm of weights: 5.577395118701635\n",
      "---------------------\n",
      "Iteration Number: 9716\n",
      "Loss: 26.74034221904627\n",
      "l2 norm of gradients: 0.16507900337650752\n",
      "l2 norm of weights: 5.577335003463574\n",
      "---------------------\n",
      "Iteration Number: 9717\n",
      "Loss: 26.73897520735916\n",
      "l2 norm of gradients: 0.16506731906320607\n",
      "l2 norm of weights: 5.577274892331973\n",
      "---------------------\n",
      "Iteration Number: 9718\n",
      "Loss: 26.737608388493577\n",
      "l2 norm of gradients: 0.16505563651674296\n",
      "l2 norm of weights: 5.577214785306332\n",
      "---------------------\n",
      "Iteration Number: 9719\n",
      "Loss: 26.7362417624064\n",
      "l2 norm of gradients: 0.1650439557366246\n",
      "l2 norm of weights: 5.577154682386153\n",
      "---------------------\n",
      "Iteration Number: 9720\n",
      "Loss: 26.73487532905622\n",
      "l2 norm of gradients: 0.16503227672235762\n",
      "l2 norm of weights: 5.577094583570939\n",
      "---------------------\n",
      "Iteration Number: 9721\n",
      "Loss: 26.733509088397295\n",
      "l2 norm of gradients: 0.16502059947344885\n",
      "l2 norm of weights: 5.57703448886019\n",
      "---------------------\n",
      "Iteration Number: 9722\n",
      "Loss: 26.7321430403888\n",
      "l2 norm of gradients: 0.16500892398940523\n",
      "l2 norm of weights: 5.576974398253409\n",
      "---------------------\n",
      "Iteration Number: 9723\n",
      "Loss: 26.730777184985087\n",
      "l2 norm of gradients: 0.16499725026973383\n",
      "l2 norm of weights: 5.576914311750098\n",
      "---------------------\n",
      "Iteration Number: 9724\n",
      "Loss: 26.729411522146115\n",
      "l2 norm of gradients: 0.164985578313942\n",
      "l2 norm of weights: 5.576854229349759\n",
      "---------------------\n",
      "Iteration Number: 9725\n",
      "Loss: 26.728046051827683\n",
      "l2 norm of gradients: 0.1649739081215372\n",
      "l2 norm of weights: 5.576794151051893\n",
      "---------------------\n",
      "Iteration Number: 9726\n",
      "Loss: 26.726680773987304\n",
      "l2 norm of gradients: 0.164962239692027\n",
      "l2 norm of weights: 5.576734076856003\n",
      "---------------------\n",
      "Iteration Number: 9727\n",
      "Loss: 26.725315688580558\n",
      "l2 norm of gradients: 0.16495057302491928\n",
      "l2 norm of weights: 5.576674006761594\n",
      "---------------------\n",
      "Iteration Number: 9728\n",
      "Loss: 26.72395079556582\n",
      "l2 norm of gradients: 0.16493890811972187\n",
      "l2 norm of weights: 5.576613940768164\n",
      "---------------------\n",
      "Iteration Number: 9729\n",
      "Loss: 26.722586094900215\n",
      "l2 norm of gradients: 0.16492724497594294\n",
      "l2 norm of weights: 5.576553878875218\n",
      "---------------------\n",
      "Iteration Number: 9730\n",
      "Loss: 26.721221586539546\n",
      "l2 norm of gradients: 0.16491558359309078\n",
      "l2 norm of weights: 5.576493821082257\n",
      "---------------------\n",
      "Iteration Number: 9731\n",
      "Loss: 26.71985727044106\n",
      "l2 norm of gradients: 0.16490392397067383\n",
      "l2 norm of weights: 5.576433767388786\n",
      "---------------------\n",
      "Iteration Number: 9732\n",
      "Loss: 26.718493146564306\n",
      "l2 norm of gradients: 0.16489226610820068\n",
      "l2 norm of weights: 5.576373717794308\n",
      "---------------------\n",
      "Iteration Number: 9733\n",
      "Loss: 26.717129214864336\n",
      "l2 norm of gradients: 0.16488061000518017\n",
      "l2 norm of weights: 5.5763136722983235\n",
      "---------------------\n",
      "Iteration Number: 9734\n",
      "Loss: 26.715765475297562\n",
      "l2 norm of gradients: 0.16486895566112114\n",
      "l2 norm of weights: 5.576253630900337\n",
      "---------------------\n",
      "Iteration Number: 9735\n",
      "Loss: 26.714401927822877\n",
      "l2 norm of gradients: 0.16485730307553276\n",
      "l2 norm of weights: 5.576193593599851\n",
      "---------------------\n",
      "Iteration Number: 9736\n",
      "Loss: 26.713038572397345\n",
      "l2 norm of gradients: 0.1648456522479243\n",
      "l2 norm of weights: 5.576133560396371\n",
      "---------------------\n",
      "Iteration Number: 9737\n",
      "Loss: 26.71167540897686\n",
      "l2 norm of gradients: 0.16483400317780517\n",
      "l2 norm of weights: 5.576073531289398\n",
      "---------------------\n",
      "Iteration Number: 9738\n",
      "Loss: 26.71031243752138\n",
      "l2 norm of gradients: 0.16482235586468497\n",
      "l2 norm of weights: 5.576013506278437\n",
      "---------------------\n",
      "Iteration Number: 9739\n",
      "Loss: 26.708949657983826\n",
      "l2 norm of gradients: 0.16481071030807343\n",
      "l2 norm of weights: 5.57595348536299\n",
      "---------------------\n",
      "Iteration Number: 9740\n",
      "Loss: 26.707587070325836\n",
      "l2 norm of gradients: 0.1647990665074805\n",
      "l2 norm of weights: 5.575893468542561\n",
      "---------------------\n",
      "Iteration Number: 9741\n",
      "Loss: 26.706224674500266\n",
      "l2 norm of gradients: 0.1647874244624163\n",
      "l2 norm of weights: 5.575833455816655\n",
      "---------------------\n",
      "Iteration Number: 9742\n",
      "Loss: 26.70486247046773\n",
      "l2 norm of gradients: 0.16477578417239097\n",
      "l2 norm of weights: 5.575773447184775\n",
      "---------------------\n",
      "Iteration Number: 9743\n",
      "Loss: 26.70350045818458\n",
      "l2 norm of gradients: 0.16476414563691497\n",
      "l2 norm of weights: 5.575713442646426\n",
      "---------------------\n",
      "Iteration Number: 9744\n",
      "Loss: 26.702138637608876\n",
      "l2 norm of gradients: 0.16475250885549894\n",
      "l2 norm of weights: 5.575653442201111\n",
      "---------------------\n",
      "Iteration Number: 9745\n",
      "Loss: 26.700777008694047\n",
      "l2 norm of gradients: 0.16474087382765348\n",
      "l2 norm of weights: 5.575593445848335\n",
      "---------------------\n",
      "Iteration Number: 9746\n",
      "Loss: 26.69941557140274\n",
      "l2 norm of gradients: 0.1647292405528896\n",
      "l2 norm of weights: 5.5755334535876\n",
      "---------------------\n",
      "Iteration Number: 9747\n",
      "Loss: 26.698054325689128\n",
      "l2 norm of gradients: 0.1647176090307183\n",
      "l2 norm of weights: 5.575473465418413\n",
      "---------------------\n",
      "Iteration Number: 9748\n",
      "Loss: 26.69669327151077\n",
      "l2 norm of gradients: 0.1647059792606508\n",
      "l2 norm of weights: 5.575413481340278\n",
      "---------------------\n",
      "Iteration Number: 9749\n",
      "Loss: 26.69533240882646\n",
      "l2 norm of gradients: 0.16469435124219853\n",
      "l2 norm of weights: 5.575353501352699\n",
      "---------------------\n",
      "Iteration Number: 9750\n",
      "Loss: 26.693971737593383\n",
      "l2 norm of gradients: 0.1646827249748729\n",
      "l2 norm of weights: 5.575293525455181\n",
      "---------------------\n",
      "Iteration Number: 9751\n",
      "Loss: 26.692611257766494\n",
      "l2 norm of gradients: 0.16467110045818575\n",
      "l2 norm of weights: 5.575233553647228\n",
      "---------------------\n",
      "Iteration Number: 9752\n",
      "Loss: 26.691250969304967\n",
      "l2 norm of gradients: 0.16465947769164893\n",
      "l2 norm of weights: 5.575173585928347\n",
      "---------------------\n",
      "Iteration Number: 9753\n",
      "Loss: 26.689890872167528\n",
      "l2 norm of gradients: 0.16464785667477436\n",
      "l2 norm of weights: 5.5751136222980415\n",
      "---------------------\n",
      "Iteration Number: 9754\n",
      "Loss: 26.688530966309685\n",
      "l2 norm of gradients: 0.1646362374070743\n",
      "l2 norm of weights: 5.575053662755817\n",
      "---------------------\n",
      "Iteration Number: 9755\n",
      "Loss: 26.687171251689197\n",
      "l2 norm of gradients: 0.1646246198880611\n",
      "l2 norm of weights: 5.574993707301178\n",
      "---------------------\n",
      "Iteration Number: 9756\n",
      "Loss: 26.685811728264245\n",
      "l2 norm of gradients: 0.16461300411724727\n",
      "l2 norm of weights: 5.574933755933629\n",
      "---------------------\n",
      "Iteration Number: 9757\n",
      "Loss: 26.68445239599241\n",
      "l2 norm of gradients: 0.16460139009414546\n",
      "l2 norm of weights: 5.574873808652678\n",
      "---------------------\n",
      "Iteration Number: 9758\n",
      "Loss: 26.68309325482932\n",
      "l2 norm of gradients: 0.16458977781826845\n",
      "l2 norm of weights: 5.574813865457829\n",
      "---------------------\n",
      "Iteration Number: 9759\n",
      "Loss: 26.681734304733393\n",
      "l2 norm of gradients: 0.16457816728912933\n",
      "l2 norm of weights: 5.5747539263485875\n",
      "---------------------\n",
      "Iteration Number: 9760\n",
      "Loss: 26.680375545664276\n",
      "l2 norm of gradients: 0.16456655850624108\n",
      "l2 norm of weights: 5.574693991324461\n",
      "---------------------\n",
      "Iteration Number: 9761\n",
      "Loss: 26.67901697757591\n",
      "l2 norm of gradients: 0.16455495146911717\n",
      "l2 norm of weights: 5.574634060384952\n",
      "---------------------\n",
      "Iteration Number: 9762\n",
      "Loss: 26.677658600428884\n",
      "l2 norm of gradients: 0.164543346177271\n",
      "l2 norm of weights: 5.574574133529568\n",
      "---------------------\n",
      "Iteration Number: 9763\n",
      "Loss: 26.67630041417878\n",
      "l2 norm of gradients: 0.16453174263021614\n",
      "l2 norm of weights: 5.574514210757816\n",
      "---------------------\n",
      "Iteration Number: 9764\n",
      "Loss: 26.674942418783743\n",
      "l2 norm of gradients: 0.16452014082746647\n",
      "l2 norm of weights: 5.574454292069202\n",
      "---------------------\n",
      "Iteration Number: 9765\n",
      "Loss: 26.673584614203747\n",
      "l2 norm of gradients: 0.16450854076853585\n",
      "l2 norm of weights: 5.57439437746323\n",
      "---------------------\n",
      "Iteration Number: 9766\n",
      "Loss: 26.672227000391022\n",
      "l2 norm of gradients: 0.16449694245293844\n",
      "l2 norm of weights: 5.574334466939409\n",
      "---------------------\n",
      "Iteration Number: 9767\n",
      "Loss: 26.670869577307226\n",
      "l2 norm of gradients: 0.16448534588018845\n",
      "l2 norm of weights: 5.574274560497244\n",
      "---------------------\n",
      "Iteration Number: 9768\n",
      "Loss: 26.669512344910466\n",
      "l2 norm of gradients: 0.1644737510498003\n",
      "l2 norm of weights: 5.574214658136242\n",
      "---------------------\n",
      "Iteration Number: 9769\n",
      "Loss: 26.668155303155448\n",
      "l2 norm of gradients: 0.16446215796128855\n",
      "l2 norm of weights: 5.574154759855909\n",
      "---------------------\n",
      "Iteration Number: 9770\n",
      "Loss: 26.666798452002453\n",
      "l2 norm of gradients: 0.164450566614168\n",
      "l2 norm of weights: 5.574094865655752\n",
      "---------------------\n",
      "Iteration Number: 9771\n",
      "Loss: 26.665441791408227\n",
      "l2 norm of gradients: 0.16443897700795349\n",
      "l2 norm of weights: 5.5740349755352785\n",
      "---------------------\n",
      "Iteration Number: 9772\n",
      "Loss: 26.664085321329903\n",
      "l2 norm of gradients: 0.16442738914216004\n",
      "l2 norm of weights: 5.573975089493994\n",
      "---------------------\n",
      "Iteration Number: 9773\n",
      "Loss: 26.662729041725505\n",
      "l2 norm of gradients: 0.16441580301630287\n",
      "l2 norm of weights: 5.573915207531407\n",
      "---------------------\n",
      "Iteration Number: 9774\n",
      "Loss: 26.661372952552355\n",
      "l2 norm of gradients: 0.1644042186298974\n",
      "l2 norm of weights: 5.5738553296470235\n",
      "---------------------\n",
      "Iteration Number: 9775\n",
      "Loss: 26.660017053770282\n",
      "l2 norm of gradients: 0.16439263598245912\n",
      "l2 norm of weights: 5.573795455840352\n",
      "---------------------\n",
      "Iteration Number: 9776\n",
      "Loss: 26.658661345333435\n",
      "l2 norm of gradients: 0.16438105507350365\n",
      "l2 norm of weights: 5.573735586110898\n",
      "---------------------\n",
      "Iteration Number: 9777\n",
      "Loss: 26.657305827202812\n",
      "l2 norm of gradients: 0.16436947590254686\n",
      "l2 norm of weights: 5.57367572045817\n",
      "---------------------\n",
      "Iteration Number: 9778\n",
      "Loss: 26.65595049933481\n",
      "l2 norm of gradients: 0.16435789846910476\n",
      "l2 norm of weights: 5.573615858881675\n",
      "---------------------\n",
      "Iteration Number: 9779\n",
      "Loss: 26.654595361686773\n",
      "l2 norm of gradients: 0.16434632277269345\n",
      "l2 norm of weights: 5.573556001380922\n",
      "---------------------\n",
      "Iteration Number: 9780\n",
      "Loss: 26.65324041421719\n",
      "l2 norm of gradients: 0.16433474881282925\n",
      "l2 norm of weights: 5.573496147955418\n",
      "---------------------\n",
      "Iteration Number: 9781\n",
      "Loss: 26.651885656884357\n",
      "l2 norm of gradients: 0.16432317658902867\n",
      "l2 norm of weights: 5.57343629860467\n",
      "---------------------\n",
      "Iteration Number: 9782\n",
      "Loss: 26.650531089645302\n",
      "l2 norm of gradients: 0.16431160610080825\n",
      "l2 norm of weights: 5.573376453328186\n",
      "---------------------\n",
      "Iteration Number: 9783\n",
      "Loss: 26.649176712457763\n",
      "l2 norm of gradients: 0.1643000373476848\n",
      "l2 norm of weights: 5.573316612125475\n",
      "---------------------\n",
      "Iteration Number: 9784\n",
      "Loss: 26.647822525280585\n",
      "l2 norm of gradients: 0.1642884703291752\n",
      "l2 norm of weights: 5.573256774996045\n",
      "---------------------\n",
      "Iteration Number: 9785\n",
      "Loss: 26.64646852806871\n",
      "l2 norm of gradients: 0.1642769050447966\n",
      "l2 norm of weights: 5.573196941939403\n",
      "---------------------\n",
      "Iteration Number: 9786\n",
      "Loss: 26.645114720785426\n",
      "l2 norm of gradients: 0.16426534149406616\n",
      "l2 norm of weights: 5.573137112955059\n",
      "---------------------\n",
      "Iteration Number: 9787\n",
      "Loss: 26.643761103383643\n",
      "l2 norm of gradients: 0.16425377967650132\n",
      "l2 norm of weights: 5.57307728804252\n",
      "---------------------\n",
      "Iteration Number: 9788\n",
      "Loss: 26.6424076758226\n",
      "l2 norm of gradients: 0.1642422195916196\n",
      "l2 norm of weights: 5.573017467201295\n",
      "---------------------\n",
      "Iteration Number: 9789\n",
      "Loss: 26.641054438061072\n",
      "l2 norm of gradients: 0.1642306612389387\n",
      "l2 norm of weights: 5.572957650430893\n",
      "---------------------\n",
      "Iteration Number: 9790\n",
      "Loss: 26.639701390056917\n",
      "l2 norm of gradients: 0.16421910461797656\n",
      "l2 norm of weights: 5.572897837730823\n",
      "---------------------\n",
      "Iteration Number: 9791\n",
      "Loss: 26.638348531767463\n",
      "l2 norm of gradients: 0.16420754972825102\n",
      "l2 norm of weights: 5.572838029100592\n",
      "---------------------\n",
      "Iteration Number: 9792\n",
      "Loss: 26.63699586315292\n",
      "l2 norm of gradients: 0.16419599656928038\n",
      "l2 norm of weights: 5.572778224539711\n",
      "---------------------\n",
      "Iteration Number: 9793\n",
      "Loss: 26.635643384166166\n",
      "l2 norm of gradients: 0.16418444514058297\n",
      "l2 norm of weights: 5.572718424047688\n",
      "---------------------\n",
      "Iteration Number: 9794\n",
      "Loss: 26.634291094770248\n",
      "l2 norm of gradients: 0.16417289544167712\n",
      "l2 norm of weights: 5.5726586276240315\n",
      "---------------------\n",
      "Iteration Number: 9795\n",
      "Loss: 26.63293899492277\n",
      "l2 norm of gradients: 0.1641613474720816\n",
      "l2 norm of weights: 5.572598835268253\n",
      "---------------------\n",
      "Iteration Number: 9796\n",
      "Loss: 26.631587084577845\n",
      "l2 norm of gradients: 0.16414980123131515\n",
      "l2 norm of weights: 5.572539046979859\n",
      "---------------------\n",
      "Iteration Number: 9797\n",
      "Loss: 26.6302353636965\n",
      "l2 norm of gradients: 0.16413825671889665\n",
      "l2 norm of weights: 5.572479262758361\n",
      "---------------------\n",
      "Iteration Number: 9798\n",
      "Loss: 26.62888383223755\n",
      "l2 norm of gradients: 0.16412671393434522\n",
      "l2 norm of weights: 5.572419482603268\n",
      "---------------------\n",
      "Iteration Number: 9799\n",
      "Loss: 26.627532490157307\n",
      "l2 norm of gradients: 0.16411517287718014\n",
      "l2 norm of weights: 5.572359706514089\n",
      "---------------------\n",
      "Iteration Number: 9800\n",
      "Loss: 26.626181337413158\n",
      "l2 norm of gradients: 0.16410363354692076\n",
      "l2 norm of weights: 5.572299934490334\n",
      "---------------------\n",
      "Iteration Number: 9801\n",
      "Loss: 26.62483037396595\n",
      "l2 norm of gradients: 0.16409209594308657\n",
      "l2 norm of weights: 5.572240166531513\n",
      "---------------------\n",
      "Iteration Number: 9802\n",
      "Loss: 26.623479599771148\n",
      "l2 norm of gradients: 0.16408056006519736\n",
      "l2 norm of weights: 5.572180402637136\n",
      "---------------------\n",
      "Iteration Number: 9803\n",
      "Loss: 26.62212901478844\n",
      "l2 norm of gradients: 0.16406902591277298\n",
      "l2 norm of weights: 5.572120642806713\n",
      "---------------------\n",
      "Iteration Number: 9804\n",
      "Loss: 26.620778618974676\n",
      "l2 norm of gradients: 0.16405749348533336\n",
      "l2 norm of weights: 5.572060887039753\n",
      "---------------------\n",
      "Iteration Number: 9805\n",
      "Loss: 26.619428412289015\n",
      "l2 norm of gradients: 0.16404596278239872\n",
      "l2 norm of weights: 5.572001135335768\n",
      "---------------------\n",
      "Iteration Number: 9806\n",
      "Loss: 26.618078394691008\n",
      "l2 norm of gradients: 0.16403443380348934\n",
      "l2 norm of weights: 5.571941387694267\n",
      "---------------------\n",
      "Iteration Number: 9807\n",
      "Loss: 26.616728566135627\n",
      "l2 norm of gradients: 0.16402290654812568\n",
      "l2 norm of weights: 5.571881644114761\n",
      "---------------------\n",
      "Iteration Number: 9808\n",
      "Loss: 26.61537892658249\n",
      "l2 norm of gradients: 0.16401138101582835\n",
      "l2 norm of weights: 5.57182190459676\n",
      "---------------------\n",
      "Iteration Number: 9809\n",
      "Loss: 26.6140294759901\n",
      "l2 norm of gradients: 0.16399985720611812\n",
      "l2 norm of weights: 5.571762169139776\n",
      "---------------------\n",
      "Iteration Number: 9810\n",
      "Loss: 26.612680214317066\n",
      "l2 norm of gradients: 0.16398833511851588\n",
      "l2 norm of weights: 5.571702437743317\n",
      "---------------------\n",
      "Iteration Number: 9811\n",
      "Loss: 26.611331141522726\n",
      "l2 norm of gradients: 0.16397681475254272\n",
      "l2 norm of weights: 5.5716427104068975\n",
      "---------------------\n",
      "Iteration Number: 9812\n",
      "Loss: 26.60998225756108\n",
      "l2 norm of gradients: 0.16396529610771987\n",
      "l2 norm of weights: 5.571582987130025\n",
      "---------------------\n",
      "Iteration Number: 9813\n",
      "Loss: 26.608633562394697\n",
      "l2 norm of gradients: 0.16395377918356868\n",
      "l2 norm of weights: 5.571523267912212\n",
      "---------------------\n",
      "Iteration Number: 9814\n",
      "Loss: 26.60728505597845\n",
      "l2 norm of gradients: 0.16394226397961062\n",
      "l2 norm of weights: 5.57146355275297\n",
      "---------------------\n",
      "Iteration Number: 9815\n",
      "Loss: 26.605936738274234\n",
      "l2 norm of gradients: 0.16393075049536748\n",
      "l2 norm of weights: 5.571403841651811\n",
      "---------------------\n",
      "Iteration Number: 9816\n",
      "Loss: 26.604588609237307\n",
      "l2 norm of gradients: 0.16391923873036096\n",
      "l2 norm of weights: 5.5713441346082435\n",
      "---------------------\n",
      "Iteration Number: 9817\n",
      "Loss: 26.60324066882687\n",
      "l2 norm of gradients: 0.16390772868411307\n",
      "l2 norm of weights: 5.57128443162178\n",
      "---------------------\n",
      "Iteration Number: 9818\n",
      "Loss: 26.60189291700293\n",
      "l2 norm of gradients: 0.16389622035614593\n",
      "l2 norm of weights: 5.571224732691934\n",
      "---------------------\n",
      "Iteration Number: 9819\n",
      "Loss: 26.6005453537224\n",
      "l2 norm of gradients: 0.16388471374598182\n",
      "l2 norm of weights: 5.571165037818214\n",
      "---------------------\n",
      "Iteration Number: 9820\n",
      "Loss: 26.59919797894385\n",
      "l2 norm of gradients: 0.1638732088531432\n",
      "l2 norm of weights: 5.571105347000135\n",
      "---------------------\n",
      "Iteration Number: 9821\n",
      "Loss: 26.597850792622882\n",
      "l2 norm of gradients: 0.1638617056771525\n",
      "l2 norm of weights: 5.571045660237205\n",
      "---------------------\n",
      "Iteration Number: 9822\n",
      "Loss: 26.596503794722388\n",
      "l2 norm of gradients: 0.16385020421753257\n",
      "l2 norm of weights: 5.570985977528939\n",
      "---------------------\n",
      "Iteration Number: 9823\n",
      "Loss: 26.59515698519963\n",
      "l2 norm of gradients: 0.16383870447380625\n",
      "l2 norm of weights: 5.5709262988748485\n",
      "---------------------\n",
      "Iteration Number: 9824\n",
      "Loss: 26.593810364011226\n",
      "l2 norm of gradients: 0.16382720644549653\n",
      "l2 norm of weights: 5.570866624274445\n",
      "---------------------\n",
      "Iteration Number: 9825\n",
      "Loss: 26.592463931117557\n",
      "l2 norm of gradients: 0.1638157101321266\n",
      "l2 norm of weights: 5.57080695372724\n",
      "---------------------\n",
      "Iteration Number: 9826\n",
      "Loss: 26.59111768647566\n",
      "l2 norm of gradients: 0.16380421553321978\n",
      "l2 norm of weights: 5.570747287232747\n",
      "---------------------\n",
      "Iteration Number: 9827\n",
      "Loss: 26.589771630044403\n",
      "l2 norm of gradients: 0.1637927226482995\n",
      "l2 norm of weights: 5.5706876247904775\n",
      "---------------------\n",
      "Iteration Number: 9828\n",
      "Loss: 26.588425761781927\n",
      "l2 norm of gradients: 0.1637812314768894\n",
      "l2 norm of weights: 5.570627966399945\n",
      "---------------------\n",
      "Iteration Number: 9829\n",
      "Loss: 26.587080081648104\n",
      "l2 norm of gradients: 0.16376974201851324\n",
      "l2 norm of weights: 5.570568312060661\n",
      "---------------------\n",
      "Iteration Number: 9830\n",
      "Loss: 26.58573458959954\n",
      "l2 norm of gradients: 0.16375825427269494\n",
      "l2 norm of weights: 5.5705086617721395\n",
      "---------------------\n",
      "Iteration Number: 9831\n",
      "Loss: 26.584389285596753\n",
      "l2 norm of gradients: 0.16374676823895853\n",
      "l2 norm of weights: 5.5704490155338915\n",
      "---------------------\n",
      "Iteration Number: 9832\n",
      "Loss: 26.58304416959506\n",
      "l2 norm of gradients: 0.16373528391682826\n",
      "l2 norm of weights: 5.570389373345432\n",
      "---------------------\n",
      "Iteration Number: 9833\n",
      "Loss: 26.581699241557487\n",
      "l2 norm of gradients: 0.16372380130582842\n",
      "l2 norm of weights: 5.570329735206272\n",
      "---------------------\n",
      "Iteration Number: 9834\n",
      "Loss: 26.580354501440144\n",
      "l2 norm of gradients: 0.16371232040548356\n",
      "l2 norm of weights: 5.570270101115925\n",
      "---------------------\n",
      "Iteration Number: 9835\n",
      "Loss: 26.57900994919922\n",
      "l2 norm of gradients: 0.16370084121531833\n",
      "l2 norm of weights: 5.570210471073906\n",
      "---------------------\n",
      "Iteration Number: 9836\n",
      "Loss: 26.577665584798897\n",
      "l2 norm of gradients: 0.16368936373485748\n",
      "l2 norm of weights: 5.570150845079726\n",
      "---------------------\n",
      "Iteration Number: 9837\n",
      "Loss: 26.57632140819364\n",
      "l2 norm of gradients: 0.163677887963626\n",
      "l2 norm of weights: 5.5700912231329\n",
      "---------------------\n",
      "Iteration Number: 9838\n",
      "Loss: 26.574977419342016\n",
      "l2 norm of gradients: 0.163666413901149\n",
      "l2 norm of weights: 5.57003160523294\n",
      "---------------------\n",
      "Iteration Number: 9839\n",
      "Loss: 26.57363361820533\n",
      "l2 norm of gradients: 0.16365494154695168\n",
      "l2 norm of weights: 5.5699719913793615\n",
      "---------------------\n",
      "Iteration Number: 9840\n",
      "Loss: 26.572290004738644\n",
      "l2 norm of gradients: 0.16364347090055942\n",
      "l2 norm of weights: 5.569912381571677\n",
      "---------------------\n",
      "Iteration Number: 9841\n",
      "Loss: 26.570946578904508\n",
      "l2 norm of gradients: 0.16363200196149774\n",
      "l2 norm of weights: 5.569852775809399\n",
      "---------------------\n",
      "Iteration Number: 9842\n",
      "Loss: 26.56960334065862\n",
      "l2 norm of gradients: 0.16362053472929244\n",
      "l2 norm of weights: 5.569793174092044\n",
      "---------------------\n",
      "Iteration Number: 9843\n",
      "Loss: 26.56826028996091\n",
      "l2 norm of gradients: 0.16360906920346913\n",
      "l2 norm of weights: 5.569733576419124\n",
      "---------------------\n",
      "Iteration Number: 9844\n",
      "Loss: 26.566917426769006\n",
      "l2 norm of gradients: 0.16359760538355395\n",
      "l2 norm of weights: 5.569673982790154\n",
      "---------------------\n",
      "Iteration Number: 9845\n",
      "Loss: 26.565574751044107\n",
      "l2 norm of gradients: 0.163586143269073\n",
      "l2 norm of weights: 5.569614393204648\n",
      "---------------------\n",
      "Iteration Number: 9846\n",
      "Loss: 26.56423226274266\n",
      "l2 norm of gradients: 0.16357468285955248\n",
      "l2 norm of weights: 5.5695548076621195\n",
      "---------------------\n",
      "Iteration Number: 9847\n",
      "Loss: 26.56288996182316\n",
      "l2 norm of gradients: 0.16356322415451882\n",
      "l2 norm of weights: 5.569495226162084\n",
      "---------------------\n",
      "Iteration Number: 9848\n",
      "Loss: 26.56154784824705\n",
      "l2 norm of gradients: 0.16355176715349865\n",
      "l2 norm of weights: 5.569435648704056\n",
      "---------------------\n",
      "Iteration Number: 9849\n",
      "Loss: 26.560205921969235\n",
      "l2 norm of gradients: 0.1635403118560185\n",
      "l2 norm of weights: 5.569376075287549\n",
      "---------------------\n",
      "Iteration Number: 9850\n",
      "Loss: 26.558864182951613\n",
      "l2 norm of gradients: 0.16352885826160538\n",
      "l2 norm of weights: 5.569316505912078\n",
      "---------------------\n",
      "Iteration Number: 9851\n",
      "Loss: 26.557522631151613\n",
      "l2 norm of gradients: 0.1635174063697862\n",
      "l2 norm of weights: 5.569256940577159\n",
      "---------------------\n",
      "Iteration Number: 9852\n",
      "Loss: 26.55618126652888\n",
      "l2 norm of gradients: 0.16350595618008815\n",
      "l2 norm of weights: 5.569197379282305\n",
      "---------------------\n",
      "Iteration Number: 9853\n",
      "Loss: 26.55484008904075\n",
      "l2 norm of gradients: 0.16349450769203844\n",
      "l2 norm of weights: 5.569137822027032\n",
      "---------------------\n",
      "Iteration Number: 9854\n",
      "Loss: 26.55349909864757\n",
      "l2 norm of gradients: 0.16348306090516457\n",
      "l2 norm of weights: 5.569078268810855\n",
      "---------------------\n",
      "Iteration Number: 9855\n",
      "Loss: 26.5521582953065\n",
      "l2 norm of gradients: 0.16347161581899403\n",
      "l2 norm of weights: 5.569018719633289\n",
      "---------------------\n",
      "Iteration Number: 9856\n",
      "Loss: 26.55081767897949\n",
      "l2 norm of gradients: 0.16346017243305463\n",
      "l2 norm of weights: 5.568959174493849\n",
      "---------------------\n",
      "Iteration Number: 9857\n",
      "Loss: 26.549477249621642\n",
      "l2 norm of gradients: 0.16344873074687416\n",
      "l2 norm of weights: 5.568899633392051\n",
      "---------------------\n",
      "Iteration Number: 9858\n",
      "Loss: 26.548137007194974\n",
      "l2 norm of gradients: 0.1634372907599806\n",
      "l2 norm of weights: 5.56884009632741\n",
      "---------------------\n",
      "Iteration Number: 9859\n",
      "Loss: 26.546796951654688\n",
      "l2 norm of gradients: 0.16342585247190217\n",
      "l2 norm of weights: 5.568780563299441\n",
      "---------------------\n",
      "Iteration Number: 9860\n",
      "Loss: 26.54545708296388\n",
      "l2 norm of gradients: 0.16341441588216712\n",
      "l2 norm of weights: 5.568721034307661\n",
      "---------------------\n",
      "Iteration Number: 9861\n",
      "Loss: 26.544117401078605\n",
      "l2 norm of gradients: 0.16340298099030393\n",
      "l2 norm of weights: 5.5686615093515845\n",
      "---------------------\n",
      "Iteration Number: 9862\n",
      "Loss: 26.542777905960076\n",
      "l2 norm of gradients: 0.1633915477958411\n",
      "l2 norm of weights: 5.5686019884307285\n",
      "---------------------\n",
      "Iteration Number: 9863\n",
      "Loss: 26.541438597566188\n",
      "l2 norm of gradients: 0.16338011629830745\n",
      "l2 norm of weights: 5.568542471544608\n",
      "---------------------\n",
      "Iteration Number: 9864\n",
      "Loss: 26.540099475853722\n",
      "l2 norm of gradients: 0.16336868649723174\n",
      "l2 norm of weights: 5.568482958692739\n",
      "---------------------\n",
      "Iteration Number: 9865\n",
      "Loss: 26.538760540785503\n",
      "l2 norm of gradients: 0.16335725839214302\n",
      "l2 norm of weights: 5.568423449874637\n",
      "---------------------\n",
      "Iteration Number: 9866\n",
      "Loss: 26.53742179231734\n",
      "l2 norm of gradients: 0.1633458319825705\n",
      "l2 norm of weights: 5.56836394508982\n",
      "---------------------\n",
      "Iteration Number: 9867\n",
      "Loss: 26.53608323040946\n",
      "l2 norm of gradients: 0.16333440726804338\n",
      "l2 norm of weights: 5.568304444337804\n",
      "---------------------\n",
      "Iteration Number: 9868\n",
      "Loss: 26.53474485502169\n",
      "l2 norm of gradients: 0.1633229842480912\n",
      "l2 norm of weights: 5.568244947618104\n",
      "---------------------\n",
      "Iteration Number: 9869\n",
      "Loss: 26.53340666611227\n",
      "l2 norm of gradients: 0.16331156292224344\n",
      "l2 norm of weights: 5.568185454930239\n",
      "---------------------\n",
      "Iteration Number: 9870\n",
      "Loss: 26.53206866364095\n",
      "l2 norm of gradients: 0.1633001432900299\n",
      "l2 norm of weights: 5.568125966273723\n",
      "---------------------\n",
      "Iteration Number: 9871\n",
      "Loss: 26.530730847566645\n",
      "l2 norm of gradients: 0.16328872535098035\n",
      "l2 norm of weights: 5.5680664816480725\n",
      "---------------------\n",
      "Iteration Number: 9872\n",
      "Loss: 26.52939321784587\n",
      "l2 norm of gradients: 0.16327730910462493\n",
      "l2 norm of weights: 5.568007001052807\n",
      "---------------------\n",
      "Iteration Number: 9873\n",
      "Loss: 26.528055774441427\n",
      "l2 norm of gradients: 0.16326589455049367\n",
      "l2 norm of weights: 5.567947524487442\n",
      "---------------------\n",
      "Iteration Number: 9874\n",
      "Loss: 26.526718517310115\n",
      "l2 norm of gradients: 0.1632544816881169\n",
      "l2 norm of weights: 5.567888051951494\n",
      "---------------------\n",
      "Iteration Number: 9875\n",
      "Loss: 26.525381446411345\n",
      "l2 norm of gradients: 0.1632430705170251\n",
      "l2 norm of weights: 5.567828583444481\n",
      "---------------------\n",
      "Iteration Number: 9876\n",
      "Loss: 26.524044561705907\n",
      "l2 norm of gradients: 0.1632316610367488\n",
      "l2 norm of weights: 5.567769118965919\n",
      "---------------------\n",
      "Iteration Number: 9877\n",
      "Loss: 26.522707863150767\n",
      "l2 norm of gradients: 0.16322025324681871\n",
      "l2 norm of weights: 5.5677096585153265\n",
      "---------------------\n",
      "Iteration Number: 9878\n",
      "Loss: 26.52137135070623\n",
      "l2 norm of gradients: 0.1632088471467657\n",
      "l2 norm of weights: 5.567650202092222\n",
      "---------------------\n",
      "Iteration Number: 9879\n",
      "Loss: 26.520035024332373\n",
      "l2 norm of gradients: 0.1631974427361208\n",
      "l2 norm of weights: 5.5675907496961194\n",
      "---------------------\n",
      "Iteration Number: 9880\n",
      "Loss: 26.51869888398529\n",
      "l2 norm of gradients: 0.1631860400144151\n",
      "l2 norm of weights: 5.5675313013265395\n",
      "---------------------\n",
      "Iteration Number: 9881\n",
      "Loss: 26.5173629296267\n",
      "l2 norm of gradients: 0.16317463898117993\n",
      "l2 norm of weights: 5.567471856982999\n",
      "---------------------\n",
      "Iteration Number: 9882\n",
      "Loss: 26.51602716121585\n",
      "l2 norm of gradients: 0.16316323963594664\n",
      "l2 norm of weights: 5.5674124166650145\n",
      "---------------------\n",
      "Iteration Number: 9883\n",
      "Loss: 26.514691578711638\n",
      "l2 norm of gradients: 0.1631518419782469\n",
      "l2 norm of weights: 5.567352980372105\n",
      "---------------------\n",
      "Iteration Number: 9884\n",
      "Loss: 26.513356182071174\n",
      "l2 norm of gradients: 0.16314044600761227\n",
      "l2 norm of weights: 5.567293548103788\n",
      "---------------------\n",
      "Iteration Number: 9885\n",
      "Loss: 26.512020971258206\n",
      "l2 norm of gradients: 0.1631290517235747\n",
      "l2 norm of weights: 5.5672341198595845\n",
      "---------------------\n",
      "Iteration Number: 9886\n",
      "Loss: 26.5106859462277\n",
      "l2 norm of gradients: 0.16311765912566614\n",
      "l2 norm of weights: 5.567174695639008\n",
      "---------------------\n",
      "Iteration Number: 9887\n",
      "Loss: 26.509351106940876\n",
      "l2 norm of gradients: 0.16310626821341873\n",
      "l2 norm of weights: 5.56711527544158\n",
      "---------------------\n",
      "Iteration Number: 9888\n",
      "Loss: 26.508016453356856\n",
      "l2 norm of gradients: 0.16309487898636468\n",
      "l2 norm of weights: 5.567055859266818\n",
      "---------------------\n",
      "Iteration Number: 9889\n",
      "Loss: 26.506681985434284\n",
      "l2 norm of gradients: 0.16308349144403647\n",
      "l2 norm of weights: 5.566996447114239\n",
      "---------------------\n",
      "Iteration Number: 9890\n",
      "Loss: 26.505347703133374\n",
      "l2 norm of gradients: 0.1630721055859666\n",
      "l2 norm of weights: 5.5669370389833635\n",
      "---------------------\n",
      "Iteration Number: 9891\n",
      "Loss: 26.504013606414453\n",
      "l2 norm of gradients: 0.1630607214116878\n",
      "l2 norm of weights: 5.566877634873709\n",
      "---------------------\n",
      "Iteration Number: 9892\n",
      "Loss: 26.502679695233727\n",
      "l2 norm of gradients: 0.16304933892073278\n",
      "l2 norm of weights: 5.566818234784796\n",
      "---------------------\n",
      "Iteration Number: 9893\n",
      "Loss: 26.501345969553974\n",
      "l2 norm of gradients: 0.16303795811263463\n",
      "l2 norm of weights: 5.566758838716142\n",
      "---------------------\n",
      "Iteration Number: 9894\n",
      "Loss: 26.500012429332404\n",
      "l2 norm of gradients: 0.1630265789869264\n",
      "l2 norm of weights: 5.566699446667265\n",
      "---------------------\n",
      "Iteration Number: 9895\n",
      "Loss: 26.49867907452911\n",
      "l2 norm of gradients: 0.16301520154314128\n",
      "l2 norm of weights: 5.566640058637686\n",
      "---------------------\n",
      "Iteration Number: 9896\n",
      "Loss: 26.497345905102463\n",
      "l2 norm of gradients: 0.16300382578081277\n",
      "l2 norm of weights: 5.566580674626923\n",
      "---------------------\n",
      "Iteration Number: 9897\n",
      "Loss: 26.496012921013545\n",
      "l2 norm of gradients: 0.16299245169947424\n",
      "l2 norm of weights: 5.566521294634496\n",
      "---------------------\n",
      "Iteration Number: 9898\n",
      "Loss: 26.494680122220785\n",
      "l2 norm of gradients: 0.16298107929865943\n",
      "l2 norm of weights: 5.566461918659924\n",
      "---------------------\n",
      "Iteration Number: 9899\n",
      "Loss: 26.493347508683616\n",
      "l2 norm of gradients: 0.16296970857790216\n",
      "l2 norm of weights: 5.566402546702726\n",
      "---------------------\n",
      "Iteration Number: 9900\n",
      "Loss: 26.49201508036133\n",
      "l2 norm of gradients: 0.1629583395367363\n",
      "l2 norm of weights: 5.566343178762422\n",
      "---------------------\n",
      "Iteration Number: 9901\n",
      "Loss: 26.490682837214827\n",
      "l2 norm of gradients: 0.16294697217469595\n",
      "l2 norm of weights: 5.56628381483853\n",
      "---------------------\n",
      "Iteration Number: 9902\n",
      "Loss: 26.48935077920249\n",
      "l2 norm of gradients: 0.1629356064913153\n",
      "l2 norm of weights: 5.566224454930572\n",
      "---------------------\n",
      "Iteration Number: 9903\n",
      "Loss: 26.488018906283845\n",
      "l2 norm of gradients: 0.16292424248612872\n",
      "l2 norm of weights: 5.566165099038067\n",
      "---------------------\n",
      "Iteration Number: 9904\n",
      "Loss: 26.486687218418613\n",
      "l2 norm of gradients: 0.1629128801586707\n",
      "l2 norm of weights: 5.566105747160536\n",
      "---------------------\n",
      "Iteration Number: 9905\n",
      "Loss: 26.48535571556578\n",
      "l2 norm of gradients: 0.16290151950847584\n",
      "l2 norm of weights: 5.566046399297497\n",
      "---------------------\n",
      "Iteration Number: 9906\n",
      "Loss: 26.484024397685367\n",
      "l2 norm of gradients: 0.16289016053507896\n",
      "l2 norm of weights: 5.565987055448472\n",
      "---------------------\n",
      "Iteration Number: 9907\n",
      "Loss: 26.482693264737158\n",
      "l2 norm of gradients: 0.16287880323801485\n",
      "l2 norm of weights: 5.56592771561298\n",
      "---------------------\n",
      "Iteration Number: 9908\n",
      "Loss: 26.481362316679174\n",
      "l2 norm of gradients: 0.16286744761681862\n",
      "l2 norm of weights: 5.565868379790541\n",
      "---------------------\n",
      "Iteration Number: 9909\n",
      "Loss: 26.48003155347351\n",
      "l2 norm of gradients: 0.16285609367102544\n",
      "l2 norm of weights: 5.565809047980677\n",
      "---------------------\n",
      "Iteration Number: 9910\n",
      "Loss: 26.478700975078052\n",
      "l2 norm of gradients: 0.16284474140017058\n",
      "l2 norm of weights: 5.5657497201829065\n",
      "---------------------\n",
      "Iteration Number: 9911\n",
      "Loss: 26.477370581451904\n",
      "l2 norm of gradients: 0.16283339080378953\n",
      "l2 norm of weights: 5.565690396396752\n",
      "---------------------\n",
      "Iteration Number: 9912\n",
      "Loss: 26.476040372557268\n",
      "l2 norm of gradients: 0.16282204188141788\n",
      "l2 norm of weights: 5.565631076621733\n",
      "---------------------\n",
      "Iteration Number: 9913\n",
      "Loss: 26.474710348351735\n",
      "l2 norm of gradients: 0.1628106946325913\n",
      "l2 norm of weights: 5.56557176085737\n",
      "---------------------\n",
      "Iteration Number: 9914\n",
      "Loss: 26.473380508795003\n",
      "l2 norm of gradients: 0.16279934905684568\n",
      "l2 norm of weights: 5.565512449103185\n",
      "---------------------\n",
      "Iteration Number: 9915\n",
      "Loss: 26.47205085384708\n",
      "l2 norm of gradients: 0.162788005153717\n",
      "l2 norm of weights: 5.565453141358699\n",
      "---------------------\n",
      "Iteration Number: 9916\n",
      "Loss: 26.470721383467573\n",
      "l2 norm of gradients: 0.1627766629227414\n",
      "l2 norm of weights: 5.565393837623431\n",
      "---------------------\n",
      "Iteration Number: 9917\n",
      "Loss: 26.46939209761468\n",
      "l2 norm of gradients: 0.16276532236345514\n",
      "l2 norm of weights: 5.565334537896906\n",
      "---------------------\n",
      "Iteration Number: 9918\n",
      "Loss: 26.468062996252694\n",
      "l2 norm of gradients: 0.16275398347539458\n",
      "l2 norm of weights: 5.565275242178641\n",
      "---------------------\n",
      "Iteration Number: 9919\n",
      "Loss: 26.466734079334636\n",
      "l2 norm of gradients: 0.16274264625809637\n",
      "l2 norm of weights: 5.56521595046816\n",
      "---------------------\n",
      "Iteration Number: 9920\n",
      "Loss: 26.4654053468264\n",
      "l2 norm of gradients: 0.1627313107110971\n",
      "l2 norm of weights: 5.565156662764984\n",
      "---------------------\n",
      "Iteration Number: 9921\n",
      "Loss: 26.464076798683713\n",
      "l2 norm of gradients: 0.16271997683393352\n",
      "l2 norm of weights: 5.565097379068635\n",
      "---------------------\n",
      "Iteration Number: 9922\n",
      "Loss: 26.462748434868793\n",
      "l2 norm of gradients: 0.16270864462614268\n",
      "l2 norm of weights: 5.565038099378633\n",
      "---------------------\n",
      "Iteration Number: 9923\n",
      "Loss: 26.461420255339263\n",
      "l2 norm of gradients: 0.16269731408726165\n",
      "l2 norm of weights: 5.564978823694501\n",
      "---------------------\n",
      "Iteration Number: 9924\n",
      "Loss: 26.460092260057536\n",
      "l2 norm of gradients: 0.16268598521682756\n",
      "l2 norm of weights: 5.564919552015762\n",
      "---------------------\n",
      "Iteration Number: 9925\n",
      "Loss: 26.458764448981988\n",
      "l2 norm of gradients: 0.16267465801437786\n",
      "l2 norm of weights: 5.564860284341934\n",
      "---------------------\n",
      "Iteration Number: 9926\n",
      "Loss: 26.457436822071088\n",
      "l2 norm of gradients: 0.16266333247945\n",
      "l2 norm of weights: 5.564801020672544\n",
      "---------------------\n",
      "Iteration Number: 9927\n",
      "Loss: 26.45610937928711\n",
      "l2 norm of gradients: 0.16265200861158155\n",
      "l2 norm of weights: 5.56474176100711\n",
      "---------------------\n",
      "Iteration Number: 9928\n",
      "Loss: 26.454782120587673\n",
      "l2 norm of gradients: 0.1626406864103103\n",
      "l2 norm of weights: 5.564682505345157\n",
      "---------------------\n",
      "Iteration Number: 9929\n",
      "Loss: 26.453455045934543\n",
      "l2 norm of gradients: 0.1626293658751742\n",
      "l2 norm of weights: 5.564623253686207\n",
      "---------------------\n",
      "Iteration Number: 9930\n",
      "Loss: 26.452128155284854\n",
      "l2 norm of gradients: 0.16261804700571114\n",
      "l2 norm of weights: 5.56456400602978\n",
      "---------------------\n",
      "Iteration Number: 9931\n",
      "Loss: 26.450801448602167\n",
      "l2 norm of gradients: 0.16260672980145943\n",
      "l2 norm of weights: 5.564504762375402\n",
      "---------------------\n",
      "Iteration Number: 9932\n",
      "Loss: 26.449474925843216\n",
      "l2 norm of gradients: 0.16259541426195723\n",
      "l2 norm of weights: 5.564445522722593\n",
      "---------------------\n",
      "Iteration Number: 9933\n",
      "Loss: 26.4481485869698\n",
      "l2 norm of gradients: 0.16258410038674304\n",
      "l2 norm of weights: 5.564386287070877\n",
      "---------------------\n",
      "Iteration Number: 9934\n",
      "Loss: 26.44682243194045\n",
      "l2 norm of gradients: 0.1625727881753554\n",
      "l2 norm of weights: 5.564327055419777\n",
      "---------------------\n",
      "Iteration Number: 9935\n",
      "Loss: 26.44549646071764\n",
      "l2 norm of gradients: 0.162561477627333\n",
      "l2 norm of weights: 5.564267827768814\n",
      "---------------------\n",
      "Iteration Number: 9936\n",
      "Loss: 26.44417067325845\n",
      "l2 norm of gradients: 0.16255016874221467\n",
      "l2 norm of weights: 5.564208604117513\n",
      "---------------------\n",
      "Iteration Number: 9937\n",
      "Loss: 26.44284506952298\n",
      "l2 norm of gradients: 0.16253886151953933\n",
      "l2 norm of weights: 5.564149384465396\n",
      "---------------------\n",
      "Iteration Number: 9938\n",
      "Loss: 26.441519649473463\n",
      "l2 norm of gradients: 0.16252755595884621\n",
      "l2 norm of weights: 5.564090168811987\n",
      "---------------------\n",
      "Iteration Number: 9939\n",
      "Loss: 26.44019441306598\n",
      "l2 norm of gradients: 0.16251625205967438\n",
      "l2 norm of weights: 5.56403095715681\n",
      "---------------------\n",
      "Iteration Number: 9940\n",
      "Loss: 26.438869360265752\n",
      "l2 norm of gradients: 0.16250494982156327\n",
      "l2 norm of weights: 5.563971749499385\n",
      "---------------------\n",
      "Iteration Number: 9941\n",
      "Loss: 26.437544491029957\n",
      "l2 norm of gradients: 0.16249364924405238\n",
      "l2 norm of weights: 5.563912545839239\n",
      "---------------------\n",
      "Iteration Number: 9942\n",
      "Loss: 26.436219805318185\n",
      "l2 norm of gradients: 0.16248235032668135\n",
      "l2 norm of weights: 5.563853346175894\n",
      "---------------------\n",
      "Iteration Number: 9943\n",
      "Loss: 26.4348953030899\n",
      "l2 norm of gradients: 0.16247105306898987\n",
      "l2 norm of weights: 5.563794150508874\n",
      "---------------------\n",
      "Iteration Number: 9944\n",
      "Loss: 26.43357098430776\n",
      "l2 norm of gradients: 0.16245975747051788\n",
      "l2 norm of weights: 5.563734958837704\n",
      "---------------------\n",
      "Iteration Number: 9945\n",
      "Loss: 26.43224684892925\n",
      "l2 norm of gradients: 0.16244846353080544\n",
      "l2 norm of weights: 5.563675771161905\n",
      "---------------------\n",
      "Iteration Number: 9946\n",
      "Loss: 26.4309228969163\n",
      "l2 norm of gradients: 0.16243717124939266\n",
      "l2 norm of weights: 5.563616587481004\n",
      "---------------------\n",
      "Iteration Number: 9947\n",
      "Loss: 26.429599128228187\n",
      "l2 norm of gradients: 0.16242588062581984\n",
      "l2 norm of weights: 5.563557407794522\n",
      "---------------------\n",
      "Iteration Number: 9948\n",
      "Loss: 26.428275542825244\n",
      "l2 norm of gradients: 0.16241459165962746\n",
      "l2 norm of weights: 5.563498232101987\n",
      "---------------------\n",
      "Iteration Number: 9949\n",
      "Loss: 26.42695214066804\n",
      "l2 norm of gradients: 0.16240330435035594\n",
      "l2 norm of weights: 5.563439060402919\n",
      "---------------------\n",
      "Iteration Number: 9950\n",
      "Loss: 26.42562892171435\n",
      "l2 norm of gradients: 0.1623920186975461\n",
      "l2 norm of weights: 5.563379892696846\n",
      "---------------------\n",
      "Iteration Number: 9951\n",
      "Loss: 26.424305885927165\n",
      "l2 norm of gradients: 0.16238073470073866\n",
      "l2 norm of weights: 5.56332072898329\n",
      "---------------------\n",
      "Iteration Number: 9952\n",
      "Loss: 26.422983033263712\n",
      "l2 norm of gradients: 0.16236945235947464\n",
      "l2 norm of weights: 5.563261569261775\n",
      "---------------------\n",
      "Iteration Number: 9953\n",
      "Loss: 26.42166036368906\n",
      "l2 norm of gradients: 0.1623581716732951\n",
      "l2 norm of weights: 5.563202413531829\n",
      "---------------------\n",
      "Iteration Number: 9954\n",
      "Loss: 26.42033787715809\n",
      "l2 norm of gradients: 0.16234689264174126\n",
      "l2 norm of weights: 5.563143261792974\n",
      "---------------------\n",
      "Iteration Number: 9955\n",
      "Loss: 26.41901557363279\n",
      "l2 norm of gradients: 0.16233561526435444\n",
      "l2 norm of weights: 5.5630841140447345\n",
      "---------------------\n",
      "Iteration Number: 9956\n",
      "Loss: 26.41769345307398\n",
      "l2 norm of gradients: 0.16232433954067613\n",
      "l2 norm of weights: 5.563024970286638\n",
      "---------------------\n",
      "Iteration Number: 9957\n",
      "Loss: 26.416371515441814\n",
      "l2 norm of gradients: 0.16231306547024793\n",
      "l2 norm of weights: 5.562965830518207\n",
      "---------------------\n",
      "Iteration Number: 9958\n",
      "Loss: 26.41504976069664\n",
      "l2 norm of gradients: 0.1623017930526116\n",
      "l2 norm of weights: 5.5629066947389685\n",
      "---------------------\n",
      "Iteration Number: 9959\n",
      "Loss: 26.413728188798338\n",
      "l2 norm of gradients: 0.162290522287309\n",
      "l2 norm of weights: 5.562847562948446\n",
      "---------------------\n",
      "Iteration Number: 9960\n",
      "Loss: 26.41240679970766\n",
      "l2 norm of gradients: 0.1622792531738821\n",
      "l2 norm of weights: 5.562788435146165\n",
      "---------------------\n",
      "Iteration Number: 9961\n",
      "Loss: 26.411085593383486\n",
      "l2 norm of gradients: 0.1622679857118731\n",
      "l2 norm of weights: 5.562729311331651\n",
      "---------------------\n",
      "Iteration Number: 9962\n",
      "Loss: 26.409764569788166\n",
      "l2 norm of gradients: 0.1622567199008242\n",
      "l2 norm of weights: 5.562670191504431\n",
      "---------------------\n",
      "Iteration Number: 9963\n",
      "Loss: 26.40844372887991\n",
      "l2 norm of gradients: 0.1622454557402778\n",
      "l2 norm of weights: 5.562611075664029\n",
      "---------------------\n",
      "Iteration Number: 9964\n",
      "Loss: 26.407123070620244\n",
      "l2 norm of gradients: 0.16223419322977642\n",
      "l2 norm of weights: 5.562551963809971\n",
      "---------------------\n",
      "Iteration Number: 9965\n",
      "Loss: 26.40580259496848\n",
      "l2 norm of gradients: 0.1622229323688628\n",
      "l2 norm of weights: 5.562492855941784\n",
      "---------------------\n",
      "Iteration Number: 9966\n",
      "Loss: 26.40448230188659\n",
      "l2 norm of gradients: 0.16221167315707954\n",
      "l2 norm of weights: 5.5624337520589915\n",
      "---------------------\n",
      "Iteration Number: 9967\n",
      "Loss: 26.403162191333895\n",
      "l2 norm of gradients: 0.1622004155939697\n",
      "l2 norm of weights: 5.562374652161121\n",
      "---------------------\n",
      "Iteration Number: 9968\n",
      "Loss: 26.40184226327069\n",
      "l2 norm of gradients: 0.1621891596790763\n",
      "l2 norm of weights: 5.562315556247698\n",
      "---------------------\n",
      "Iteration Number: 9969\n",
      "Loss: 26.40052251765798\n",
      "l2 norm of gradients: 0.1621779054119425\n",
      "l2 norm of weights: 5.562256464318248\n",
      "---------------------\n",
      "Iteration Number: 9970\n",
      "Loss: 26.399202954455692\n",
      "l2 norm of gradients: 0.16216665279211154\n",
      "l2 norm of weights: 5.562197376372301\n",
      "---------------------\n",
      "Iteration Number: 9971\n",
      "Loss: 26.397883573623282\n",
      "l2 norm of gradients: 0.16215540181912697\n",
      "l2 norm of weights: 5.5621382924093785\n",
      "---------------------\n",
      "Iteration Number: 9972\n",
      "Loss: 26.396564375123894\n",
      "l2 norm of gradients: 0.16214415249253228\n",
      "l2 norm of weights: 5.562079212429008\n",
      "---------------------\n",
      "Iteration Number: 9973\n",
      "Loss: 26.39524535891391\n",
      "l2 norm of gradients: 0.16213290481187115\n",
      "l2 norm of weights: 5.562020136430719\n",
      "---------------------\n",
      "Iteration Number: 9974\n",
      "Loss: 26.393926524958385\n",
      "l2 norm of gradients: 0.1621216587766874\n",
      "l2 norm of weights: 5.561961064414034\n",
      "---------------------\n",
      "Iteration Number: 9975\n",
      "Loss: 26.39260787321266\n",
      "l2 norm of gradients: 0.16211041438652504\n",
      "l2 norm of weights: 5.5619019963784835\n",
      "---------------------\n",
      "Iteration Number: 9976\n",
      "Loss: 26.39128940364231\n",
      "l2 norm of gradients: 0.16209917164092807\n",
      "l2 norm of weights: 5.561842932323592\n",
      "---------------------\n",
      "Iteration Number: 9977\n",
      "Loss: 26.389971116205118\n",
      "l2 norm of gradients: 0.1620879305394407\n",
      "l2 norm of weights: 5.561783872248887\n",
      "---------------------\n",
      "Iteration Number: 9978\n",
      "Loss: 26.38865301086146\n",
      "l2 norm of gradients: 0.16207669108160733\n",
      "l2 norm of weights: 5.561724816153895\n",
      "---------------------\n",
      "Iteration Number: 9979\n",
      "Loss: 26.3873350875727\n",
      "l2 norm of gradients: 0.1620654532669724\n",
      "l2 norm of weights: 5.561665764038143\n",
      "---------------------\n",
      "Iteration Number: 9980\n",
      "Loss: 26.38601734629882\n",
      "l2 norm of gradients: 0.1620542170950804\n",
      "l2 norm of weights: 5.56160671590116\n",
      "---------------------\n",
      "Iteration Number: 9981\n",
      "Loss: 26.384699787000976\n",
      "l2 norm of gradients: 0.16204298256547622\n",
      "l2 norm of weights: 5.5615476717424706\n",
      "---------------------\n",
      "Iteration Number: 9982\n",
      "Loss: 26.383382409639058\n",
      "l2 norm of gradients: 0.16203174967770456\n",
      "l2 norm of weights: 5.561488631561605\n",
      "---------------------\n",
      "Iteration Number: 9983\n",
      "Loss: 26.38206521417287\n",
      "l2 norm of gradients: 0.1620205184313105\n",
      "l2 norm of weights: 5.561429595358089\n",
      "---------------------\n",
      "Iteration Number: 9984\n",
      "Loss: 26.380748200565467\n",
      "l2 norm of gradients: 0.1620092888258391\n",
      "l2 norm of weights: 5.561370563131449\n",
      "---------------------\n",
      "Iteration Number: 9985\n",
      "Loss: 26.37943136877489\n",
      "l2 norm of gradients: 0.16199806086083562\n",
      "l2 norm of weights: 5.561311534881216\n",
      "---------------------\n",
      "Iteration Number: 9986\n",
      "Loss: 26.37811471876197\n",
      "l2 norm of gradients: 0.1619868345358454\n",
      "l2 norm of weights: 5.561252510606915\n",
      "---------------------\n",
      "Iteration Number: 9987\n",
      "Loss: 26.376798250488843\n",
      "l2 norm of gradients: 0.16197560985041384\n",
      "l2 norm of weights: 5.5611934903080735\n",
      "---------------------\n",
      "Iteration Number: 9988\n",
      "Loss: 26.37548196391586\n",
      "l2 norm of gradients: 0.16196438680408673\n",
      "l2 norm of weights: 5.561134473984222\n",
      "---------------------\n",
      "Iteration Number: 9989\n",
      "Loss: 26.374165859002968\n",
      "l2 norm of gradients: 0.1619531653964097\n",
      "l2 norm of weights: 5.561075461634887\n",
      "---------------------\n",
      "Iteration Number: 9990\n",
      "Loss: 26.372849935711354\n",
      "l2 norm of gradients: 0.1619419456269286\n",
      "l2 norm of weights: 5.5610164532595965\n",
      "---------------------\n",
      "Iteration Number: 9991\n",
      "Loss: 26.371534194000585\n",
      "l2 norm of gradients: 0.1619307274951895\n",
      "l2 norm of weights: 5.560957448857879\n",
      "---------------------\n",
      "Iteration Number: 9992\n",
      "Loss: 26.37021863383227\n",
      "l2 norm of gradients: 0.16191951100073854\n",
      "l2 norm of weights: 5.560898448429263\n",
      "---------------------\n",
      "Iteration Number: 9993\n",
      "Loss: 26.368903255167176\n",
      "l2 norm of gradients: 0.16190829614312185\n",
      "l2 norm of weights: 5.560839451973277\n",
      "---------------------\n",
      "Iteration Number: 9994\n",
      "Loss: 26.36758805796518\n",
      "l2 norm of gradients: 0.16189708292188593\n",
      "l2 norm of weights: 5.5607804594894485\n",
      "---------------------\n",
      "Iteration Number: 9995\n",
      "Loss: 26.366273042188276\n",
      "l2 norm of gradients: 0.16188587133657725\n",
      "l2 norm of weights: 5.560721470977307\n",
      "---------------------\n",
      "Iteration Number: 9996\n",
      "Loss: 26.364958207796427\n",
      "l2 norm of gradients: 0.1618746613867424\n",
      "l2 norm of weights: 5.5606624864363825\n",
      "---------------------\n",
      "Iteration Number: 9997\n",
      "Loss: 26.363643554750873\n",
      "l2 norm of gradients: 0.1618634530719282\n",
      "l2 norm of weights: 5.560603505866201\n",
      "---------------------\n",
      "Iteration Number: 9998\n",
      "Loss: 26.362329083010465\n",
      "l2 norm of gradients: 0.16185224639168153\n",
      "l2 norm of weights: 5.560544529266292\n",
      "---------------------\n",
      "Iteration Number: 9999\n",
      "Loss: 26.36101479253773\n",
      "l2 norm of gradients: 0.16184104134554933\n",
      "l2 norm of weights: 5.560485556636186\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "lambda_ = 1.0\n",
    "max_iter = 10000\n",
    "model = fit(xtrain_normal, ytrain, learning_rate, lambda_, max_iter, verbose=1) #keep the verbose on here for your submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8307692307692308\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: \", accuracy(xtrain_normal, ytrain, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 5 0.9280719280719281\n",
      "0.01 2 0.922077922077922\n",
      "0.01 1 0.9250749250749251\n",
      "0.01 0.1 0.9270729270729271\n",
      "0.01 0.01 0.9180819180819181\n",
      "0.001 5 0.8111888111888111\n",
      "0.001 2 0.8231768231768232\n",
      "0.001 1 0.8221778221778222\n",
      "0.001 0.1 0.8251748251748252\n",
      "0.001 0.01 0.8331668331668332\n",
      "0.0001 5 0.6823176823176823\n",
      "0.0001 2 0.7612387612387612\n",
      "0.0001 1 0.46053946053946054\n",
      "0.0001 0.1 0.7462537462537463\n",
      "0.0001 0.01 0.7412587412587412\n",
      "1e-05 5 0.5894105894105894\n",
      "1e-05 2 0.5724275724275725\n",
      "1e-05 1 0.4965034965034965\n",
      "1e-05 0.1 0.5634365634365635\n",
      "1e-05 0.01 0.4565434565434565\n"
     ]
    }
   ],
   "source": [
    "#grid search for finding the best hyperparams and model\n",
    "\n",
    "best_model = None\n",
    "best_val = -1\n",
    "for lr in [0.01, 0.001, 0.0001, 0.00001]:\n",
    "    for la in [5, 2, 1, 0.1, 0.01]:\n",
    "        model = fit(xtrain_normal, ytrain, lr, la, 10000, verbose=0)\n",
    "        val_acc = accuracy(xval_normal, yval, model)\n",
    "        print(lr, la, val_acc)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_model = model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.941\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: \", accuracy(xtest_normal, ytest, best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_xlabel('Cross Entropy Loss')\n",
    "ax.set_ylabel('Number of Iterations')\n",
    "ax.plot(cost_list,np.arange(max_iter))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
